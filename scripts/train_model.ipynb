{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('concatenated_snps_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chr2:192381934:C:T_maternal</th>\n",
       "      <th>chr2:192381934:C:T_paternal</th>\n",
       "      <th>chr8:129199566:G:A_maternal</th>\n",
       "      <th>chr8:129199566:G:A_paternal</th>\n",
       "      <th>chr8:29509616:A:C_maternal</th>\n",
       "      <th>chr8:29509616:A:C_paternal</th>\n",
       "      <th>chr2:121089731:T:C_maternal</th>\n",
       "      <th>chr2:121089731:T:C_paternal</th>\n",
       "      <th>chr15:75750383:T:C_maternal</th>\n",
       "      <th>chr15:75750383:T:C_paternal</th>\n",
       "      <th>...</th>\n",
       "      <th>chr5:176134882:T:A,C_maternal</th>\n",
       "      <th>chr5:176134882:T:A,C_paternal</th>\n",
       "      <th>chr5:52679539:C:CA,CAA_maternal</th>\n",
       "      <th>chr5:52679539:C:CA,CAA_paternal</th>\n",
       "      <th>chr7:91459189:A:AT,ATT_maternal</th>\n",
       "      <th>chr7:91459189:A:AT,ATT_paternal</th>\n",
       "      <th>chr10:22861490:A:C,T_maternal</th>\n",
       "      <th>chr10:22861490:A:C,T_paternal</th>\n",
       "      <th>chr22:38583315:AAAAG:AAAAGAAAG,AAAAGAAAGAAAG,A_maternal</th>\n",
       "      <th>chr22:38583315:AAAAG:AAAAGAAAG,AAAAGAAAGAAAG,A_paternal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>HG00096</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HG00097</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HG00099</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HG00100</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HG00101</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NA21137</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NA21141</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NA21142</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NA21143</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NA21144</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2504 rows × 626 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         chr2:192381934:C:T_maternal  chr2:192381934:C:T_paternal  \\\n",
       "HG00096                         True                         True   \n",
       "HG00097                         True                         True   \n",
       "HG00099                         True                         True   \n",
       "HG00100                        False                         True   \n",
       "HG00101                         True                         True   \n",
       "...                              ...                          ...   \n",
       "NA21137                         True                         True   \n",
       "NA21141                         True                         True   \n",
       "NA21142                         True                        False   \n",
       "NA21143                         True                         True   \n",
       "NA21144                         True                         True   \n",
       "\n",
       "         chr8:129199566:G:A_maternal  chr8:129199566:G:A_paternal  \\\n",
       "HG00096                        False                         True   \n",
       "HG00097                         True                        False   \n",
       "HG00099                        False                        False   \n",
       "HG00100                        False                        False   \n",
       "HG00101                        False                        False   \n",
       "...                              ...                          ...   \n",
       "NA21137                        False                        False   \n",
       "NA21141                        False                        False   \n",
       "NA21142                         True                        False   \n",
       "NA21143                        False                        False   \n",
       "NA21144                        False                        False   \n",
       "\n",
       "         chr8:29509616:A:C_maternal  chr8:29509616:A:C_paternal  \\\n",
       "HG00096                        True                        True   \n",
       "HG00097                        True                        True   \n",
       "HG00099                       False                       False   \n",
       "HG00100                       False                        True   \n",
       "HG00101                        True                        True   \n",
       "...                             ...                         ...   \n",
       "NA21137                        True                        True   \n",
       "NA21141                       False                       False   \n",
       "NA21142                       False                        True   \n",
       "NA21143                        True                        True   \n",
       "NA21144                       False                        True   \n",
       "\n",
       "         chr2:121089731:T:C_maternal  chr2:121089731:T:C_paternal  \\\n",
       "HG00096                         True                        False   \n",
       "HG00097                        False                        False   \n",
       "HG00099                        False                         True   \n",
       "HG00100                        False                        False   \n",
       "HG00101                         True                        False   \n",
       "...                              ...                          ...   \n",
       "NA21137                        False                        False   \n",
       "NA21141                        False                        False   \n",
       "NA21142                        False                        False   \n",
       "NA21143                        False                        False   \n",
       "NA21144                        False                        False   \n",
       "\n",
       "         chr15:75750383:T:C_maternal  chr15:75750383:T:C_paternal  ...  \\\n",
       "HG00096                         True                        False  ...   \n",
       "HG00097                        False                        False  ...   \n",
       "HG00099                        False                        False  ...   \n",
       "HG00100                        False                        False  ...   \n",
       "HG00101                         True                         True  ...   \n",
       "...                              ...                          ...  ...   \n",
       "NA21137                        False                        False  ...   \n",
       "NA21141                        False                         True  ...   \n",
       "NA21142                         True                        False  ...   \n",
       "NA21143                         True                         True  ...   \n",
       "NA21144                        False                         True  ...   \n",
       "\n",
       "         chr5:176134882:T:A,C_maternal  chr5:176134882:T:A,C_paternal  \\\n",
       "HG00096                          False                           True   \n",
       "HG00097                           True                          False   \n",
       "HG00099                          False                          False   \n",
       "HG00100                           True                           True   \n",
       "HG00101                           True                          False   \n",
       "...                                ...                            ...   \n",
       "NA21137                           True                          False   \n",
       "NA21141                           True                           True   \n",
       "NA21142                           True                           True   \n",
       "NA21143                          False                          False   \n",
       "NA21144                           True                           True   \n",
       "\n",
       "         chr5:52679539:C:CA,CAA_maternal  chr5:52679539:C:CA,CAA_paternal  \\\n",
       "HG00096                            False                            False   \n",
       "HG00097                            False                            False   \n",
       "HG00099                            False                            False   \n",
       "HG00100                            False                            False   \n",
       "HG00101                            False                            False   \n",
       "...                                  ...                              ...   \n",
       "NA21137                            False                            False   \n",
       "NA21141                            False                            False   \n",
       "NA21142                            False                            False   \n",
       "NA21143                             True                            False   \n",
       "NA21144                            False                            False   \n",
       "\n",
       "         chr7:91459189:A:AT,ATT_maternal  chr7:91459189:A:AT,ATT_paternal  \\\n",
       "HG00096                             True                            False   \n",
       "HG00097                             True                            False   \n",
       "HG00099                             True                             True   \n",
       "HG00100                            False                             True   \n",
       "HG00101                            False                            False   \n",
       "...                                  ...                              ...   \n",
       "NA21137                            False                            False   \n",
       "NA21141                             True                             True   \n",
       "NA21142                            False                             True   \n",
       "NA21143                             True                            False   \n",
       "NA21144                             True                            False   \n",
       "\n",
       "         chr10:22861490:A:C,T_maternal  chr10:22861490:A:C,T_paternal  \\\n",
       "HG00096                           True                           True   \n",
       "HG00097                           True                           True   \n",
       "HG00099                           True                           True   \n",
       "HG00100                          False                           True   \n",
       "HG00101                           True                           True   \n",
       "...                                ...                            ...   \n",
       "NA21137                           True                           True   \n",
       "NA21141                           True                           True   \n",
       "NA21142                           True                           True   \n",
       "NA21143                          False                           True   \n",
       "NA21144                           True                           True   \n",
       "\n",
       "         chr22:38583315:AAAAG:AAAAGAAAG,AAAAGAAAGAAAG,A_maternal  \\\n",
       "HG00096                                              False         \n",
       "HG00097                                              False         \n",
       "HG00099                                               True         \n",
       "HG00100                                              False         \n",
       "HG00101                                              False         \n",
       "...                                                    ...         \n",
       "NA21137                                               True         \n",
       "NA21141                                              False         \n",
       "NA21142                                              False         \n",
       "NA21143                                               True         \n",
       "NA21144                                               True         \n",
       "\n",
       "         chr22:38583315:AAAAG:AAAAGAAAG,AAAAGAAAGAAAG,A_paternal  \n",
       "HG00096                                              False        \n",
       "HG00097                                              False        \n",
       "HG00099                                               True        \n",
       "HG00100                                              False        \n",
       "HG00101                                              False        \n",
       "...                                                    ...        \n",
       "NA21137                                              False        \n",
       "NA21141                                               True        \n",
       "NA21142                                               True        \n",
       "NA21143                                              False        \n",
       "NA21144                                              False        \n",
       "\n",
       "[2504 rows x 626 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "# Define the transformer model architecture\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_heads, dropout):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Linear(input_size, hidden_size)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_size,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_size * 4,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.output = nn.Linear(hidden_size, input_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.output(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 0.6890, Train Acc: 0.3339, Train Prec: 0.3339, Train Rec: 1.0000, Val Loss: 0.6712, Val Acc: 0.3348, Val Prec: 0.3348, Val Rec: 1.0000\n",
      "Epoch [2/50], Train Loss: 0.6716, Train Acc: 0.3339, Train Prec: 0.3339, Train Rec: 1.0000, Val Loss: 0.6709, Val Acc: 0.3348, Val Prec: 0.3348, Val Rec: 1.0000\n",
      "Epoch [3/50], Train Loss: 0.6714, Train Acc: 0.3339, Train Prec: 0.3339, Train Rec: 1.0000, Val Loss: 0.6706, Val Acc: 0.3348, Val Prec: 0.3348, Val Rec: 1.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 67\u001b[0m\n\u001b[1;32m     64\u001b[0m         val_preds\u001b[38;5;241m.\u001b[39mextend((output[batch_mask]\u001b[38;5;241m.\u001b[39msigmoid() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten())\n\u001b[1;32m     65\u001b[0m         val_targets\u001b[38;5;241m.\u001b[39mextend(batch_data[batch_mask]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten())\n\u001b[0;32m---> 67\u001b[0m train_acc \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_preds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m train_prec \u001b[38;5;241m=\u001b[39m precision_score(train_targets, train_preds)\n\u001b[1;32m     69\u001b[0m train_rec \u001b[38;5;241m=\u001b[39m recall_score(train_targets, train_preds)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n\u001b[0;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    224\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:220\u001b[0m, in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[0;32m--> 220\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:85\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03mThis converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03my_pred : array or indicator matrix\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     84\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[0;32m---> 85\u001b[0m type_true \u001b[38;5;241m=\u001b[39m \u001b[43mtype_of_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my_true\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     88\u001b[0m y_type \u001b[38;5;241m=\u001b[39m {type_true, type_pred}\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/multiclass.py:388\u001b[0m, in \u001b[0;36mtype_of_target\u001b[0;34m(y, input_name)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;66;03m# Check multiclass\u001b[39;00m\n\u001b[1;32m    387\u001b[0m first_row \u001b[38;5;241m=\u001b[39m y[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issparse(y) \u001b[38;5;28;01melse\u001b[39;00m y\u001b[38;5;241m.\u001b[39mgetrow(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m--> 388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(first_row) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;66;03m# [1, 2, 3] or [[1., 2., 3]] or [[1, 2]]\u001b[39;00m\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m suffix\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/_array_api.py:262\u001b[0m, in \u001b[0;36m_NumPyAPIWrapper.unique_values\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munique_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36munique\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/lib/arraysetops.py:274\u001b[0m, in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[1;32m    272\u001b[0m ar \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(ar)\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43m_unique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mequal_nan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# axis was specified and not None\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/lib/arraysetops.py:336\u001b[0m, in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[0m\n\u001b[1;32m    334\u001b[0m     aux \u001b[38;5;241m=\u001b[39m ar[perm]\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 336\u001b[0m     ar\u001b[38;5;241m.\u001b[39msort()\n\u001b[1;32m    337\u001b[0m     aux \u001b[38;5;241m=\u001b[39m ar\n\u001b[1;32m    338\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(aux\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mbool_)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set hyperparameters\n",
    "input_size = 626\n",
    "hidden_size = 256\n",
    "num_layers = 3\n",
    "num_heads = 8\n",
    "dropout = 0.1\n",
    "batch_size = 32\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "masking_percentage = 0.2\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "# Prepare the data\n",
    "def mask_data(data, masking_percentage):\n",
    "    mask = torch.rand(data.shape) < masking_percentage\n",
    "    masked_data = data.clone()\n",
    "    masked_data[mask] = -1  # Set masked values to -1 or any other appropriate value\n",
    "    return masked_data, mask, data\n",
    "\n",
    "# Assuming you have the data frame loaded as 'df'\n",
    "data = torch.tensor(df.values, dtype=torch.float32)\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_masked_data, train_mask, train_data = mask_data(train_data, masking_percentage)\n",
    "val_masked_data, val_mask, val_data = mask_data(val_data, masking_percentage)\n",
    "\n",
    "train_dataset = TensorDataset(train_masked_data, train_mask, train_data)\n",
    "val_dataset = TensorDataset(val_masked_data, val_mask, val_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = TransformerModel(input_size, hidden_size, num_layers, num_heads, dropout)\n",
    "criterion = nn.BCEWithLogitsLoss(reduction='none')  # Use BCEWithLogitsLoss for numerical stability\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_preds = []\n",
    "    train_targets = []\n",
    "    for batch_masked_data, batch_mask, batch_data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_masked_data)\n",
    "        loss = criterion(output[batch_mask], batch_data[batch_mask])\n",
    "        loss = loss.mean()  # Calculate average loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        train_preds.extend((output[batch_mask].sigmoid() > 0.5).cpu().numpy().flatten())\n",
    "        train_targets.extend(batch_data[batch_mask].cpu().numpy().flatten())\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_preds = []\n",
    "    val_targets = []\n",
    "    with torch.no_grad():\n",
    "        for batch_masked_data, batch_mask, batch_data in val_loader:\n",
    "            output = model(batch_masked_data)\n",
    "            loss = criterion(output[batch_mask], batch_data[batch_mask])\n",
    "            loss = loss.mean()  # Calculate average loss\n",
    "            val_loss += loss.item()\n",
    "            val_preds.extend((output[batch_mask].sigmoid() > 0.5).cpu().numpy().flatten())\n",
    "            val_targets.extend(batch_data[batch_mask].cpu().numpy().flatten())\n",
    "    \n",
    "    train_acc = accuracy_score(train_targets, train_preds)\n",
    "    train_prec = precision_score(train_targets, train_preds)\n",
    "    train_rec = recall_score(train_targets, train_preds)\n",
    "    \n",
    "    val_acc = accuracy_score(val_targets, val_preds)\n",
    "    val_prec = precision_score(val_targets, val_preds)\n",
    "    val_rec = recall_score(val_targets, val_preds)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "          f\"Train Loss: {train_loss / len(train_loader):.4f}, \"\n",
    "          f\"Train Acc: {train_acc:.4f}, \"\n",
    "          f\"Train Prec: {train_prec:.4f}, \"\n",
    "          f\"Train Rec: {train_rec:.4f}, \"\n",
    "          f\"Val Loss: {val_loss / len(val_loader):.4f}, \"\n",
    "          f\"Val Acc: {val_acc:.4f}, \"\n",
    "          f\"Val Prec: {val_prec:.4f}, \"\n",
    "          f\"Val Rec: {val_rec:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"genomic_imputation_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 0.6723, Train Acc: 0.6976, Train Prec: 0.5338, Train Rec: 0.7446, Val Loss: 0.6692, Val Acc: 0.7130, Val Prec: 0.5539, Val Rec: 0.7331, Learning Rate: 0.001000\n",
      "Epoch [2/50], Train Loss: 0.6668, Train Acc: 0.7108, Train Prec: 0.5492, Train Rec: 0.7483, Val Loss: 0.6683, Val Acc: 0.7074, Val Prec: 0.5468, Val Rec: 0.7375, Learning Rate: 0.001000\n",
      "Epoch [3/50], Train Loss: 0.6627, Train Acc: 0.7074, Train Prec: 0.5446, Train Rec: 0.7557, Val Loss: 0.6703, Val Acc: 0.6915, Val Prec: 0.5265, Val Rec: 0.7810, Learning Rate: 0.001000\n",
      "Epoch [4/50], Train Loss: 0.6578, Train Acc: 0.7046, Train Prec: 0.5407, Train Rec: 0.7653, Val Loss: 0.6691, Val Acc: 0.7017, Val Prec: 0.5390, Val Rec: 0.7540, Learning Rate: 0.001000\n",
      "Epoch [5/50], Train Loss: 0.6531, Train Acc: 0.6998, Train Prec: 0.5351, Train Rec: 0.7711, Val Loss: 0.6697, Val Acc: 0.7041, Val Prec: 0.5423, Val Rec: 0.7450, Learning Rate: 0.001000\n",
      "Epoch [6/50], Train Loss: 0.6486, Train Acc: 0.6943, Train Prec: 0.5287, Train Rec: 0.7792, Val Loss: 0.6701, Val Acc: 0.7046, Val Prec: 0.5426, Val Rec: 0.7485, Learning Rate: 0.001000\n",
      "Epoch [7/50], Train Loss: 0.6448, Train Acc: 0.6855, Train Prec: 0.5189, Train Rec: 0.7954, Val Loss: 0.6721, Val Acc: 0.6771, Val Prec: 0.5114, Val Rec: 0.7946, Learning Rate: 0.001000\n",
      "Epoch 00008: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch [8/50], Train Loss: 0.6413, Train Acc: 0.6790, Train Prec: 0.5124, Train Rec: 0.8051, Val Loss: 0.6727, Val Acc: 0.6689, Val Prec: 0.5034, Val Rec: 0.8039, Learning Rate: 0.000500\n",
      "Epoch [9/50], Train Loss: 0.6373, Train Acc: 0.6714, Train Prec: 0.5050, Train Rec: 0.8162, Val Loss: 0.6733, Val Acc: 0.6587, Val Prec: 0.4942, Val Rec: 0.8178, Learning Rate: 0.000500\n",
      "Epoch [10/50], Train Loss: 0.6335, Train Acc: 0.6626, Train Prec: 0.4969, Train Rec: 0.8273, Val Loss: 0.6743, Val Acc: 0.6502, Val Prec: 0.4869, Val Rec: 0.8326, Learning Rate: 0.000500\n",
      "Epoch [11/50], Train Loss: 0.6310, Train Acc: 0.6536, Train Prec: 0.4891, Train Rec: 0.8345, Val Loss: 0.6744, Val Acc: 0.6503, Val Prec: 0.4869, Val Rec: 0.8267, Learning Rate: 0.000500\n",
      "Epoch [12/50], Train Loss: 0.6291, Train Acc: 0.6461, Train Prec: 0.4829, Train Rec: 0.8444, Val Loss: 0.6747, Val Acc: 0.6449, Val Prec: 0.4825, Val Rec: 0.8347, Learning Rate: 0.000500\n",
      "Epoch [13/50], Train Loss: 0.6273, Train Acc: 0.6403, Train Prec: 0.4783, Train Rec: 0.8487, Val Loss: 0.6758, Val Acc: 0.6371, Val Prec: 0.4763, Val Rec: 0.8453, Learning Rate: 0.000500\n",
      "Epoch 00014: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch [14/50], Train Loss: 0.6259, Train Acc: 0.6359, Train Prec: 0.4749, Train Rec: 0.8545, Val Loss: 0.6766, Val Acc: 0.6277, Val Prec: 0.4693, Val Rec: 0.8562, Learning Rate: 0.000250\n",
      "Epoch [15/50], Train Loss: 0.6243, Train Acc: 0.6305, Train Prec: 0.4708, Train Rec: 0.8596, Val Loss: 0.6768, Val Acc: 0.6242, Val Prec: 0.4667, Val Rec: 0.8589, Learning Rate: 0.000250\n",
      "Epoch [16/50], Train Loss: 0.6231, Train Acc: 0.6259, Train Prec: 0.4674, Train Rec: 0.8634, Val Loss: 0.6770, Val Acc: 0.6233, Val Prec: 0.4661, Val Rec: 0.8601, Learning Rate: 0.000250\n",
      "Epoch [17/50], Train Loss: 0.6223, Train Acc: 0.6221, Train Prec: 0.4647, Train Rec: 0.8671, Val Loss: 0.6775, Val Acc: 0.6157, Val Prec: 0.4607, Val Rec: 0.8675, Learning Rate: 0.000250\n",
      "Epoch [18/50], Train Loss: 0.6215, Train Acc: 0.6189, Train Prec: 0.4624, Train Rec: 0.8688, Val Loss: 0.6774, Val Acc: 0.6169, Val Prec: 0.4615, Val Rec: 0.8660, Learning Rate: 0.000250\n",
      "Epoch [19/50], Train Loss: 0.6208, Train Acc: 0.6165, Train Prec: 0.4607, Train Rec: 0.8710, Val Loss: 0.6779, Val Acc: 0.6149, Val Prec: 0.4602, Val Rec: 0.8676, Learning Rate: 0.000250\n",
      "Epoch 00020: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch [20/50], Train Loss: 0.6202, Train Acc: 0.6145, Train Prec: 0.4594, Train Rec: 0.8729, Val Loss: 0.6782, Val Acc: 0.6106, Val Prec: 0.4572, Val Rec: 0.8719, Learning Rate: 0.000125\n",
      "Epoch [21/50], Train Loss: 0.6196, Train Acc: 0.6123, Train Prec: 0.4579, Train Rec: 0.8758, Val Loss: 0.6784, Val Acc: 0.6085, Val Prec: 0.4558, Val Rec: 0.8733, Learning Rate: 0.000125\n",
      "Epoch [22/50], Train Loss: 0.6192, Train Acc: 0.6099, Train Prec: 0.4563, Train Rec: 0.8789, Val Loss: 0.6787, Val Acc: 0.6062, Val Prec: 0.4543, Val Rec: 0.8748, Learning Rate: 0.000125\n",
      "Epoch [23/50], Train Loss: 0.6187, Train Acc: 0.6071, Train Prec: 0.4543, Train Rec: 0.8788, Val Loss: 0.6787, Val Acc: 0.6058, Val Prec: 0.4540, Val Rec: 0.8766, Learning Rate: 0.000125\n",
      "Epoch [24/50], Train Loss: 0.6184, Train Acc: 0.6045, Train Prec: 0.4526, Train Rec: 0.8814, Val Loss: 0.6789, Val Acc: 0.6037, Val Prec: 0.4527, Val Rec: 0.8780, Learning Rate: 0.000125\n",
      "Epoch [25/50], Train Loss: 0.6180, Train Acc: 0.6045, Train Prec: 0.4527, Train Rec: 0.8814, Val Loss: 0.6788, Val Acc: 0.6053, Val Prec: 0.4538, Val Rec: 0.8778, Learning Rate: 0.000125\n",
      "Epoch 00026: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch [26/50], Train Loss: 0.6176, Train Acc: 0.6015, Train Prec: 0.4507, Train Rec: 0.8839, Val Loss: 0.6792, Val Acc: 0.6018, Val Prec: 0.4515, Val Rec: 0.8803, Learning Rate: 0.000063\n",
      "Epoch [27/50], Train Loss: 0.6174, Train Acc: 0.6010, Train Prec: 0.4504, Train Rec: 0.8850, Val Loss: 0.6794, Val Acc: 0.5990, Val Prec: 0.4496, Val Rec: 0.8827, Learning Rate: 0.000063\n",
      "Epoch [28/50], Train Loss: 0.6173, Train Acc: 0.5990, Train Prec: 0.4490, Train Rec: 0.8846, Val Loss: 0.6793, Val Acc: 0.6004, Val Prec: 0.4506, Val Rec: 0.8821, Learning Rate: 0.000063\n",
      "Epoch [29/50], Train Loss: 0.6170, Train Acc: 0.5985, Train Prec: 0.4488, Train Rec: 0.8855, Val Loss: 0.6798, Val Acc: 0.5958, Val Prec: 0.4476, Val Rec: 0.8853, Learning Rate: 0.000063\n",
      "Epoch [30/50], Train Loss: 0.6168, Train Acc: 0.5973, Train Prec: 0.4480, Train Rec: 0.8878, Val Loss: 0.6796, Val Acc: 0.5986, Val Prec: 0.4494, Val Rec: 0.8838, Learning Rate: 0.000063\n",
      "Epoch [31/50], Train Loss: 0.6166, Train Acc: 0.5956, Train Prec: 0.4469, Train Rec: 0.8881, Val Loss: 0.6797, Val Acc: 0.5962, Val Prec: 0.4479, Val Rec: 0.8856, Learning Rate: 0.000063\n",
      "Epoch 00032: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch [32/50], Train Loss: 0.6164, Train Acc: 0.5946, Train Prec: 0.4463, Train Rec: 0.8886, Val Loss: 0.6798, Val Acc: 0.5957, Val Prec: 0.4475, Val Rec: 0.8858, Learning Rate: 0.000031\n",
      "Epoch [33/50], Train Loss: 0.6162, Train Acc: 0.5939, Train Prec: 0.4458, Train Rec: 0.8887, Val Loss: 0.6799, Val Acc: 0.5948, Val Prec: 0.4469, Val Rec: 0.8863, Learning Rate: 0.000031\n",
      "Epoch [34/50], Train Loss: 0.6162, Train Acc: 0.5925, Train Prec: 0.4449, Train Rec: 0.8895, Val Loss: 0.6800, Val Acc: 0.5938, Val Prec: 0.4463, Val Rec: 0.8869, Learning Rate: 0.000031\n",
      "Epoch [35/50], Train Loss: 0.6160, Train Acc: 0.5939, Train Prec: 0.4459, Train Rec: 0.8900, Val Loss: 0.6799, Val Acc: 0.5952, Val Prec: 0.4472, Val Rec: 0.8855, Learning Rate: 0.000031\n",
      "Epoch [36/50], Train Loss: 0.6159, Train Acc: 0.5934, Train Prec: 0.4454, Train Rec: 0.8885, Val Loss: 0.6801, Val Acc: 0.5926, Val Prec: 0.4456, Val Rec: 0.8876, Learning Rate: 0.000031\n",
      "Epoch [37/50], Train Loss: 0.6158, Train Acc: 0.5912, Train Prec: 0.4441, Train Rec: 0.8909, Val Loss: 0.6801, Val Acc: 0.5924, Val Prec: 0.4455, Val Rec: 0.8877, Learning Rate: 0.000031\n",
      "Epoch 00038: reducing learning rate of group 0 to 1.5625e-05.\n",
      "Epoch [38/50], Train Loss: 0.6158, Train Acc: 0.5921, Train Prec: 0.4447, Train Rec: 0.8904, Val Loss: 0.6802, Val Acc: 0.5920, Val Prec: 0.4452, Val Rec: 0.8880, Learning Rate: 0.000016\n",
      "Epoch [39/50], Train Loss: 0.6157, Train Acc: 0.5906, Train Prec: 0.4438, Train Rec: 0.8917, Val Loss: 0.6802, Val Acc: 0.5927, Val Prec: 0.4457, Val Rec: 0.8880, Learning Rate: 0.000016\n",
      "Epoch [40/50], Train Loss: 0.6156, Train Acc: 0.5912, Train Prec: 0.4441, Train Rec: 0.8909, Val Loss: 0.6802, Val Acc: 0.5922, Val Prec: 0.4454, Val Rec: 0.8883, Learning Rate: 0.000016\n",
      "Epoch [41/50], Train Loss: 0.6156, Train Acc: 0.5906, Train Prec: 0.4438, Train Rec: 0.8921, Val Loss: 0.6802, Val Acc: 0.5920, Val Prec: 0.4452, Val Rec: 0.8881, Learning Rate: 0.000016\n",
      "Epoch [42/50], Train Loss: 0.6154, Train Acc: 0.5912, Train Prec: 0.4441, Train Rec: 0.8911, Val Loss: 0.6803, Val Acc: 0.5917, Val Prec: 0.4450, Val Rec: 0.8885, Learning Rate: 0.000016\n",
      "Epoch [43/50], Train Loss: 0.6155, Train Acc: 0.5897, Train Prec: 0.4431, Train Rec: 0.8912, Val Loss: 0.6803, Val Acc: 0.5917, Val Prec: 0.4450, Val Rec: 0.8883, Learning Rate: 0.000016\n",
      "Epoch 00044: reducing learning rate of group 0 to 7.8125e-06.\n",
      "Epoch [44/50], Train Loss: 0.6154, Train Acc: 0.5907, Train Prec: 0.4439, Train Rec: 0.8917, Val Loss: 0.6802, Val Acc: 0.5917, Val Prec: 0.4450, Val Rec: 0.8881, Learning Rate: 0.000008\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     18\u001b[0m output \u001b[38;5;241m=\u001b[39m model(batch_masked_data)\n\u001b[0;32m---> 19\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_mask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_mask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# Calculate average loss\u001b[39;00m\n\u001b[1;32m     21\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1514\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1511\u001b[0m             tracing_state\u001b[38;5;241m.\u001b[39mpop_scope()\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m-> 1514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapped_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1515\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1516\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load the trained model and keep training it with a learning rate scheduler\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = TransformerModel(input_size, hidden_size, num_layers, num_heads, dropout)\n",
    "model.load_state_dict(torch.load(\"genomic_imputation_model.pth\"))\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_preds = []\n",
    "    train_targets = []\n",
    "    for batch_masked_data, batch_mask, batch_data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_masked_data)\n",
    "        loss = criterion(output[batch_mask], batch_data[batch_mask])\n",
    "        loss = loss.mean()  # Calculate average loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        train_preds.extend((output[batch_mask].sigmoid() > 0.5).cpu().numpy().flatten())\n",
    "        train_targets.extend(batch_data[batch_mask].cpu().numpy().flatten())\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_preds = []\n",
    "    val_targets = []\n",
    "    with torch.no_grad():\n",
    "        for batch_masked_data, batch_mask, batch_data in val_loader:\n",
    "            output = model(batch_masked_data)\n",
    "            loss = criterion(output[batch_mask], batch_data[batch_mask])\n",
    "            loss = loss.mean()  # Calculate average loss\n",
    "            val_loss += loss.item()\n",
    "            val_preds.extend((output[batch_mask].sigmoid() > 0.5).cpu().numpy().flatten())\n",
    "            val_targets.extend(batch_data[batch_mask].cpu().numpy().flatten())\n",
    "    \n",
    "    # Update the learning rate based on the validation loss\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    train_acc = accuracy_score(train_targets, train_preds)\n",
    "    train_prec = precision_score(train_targets, train_preds)\n",
    "    train_rec = recall_score(train_targets, train_preds)\n",
    "    \n",
    "    val_acc = accuracy_score(val_targets, val_preds)\n",
    "    val_prec = precision_score(val_targets, val_preds)\n",
    "    val_rec = recall_score(val_targets, val_preds)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "          f\"Train Loss: {train_loss / len(train_loader):.4f}, \"\n",
    "          f\"Train Acc: {train_acc:.4f}, \"\n",
    "          f\"Train Prec: {train_prec:.4f}, \"\n",
    "          f\"Train Rec: {train_rec:.4f}, \"\n",
    "          f\"Val Loss: {val_loss / len(val_loader):.4f}, \"\n",
    "          f\"Val Acc: {val_acc:.4f}, \"\n",
    "          f\"Val Prec: {val_prec:.4f}, \"\n",
    "          f\"Val Rec: {val_rec:.4f}, \"\n",
    "          f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"genomic_imputation_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
