{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown PRS313 SNPs:  28\n",
      "Known PRS313 SNPs:  14\n",
      "23AndMe SNPs with LD to Unknown PRS313 SNPs:  1262\n",
      "Validation Accuracy is  0.6070715711434275\n",
      "Epoch 1: Val Loss improved to 0.6113, saved checkpoint\n",
      "Validation Accuracy is  0.7159965782720273\n",
      "Epoch 2: Val Loss improved to 0.5567, saved checkpoint\n",
      "Validation Accuracy is  0.7318220701454234\n",
      "Epoch 3: Val Loss improved to 0.5314, saved checkpoint\n",
      "Validation Accuracy is  0.7322497861420018\n",
      "Epoch 4: Val Loss improved to 0.5212, saved checkpoint\n",
      "Validation Accuracy is  0.7391645280866838\n",
      "Epoch 5: Val Loss improved to 0.5160, saved checkpoint\n",
      "Validation Accuracy is  0.7403763900769889\n",
      "Epoch 6: Val Loss improved to 0.5123, saved checkpoint\n",
      "Validation Accuracy is  0.7432278300541774\n",
      "Epoch 7: Val Loss improved to 0.5087, saved checkpoint\n",
      "Validation Accuracy is  0.7517821499857428\n",
      "Epoch 8: Val Loss improved to 0.5051, saved checkpoint\n",
      "Validation Accuracy is  0.7542771599657827\n",
      "Epoch 9: Val Loss improved to 0.5018, saved checkpoint\n",
      "Validation Accuracy is  0.7572711719418306\n",
      "Epoch 10: Val Loss improved to 0.4980, saved checkpoint\n",
      "Validation Accuracy is  0.7613344739093242\n",
      "Epoch 11: Val Loss improved to 0.4945, saved checkpoint\n",
      "Validation Accuracy is  0.7628314798973481\n",
      "Epoch 12: Val Loss improved to 0.4914, saved checkpoint\n",
      "Validation Accuracy is  0.7650413458796692\n",
      "Epoch 13: Val Loss improved to 0.4879, saved checkpoint\n",
      "Validation Accuracy is  0.7680353578557171\n",
      "Epoch 14: Val Loss improved to 0.4843, saved checkpoint\n",
      "Validation Accuracy is  0.7721699458226404\n",
      "Epoch 15: Val Loss improved to 0.4809, saved checkpoint\n",
      "Validation Accuracy is  0.771528371827773\n",
      "Epoch 16: Val Loss improved to 0.4778, saved checkpoint\n",
      "Validation Accuracy is  0.7753065297975478\n",
      "Epoch 17: Val Loss improved to 0.4746, saved checkpoint\n",
      "Validation Accuracy is  0.7759481037924152\n",
      "Epoch 18: Val Loss improved to 0.4717, saved checkpoint\n",
      "Validation Accuracy is  0.7791559737667522\n",
      "Epoch 19: Val Loss improved to 0.4690, saved checkpoint\n",
      "Validation Accuracy is  0.7797975477616196\n",
      "Epoch 20: Val Loss improved to 0.4666, saved checkpoint\n",
      "Validation Accuracy is  0.7822212717422298\n",
      "Epoch 21: Val Loss improved to 0.4633, saved checkpoint\n",
      "Validation Accuracy is  0.7830767037353864\n",
      "Epoch 22: Val Loss improved to 0.4615, saved checkpoint\n",
      "Validation Accuracy is  0.7836469917308241\n",
      "Epoch 23: Val Loss improved to 0.4584, saved checkpoint\n",
      "Validation Accuracy is  0.7840747077274023\n",
      "Epoch 24: Val Loss improved to 0.4564, saved checkpoint\n",
      "Validation Accuracy is  0.7838608497291132\n",
      "Epoch 25: Val Loss improved to 0.4548, saved checkpoint\n",
      "Validation Accuracy is  0.7852152837182778\n",
      "Epoch 26: Val Loss improved to 0.4535, saved checkpoint\n",
      "Validation Accuracy is  0.7860707157114343\n",
      "Epoch 27: Val Loss improved to 0.4521, saved checkpoint\n",
      "Validation Accuracy is  0.787852865697177\n",
      "Epoch 28: Val Loss improved to 0.4504, saved checkpoint\n",
      "Validation Accuracy is  0.787639007698888\n",
      "Epoch 29: Val Loss improved to 0.4491, saved checkpoint\n",
      "Validation Accuracy is  0.789349871685201\n",
      "Epoch 30: Val Loss improved to 0.4478, saved checkpoint\n",
      "Validation Accuracy is  0.7895637296834902\n",
      "Epoch 31: Val Loss improved to 0.4470, saved checkpoint\n",
      "Validation Accuracy is  0.7894924436840605\n",
      "Epoch 32: Val Loss improved to 0.4458, saved checkpoint\n",
      "Validation Accuracy is  0.7902053036783576\n",
      "Epoch 33: Val Loss improved to 0.4445, saved checkpoint\n",
      "Validation Accuracy is  0.7902053036783576\n",
      "Epoch 34: Val Loss improved to 0.4441, saved checkpoint\n",
      "Validation Accuracy is  0.7899201596806387\n",
      "Epoch 35: Val Loss improved to 0.4435, saved checkpoint\n",
      "Validation Accuracy is  0.7906330196749358\n",
      "Epoch 36: Val Loss improved to 0.4424, saved checkpoint\n",
      "Validation Accuracy is  0.7904191616766467\n",
      "Epoch 37: Val Loss improved to 0.4421, saved checkpoint\n",
      "Validation Accuracy is  0.7920587396635301\n",
      "Epoch 38: Val Loss improved to 0.4408, saved checkpoint\n",
      "Validation Accuracy is  0.7912033076703735\n",
      "Epoch 39: Val Loss improved to 0.4403, saved checkpoint\n",
      "Validation Accuracy is  0.7934131736526946\n",
      "Epoch 40: Val Loss improved to 0.4401, saved checkpoint\n",
      "Validation Accuracy is  0.7927003136583974\n",
      "Epoch 41: Val Loss improved to 0.4398, saved checkpoint\n",
      "Validation Accuracy is  0.7927715996578272\n",
      "Epoch 42: Val Loss improved to 0.4385, saved checkpoint\n",
      "Epoch 43: Val Loss did not improve from 0.4385\n",
      "Validation Accuracy is  0.7922013116623895\n",
      "Epoch 44: Val Loss improved to 0.4380, saved checkpoint\n",
      "Validation Accuracy is  0.793056743655546\n",
      "Epoch 45: Val Loss improved to 0.4372, saved checkpoint\n",
      "Epoch 46: Val Loss did not improve from 0.4372\n",
      "Validation Accuracy is  0.7924151696606786\n",
      "Epoch 47: Val Loss improved to 0.4365, saved checkpoint\n",
      "Epoch 48: Val Loss did not improve from 0.4365\n",
      "Validation Accuracy is  0.7951953236384374\n",
      "Epoch 49: Val Loss improved to 0.4359, saved checkpoint\n",
      "Validation Accuracy is  0.7936983176504134\n",
      "Epoch 50: Val Loss improved to 0.4353, saved checkpoint\n",
      "Epoch 51: Val Loss did not improve from 0.4353\n",
      "Validation Accuracy is  0.7934131736526946\n",
      "Epoch 52: Val Loss improved to 0.4348, saved checkpoint\n",
      "Validation Accuracy is  0.7936983176504134\n",
      "Epoch 53: Val Loss improved to 0.4345, saved checkpoint\n",
      "Validation Accuracy is  0.7932706016538352\n",
      "Epoch 54: Val Loss improved to 0.4344, saved checkpoint\n",
      "Validation Accuracy is  0.794054747647562\n",
      "Epoch 55: Val Loss improved to 0.4342, saved checkpoint\n",
      "Validation Accuracy is  0.7934131736526946\n",
      "Epoch 56: Val Loss improved to 0.4340, saved checkpoint\n",
      "Validation Accuracy is  0.7944111776447106\n",
      "Epoch 57: Val Loss improved to 0.4334, saved checkpoint\n",
      "Epoch 58: Val Loss did not improve from 0.4334\n",
      "Epoch 59: Val Loss did not improve from 0.4334\n",
      "Validation Accuracy is  0.794054747647562\n",
      "Epoch 60: Val Loss improved to 0.4328, saved checkpoint\n",
      "Epoch 61: Val Loss did not improve from 0.4328\n",
      "Validation Accuracy is  0.7951240376390077\n",
      "Epoch 62: Val Loss improved to 0.4326, saved checkpoint\n",
      "Epoch 63: Val Loss did not improve from 0.4326\n",
      "Validation Accuracy is  0.7947676076418592\n",
      "Epoch 64: Val Loss improved to 0.4325, saved checkpoint\n",
      "Validation Accuracy is  0.7941260336469917\n",
      "Epoch 65: Val Loss improved to 0.4323, saved checkpoint\n",
      "Validation Accuracy is  0.7947676076418592\n",
      "Epoch 66: Val Loss improved to 0.4321, saved checkpoint\n",
      "Validation Accuracy is  0.7951240376390077\n",
      "Epoch 67: Val Loss improved to 0.4316, saved checkpoint\n",
      "Epoch 68: Val Loss did not improve from 0.4316\n",
      "Epoch 69: Val Loss did not improve from 0.4316\n",
      "Validation Accuracy is  0.7926290276589678\n",
      "Epoch 70: Val Loss improved to 0.4315, saved checkpoint\n",
      "Epoch 71: Val Loss did not improve from 0.4315\n",
      "Validation Accuracy is  0.7943398916452808\n",
      "Epoch 72: Val Loss improved to 0.4312, saved checkpoint\n",
      "Epoch 73: Val Loss did not improve from 0.4312\n",
      "Epoch 74: Val Loss did not improve from 0.4312\n",
      "Validation Accuracy is  0.7934844596521243\n",
      "Epoch 75: Val Loss improved to 0.4312, saved checkpoint\n",
      "Validation Accuracy is  0.7931280296549758\n",
      "Epoch 76: Val Loss improved to 0.4311, saved checkpoint\n",
      "Epoch 77: Val Loss did not improve from 0.4311\n",
      "Epoch 78: Val Loss did not improve from 0.4311\n",
      "Epoch 79: Val Loss did not improve from 0.4311\n",
      "Validation Accuracy is  0.7931993156544055\n",
      "Epoch 80: Val Loss improved to 0.4309, saved checkpoint\n",
      "Epoch 81: Val Loss did not improve from 0.4309\n",
      "Validation Accuracy is  0.7928428856572569\n",
      "Epoch 82: Val Loss improved to 0.4308, saved checkpoint\n",
      "Epoch 83: Val Loss did not improve from 0.4308\n",
      "Epoch 84: Val Loss did not improve from 0.4308\n",
      "Epoch 85: Val Loss did not improve from 0.4308\n",
      "Epoch 86: Val Loss did not improve from 0.4308\n",
      "Epoch 87: Val Loss did not improve from 0.4308\n",
      "Epoch 00088: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 88: Val Loss did not improve from 0.4308\n",
      "Epoch 89: Val Loss did not improve from 0.4308\n",
      "Epoch 90: Val Loss did not improve from 0.4308\n",
      "Epoch 91: Val Loss did not improve from 0.4308\n",
      "Epoch 92: Val Loss did not improve from 0.4308\n",
      "Epoch 93: Val Loss did not improve from 0.4308\n",
      "Epoch 00094: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 94: Val Loss did not improve from 0.4308\n",
      "Epoch 95: Val Loss did not improve from 0.4308\n",
      "Epoch 96: Val Loss did not improve from 0.4308\n",
      "Epoch 97: Val Loss did not improve from 0.4308\n",
      "Epoch 98: Val Loss did not improve from 0.4308\n",
      "Epoch 99: Val Loss did not improve from 0.4308\n",
      "Epoch 00100: reducing learning rate of group 0 to 1.0000e-09.\n",
      "Epoch 100: Val Loss did not improve from 0.4308\n",
      "0.7928733\n",
      "Best Model - Val Accuracy: 0.7943\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "\n",
    "# Define the updated neural network architecture\n",
    "class SimpleFFNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(SimpleFFNN, self).__init__()\n",
    "        self.hidden1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.hidden2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.output = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.hidden1(x))\n",
    "        x = torch.relu(self.hidden2(x))\n",
    "        x = torch.sigmoid(self.output(x))\n",
    "        return x\n",
    "\n",
    "def train_and_evaluate_model(model, criterion, optimizer, scheduler, train_loader, val_loader, num_epochs, device, writer, checkpoint_path):\n",
    "    best_val_loss = float('inf')\n",
    "    start_epoch = 0\n",
    "\n",
    "    # Load checkpoint if it exists\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        best_val_loss = checkpoint['best_val_loss']\n",
    "        print(f\"Loaded checkpoint from {checkpoint_path}. Starting from epoch {start_epoch + 1}\")\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            val_outputs = []\n",
    "            val_labels = []\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_X)\n",
    "                val_loss += criterion(outputs, batch_y).item()\n",
    "                val_outputs.extend(outputs.cpu().numpy())\n",
    "                val_labels.extend(batch_y.cpu().numpy())\n",
    "            \n",
    "            val_loss /= len(val_loader)\n",
    "            scheduler.step(val_loss)  # Update the learning rate based on validation loss\n",
    "            val_outputs = np.array(val_outputs)\n",
    "            val_labels = np.array(val_labels)\n",
    "            val_accuracy = ((val_outputs > 0.5) == val_labels).mean()\n",
    "            # val_accuracy = accuracy_score((val_labels == 1), np.round(val_outputs))\n",
    "            # val_roc_auc = roc_auc_score(val_labels, val_outputs, average=\"macro\")\n",
    "            \n",
    "            writer.add_scalar('Val_Loss', val_loss, epoch)\n",
    "            writer.add_scalar('Val_Accuracy', val_accuracy, epoch)\n",
    "            # writer.add_scalar('Val_ROC_AUC', val_roc_auc, epoch)\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'best_val_loss': best_val_loss\n",
    "                }, checkpoint_path)\n",
    "                print(\"Validation Accuracy is \", val_accuracy)\n",
    "                print(f\"Epoch {epoch+1}: Val Loss improved to {val_loss:.4f}, saved checkpoint\")\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}: Val Loss did not improve from {best_val_loss:.4f}\")\n",
    "    \n",
    "    return best_val_loss\n",
    "\n",
    "# Load and preprocess the data\n",
    "data_directory = '../../Data/Filtered_split_training_data/'\n",
    "chromosome_number = 8\n",
    "file_name = data_directory + f\"23AndMe_PRS313_merged_chr{chromosome_number}_matching_split.parquet\"\n",
    "data = pd.read_parquet(file_name)\n",
    "\n",
    "print(\"Unknown PRS313 SNPs: \", data[[col for col in data.columns if \"PRS313_Unknown\" in col]].shape[1])\n",
    "print(\"Known PRS313 SNPs: \", data[[col for col in data.columns if \"PRS313_Known\" in col]].shape[1])\n",
    "print(\"23AndMe SNPs with LD to Unknown PRS313 SNPs: \", data[[col for col in data.columns if \"PRS313_\" not in col]].shape[1])\n",
    "\n",
    "# Split the data into train and validation sets\n",
    "X = torch.tensor(data.filter(regex='^(?!.*Unknown)').values, dtype=torch.float32)\n",
    "y = torch.tensor(data.filter(regex='Unknown').values, dtype=torch.float32)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128)\n",
    "\n",
    "# Set up the model, loss function, optimizer, and learning rate scheduler\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size1 = 20\n",
    "hidden_size2 = 20\n",
    "output_size = y_train.shape[1]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SimpleFFNN(input_size, hidden_size1, hidden_size2, output_size).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.01, patience=5, verbose=True)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 100\n",
    "checkpoint_path = f'checkpoint_chr{chromosome_number}.pth'\n",
    "writer = SummaryWriter()\n",
    "best_val_loss = train_and_evaluate_model(model, criterion, optimizer, scheduler, train_loader, val_loader, num_epochs, device, writer, checkpoint_path)\n",
    "writer.close()\n",
    "\n",
    "# Load the best model and evaluate on the validation set\n",
    "best_model = SimpleFFNN(input_size, hidden_size1, hidden_size2, output_size).to(device)\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "best_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "best_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for batch_X, batch_y in val_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        outputs = best_model(batch_X)\n",
    "        val_accuracy = ((outputs > 0.5) == batch_y.to(device)).float().mean()\n",
    "\n",
    "        val_accuracies.append(val_accuracy.cpu())\n",
    "\n",
    "    print(np.mean(val_accuracies))\n",
    "    # val_roc_auc = roc_auc_score(val_labels, val_outputs, average=\"macro\")\n",
    "    \n",
    "    print(f\"Best Model - Val Accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown PRS313 SNPs:  54\n",
      "Known PRS313 SNPs:  14\n",
      "23AndMe SNPs with LD to Unknown PRS313 SNPs:  1850\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(data\u001b[38;5;241m.\u001b[39mfilter(regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnknown\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Define the logistic regression model\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLogisticRegression\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_dim, output_dim):\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;28msuper\u001b[39m(LogisticRegression, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "Cell \u001b[0;32mIn[51], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(data\u001b[38;5;241m.\u001b[39mfilter(regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnknown\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Define the logistic regression model\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLogisticRegression\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_dim, output_dim):\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;28msuper\u001b[39m(LogisticRegression, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_frame.py:988\u001b[0m, in \u001b[0;36mPyDBFrame.trace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# if thread has a suspend flag, we suspend with a busy wait\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info\u001b[38;5;241m.\u001b[39mpydev_state \u001b[38;5;241m==\u001b[39m STATE_SUSPEND:\n\u001b[0;32m--> 988\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrace_dispatch\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_frame.py:165\u001b[0m, in \u001b[0;36mPyDBFrame.do_wait_suspend\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_wait_suspend\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Load and preprocess the data\n",
    "data_directory = '../../Data/Filtered_split_training_data/'\n",
    "chromosome_number = 1\n",
    "\n",
    "for chromosome_number in range (5,23):\n",
    "    file_name = data_directory + f\"23AndMe_PRS313_merged_chr{chromosome_number}_matching_split.parquet\"\n",
    "    data = pd.read_parquet(file_name)\n",
    "    print(\"Unknown PRS313 SNPs: \", data[[col for col in data.columns if \"PRS313_Unknown\" in col]].shape[1])\n",
    "    print(\"Known PRS313 SNPs: \", data[[col for col in data.columns if \"PRS313_Known\" in col]].shape[1])\n",
    "    print(\"23AndMe SNPs with LD to Unknown PRS313 SNPs: \", data[[col for col in data.columns if \"PRS313_\" not in col]].shape[1])\n",
    "\n",
    "    # Split the data into features and target\n",
    "    X = torch.tensor(data.filter(regex='^(?!.*Unknown)').values, dtype=torch.float32)\n",
    "    y = torch.tensor(data.filter(regex='Unknown').values, dtype=torch.float32)\n",
    "\n",
    "    # Define the logistic regression model\n",
    "    class LogisticRegression(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim):\n",
    "            super(LogisticRegression, self).__init__()\n",
    "            self.linear = nn.Linear(input_dim, output_dim)\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        def forward(self, x):\n",
    "            out = self.linear(x)\n",
    "            out = self.sigmoid(out)\n",
    "            return out\n",
    "\n",
    "    # Set the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Set the hyperparameters\n",
    "    input_dim = X.shape[1]\n",
    "    output_dim = y.shape[1]\n",
    "    learning_rate = 0.001\n",
    "    num_epochs = 400\n",
    "    batch_size = 128\n",
    "    num_folds = 5\n",
    "\n",
    "    # Define the loss function\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # K-fold cross-validation\n",
    "    kfold = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "    fold_accuracies = []\n",
    "    fold_precisions = []\n",
    "    fold_recalls = []\n",
    "    fold_f1_scores = []\n",
    "    fold_roc_auc_scores = []\n",
    "    fold_train_losses = []\n",
    "    fold_val_losses = []\n",
    "\n",
    "\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X)):\n",
    "        print(f\"Fold {fold + 1}/{num_folds}\")\n",
    "        \n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        train_dataset = TensorDataset(X_train, y_train)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        val_dataset = TensorDataset(X_val, y_val)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "        \n",
    "        model = LogisticRegression(input_dim, output_dim).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss = 0.0\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                \n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            train_loss /= len(train_loader)\n",
    "            fold_train_losses.append(train_loss)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val.to(device))\n",
    "            val_loss = criterion(val_outputs, y_val.to(device))\n",
    "            fold_val_losses.append(val_loss.item())\n",
    "            \n",
    "            val_preds = (val_outputs > 0.5).float()\n",
    "            val_accuracy = ((val_preds > 0.5) == y_val).float().mean()\n",
    "            val_precision = precision_score(y_val.cpu().numpy(), val_preds.cpu().numpy(), average='micro')\n",
    "            val_recall = recall_score(y_val.cpu().numpy(), val_preds.cpu().numpy(), average='micro')\n",
    "            val_f1 = f1_score(y_val.cpu().numpy(), val_preds.cpu().numpy(), average='micro')\n",
    "            val_roc_auc = roc_auc_score(y_val.cpu().numpy(), val_outputs.cpu().numpy(), average='micro')\n",
    "            \n",
    "            fold_accuracies.append(val_accuracy)\n",
    "            fold_precisions.append(val_precision)\n",
    "            fold_recalls.append(val_recall)\n",
    "            fold_f1_scores.append(val_f1)\n",
    "            fold_roc_auc_scores.append(val_roc_auc)\n",
    "            \n",
    "            print(f\"Fold {fold + 1}/{num_folds}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss.item():.4f}, Val Accuracy: {val_accuracy:.4f}, Val Precision: {val_precision:.4f}, Val Recall: {val_recall:.4f}, Val F1: {val_f1:.4f}, Val ROC AUC: {val_roc_auc:.4f}\")\n",
    "\n",
    "    print(f\"Average Accuracy: {np.mean(fold_accuracies):.4f} +/- {np.std(fold_accuracies):.4f}\")\n",
    "    print(f\"Average Precision: {np.mean(fold_precisions):.4f} +/- {np.std(fold_precisions):.4f}\")\n",
    "    print(f\"Average Recall: {np.mean(fold_recalls):.4f} +/- {np.std(fold_recalls):.4f}\")\n",
    "    print(f\"Average F1 Score: {np.mean(fold_f1_scores):.4f} +/- {np.std(fold_f1_scores):.4f}\")\n",
    "    print(f\"Average ROC AUC: {np.mean(fold_roc_auc_scores):.4f} +/- {np.std(fold_roc_auc_scores):.4f}\")\n",
    "\n",
    "    import csv\n",
    "\n",
    "    # Export results to CSV\n",
    "\n",
    "    output_folder = \"../../Data/model_results/logistic_regression/\"\n",
    "\n",
    "    csv_file = output_folder + f'cross_validation_results_chr{chromosome_number}.csv'\n",
    "    fieldnames = ['Fold', 'Train Loss', 'Val Loss', 'Val Accuracy', 'Val Precision', 'Val Recall', 'Val F1', 'Val ROC AUC']\n",
    "\n",
    "    with open(csv_file, 'w', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for fold in range(num_folds):\n",
    "            writer.writerow({\n",
    "                'Fold': fold + 1,\n",
    "                'Train Loss': fold_train_losses[fold],\n",
    "                'Val Loss': fold_val_losses[fold],\n",
    "                'Val Accuracy': fold_accuracies[fold],\n",
    "                'Val Precision': fold_precisions[fold],\n",
    "                'Val Recall': fold_recalls[fold],\n",
    "                'Val F1': fold_f1_scores[fold],\n",
    "                'Val ROC AUC': fold_roc_auc_scores[fold]\n",
    "            })\n",
    "\n",
    "        writer.writerow({})  # Empty row for separation\n",
    "        writer.writerow({\n",
    "            'Fold': 'Average',\n",
    "            'Train Loss': np.mean(fold_train_losses),\n",
    "            'Val Loss': np.mean(fold_val_losses),\n",
    "            'Val Accuracy': np.mean(fold_accuracies),\n",
    "            'Val Precision': np.mean(fold_precisions),\n",
    "            'Val Recall': np.mean(fold_recalls),\n",
    "            'Val F1': np.mean(fold_f1_scores),\n",
    "            'Val ROC AUC': np.mean(fold_roc_auc_scores)\n",
    "        })\n",
    "        writer.writerow({\n",
    "            'Fold': 'Std Dev',\n",
    "            'Train Loss': np.std(fold_train_losses),\n",
    "            'Val Loss': np.std(fold_val_losses),\n",
    "            'Val Accuracy': np.std(fold_accuracies),\n",
    "            'Val Precision': np.std(fold_precisions),\n",
    "            'Val Recall': np.std(fold_recalls),\n",
    "            'Val F1': np.std(fold_f1_scores),\n",
    "            'Val ROC AUC': np.std(fold_roc_auc_scores)\n",
    "        })\n",
    "\n",
    "    print(f\"Results exported to {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Dim: 64, Num Layers: 2, Num Heads: 2, Dropout: 0.1, Dim Feedforward: 128, Batch Size: 32, Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype, but got Bool and Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[41], line 92\u001b[0m\n",
      "\u001b[1;32m     89\u001b[0m X_val, X_test \u001b[38;5;241m=\u001b[39m X_test[:\u001b[38;5;28mlen\u001b[39m(X_test)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m], X_test[\u001b[38;5;28mlen\u001b[39m(X_test)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m:]\n",
      "\u001b[1;32m     90\u001b[0m y_val, y_test \u001b[38;5;241m=\u001b[39m y_test[:\u001b[38;5;28mlen\u001b[39m(y_test)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m], y_test[\u001b[38;5;28mlen\u001b[39m(y_test)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m:]\n",
      "\u001b[0;32m---> 92\u001b[0m \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_feedforward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     94\u001b[0m model \u001b[38;5;241m=\u001b[39m TransformerImputer(input_dim, hidden_dim, num_layers, num_heads, dropout, dim_feedforward)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;32m     95\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model.pt\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\n",
      "Cell \u001b[0;32mIn[41], line 52\u001b[0m, in \u001b[0;36mrun_training\u001b[0;34m(X_train, y_train, X_val, y_val, hidden_dim, num_layers, num_heads, dropout, dim_feedforward, batch_size, learning_rate, num_epochs, device)\u001b[0m\n",
      "\u001b[1;32m     50\u001b[0m best_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n",
      "\u001b[0;32m---> 52\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     53\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m evaluate(model, val_dataloader, criterion, device)\n",
      "\u001b[1;32m     55\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "Cell \u001b[0;32mIn[41], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, criterion, device)\u001b[0m\n",
      "\u001b[1;32m     12\u001b[0m X_batch, y_batch, mask_batch \u001b[38;5;241m=\u001b[39m X_batch\u001b[38;5;241m.\u001b[39mto(device), y_batch\u001b[38;5;241m.\u001b[39mto(device), mask_batch\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[0;32m---> 15\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_batch\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output[mask_batch], y_batch[mask_batch])\n",
      "\u001b[1;32m     17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "Cell \u001b[0;32mIn[40], line 15\u001b[0m, in \u001b[0;36mTransformerImputer.forward\u001b[0;34m(self, x, mask)\u001b[0m\n",
      "\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask):\n",
      "\u001b[0;32m---> 15\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     16\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Shape: (seq_len, batch_size, hidden_dim)\u001b[39;00m\n",
      "\u001b[1;32m     17\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(x, src_key_padding_mask\u001b[38;5;241m=\u001b[39mmask)\n",
      "\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n",
      "\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n",
      "\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype, but got Bool and Float"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in dataloader:\n",
    "        X_batch, y_batch, mask_batch = batch\n",
    "        X_batch, y_batch, mask_batch = X_batch.to(device), y_batch.to(device), mask_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch, mask_batch)\n",
    "        loss = criterion(output[mask_batch], y_batch[mask_batch])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    return train_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            X_batch, y_batch, mask_batch = batch\n",
    "            X_batch, y_batch, mask_batch = X_batch.to(device), y_batch.to(device), mask_batch.to(device)\n",
    "            \n",
    "            output = model(X_batch, mask_batch)\n",
    "            loss = criterion(output[mask_batch], y_batch[mask_batch])\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "    return val_loss / len(dataloader)\n",
    "\n",
    "def run_training(X_train, y_train, X_val, y_val, hidden_dim, num_layers, num_heads, dropout, dim_feedforward, batch_size, learning_rate, num_epochs, device):\n",
    "    input_dim = X_train.shape[1]\n",
    "    \n",
    "    train_dataset = TensorDataset(torch.tensor(X_train), torch.tensor(y_train), torch.tensor(np.random.rand(*X_train.shape) < 0.2))\n",
    "    val_dataset = TensorDataset(torch.tensor(X_val), torch.tensor(y_val), torch.tensor(np.random.rand(*X_val.shape) < 0.2))\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    model = TransformerImputer(input_dim, hidden_dim, num_layers, num_heads, dropout, dim_feedforward).to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = BCEWithLogitsLoss()\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train(model, train_dataloader, optimizer, criterion, device)\n",
    "        val_loss = evaluate(model, val_dataloader, criterion, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "\n",
    "# Hyperparameter tuning with 10-fold cross-validation\n",
    "hidden_dims = [64, 128, 256]\n",
    "num_layers_options = [2, 4, 6]\n",
    "num_heads_options = [2, 4, 8]\n",
    "dropout_options = [0.1, 0.2, 0.3]\n",
    "dim_feedforward_options = [128, 256, 512]  # Add dim_feedforward options\n",
    "batch_sizes = [32, 64, 128]\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "num_epochs = 50\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "best_hyperparams = None\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for hidden_dim in hidden_dims:\n",
    "    for num_layers in num_layers_options:\n",
    "        for num_heads in num_heads_options:\n",
    "            for dropout in dropout_options:\n",
    "                for dim_feedforward in dim_feedforward_options:  # Add dim_feedforward loop\n",
    "                    for batch_size in batch_sizes:\n",
    "                        for learning_rate in learning_rates:\n",
    "                            print(f\"Hidden Dim: {hidden_dim}, Num Layers: {num_layers}, Num Heads: {num_heads}, Dropout: {dropout}, Dim Feedforward: {dim_feedforward}, Batch Size: {batch_size}, Learning Rate: {learning_rate}\")\n",
    "                            \n",
    "                            val_losses = []\n",
    "                            for train_idx, test_idx in kfold.split(X):\n",
    "                                X_train, X_test = X[train_idx], X[test_idx]\n",
    "                                y_train, y_test = y[train_idx], y[test_idx]\n",
    "                                \n",
    "                                X_val, X_test = X_test[:len(X_test)//2], X_test[len(X_test)//2:]\n",
    "                                y_val, y_test = y_test[:len(y_test)//2], y_test[len(y_test)//2:]\n",
    "                                \n",
    "                                run_training(X_train, y_train, X_val, y_val, hidden_dim, num_layers, num_heads, dropout, dim_feedforward, batch_size, learning_rate, num_epochs, device)\n",
    "                                \n",
    "                                model = TransformerImputer(input_dim, hidden_dim, num_layers, num_heads, dropout, dim_feedforward).to(device)\n",
    "                                model.load_state_dict(torch.load('best_model.pt'))\n",
    "                                \n",
    "                                test_dataset = TensorDataset(torch.tensor(X_test), torch.tensor(y_test), torch.tensor(np.random.rand(*X_test.shape) < 0.2))\n",
    "                                test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "                                \n",
    "                                val_loss = evaluate(model, test_dataloader, criterion, device)\n",
    "                                val_losses.append(val_loss)\n",
    "                            \n",
    "                            mean_val_loss = np.mean(val_losses)\n",
    "                            print(f\"Mean Val Loss: {mean_val_loss:.4f}\")\n",
    "                            \n",
    "                            if mean_val_loss < best_val_loss:\n",
    "                                best_val_loss = mean_val_loss\n",
    "                                best_hyperparams = (hidden_dim, num_layers, num_heads, dropout, dim_feedforward, batch_size, learning_rate)\n",
    "\n",
    "print(f\"Best Hyperparameters: Hidden Dim: {best_hyperparams[0]}, Num Layers: {best_hyperparams[1]}, Num Heads: {best_hyperparams[2]}, Dropout: {best_hyperparams[3]}, Dim Feedforward: {best_hyperparams[4]}, Batch Size: {best_hyperparams[5]}, Learning Rate: {best_hyperparams[6]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
