{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown PRS313 SNPs:  40\n",
      "Known PRS313 SNPs:  20\n",
      "23AndMe SNPs with LD to Unknown PRS313 SNPs:  2204\n",
      "Fold 1/5\n",
      "Fold 1/5, Train Loss: 0.0000, Val Loss: 0.8972, Val Accuracy: 0.9076, Val Precision: 0.8658, Val Recall: 0.8268, Val F1: 0.8458, Val ROC AUC: 0.9522, Val R2: 0.5074\n",
      "Fold 2/5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 88\u001b[0m\n\u001b[1;32m     86\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_X, batch_y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m---> 88\u001b[0m     batch_X, batch_y \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_X\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, batch_y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     90\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(batch_X)\n\u001b[1;32m     91\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_y)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import r2_score as sklearn_r2_score\n",
    "\n",
    "\n",
    "# Load and preprocess the data\n",
    "data_directory = '../../Data/Filtered_split_training_data/'\n",
    "chromosome_number = 1\n",
    "\n",
    "hidden_size1 = 150\n",
    "hidden_size2 = 150\n",
    "\n",
    "for chromosome_number in range (1,23):\n",
    "    file_name = data_directory + f\"23AndMe_PRS313_merged_chr{chromosome_number}_matching_split.parquet\"\n",
    "    data = pd.read_parquet(file_name)\n",
    "    print(\"Unknown PRS313 SNPs: \", data[[col for col in data.columns if \"PRS313_Unknown\" in col]].shape[1])\n",
    "    print(\"Known PRS313 SNPs: \", data[[col for col in data.columns if \"PRS313_Known\" in col]].shape[1])\n",
    "    print(\"23AndMe SNPs with LD to Unknown PRS313 SNPs: \", data[[col for col in data.columns if \"PRS313_\" not in col]].shape[1])\n",
    "\n",
    "    # Split the data into features and target\n",
    "    X = torch.tensor(data.filter(regex='^(?!.*Unknown)').values, dtype=torch.float32)\n",
    "    y = torch.tensor(data.filter(regex='Unknown').values, dtype=torch.float32)\n",
    "\n",
    "        \n",
    "        # Define the updated neural network architecture\n",
    "    class SimpleFFNN(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "            super(SimpleFFNN, self).__init__()\n",
    "            self.hidden1 = nn.Linear(input_size, hidden_size1)\n",
    "            self.hidden2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "            self.output = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = torch.relu(self.hidden1(x))\n",
    "            x = torch.relu(self.hidden2(x))\n",
    "            x = torch.sigmoid(self.output(x))\n",
    "            return x\n",
    "\n",
    "    # Set the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Set the hyperparameters\n",
    "    input_dim = X.shape[1]\n",
    "    output_dim = y.shape[1]\n",
    "    learning_rate = 0.001\n",
    "    num_epochs = 400\n",
    "    batch_size = 128\n",
    "    num_folds = 5\n",
    "\n",
    "    # Define the loss function\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # K-fold cross-validation\n",
    "    kfold = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "    fold_accuracies = []\n",
    "    fold_precisions = []\n",
    "    fold_recalls = []\n",
    "    fold_f1_scores = []\n",
    "    fold_roc_auc_scores = []\n",
    "    fold_r2_scores = []\n",
    "    fold_train_losses = []\n",
    "    fold_val_losses = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X)):\n",
    "        print(f\"Fold {fold + 1}/{num_folds}\")\n",
    "        \n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        train_dataset = TensorDataset(X_train, y_train)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        val_dataset = TensorDataset(X_val, y_val)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "        \n",
    "        model = SimpleFFNN(input_dim,hidden_size1, hidden_size2, output_dim).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss = 0.0\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                \n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            train_loss /= len(train_loader)\n",
    "            fold_train_losses.append(train_loss)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val.to(device))\n",
    "            val_loss = criterion(val_outputs, y_val.to(device))\n",
    "            fold_val_losses.append(val_loss.item())\n",
    "            \n",
    "            val_preds = (val_outputs > 0.5).float()\n",
    "            val_accuracy = float(((val_preds > 0.5) == y_val).float().mean())\n",
    "            val_precision = precision_score(y_val.cpu().numpy(), val_preds.cpu().numpy(), average='micro')\n",
    "            val_recall = recall_score(y_val.cpu().numpy(), val_preds.cpu().numpy(), average='micro')\n",
    "            val_f1 = f1_score(y_val.cpu().numpy(), val_preds.cpu().numpy(), average='micro')\n",
    "            val_roc_auc = roc_auc_score(y_val.cpu().numpy(), val_outputs.cpu().numpy(), average='micro')\n",
    "            val_r2 = sklearn_r2_score(y_val.cpu().numpy(), val_outputs.cpu().numpy())\n",
    "\n",
    "            fold_accuracies.append(val_accuracy)\n",
    "            fold_precisions.append(val_precision)\n",
    "            fold_recalls.append(val_recall)\n",
    "            fold_f1_scores.append(val_f1)\n",
    "            fold_roc_auc_scores.append(val_roc_auc)\n",
    "            fold_r2_scores.append(val_r2)\n",
    "\n",
    "            print(f\"Fold {fold + 1}/{num_folds}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss.item():.4f}, Val Accuracy: {val_accuracy:.4f}, Val Precision: {val_precision:.4f}, Val Recall: {val_recall:.4f}, Val F1: {val_f1:.4f}, Val ROC AUC: {val_roc_auc:.4f}, Val R2: {val_r2:.4f}\")\n",
    "\n",
    "\n",
    "    print(f\"Average Accuracy: {np.mean(fold_accuracies):.4f} +/- {np.std(fold_accuracies):.4f}\")\n",
    "    print(f\"Average Precision: {np.mean(fold_precisions):.4f} +/- {np.std(fold_precisions):.4f}\")\n",
    "    print(f\"Average Recall: {np.mean(fold_recalls):.4f} +/- {np.std(fold_recalls):.4f}\")\n",
    "    print(f\"Average F1 Score: {np.mean(fold_f1_scores):.4f} +/- {np.std(fold_f1_scores):.4f}\")\n",
    "    print(f\"Average ROC AUC: {np.mean(fold_roc_auc_scores):.4f} +/- {np.std(fold_roc_auc_scores):.4f}\")\n",
    "    print(f\"Average R2 Score: {np.mean(fold_r2_scores):.4f} +/- {np.std(fold_r2_scores):.4f}\")\n",
    "\n",
    "    import csv\n",
    "\n",
    "    # Export results to CSV\n",
    "\n",
    "    output_folder = \"../../Data/model_results/logistic_regression/\"\n",
    "\n",
    "    csv_file = output_folder + f'cross_validation_results_chr{chromosome_number}.csv'\n",
    "    fieldnames = ['Fold', 'Train Loss', 'Val Loss', 'Val Accuracy', 'Val Precision', 'Val Recall', 'Val F1', 'Val ROC AUC', 'Val R2']\n",
    "\n",
    "    with open(csv_file, 'w', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for fold in range(num_folds):\n",
    "            writer.writerow({\n",
    "                'Fold': fold + 1,\n",
    "                'Train Loss': fold_train_losses[fold],\n",
    "                'Val Loss': fold_val_losses[fold],\n",
    "                'Val Accuracy': fold_accuracies[fold],\n",
    "                'Val Precision': fold_precisions[fold],\n",
    "                'Val Recall': fold_recalls[fold],\n",
    "                'Val F1': fold_f1_scores[fold],\n",
    "                'Val ROC AUC': fold_roc_auc_scores[fold],\n",
    "                'Val R2': fold_r2_scores[fold]\n",
    "            })\n",
    "\n",
    "        writer.writerow({})  # Empty row for separation\n",
    "        writer.writerow({\n",
    "            'Fold': 'Average',\n",
    "            'Train Loss': np.mean(fold_train_losses),\n",
    "            'Val Loss': np.mean(fold_val_losses),\n",
    "            'Val Accuracy': np.mean(fold_accuracies),\n",
    "            'Val Precision': np.mean(fold_precisions),\n",
    "            'Val Recall': np.mean(fold_recalls),\n",
    "            'Val F1': np.mean(fold_f1_scores),\n",
    "            'Val ROC AUC': np.mean(fold_roc_auc_scores),\n",
    "            'Val R2': np.mean(fold_r2_scores)\n",
    "        })\n",
    "        writer.writerow({\n",
    "            'Fold': 'Std Dev',\n",
    "            'Train Loss': np.std(fold_train_losses),\n",
    "            'Val Loss': np.std(fold_val_losses),\n",
    "            'Val Accuracy': np.std(fold_accuracies),\n",
    "            'Val Precision': np.std(fold_precisions),\n",
    "            'Val Recall': np.std(fold_recalls),\n",
    "            'Val F1': np.std(fold_f1_scores),\n",
    "            'Val ROC AUC': np.std(fold_roc_auc_scores),\n",
    "            'Val R2': np.std(fold_r2_scores)\n",
    "        })\n",
    "\n",
    "    print(f\"Results exported to {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown PRS313 SNPs:  8\n",
      "Known PRS313 SNPs:  2\n",
      "23AndMe SNPs with LD to Unknown PRS313 SNPs:  210\n",
      "Loaded checkpoint from checkpoint_chr13.pth. Starting from epoch 44\n",
      "Epoch 44: Val Loss did not improve from 0.1294\n",
      "Epoch 45: Val Loss did not improve from 0.1294\n",
      "Epoch 46: Val Loss did not improve from 0.1294\n",
      "Epoch 47: Val Loss did not improve from 0.1294\n",
      "Epoch 48: Val Loss did not improve from 0.1294\n",
      "Epoch 49: Val Loss did not improve from 0.1294\n",
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 50: Val Loss did not improve from 0.1294\n",
      "Epoch 51: Val Loss did not improve from 0.1294\n",
      "Epoch 52: Val Loss did not improve from 0.1294\n",
      "Epoch 53: Val Loss did not improve from 0.1294\n",
      "Epoch 54: Val Loss did not improve from 0.1294\n",
      "Epoch 55: Val Loss did not improve from 0.1294\n",
      "Epoch 56: Val Loss did not improve from 0.1294\n",
      "Epoch 57: Val Loss did not improve from 0.1294\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 58: Val Loss did not improve from 0.1294\n",
      "Epoch 59: Val Loss did not improve from 0.1294\n",
      "Epoch 60: Val Loss did not improve from 0.1294\n",
      "Epoch 61: Val Loss did not improve from 0.1294\n",
      "Epoch 62: Val Loss did not improve from 0.1294\n",
      "Epoch 63: Val Loss did not improve from 0.1294\n",
      "Epoch 00021: reducing learning rate of group 0 to 1.0000e-09.\n",
      "Epoch 64: Val Loss did not improve from 0.1294\n",
      "Epoch 65: Val Loss did not improve from 0.1294\n",
      "Epoch 66: Val Loss did not improve from 0.1294\n",
      "Epoch 67: Val Loss did not improve from 0.1294\n",
      "Epoch 68: Val Loss did not improve from 0.1294\n",
      "Epoch 69: Val Loss did not improve from 0.1294\n",
      "Epoch 70: Val Loss did not improve from 0.1294\n",
      "Epoch 71: Val Loss did not improve from 0.1294\n",
      "Epoch 72: Val Loss did not improve from 0.1294\n",
      "Epoch 73: Val Loss did not improve from 0.1294\n",
      "Epoch 74: Val Loss did not improve from 0.1294\n",
      "Epoch 75: Val Loss did not improve from 0.1294\n",
      "Epoch 76: Val Loss did not improve from 0.1294\n",
      "Epoch 77: Val Loss did not improve from 0.1294\n",
      "Epoch 78: Val Loss did not improve from 0.1294\n",
      "Epoch 79: Val Loss did not improve from 0.1294\n",
      "Epoch 80: Val Loss did not improve from 0.1294\n",
      "Epoch 81: Val Loss did not improve from 0.1294\n",
      "Epoch 82: Val Loss did not improve from 0.1294\n",
      "Epoch 83: Val Loss did not improve from 0.1294\n",
      "Epoch 84: Val Loss did not improve from 0.1294\n",
      "Epoch 85: Val Loss did not improve from 0.1294\n",
      "Epoch 86: Val Loss did not improve from 0.1294\n",
      "Epoch 87: Val Loss did not improve from 0.1294\n",
      "Epoch 88: Val Loss did not improve from 0.1294\n",
      "Epoch 89: Val Loss did not improve from 0.1294\n",
      "Epoch 90: Val Loss did not improve from 0.1294\n",
      "Epoch 91: Val Loss did not improve from 0.1294\n",
      "Epoch 92: Val Loss did not improve from 0.1294\n",
      "Epoch 93: Val Loss did not improve from 0.1294\n",
      "Epoch 94: Val Loss did not improve from 0.1294\n",
      "Epoch 95: Val Loss did not improve from 0.1294\n",
      "Epoch 96: Val Loss did not improve from 0.1294\n",
      "Epoch 97: Val Loss did not improve from 0.1294\n",
      "Epoch 98: Val Loss did not improve from 0.1294\n",
      "Epoch 99: Val Loss did not improve from 0.1294\n",
      "Epoch 100: Val Loss did not improve from 0.1294\n",
      "Epoch 101: Val Loss did not improve from 0.1294\n",
      "Epoch 102: Val Loss did not improve from 0.1294\n",
      "Epoch 103: Val Loss did not improve from 0.1294\n",
      "Epoch 104: Val Loss did not improve from 0.1294\n",
      "Epoch 105: Val Loss did not improve from 0.1294\n",
      "Epoch 106: Val Loss did not improve from 0.1294\n",
      "Epoch 107: Val Loss did not improve from 0.1294\n",
      "Epoch 108: Val Loss did not improve from 0.1294\n",
      "Epoch 109: Val Loss did not improve from 0.1294\n",
      "Epoch 110: Val Loss did not improve from 0.1294\n",
      "Epoch 111: Val Loss did not improve from 0.1294\n",
      "Epoch 112: Val Loss did not improve from 0.1294\n",
      "Epoch 113: Val Loss did not improve from 0.1294\n",
      "Epoch 114: Val Loss did not improve from 0.1294\n",
      "Epoch 115: Val Loss did not improve from 0.1294\n",
      "Epoch 116: Val Loss did not improve from 0.1294\n",
      "Epoch 117: Val Loss did not improve from 0.1294\n",
      "Epoch 118: Val Loss did not improve from 0.1294\n",
      "Epoch 119: Val Loss did not improve from 0.1294\n",
      "Epoch 120: Val Loss did not improve from 0.1294\n",
      "Epoch 121: Val Loss did not improve from 0.1294\n",
      "Epoch 122: Val Loss did not improve from 0.1294\n",
      "Epoch 123: Val Loss did not improve from 0.1294\n",
      "Epoch 124: Val Loss did not improve from 0.1294\n",
      "Epoch 125: Val Loss did not improve from 0.1294\n",
      "Epoch 126: Val Loss did not improve from 0.1294\n",
      "Epoch 127: Val Loss did not improve from 0.1294\n",
      "Epoch 128: Val Loss did not improve from 0.1294\n",
      "Epoch 129: Val Loss did not improve from 0.1294\n",
      "Epoch 130: Val Loss did not improve from 0.1294\n",
      "Epoch 131: Val Loss did not improve from 0.1294\n",
      "Epoch 132: Val Loss did not improve from 0.1294\n",
      "Epoch 133: Val Loss did not improve from 0.1294\n",
      "Epoch 134: Val Loss did not improve from 0.1294\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 125\u001b[0m\n\u001b[1;32m    123\u001b[0m checkpoint_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint_chr\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchromosome_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    124\u001b[0m writer \u001b[38;5;241m=\u001b[39m SummaryWriter()\n\u001b[0;32m--> 125\u001b[0m best_val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m writer\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# Load the best model and evaluate on the validation set\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 46\u001b[0m, in \u001b[0;36mtrain_and_evaluate_model\u001b[0;34m(model, criterion, optimizer, scheduler, train_loader, val_loader, num_epochs, device, writer, checkpoint_path)\u001b[0m\n\u001b[1;32m     44\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(batch_X)\n\u001b[1;32m     45\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_y)\n\u001b[0;32m---> 46\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     49\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "\n",
    "# Define the updated neural network architecture\n",
    "class SimpleFFNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(SimpleFFNN, self).__init__()\n",
    "        self.hidden1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.hidden2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.output = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.hidden1(x))\n",
    "        x = torch.relu(self.hidden2(x))\n",
    "        x = torch.sigmoid(self.output(x))\n",
    "        return x\n",
    "\n",
    "def train_and_evaluate_model(model, criterion, optimizer, scheduler, train_loader, val_loader, num_epochs, device, writer, checkpoint_path):\n",
    "    best_val_loss = float('inf')\n",
    "    start_epoch = 0\n",
    "\n",
    "    # Load checkpoint if it exists\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        best_val_loss = checkpoint['best_val_loss']\n",
    "        print(f\"Loaded checkpoint from {checkpoint_path}. Starting from epoch {start_epoch + 1}\")\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            val_outputs = []\n",
    "            val_labels = []\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_X)\n",
    "                val_loss += criterion(outputs, batch_y).item()\n",
    "                val_outputs.extend(outputs.cpu().numpy())\n",
    "                val_labels.extend(batch_y.cpu().numpy())\n",
    "            \n",
    "            val_loss /= len(val_loader)\n",
    "            scheduler.step(val_loss)  # Update the learning rate based on validation loss\n",
    "            val_outputs = np.array(val_outputs)\n",
    "            val_labels = np.array(val_labels)\n",
    "            val_accuracy = ((val_outputs > 0.5) == val_labels).mean()\n",
    "            # val_accuracy = accuracy_score((val_labels == 1), np.round(val_outputs))\n",
    "            # val_roc_auc = roc_auc_score(val_labels, val_outputs, average=\"macro\")\n",
    "            \n",
    "            writer.add_scalar('Val_Loss', val_loss, epoch)\n",
    "            writer.add_scalar('Val_Accuracy', val_accuracy, epoch)\n",
    "            # writer.add_scalar('Val_ROC_AUC', val_roc_auc, epoch)\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'best_val_loss': best_val_loss\n",
    "                }, checkpoint_path)\n",
    "                print(\"Validation Accuracy is \", val_accuracy)\n",
    "                print(f\"Epoch {epoch+1}: Val Loss improved to {val_loss:.4f}, saved checkpoint\")\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}: Val Loss did not improve from {best_val_loss:.4f}\")\n",
    "    \n",
    "    return best_val_loss\n",
    "\n",
    "# Load and preprocess the data\n",
    "data_directory = '../../Data/Filtered_split_training_data/'\n",
    "chromosome_number = 13\n",
    "file_name = data_directory + f\"23AndMe_PRS313_merged_chr{chromosome_number}_matching_split.parquet\"\n",
    "data = pd.read_parquet(file_name)\n",
    "\n",
    "print(\"Unknown PRS313 SNPs: \", data[[col for col in data.columns if \"PRS313_Unknown\" in col]].shape[1])\n",
    "print(\"Known PRS313 SNPs: \", data[[col for col in data.columns if \"PRS313_Known\" in col]].shape[1])\n",
    "print(\"23AndMe SNPs with LD to Unknown PRS313 SNPs: \", data[[col for col in data.columns if \"PRS313_\" not in col]].shape[1])\n",
    "\n",
    "# Split the data into train and validation sets\n",
    "X = torch.tensor(data.filter(regex='^(?!.*Unknown)').values, dtype=torch.float32)\n",
    "y = torch.tensor(data.filter(regex='Unknown').values, dtype=torch.float32)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128)\n",
    "\n",
    "# Set up the model, loss function, optimizer, and learning rate scheduler\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size1 = 100\n",
    "hidden_size2 = 100\n",
    "output_size = y_train.shape[1]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SimpleFFNN(input_size, hidden_size1, hidden_size2, output_size).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.01, patience=5, verbose=True)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 400\n",
    "checkpoint_path = f'checkpoint_chr{chromosome_number}.pth'\n",
    "writer = SummaryWriter()\n",
    "best_val_loss = train_and_evaluate_model(model, criterion, optimizer, scheduler, train_loader, val_loader, num_epochs, device, writer, checkpoint_path)\n",
    "writer.close()\n",
    "\n",
    "# Load the best model and evaluate on the validation set\n",
    "best_model = SimpleFFNN(input_size, hidden_size1, hidden_size2, output_size).to(device)\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "best_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "best_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for batch_X, batch_y in val_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        outputs = best_model(batch_X)\n",
    "        val_accuracy = ((outputs > 0.5) == batch_y.to(device)).float().mean()\n",
    "\n",
    "        val_accuracies.append(val_accuracy.cpu())\n",
    "\n",
    "    print(np.mean(val_accuracies))\n",
    "    # val_roc_auc = roc_auc_score(val_labels, val_outputs, average=\"macro\")\n",
    "    \n",
    "    print(f\"Best Model - Val Accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chr22_46255004_A_G_maternal',\n",
       " 'chr22_46255004_A_G_paternal',\n",
       " 'chr22_46391695_C_T_maternal',\n",
       " 'chr22_46391695_C_T_paternal',\n",
       " 'chr22_46320275_A_G_maternal',\n",
       " 'chr22_46320275_A_G_paternal',\n",
       " 'chr22_46194973_C_T_maternal',\n",
       " 'chr22_46194973_C_T_paternal',\n",
       " 'chr22_46290571_T_C_maternal',\n",
       " 'chr22_46290571_T_C_paternal',\n",
       " 'chr22_46610409_A_G_maternal',\n",
       " 'chr22_46610409_A_G_paternal',\n",
       " 'chr22_46251927_A_G_maternal',\n",
       " 'chr22_46251927_A_G_paternal',\n",
       " 'chr22_46324957_T_C_maternal',\n",
       " 'chr22_46324957_T_C_paternal',\n",
       " 'chr22_46391888_A_G_maternal',\n",
       " 'chr22_46391888_A_G_paternal',\n",
       " 'chr22_46487902_A_G_maternal',\n",
       " 'chr22_46487902_A_G_paternal',\n",
       " 'chr22_46009063_A_G_maternal',\n",
       " 'chr22_46009063_A_G_paternal',\n",
       " 'chr22_46428306_T_C_maternal',\n",
       " 'chr22_46428306_T_C_paternal',\n",
       " 'chr22_46298649_G_A_maternal',\n",
       " 'chr22_46298649_G_A_paternal',\n",
       " 'chr22_46458123_T_G_maternal',\n",
       " 'chr22_46458123_T_G_paternal',\n",
       " 'chr22_46731689_G_T_maternal',\n",
       " 'chr22_46731689_G_T_paternal',\n",
       " 'chr22_46630634_G_C_maternal',\n",
       " 'chr22_46630634_G_C_paternal',\n",
       " 'chr22_46475878_G_A_maternal',\n",
       " 'chr22_46475878_G_A_paternal',\n",
       " 'chr22_46644177_G_A_maternal',\n",
       " 'chr22_46644177_G_A_paternal',\n",
       " 'chr22_46261909_T_C_maternal',\n",
       " 'chr22_46261909_T_C_paternal',\n",
       " 'chr22_46259060_T_C_maternal',\n",
       " 'chr22_46259060_T_C_paternal',\n",
       " 'chr22_45970957_T_C_maternal',\n",
       " 'chr22_45970957_T_C_paternal',\n",
       " 'chr22_46780521_T_C_maternal',\n",
       " 'chr22_46780521_T_C_paternal',\n",
       " 'chr22_46490826_A_G_maternal',\n",
       " 'chr22_46490826_A_G_paternal',\n",
       " 'chr22_46385917_T_C_maternal',\n",
       " 'chr22_46385917_T_C_paternal',\n",
       " 'chr22_45994189_T_G_maternal',\n",
       " 'chr22_45994189_T_G_paternal',\n",
       " 'chr22_46035521_A_G_maternal',\n",
       " 'chr22_46035521_A_G_paternal',\n",
       " 'chr22_46553308_C_T_maternal',\n",
       " 'chr22_46553308_C_T_paternal',\n",
       " 'chr22_46714748_C_T_maternal',\n",
       " 'chr22_46714748_C_T_paternal',\n",
       " 'chr22_46145176_G_T_maternal',\n",
       " 'chr22_46145176_G_T_paternal',\n",
       " 'chr22_46313215_T_C_maternal',\n",
       " 'chr22_46313215_T_C_paternal',\n",
       " 'chr22_46177845_T_C_maternal',\n",
       " 'chr22_46177845_T_C_paternal',\n",
       " 'chr22_46515210_T_C_maternal',\n",
       " 'chr22_46515210_T_C_paternal',\n",
       " 'chr22_45996298_A_G_maternal',\n",
       " 'chr22_45996298_A_G_paternal',\n",
       " 'chr22_46394928_C_T_maternal',\n",
       " 'chr22_46394928_C_T_paternal',\n",
       " 'chr22_46652959_A_G_maternal',\n",
       " 'chr22_46652959_A_G_paternal',\n",
       " 'chr22_45989581_A_G_maternal',\n",
       " 'chr22_45989581_A_G_paternal',\n",
       " 'chr22_46281667_C_T_maternal',\n",
       " 'chr22_46281667_C_T_paternal',\n",
       " 'chr22_45971158_T_C_maternal',\n",
       " 'chr22_45971158_T_C_paternal',\n",
       " 'chr22_46553234_C_T_maternal',\n",
       " 'chr22_46553234_C_T_paternal',\n",
       " 'chr22_46594035_T_C_maternal',\n",
       " 'chr22_46594035_T_C_paternal',\n",
       " 'chr22_46421842_G_T_maternal',\n",
       " 'chr22_46421842_G_T_paternal',\n",
       " 'chr22_45856865_A_G_maternal',\n",
       " 'chr22_45856865_A_G_paternal',\n",
       " 'chr22_46539639_A_G_maternal',\n",
       " 'chr22_46539639_A_G_paternal',\n",
       " 'chr22_46038908_C_T_maternal',\n",
       " 'chr22_46038908_C_T_paternal',\n",
       " 'chr22_46107508_G_A_maternal',\n",
       " 'chr22_46107508_G_A_paternal',\n",
       " 'chr22_46304440_A_C_maternal',\n",
       " 'chr22_46304440_A_C_paternal',\n",
       " 'chr22_46174057_C_T_maternal',\n",
       " 'chr22_46174057_C_T_paternal',\n",
       " 'chr22_45922466_A_G_maternal',\n",
       " 'chr22_45922466_A_G_paternal',\n",
       " 'chr22_46235677_A_G_maternal',\n",
       " 'chr22_46235677_A_G_paternal',\n",
       " 'chr22_46235105_C_T_maternal',\n",
       " 'chr22_46235105_C_T_paternal',\n",
       " 'chr22_46283297_G_A_PRS313_Unknown_maternal',\n",
       " 'chr22_46283297_G_A_PRS313_Unknown_paternal',\n",
       " 'chr22_39467803_G_A_maternal',\n",
       " 'chr22_39467803_G_A_paternal',\n",
       " 'chr22_39659773_C_T_maternal',\n",
       " 'chr22_39659773_C_T_paternal',\n",
       " 'chr22_39477391_T_C_maternal',\n",
       " 'chr22_39477391_T_C_paternal',\n",
       " 'chr22_39352175_G_A_maternal',\n",
       " 'chr22_39352175_G_A_paternal',\n",
       " 'chr22_39343083_C_T_maternal',\n",
       " 'chr22_39343083_C_T_paternal',\n",
       " 'chr22_39751711_G_A_maternal',\n",
       " 'chr22_39751711_G_A_paternal',\n",
       " 'chr22_39343916_T_A_PRS313_Known_maternal',\n",
       " 'chr22_39343916_T_A_PRS313_Known_paternal',\n",
       " 'chr22_39485921_C_T_maternal',\n",
       " 'chr22_39485921_C_T_paternal',\n",
       " 'chr22_39392250_G_A_maternal',\n",
       " 'chr22_39392250_G_A_paternal',\n",
       " 'chr22_39332623_C_T_maternal',\n",
       " 'chr22_39332623_C_T_paternal',\n",
       " 'chr22_39407399_T_C_maternal',\n",
       " 'chr22_39407399_T_C_paternal',\n",
       " 'chr22_39355109_T_C_maternal',\n",
       " 'chr22_39355109_T_C_paternal',\n",
       " 'chr22_39392480_T_C_maternal',\n",
       " 'chr22_39392480_T_C_paternal',\n",
       " 'chr22_39350072_T_C_maternal',\n",
       " 'chr22_39350072_T_C_paternal',\n",
       " 'chr22_39843409_C_T_maternal',\n",
       " 'chr22_39843409_C_T_paternal',\n",
       " 'chr22_39340670_T_G_maternal',\n",
       " 'chr22_39340670_T_G_paternal',\n",
       " 'chr22_39333937_G_T_maternal',\n",
       " 'chr22_39333937_G_T_paternal',\n",
       " 'chr22_39499763_T_C_maternal',\n",
       " 'chr22_39499763_T_C_paternal',\n",
       " 'chr22_39774525_A_G_maternal',\n",
       " 'chr22_39774525_A_G_paternal',\n",
       " 'chr22_39353184_A_C_maternal',\n",
       " 'chr22_39353184_A_C_paternal',\n",
       " 'chr22_39445554_G_A_maternal',\n",
       " 'chr22_39445554_G_A_paternal',\n",
       " 'chr22_39320828_C_T_maternal',\n",
       " 'chr22_39320828_C_T_paternal',\n",
       " 'chr22_39358037_A_C_maternal',\n",
       " 'chr22_39358037_A_C_paternal',\n",
       " 'chr22_39493294_T_C_maternal',\n",
       " 'chr22_39493294_T_C_paternal',\n",
       " 'chr22_39413668_C_T_maternal',\n",
       " 'chr22_39413668_C_T_paternal',\n",
       " 'chr22_39168987_T_C_maternal',\n",
       " 'chr22_39168987_T_C_paternal',\n",
       " 'chr22_39510995_A_G_maternal',\n",
       " 'chr22_39510995_A_G_paternal',\n",
       " 'chr22_39708357_C_T_maternal',\n",
       " 'chr22_39708357_C_T_paternal',\n",
       " 'chr22_39835587_A_G_maternal',\n",
       " 'chr22_39835587_A_G_paternal',\n",
       " 'chr22_39332229_G_A_maternal',\n",
       " 'chr22_39332229_G_A_paternal',\n",
       " 'chr22_39831278_A_G_maternal',\n",
       " 'chr22_39831278_A_G_paternal',\n",
       " 'chr22_39738725_T_C_maternal',\n",
       " 'chr22_39738725_T_C_paternal',\n",
       " 'chr22_39825322_T_G_maternal',\n",
       " 'chr22_39825322_T_G_paternal',\n",
       " 'chr22_39333340_G_A_maternal',\n",
       " 'chr22_39333340_G_A_paternal',\n",
       " 'chr22_39670851_G_T_maternal',\n",
       " 'chr22_39670851_G_T_paternal',\n",
       " 'chr22_39477566_A_G_maternal',\n",
       " 'chr22_39477566_A_G_paternal',\n",
       " 'chr22_39343916_T_A_PRS313_Unknown_maternal',\n",
       " 'chr22_39343916_T_A_PRS313_Unknown_paternal',\n",
       " 'chr22_38583315_AAAAG_AAAAGAAAG,AAAAGAAAGAAAG,A_PRS313_Unknown_maternal',\n",
       " 'chr22_38583315_AAAAG_AAAAGAAAG,AAAAGAAAGAAAG,A_PRS313_Unknown_paternal',\n",
       " 'chr22_40810951_T_C_maternal',\n",
       " 'chr22_40810951_T_C_paternal',\n",
       " 'chr22_40876234_T_C_maternal',\n",
       " 'chr22_40876234_T_C_paternal',\n",
       " 'chr22_40701516_A_G_maternal',\n",
       " 'chr22_40701516_A_G_paternal',\n",
       " 'chr22_40794243_T_C_maternal',\n",
       " 'chr22_40794243_T_C_paternal',\n",
       " 'chr22_40611012_G_A_maternal',\n",
       " 'chr22_40611012_G_A_paternal',\n",
       " 'chr22_41061691_C_A_maternal',\n",
       " 'chr22_41061691_C_A_paternal',\n",
       " 'chr22_40779964_G_A_maternal',\n",
       " 'chr22_40779964_G_A_paternal',\n",
       " 'chr22_41284843_C_T_maternal',\n",
       " 'chr22_41284843_C_T_paternal',\n",
       " 'chr22_41078473_T_C_maternal',\n",
       " 'chr22_41078473_T_C_paternal',\n",
       " 'chr22_40800071_A_G_maternal',\n",
       " 'chr22_40800071_A_G_paternal',\n",
       " 'chr22_41081561_T_C_maternal',\n",
       " 'chr22_41081561_T_C_paternal',\n",
       " 'chr22_40738486_G_A_maternal',\n",
       " 'chr22_40738486_G_A_paternal',\n",
       " 'chr22_41264541_T_G_maternal',\n",
       " 'chr22_41264541_T_G_paternal',\n",
       " 'chr22_40738280_T_C_maternal',\n",
       " 'chr22_40738280_T_C_paternal',\n",
       " 'chr22_40849704_C_A_maternal',\n",
       " 'chr22_40849704_C_A_paternal',\n",
       " 'chr22_40538762_T_C_maternal',\n",
       " 'chr22_40538762_T_C_paternal',\n",
       " 'chr22_41166135_A_G_maternal',\n",
       " 'chr22_41166135_A_G_paternal',\n",
       " 'chr22_40736472_G_A_maternal',\n",
       " 'chr22_40736472_G_A_paternal',\n",
       " 'chr22_40914949_T_C_maternal',\n",
       " 'chr22_40914949_T_C_paternal',\n",
       " 'chr22_40800518_G_A_maternal',\n",
       " 'chr22_40800518_G_A_paternal',\n",
       " 'chr22_41078533_A_G_maternal',\n",
       " 'chr22_41078533_A_G_paternal',\n",
       " 'chr22_40855131_A_G_maternal',\n",
       " 'chr22_40855131_A_G_paternal',\n",
       " 'chr22_41042091_A_C_maternal',\n",
       " 'chr22_41042091_A_C_paternal',\n",
       " 'chr22_40652873_A_G_maternal',\n",
       " 'chr22_40652873_A_G_paternal',\n",
       " 'chr22_41076595_CAT_C_maternal',\n",
       " 'chr22_41076595_CAT_C_paternal',\n",
       " 'chr22_41076597_T_C_maternal',\n",
       " 'chr22_41076597_T_C_paternal',\n",
       " 'chr22_40778231_A_C_maternal',\n",
       " 'chr22_40778231_A_C_paternal',\n",
       " 'chr22_40904707_CT_C_PRS313_Known_maternal',\n",
       " 'chr22_40904707_CT_C_PRS313_Known_paternal',\n",
       " 'chr22_41153879_T_C_maternal',\n",
       " 'chr22_41153879_T_C_paternal',\n",
       " 'chr22_40919417_C_A_maternal',\n",
       " 'chr22_40919417_C_A_paternal',\n",
       " 'chr22_40904707_CT_C_PRS313_Unknown_maternal',\n",
       " 'chr22_40904707_CT_C_PRS313_Unknown_paternal',\n",
       " 'chr22_19930121_A_G_maternal',\n",
       " 'chr22_19930121_A_G_paternal',\n",
       " 'chr22_19930109_T_C_maternal',\n",
       " 'chr22_19930109_T_C_paternal',\n",
       " 'chr22_19766782_C_T_maternal',\n",
       " 'chr22_19766782_C_T_paternal',\n",
       " 'chr22_19767051_G_T_maternal',\n",
       " 'chr22_19767051_G_T_paternal',\n",
       " 'chr22_19905802_G_A_maternal',\n",
       " 'chr22_19905802_G_A_paternal',\n",
       " 'chr22_19775444_G_A_maternal',\n",
       " 'chr22_19775444_G_A_paternal',\n",
       " 'chr22_19785006_T_C_maternal',\n",
       " 'chr22_19785006_T_C_paternal',\n",
       " 'chr22_19807031_C_T_maternal',\n",
       " 'chr22_19807031_C_T_paternal',\n",
       " 'chr22_19765631_A_G_maternal',\n",
       " 'chr22_19765631_A_G_paternal',\n",
       " 'chr22_19758228_A_G_maternal',\n",
       " 'chr22_19758228_A_G_paternal',\n",
       " 'chr22_19767420_A_G_maternal',\n",
       " 'chr22_19767420_A_G_paternal',\n",
       " 'chr22_19788347_A_G_maternal',\n",
       " 'chr22_19788347_A_G_paternal',\n",
       " 'chr22_19766137_C_T_PRS313_Unknown_maternal',\n",
       " 'chr22_19766137_C_T_PRS313_Unknown_paternal',\n",
       " 'chr22_43195296_A_G_maternal',\n",
       " 'chr22_43195296_A_G_paternal',\n",
       " 'chr22_43544823_C_T_maternal',\n",
       " 'chr22_43544823_C_T_paternal',\n",
       " 'chr22_43486745_T_C_maternal',\n",
       " 'chr22_43486745_T_C_paternal',\n",
       " 'chr22_43533515_A_C_maternal',\n",
       " 'chr22_43533515_A_C_paternal',\n",
       " 'chr22_43356699_G_A_maternal',\n",
       " 'chr22_43356699_G_A_paternal',\n",
       " 'chr22_43415116_G_A_maternal',\n",
       " 'chr22_43415116_G_A_paternal',\n",
       " 'chr22_43662468_T_C_maternal',\n",
       " 'chr22_43662468_T_C_paternal',\n",
       " 'chr22_43466217_A_G_maternal',\n",
       " 'chr22_43466217_A_G_paternal',\n",
       " 'chr22_43566559_C_T_maternal',\n",
       " 'chr22_43566559_C_T_paternal',\n",
       " 'chr22_43605708_G_A_maternal',\n",
       " 'chr22_43605708_G_A_paternal',\n",
       " 'chr22_43225174_A_C_maternal',\n",
       " 'chr22_43225174_A_C_paternal',\n",
       " 'chr22_43550149_T_C_maternal',\n",
       " 'chr22_43550149_T_C_paternal',\n",
       " 'chr22_43232973_C_T_maternal',\n",
       " 'chr22_43232973_C_T_paternal',\n",
       " 'chr22_42959624_T_C_maternal',\n",
       " 'chr22_42959624_T_C_paternal',\n",
       " 'chr22_43782937_G_A_maternal',\n",
       " 'chr22_43782937_G_A_paternal',\n",
       " 'chr22_43414330_G_A_maternal',\n",
       " 'chr22_43414330_G_A_paternal',\n",
       " 'chr22_43448117_T_C_maternal',\n",
       " 'chr22_43448117_T_C_paternal',\n",
       " 'chr22_43442599_T_C_maternal',\n",
       " 'chr22_43442599_T_C_paternal',\n",
       " 'chr22_43551052_C_T_maternal',\n",
       " 'chr22_43551052_C_T_paternal',\n",
       " 'chr22_43549609_G_A_maternal',\n",
       " 'chr22_43549609_G_A_paternal',\n",
       " 'chr22_43099621_C_T_maternal',\n",
       " 'chr22_43099621_C_T_paternal',\n",
       " 'chr22_43197034_A_G_maternal',\n",
       " 'chr22_43197034_A_G_paternal',\n",
       " 'chr22_43500212_G_T_maternal',\n",
       " 'chr22_43500212_G_T_paternal',\n",
       " 'chr22_43456450_C_T_maternal',\n",
       " 'chr22_43456450_C_T_paternal',\n",
       " 'chr22_43465529_C_T_maternal',\n",
       " 'chr22_43465529_C_T_paternal',\n",
       " 'chr22_43667423_G_A_maternal',\n",
       " 'chr22_43667423_G_A_paternal',\n",
       " 'chr22_43664665_C_T_maternal',\n",
       " 'chr22_43664665_C_T_paternal',\n",
       " 'chr22_43414728_C_T_maternal',\n",
       " 'chr22_43414728_C_T_paternal',\n",
       " 'chr22_43439296_A_G_maternal',\n",
       " 'chr22_43439296_A_G_paternal',\n",
       " 'chr22_43513244_C_A_maternal',\n",
       " 'chr22_43513244_C_A_paternal',\n",
       " 'chr22_43483242_C_T_maternal',\n",
       " 'chr22_43483242_C_T_paternal',\n",
       " 'chr22_43500003_A_G_maternal',\n",
       " 'chr22_43500003_A_G_paternal',\n",
       " 'chr22_43159368_T_G_maternal',\n",
       " 'chr22_43159368_T_G_paternal',\n",
       " 'chr22_43570068_G_A_maternal',\n",
       " 'chr22_43570068_G_A_paternal',\n",
       " 'chr22_43699774_T_G_maternal',\n",
       " 'chr22_43699774_T_G_paternal',\n",
       " 'chr22_43375701_G_A_maternal',\n",
       " 'chr22_43375701_G_A_paternal',\n",
       " 'chr22_43153807_A_G_maternal',\n",
       " 'chr22_43153807_A_G_paternal',\n",
       " 'chr22_43655427_A_G_maternal',\n",
       " 'chr22_43655427_A_G_paternal',\n",
       " 'chr22_43779867_G_T_maternal',\n",
       " 'chr22_43779867_G_T_paternal',\n",
       " 'chr22_43397036_C_T_maternal',\n",
       " 'chr22_43397036_C_T_paternal',\n",
       " 'chr22_43473662_C_T_maternal',\n",
       " 'chr22_43473662_C_T_paternal',\n",
       " 'chr22_43432826_G_A_maternal',\n",
       " 'chr22_43432826_G_A_paternal',\n",
       " 'chr22_43615005_T_C_maternal',\n",
       " 'chr22_43615005_T_C_paternal',\n",
       " 'chr22_43433100_C_T_PRS313_Unknown_maternal',\n",
       " 'chr22_43433100_C_T_PRS313_Unknown_paternal',\n",
       " 'chr22_45354918_A_G_maternal',\n",
       " 'chr22_45354918_A_G_paternal',\n",
       " 'chr22_45326141_A_G_maternal',\n",
       " 'chr22_45326141_A_G_paternal',\n",
       " 'chr22_45318725_G_A_maternal',\n",
       " 'chr22_45318725_G_A_paternal',\n",
       " 'chr22_45415847_A_G_maternal',\n",
       " 'chr22_45415847_A_G_paternal',\n",
       " 'chr22_45286318_A_C_maternal',\n",
       " 'chr22_45286318_A_C_paternal',\n",
       " 'chr22_45435202_T_C_maternal',\n",
       " 'chr22_45435202_T_C_paternal',\n",
       " 'chr22_45322493_T_C_maternal',\n",
       " 'chr22_45322493_T_C_paternal',\n",
       " 'chr22_45415412_T_C_maternal',\n",
       " 'chr22_45415412_T_C_paternal',\n",
       " 'chr22_45286357_A_G_maternal',\n",
       " 'chr22_45286357_A_G_paternal',\n",
       " 'chr22_45253839_A_G_maternal',\n",
       " 'chr22_45253839_A_G_paternal',\n",
       " 'chr22_45397507_G_T_maternal',\n",
       " 'chr22_45397507_G_T_paternal',\n",
       " 'chr22_45319317_T_C_maternal',\n",
       " 'chr22_45319317_T_C_paternal',\n",
       " 'chr22_45251423_G_A_maternal',\n",
       " 'chr22_45251423_G_A_paternal',\n",
       " 'chr22_45322792_T_C_maternal',\n",
       " 'chr22_45322792_T_C_paternal',\n",
       " 'chr22_45415832_T_C_maternal',\n",
       " 'chr22_45415832_T_C_paternal',\n",
       " 'chr22_45192621_A_G_maternal',\n",
       " 'chr22_45192621_A_G_paternal',\n",
       " 'chr22_45339209_G_A_maternal',\n",
       " 'chr22_45339209_G_A_paternal',\n",
       " 'chr22_45415430_C_A_maternal',\n",
       " 'chr22_45415430_C_A_paternal',\n",
       " 'chr22_45337145_A_C_maternal',\n",
       " 'chr22_45337145_A_C_paternal',\n",
       " 'chr22_45271266_T_C_maternal',\n",
       " 'chr22_45271266_T_C_paternal',\n",
       " 'chr22_45397719_C_T_maternal',\n",
       " 'chr22_45397719_C_T_paternal',\n",
       " 'chr22_45353413_T_C_maternal',\n",
       " 'chr22_45353413_T_C_paternal',\n",
       " 'chr22_45315232_G_T_maternal',\n",
       " 'chr22_45315232_G_T_paternal',\n",
       " 'chr22_45260929_C_T_maternal',\n",
       " 'chr22_45260929_C_T_paternal',\n",
       " 'chr22_45333182_G_A_maternal',\n",
       " 'chr22_45333182_G_A_paternal',\n",
       " 'chr22_45388501_C_A_maternal',\n",
       " 'chr22_45388501_C_A_paternal',\n",
       " 'chr22_45409136_C_T_maternal',\n",
       " 'chr22_45409136_C_T_paternal',\n",
       " 'chr22_45416349_C_T_maternal',\n",
       " 'chr22_45416349_C_T_paternal',\n",
       " 'chr22_45408177_G_A_maternal',\n",
       " 'chr22_45408177_G_A_paternal',\n",
       " 'chr22_45345833_A_G_maternal',\n",
       " 'chr22_45345833_A_G_paternal',\n",
       " 'chr22_45331817_T_C_maternal',\n",
       " 'chr22_45331817_T_C_paternal',\n",
       " 'chr22_45308878_G_A_maternal',\n",
       " 'chr22_45308878_G_A_paternal',\n",
       " 'chr22_45279529_A_G_maternal',\n",
       " 'chr22_45279529_A_G_paternal',\n",
       " 'chr22_45330934_G_A_maternal',\n",
       " 'chr22_45330934_G_A_paternal',\n",
       " 'chr22_45319953_G_A_PRS313_Unknown_maternal',\n",
       " 'chr22_45319953_G_A_PRS313_Unknown_paternal',\n",
       " 'chr22_29621477_C_T_maternal',\n",
       " 'chr22_29621477_C_T_paternal',\n",
       " 'chr22_28781758_T_C_maternal',\n",
       " 'chr22_28781758_T_C_paternal',\n",
       " 'chr22_29104959_A_C_maternal',\n",
       " 'chr22_29104959_A_C_paternal',\n",
       " 'chr22_29115066_C_T_maternal',\n",
       " 'chr22_29115066_C_T_paternal',\n",
       " 'chr22_29100711_T_C_maternal',\n",
       " 'chr22_29100711_T_C_paternal',\n",
       " 'chr22_28769172_C_T_maternal',\n",
       " 'chr22_28769172_C_T_paternal',\n",
       " 'chr22_29192670_T_C_maternal',\n",
       " 'chr22_29192670_T_C_paternal',\n",
       " 'chr22_29453027_A_G_maternal',\n",
       " 'chr22_29453027_A_G_paternal',\n",
       " 'chr22_29351381_G_A_maternal',\n",
       " 'chr22_29351381_G_A_paternal',\n",
       " 'chr22_29203724_C_T_PRS313_Unknown_maternal',\n",
       " 'chr22_29203724_C_T_PRS313_Unknown_paternal',\n",
       " 'chr22_29551872_A_G_PRS313_Unknown_maternal',\n",
       " 'chr22_29551872_A_G_PRS313_Unknown_paternal',\n",
       " 'chr22_29156448_A_G_maternal',\n",
       " 'chr22_29156448_A_G_paternal',\n",
       " 'chr22_29106733_T_C_maternal',\n",
       " 'chr22_29106733_T_C_paternal',\n",
       " 'chr22_28794069_A_G_maternal',\n",
       " 'chr22_28794069_A_G_paternal',\n",
       " 'chr22_29300306_C_T_maternal',\n",
       " 'chr22_29300306_C_T_paternal',\n",
       " 'chr22_29174770_C_A_maternal',\n",
       " 'chr22_29174770_C_A_paternal',\n",
       " 'chr22_29182795_G_A_maternal',\n",
       " 'chr22_29182795_G_A_paternal',\n",
       " 'chr22_29227971_G_A_maternal',\n",
       " 'chr22_29227971_G_A_paternal',\n",
       " 'chr22_28848293_G_A_maternal',\n",
       " 'chr22_28848293_G_A_paternal',\n",
       " 'chr22_29398459_A_G_maternal',\n",
       " 'chr22_29398459_A_G_paternal',\n",
       " 'chr22_29379891_T_C_maternal',\n",
       " 'chr22_29379891_T_C_paternal',\n",
       " 'chr22_29196757_G_C_maternal',\n",
       " 'chr22_29196757_G_C_paternal',\n",
       " 'chr22_29383927_T_C_maternal',\n",
       " 'chr22_29383927_T_C_paternal',\n",
       " 'chr22_29132990_A_G_maternal',\n",
       " 'chr22_29132990_A_G_paternal',\n",
       " 'chr22_29165766_T_C_maternal',\n",
       " 'chr22_29165766_T_C_paternal',\n",
       " 'chr22_29154237_A_G_maternal',\n",
       " 'chr22_29154237_A_G_paternal',\n",
       " 'chr22_29101636_G_A_maternal',\n",
       " 'chr22_29101636_G_A_paternal',\n",
       " 'chr22_29105527_A_C_maternal',\n",
       " 'chr22_29105527_A_C_paternal',\n",
       " 'chr22_28923317_G_A_maternal',\n",
       " 'chr22_28923317_G_A_paternal',\n",
       " 'chr22_29130012_T_C_maternal',\n",
       " 'chr22_29130012_T_C_paternal',\n",
       " 'chr22_29108112_A_C_maternal',\n",
       " 'chr22_29108112_A_C_paternal',\n",
       " 'chr22_29130300_C_T_maternal',\n",
       " 'chr22_29130300_C_T_paternal',\n",
       " 'chr22_28882973_G_A_maternal',\n",
       " 'chr22_28882973_G_A_paternal',\n",
       " 'chr22_29286180_C_T_maternal',\n",
       " 'chr22_29286180_C_T_paternal',\n",
       " 'chr22_29070251_T_C_maternal',\n",
       " 'chr22_29070251_T_C_paternal',\n",
       " 'chr22_29351030_T_G_maternal',\n",
       " 'chr22_29351030_T_G_paternal',\n",
       " 'chr22_29199790_G_T_maternal',\n",
       " 'chr22_29199790_G_T_paternal',\n",
       " 'chr22_28919200_G_A_maternal',\n",
       " 'chr22_28919200_G_A_paternal',\n",
       " 'chr22_28693496_T_C_maternal',\n",
       " 'chr22_28693496_T_C_paternal',\n",
       " 'chr22_29135543_G_A_PRS313_Unknown_maternal',\n",
       " 'chr22_29135543_G_A_PRS313_Unknown_paternal',\n",
       " 'chr22_29490148_T_C_maternal',\n",
       " 'chr22_29490148_T_C_paternal',\n",
       " 'chr22_29121087_A_G_PRS313_Known_maternal',\n",
       " 'chr22_29121087_A_G_PRS313_Known_paternal',\n",
       " 'chr22_29121087_A_G_PRS313_Unknown_maternal',\n",
       " 'chr22_29121087_A_G_PRS313_Unknown_paternal',\n",
       " 'chr22_29561826_T_G_maternal',\n",
       " 'chr22_29561826_T_G_paternal',\n",
       " 'chr22_29850959_G_A_maternal',\n",
       " 'chr22_29850959_G_A_paternal',\n",
       " 'chr22_29596992_T_C_maternal',\n",
       " 'chr22_29596992_T_C_paternal',\n",
       " 'chr22_29794913_G_T_maternal',\n",
       " 'chr22_29794913_G_T_paternal',\n",
       " 'chr22_29583608_G_A_maternal',\n",
       " 'chr22_29583608_G_A_paternal',\n",
       " 'chr22_29748859_G_A_maternal',\n",
       " 'chr22_29748859_G_A_paternal',\n",
       " 'chr22_29603796_G_A_maternal',\n",
       " 'chr22_29603796_G_A_paternal',\n",
       " 'chr22_29556739_G_A_maternal',\n",
       " 'chr22_29556739_G_A_paternal']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:42:08,178] A new study created in memory with name: no-name-bd0cc460-1c7e-4394-b13e-7fc03a509ebb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown PRS313 SNPs:  18\n",
      "Known PRS313 SNPs:  4\n",
      "23AndMe SNPs with LD to Unknown PRS313 SNPs:  496\n",
      "Early stopping at epoch 211\n",
      "Early stopping at epoch 37\n",
      "Early stopping at epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:42:16,971] Trial 0 finished with value: 0.29145585298538207 and parameters: {'learning_rate': 0.0007488122413980146, 'lasso_coef': 0.006881266906688167, 'patience': 17}. Best is trial 0 with value: 0.29145585298538207.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 34\n",
      "Early stopping at epoch 14\n",
      "Early stopping at epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:42:22,910] Trial 1 finished with value: 0.05943194907158613 and parameters: {'learning_rate': 0.0013365013610442463, 'lasso_coef': 7.860825238863613e-05, 'patience': 13}. Best is trial 1 with value: 0.05943194907158613.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 14\n",
      "Early stopping at epoch 14\n",
      "Early stopping at epoch 16\n",
      "Early stopping at epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:42:29,109] Trial 2 finished with value: 0.05005371812731028 and parameters: {'learning_rate': 0.0003811573997622702, 'lasso_coef': 2.0284323146147483e-05, 'patience': 15}. Best is trial 2 with value: 0.05005371812731028.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 16\n",
      "Early stopping at epoch 16\n",
      "Early stopping at epoch 11\n",
      "Early stopping at epoch 13\n",
      "Early stopping at epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:42:35,426] Trial 3 finished with value: 0.31778241246938704 and parameters: {'learning_rate': 0.0009400925333489598, 'lasso_coef': 0.01135351943898625, 'patience': 6}. Best is trial 2 with value: 0.05005371812731028.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 38\n",
      "Early stopping at epoch 70\n",
      "Early stopping at epoch 16\n",
      "Early stopping at epoch 16\n",
      "Early stopping at epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:42:36,923] Trial 4 finished with value: 0.05497588459402323 and parameters: {'learning_rate': 0.024327877167228132, 'lasso_coef': 5.230297810078921e-05, 'patience': 15}. Best is trial 2 with value: 0.05005371812731028.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 16\n",
      "Early stopping at epoch 302\n",
      "Early stopping at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:42:45,731] Trial 5 finished with value: 0.10818735659122466 and parameters: {'learning_rate': 0.0009753751621843064, 'lasso_coef': 0.0004343971101326955, 'patience': 8}. Best is trial 2 with value: 0.05005371812731028.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10\n",
      "Early stopping at epoch 9\n",
      "Early stopping at epoch 69\n",
      "Early stopping at epoch 10\n",
      "Early stopping at epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:42:46,919] Trial 6 finished with value: 0.0923498261719942 and parameters: {'learning_rate': 0.03308022610734025, 'lasso_coef': 0.00020177394818880434, 'patience': 7}. Best is trial 2 with value: 0.05005371812731028.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 9\n",
      "Early stopping at epoch 14\n",
      "Early stopping at epoch 160\n",
      "Early stopping at epoch 28\n",
      "Early stopping at epoch 25\n",
      "Early stopping at epoch 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:42:50,021] Trial 7 finished with value: 0.15794478729367256 and parameters: {'learning_rate': 0.009327631359125399, 'lasso_coef': 0.0009642906396963558, 'patience': 20}. Best is trial 2 with value: 0.05005371812731028.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 21\n",
      "Early stopping at epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:43:11,268] Trial 8 finished with value: 0.1336899593472481 and parameters: {'learning_rate': 0.0002642790440558609, 'lasso_coef': 0.0005299045195671482, 'patience': 6}. Best is trial 2 with value: 0.05005371812731028.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 106\n",
      "Early stopping at epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:43:25,040] Trial 9 finished with value: 0.31772398948669434 and parameters: {'learning_rate': 0.0003752602833604903, 'lasso_coef': 0.01184568756300374, 'patience': 18}. Best is trial 2 with value: 0.05005371812731028.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 84\n",
      "Early stopping at epoch 12\n",
      "Early stopping at epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:43:30,980] Trial 10 finished with value: 0.08938243351876736 and parameters: {'learning_rate': 0.0001245880853584677, 'lasso_coef': 1.2717642975672464e-05, 'patience': 11}. Best is trial 2 with value: 0.05005371812731028.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 12\n",
      "Early stopping at epoch 12\n",
      "Early stopping at epoch 43\n",
      "Early stopping at epoch 15\n",
      "Early stopping at epoch 15\n",
      "Early stopping at epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:43:32,092] Trial 11 finished with value: 0.03191335825249553 and parameters: {'learning_rate': 0.08244798350832538, 'lasso_coef': 1.0149327458874068e-05, 'patience': 14}. Best is trial 11 with value: 0.03191335825249553.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15\n",
      "Early stopping at epoch 45\n",
      "Early stopping at epoch 13\n",
      "Early stopping at epoch 13\n",
      "Early stopping at epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:43:33,153] Trial 12 finished with value: 0.033549716044217345 and parameters: {'learning_rate': 0.08264532599565531, 'lasso_coef': 1.060118605407429e-05, 'patience': 12}. Best is trial 11 with value: 0.03191335825249553.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 13\n",
      "Early stopping at epoch 40\n",
      "Early stopping at epoch 11\n",
      "Early stopping at epoch 11\n",
      "Early stopping at epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:43:34,072] Trial 13 finished with value: 0.03332691919058561 and parameters: {'learning_rate': 0.0976946951440146, 'lasso_coef': 1.0548397429774655e-05, 'patience': 10}. Best is trial 11 with value: 0.03191335825249553.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 11\n",
      "Early stopping at epoch 37\n",
      "Early stopping at epoch 20\n",
      "Early stopping at epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:43:35,289] Trial 14 finished with value: 7.037210059165955 and parameters: {'learning_rate': 0.09328958935472782, 'lasso_coef': 0.07323189825876145, 'patience': 10}. Best is trial 11 with value: 0.03191335825249553.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 22\n",
      "Early stopping at epoch 18\n",
      "Early stopping at epoch 161\n",
      "Early stopping at epoch 10\n",
      "Early stopping at epoch 10\n",
      "Early stopping at epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:43:37,445] Trial 15 finished with value: 0.06065268535166979 and parameters: {'learning_rate': 0.00612841962442229, 'lasso_coef': 7.543646871219132e-05, 'patience': 9}. Best is trial 11 with value: 0.03191335825249553.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10\n",
      "Early stopping at epoch 73\n",
      "Early stopping at epoch 15\n",
      "Early stopping at epoch 15\n",
      "Early stopping at epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:43:38,875] Trial 16 finished with value: 0.0478851567953825 and parameters: {'learning_rate': 0.029643190415864214, 'lasso_coef': 3.6644563796650354e-05, 'patience': 14}. Best is trial 11 with value: 0.03191335825249553.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15\n",
      "Early stopping at epoch 178\n",
      "Early stopping at epoch 15\n",
      "Early stopping at epoch 12\n",
      "Early stopping at epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:43:41,325] Trial 17 finished with value: 0.07785491608083248 and parameters: {'learning_rate': 0.011863853321771088, 'lasso_coef': 0.00017430628690268699, 'patience': 11}. Best is trial 11 with value: 0.03191335825249553.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 12\n",
      "Early stopping at epoch 345\n",
      "Early stopping at epoch 18\n",
      "Early stopping at epoch 22\n",
      "Early stopping at epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:43:45,887] Trial 18 finished with value: 0.2505847133696079 and parameters: {'learning_rate': 0.003380848327957833, 'lasso_coef': 0.0034127270193719003, 'patience': 17}. Best is trial 11 with value: 0.03191335825249553.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 25\n",
      "Early stopping at epoch 51\n",
      "Early stopping at epoch 14\n",
      "Early stopping at epoch 14\n",
      "Early stopping at epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:43:47,047] Trial 19 finished with value: 0.042048963531851766 and parameters: {'learning_rate': 0.051236173056326974, 'lasso_coef': 2.327459264763858e-05, 'patience': 13}. Best is trial 11 with value: 0.03191335825249553.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 14\n",
      "Early stopping at epoch 147\n",
      "Early stopping at epoch 10\n",
      "Early stopping at epoch 10\n",
      "Early stopping at epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:43:49,123] Trial 20 finished with value: 0.07692991569638252 and parameters: {'learning_rate': 0.013436111714801658, 'lasso_coef': 0.00016219917672651455, 'patience': 9}. Best is trial 11 with value: 0.03191335825249553.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10\n",
      "Early stopping at epoch 45\n",
      "Early stopping at epoch 13\n",
      "Early stopping at epoch 13\n",
      "Early stopping at epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:43:50,187] Trial 21 finished with value: 0.034052956476807596 and parameters: {'learning_rate': 0.0851639043937076, 'lasso_coef': 1.038472932400282e-05, 'patience': 12}. Best is trial 11 with value: 0.03191335825249553.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 13\n",
      "Early stopping at epoch 44\n",
      "Early stopping at epoch 13\n",
      "Early stopping at epoch 13\n",
      "Early stopping at epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:43:51,288] Trial 22 finished with value: 0.04524081628769636 and parameters: {'learning_rate': 0.06036927136616159, 'lasso_coef': 2.5959698830129178e-05, 'patience': 12}. Best is trial 11 with value: 0.03191335825249553.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 13\n",
      "Early stopping at epoch 44\n",
      "Early stopping at epoch 16\n",
      "Early stopping at epoch 16\n",
      "Early stopping at epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:43:52,509] Trial 23 finished with value: 0.028138810861855747 and parameters: {'learning_rate': 0.04981381173156519, 'lasso_coef': 1.0253899524154534e-05, 'patience': 15}. Best is trial 23 with value: 0.028138810861855747.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 16\n",
      "Early stopping at epoch 113\n",
      "Early stopping at epoch 16\n",
      "Early stopping at epoch 16\n",
      "Early stopping at epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:43:54,424] Trial 24 finished with value: 0.063673379085958 and parameters: {'learning_rate': 0.03728390232453899, 'lasso_coef': 6.916815465270042e-05, 'patience': 15}. Best is trial 23 with value: 0.028138810861855747.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 16\n",
      "Early stopping at epoch 88\n",
      "Early stopping at epoch 17\n",
      "Early stopping at epoch 17\n",
      "Early stopping at epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:43:56,110] Trial 25 finished with value: 0.038728497177362445 and parameters: {'learning_rate': 0.022666819573049158, 'lasso_coef': 2.4610120486888766e-05, 'patience': 16}. Best is trial 23 with value: 0.028138810861855747.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 17\n",
      "Early stopping at epoch 98\n",
      "Early stopping at epoch 15\n",
      "Early stopping at epoch 15\n",
      "Early stopping at epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:43:57,843] Trial 26 finished with value: 0.027447771467268466 and parameters: {'learning_rate': 0.017756987880975593, 'lasso_coef': 1.0604553653353727e-05, 'patience': 14}. Best is trial 26 with value: 0.027447771467268466.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15\n",
      "Early stopping at epoch 145\n",
      "Early stopping at epoch 21\n",
      "Early stopping at epoch 21\n",
      "Early stopping at epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:44:00,390] Trial 27 finished with value: 0.046236333809792994 and parameters: {'learning_rate': 0.01858746599087551, 'lasso_coef': 3.939214633780241e-05, 'patience': 20}. Best is trial 26 with value: 0.027447771467268466.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 21\n",
      "Early stopping at epoch 145\n",
      "Early stopping at epoch 15\n",
      "Early stopping at epoch 17\n",
      "Early stopping at epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:44:02,699] Trial 28 finished with value: 0.21645552739501 and parameters: {'learning_rate': 0.006499080477911436, 'lasso_coef': 0.0021354260549460843, 'patience': 14}. Best is trial 26 with value: 0.027447771467268466.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 16\n",
      "Early stopping at epoch 57\n",
      "Early stopping at epoch 21\n",
      "Early stopping at epoch 33\n",
      "Early stopping at epoch 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:44:04,518] Trial 29 finished with value: 4.551795554161072 and parameters: {'learning_rate': 0.050000363085735404, 'lasso_coef': 0.087758364469517, 'patience': 18}. Best is trial 26 with value: 0.027447771467268466.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 30\n",
      "Best hyperparameters: {'learning_rate': 0.017756987880975593, 'lasso_coef': 1.0604553653353727e-05, 'patience': 14}\n",
      "Best value: 0.027447771467268466\n",
      "Epoch [1/500], Train Loss: 0.3365\n",
      "Epoch [2/500], Train Loss: 0.1825\n",
      "Epoch [3/500], Train Loss: 0.1237\n",
      "Epoch [4/500], Train Loss: 0.0992\n",
      "Epoch [5/500], Train Loss: 0.0841\n",
      "Epoch [6/500], Train Loss: 0.0734\n",
      "Epoch [7/500], Train Loss: 0.0660\n",
      "Epoch [8/500], Train Loss: 0.0606\n",
      "Epoch [9/500], Train Loss: 0.0559\n",
      "Epoch [10/500], Train Loss: 0.0524\n",
      "Epoch [11/500], Train Loss: 0.0491\n",
      "Epoch [12/500], Train Loss: 0.0469\n",
      "Epoch [13/500], Train Loss: 0.0449\n",
      "Epoch [14/500], Train Loss: 0.0431\n",
      "Epoch [15/500], Train Loss: 0.0413\n",
      "Epoch [16/500], Train Loss: 0.0397\n",
      "Epoch [17/500], Train Loss: 0.0386\n",
      "Epoch [18/500], Train Loss: 0.0372\n",
      "Epoch [19/500], Train Loss: 0.0367\n",
      "Epoch [20/500], Train Loss: 0.0359\n",
      "Epoch [21/500], Train Loss: 0.0348\n",
      "Epoch [22/500], Train Loss: 0.0341\n",
      "Epoch [23/500], Train Loss: 0.0333\n",
      "Epoch [24/500], Train Loss: 0.0328\n",
      "Epoch [25/500], Train Loss: 0.0318\n",
      "Epoch [26/500], Train Loss: 0.0319\n",
      "Epoch [27/500], Train Loss: 0.0312\n",
      "Epoch [28/500], Train Loss: 0.0304\n",
      "Epoch [29/500], Train Loss: 0.0300\n",
      "Epoch [30/500], Train Loss: 0.0298\n",
      "Epoch [31/500], Train Loss: 0.0294\n",
      "Epoch [32/500], Train Loss: 0.0290\n",
      "Epoch [33/500], Train Loss: 0.0285\n",
      "Epoch [34/500], Train Loss: 0.0280\n",
      "Epoch [35/500], Train Loss: 0.0279\n",
      "Epoch [36/500], Train Loss: 0.0279\n",
      "Epoch [37/500], Train Loss: 0.0276\n",
      "Epoch [38/500], Train Loss: 0.0270\n",
      "Epoch [39/500], Train Loss: 0.0270\n",
      "Epoch [40/500], Train Loss: 0.0267\n",
      "Epoch [41/500], Train Loss: 0.0265\n",
      "Epoch [42/500], Train Loss: 0.0264\n",
      "Epoch [43/500], Train Loss: 0.0263\n",
      "Epoch [44/500], Train Loss: 0.0259\n",
      "Epoch [45/500], Train Loss: 0.0259\n",
      "Epoch [46/500], Train Loss: 0.0254\n",
      "Epoch [47/500], Train Loss: 0.0254\n",
      "Epoch [48/500], Train Loss: 0.0252\n",
      "Epoch [49/500], Train Loss: 0.0249\n",
      "Epoch [50/500], Train Loss: 0.0251\n",
      "Epoch [51/500], Train Loss: 0.0249\n",
      "Epoch [52/500], Train Loss: 0.0249\n",
      "Epoch [53/500], Train Loss: 0.0247\n",
      "Epoch [54/500], Train Loss: 0.0244\n",
      "Epoch [55/500], Train Loss: 0.0245\n",
      "Epoch [56/500], Train Loss: 0.0242\n",
      "Epoch [57/500], Train Loss: 0.0241\n",
      "Epoch [58/500], Train Loss: 0.0239\n",
      "Epoch [59/500], Train Loss: 0.0239\n",
      "Epoch [60/500], Train Loss: 0.0241\n",
      "Epoch [61/500], Train Loss: 0.0236\n",
      "Epoch [62/500], Train Loss: 0.0232\n",
      "Epoch [63/500], Train Loss: 0.0234\n",
      "Epoch [64/500], Train Loss: 0.0233\n",
      "Epoch [65/500], Train Loss: 0.0230\n",
      "Epoch [66/500], Train Loss: 0.0235\n",
      "Epoch [67/500], Train Loss: 0.0233\n",
      "Epoch [68/500], Train Loss: 0.0229\n",
      "Epoch [69/500], Train Loss: 0.0229\n",
      "Epoch [70/500], Train Loss: 0.0227\n",
      "Epoch [71/500], Train Loss: 0.0226\n",
      "Epoch [72/500], Train Loss: 0.0225\n",
      "Epoch [73/500], Train Loss: 0.0228\n",
      "Epoch [74/500], Train Loss: 0.0229\n",
      "Epoch [75/500], Train Loss: 0.0231\n",
      "Epoch [76/500], Train Loss: 0.0225\n",
      "Epoch [77/500], Train Loss: 0.0224\n",
      "Epoch [78/500], Train Loss: 0.0225\n",
      "Epoch [79/500], Train Loss: 0.0223\n",
      "Epoch [80/500], Train Loss: 0.0223\n",
      "Epoch [81/500], Train Loss: 0.0219\n",
      "Epoch [82/500], Train Loss: 0.0219\n",
      "Epoch [83/500], Train Loss: 0.0220\n",
      "Epoch [84/500], Train Loss: 0.0219\n",
      "Epoch [85/500], Train Loss: 0.0218\n",
      "Epoch [86/500], Train Loss: 0.0217\n",
      "Epoch [87/500], Train Loss: 0.0219\n",
      "Epoch [88/500], Train Loss: 0.0218\n",
      "Epoch [89/500], Train Loss: 0.0216\n",
      "Epoch [90/500], Train Loss: 0.0217\n",
      "Epoch [91/500], Train Loss: 0.0218\n",
      "Epoch [92/500], Train Loss: 0.0216\n",
      "Epoch [93/500], Train Loss: 0.0215\n",
      "Epoch [94/500], Train Loss: 0.0216\n",
      "Epoch [95/500], Train Loss: 0.0214\n",
      "Epoch [96/500], Train Loss: 0.0213\n",
      "Epoch [97/500], Train Loss: 0.0213\n",
      "Epoch [98/500], Train Loss: 0.0212\n",
      "Epoch [99/500], Train Loss: 0.0211\n",
      "Epoch [100/500], Train Loss: 0.0212\n",
      "Epoch [101/500], Train Loss: 0.0213\n",
      "Epoch [102/500], Train Loss: 0.0212\n",
      "Epoch [103/500], Train Loss: 0.0210\n",
      "Epoch [104/500], Train Loss: 0.0211\n",
      "Epoch [105/500], Train Loss: 0.0213\n",
      "Epoch [106/500], Train Loss: 0.0212\n",
      "Epoch [107/500], Train Loss: 0.0210\n",
      "Epoch [108/500], Train Loss: 0.0211\n",
      "Epoch [109/500], Train Loss: 0.0210\n",
      "Epoch [110/500], Train Loss: 0.0208\n",
      "Epoch [111/500], Train Loss: 0.0209\n",
      "Epoch [112/500], Train Loss: 0.0209\n",
      "Epoch [113/500], Train Loss: 0.0209\n",
      "Epoch [114/500], Train Loss: 0.0211\n",
      "Epoch [115/500], Train Loss: 0.0214\n",
      "Epoch [116/500], Train Loss: 0.0209\n",
      "Epoch [117/500], Train Loss: 0.0211\n",
      "Epoch [118/500], Train Loss: 0.0206\n",
      "Epoch [119/500], Train Loss: 0.0208\n",
      "Epoch [120/500], Train Loss: 0.0209\n",
      "Epoch [121/500], Train Loss: 0.0205\n",
      "Epoch [122/500], Train Loss: 0.0205\n",
      "Epoch [123/500], Train Loss: 0.0206\n",
      "Epoch [124/500], Train Loss: 0.0204\n",
      "Epoch [125/500], Train Loss: 0.0204\n",
      "Epoch [126/500], Train Loss: 0.0204\n",
      "Epoch [127/500], Train Loss: 0.0203\n",
      "Epoch [128/500], Train Loss: 0.0205\n",
      "Epoch [129/500], Train Loss: 0.0205\n",
      "Epoch [130/500], Train Loss: 0.0203\n",
      "Epoch [131/500], Train Loss: 0.0204\n",
      "Epoch [132/500], Train Loss: 0.0204\n",
      "Epoch [133/500], Train Loss: 0.0206\n",
      "Epoch [134/500], Train Loss: 0.0206\n",
      "Epoch [135/500], Train Loss: 0.0206\n",
      "Epoch [136/500], Train Loss: 0.0204\n",
      "Epoch [137/500], Train Loss: 0.0209\n",
      "Epoch [138/500], Train Loss: 0.0203\n",
      "Epoch [139/500], Train Loss: 0.0203\n",
      "Epoch [140/500], Train Loss: 0.0201\n",
      "Epoch [141/500], Train Loss: 0.0201\n",
      "Epoch [142/500], Train Loss: 0.0202\n",
      "Epoch [143/500], Train Loss: 0.0203\n",
      "Epoch [144/500], Train Loss: 0.0205\n",
      "Epoch [145/500], Train Loss: 0.0206\n",
      "Epoch [146/500], Train Loss: 0.0207\n",
      "Epoch [147/500], Train Loss: 0.0203\n",
      "Epoch [148/500], Train Loss: 0.0204\n",
      "Epoch [149/500], Train Loss: 0.0204\n",
      "Epoch [150/500], Train Loss: 0.0205\n",
      "Epoch [151/500], Train Loss: 0.0202\n",
      "Epoch [152/500], Train Loss: 0.0203\n",
      "Epoch [153/500], Train Loss: 0.0202\n",
      "Epoch [154/500], Train Loss: 0.0206\n",
      "Early stopping at epoch 154\n",
      "Final model saved at: ../../Data/model_results/logistic_regression/models/chr17/final_model_chr17.pth\n",
      "Individual R^2 scores saved at: ../../Data/model_results/logistic_regression/csv_files/chr17/individual_r2_scores_chr17.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:1133: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[I 2024-04-25 16:44:08,066] A new study created in memory with name: no-name-2ba8f112-fed7-415f-96a1-4b8dfdd430c5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping SNP chr17_40744470_G_A_PRS313_Unknown_maternal due to insufficient data\n",
      "Individual AUC ROC curves saved in: ../../Data/model_results/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../Data/model_results/logistic_regression/csv_files/chr17/performance_metrics.csv\n",
      "Unknown PRS313 SNPs:  18\n",
      "Known PRS313 SNPs:  4\n",
      "23AndMe SNPs with LD to Unknown PRS313 SNPs:  782\n",
      "Early stopping at epoch 45\n",
      "Early stopping at epoch 10\n",
      "Early stopping at epoch 9\n",
      "Early stopping at epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:44:09,115] Trial 0 finished with value: 0.7512775093317032 and parameters: {'learning_rate': 0.007920408993990732, 'lasso_coef': 0.012357795514078743, 'patience': 6}. Best is trial 0 with value: 0.7512775093317032.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 7\n",
      "Early stopping at epoch 233\n",
      "Early stopping at epoch 14\n",
      "Early stopping at epoch 12\n",
      "Early stopping at epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:44:12,562] Trial 1 finished with value: 0.5856973499059677 and parameters: {'learning_rate': 0.00099131784118714, 'lasso_coef': 0.008836037574853515, 'patience': 6}. Best is trial 1 with value: 0.5856973499059677.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 7\n",
      "Early stopping at epoch 28\n",
      "Early stopping at epoch 35\n",
      "Early stopping at epoch 33\n",
      "Early stopping at epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:44:14,493] Trial 2 finished with value: 2.389017474651337 and parameters: {'learning_rate': 0.09742265722793308, 'lasso_coef': 0.010522819991871738, 'patience': 17}. Best is trial 1 with value: 0.5856973499059677.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 40\n",
      "Early stopping at epoch 342\n",
      "Early stopping at epoch 16\n",
      "Early stopping at epoch 16\n",
      "Early stopping at epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:44:19,508] Trial 3 finished with value: 0.09509716369211674 and parameters: {'learning_rate': 0.004752768567937321, 'lasso_coef': 5.771117733520446e-05, 'patience': 15}. Best is trial 3 with value: 0.09509716369211674.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 16\n",
      "Early stopping at epoch 30\n",
      "Early stopping at epoch 24\n",
      "Early stopping at epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:44:20,920] Trial 4 finished with value: 1.1704160392284393 and parameters: {'learning_rate': 0.010716585245001593, 'lasso_coef': 0.032161811337712294, 'patience': 11}. Best is trial 3 with value: 0.09509716369211674.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 25\n",
      "Early stopping at epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:44:46,036] Trial 5 finished with value: 0.3557654991745949 and parameters: {'learning_rate': 0.00018500481463991198, 'lasso_coef': 0.001624705096086, 'patience': 13}. Best is trial 3 with value: 0.09509716369211674.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 16\n",
      "Early stopping at epoch 16\n",
      "Early stopping at epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:44:59,062] Trial 6 finished with value: 0.21006542444229126 and parameters: {'learning_rate': 0.0005121634813071286, 'lasso_coef': 0.0004999270212840541, 'patience': 14}. Best is trial 3 with value: 0.09509716369211674.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 17\n",
      "Early stopping at epoch 26\n",
      "Early stopping at epoch 17\n",
      "Early stopping at epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:44:59,990] Trial 7 finished with value: 0.36412805169820783 and parameters: {'learning_rate': 0.0439888254497266, 'lasso_coef': 0.0010150511690111635, 'patience': 5}. Best is trial 3 with value: 0.09509716369211674.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 6\n",
      "Early stopping at epoch 13\n",
      "Early stopping at epoch 28\n",
      "Early stopping at epoch 9\n",
      "Early stopping at epoch 10\n",
      "Early stopping at epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:45:00,881] Trial 8 finished with value: 1.019051617383957 and parameters: {'learning_rate': 0.013452179490060272, 'lasso_coef': 0.018703247555803253, 'patience': 5}. Best is trial 3 with value: 0.09509716369211674.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 9\n",
      "Early stopping at epoch 19\n",
      "Early stopping at epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:45:14,322] Trial 9 finished with value: 0.16945460066199303 and parameters: {'learning_rate': 0.00024730994415671296, 'lasso_coef': 0.00019031826601627227, 'patience': 18}. Best is trial 3 with value: 0.09509716369211674.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 19\n",
      "Early stopping at epoch 21\n",
      "Early stopping at epoch 21\n",
      "Early stopping at epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:45:21,929] Trial 10 finished with value: 0.06137522961944342 and parameters: {'learning_rate': 0.0017583678356566865, 'lasso_coef': 1.0924889921874367e-05, 'patience': 20}. Best is trial 10 with value: 0.06137522961944342.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 21\n",
      "Early stopping at epoch 21\n",
      "Early stopping at epoch 21\n",
      "Early stopping at epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:45:30,708] Trial 11 finished with value: 0.06358093861490488 and parameters: {'learning_rate': 0.0017977940072633153, 'lasso_coef': 1.3505179297105795e-05, 'patience': 20}. Best is trial 10 with value: 0.06137522961944342.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 21\n",
      "Early stopping at epoch 21\n",
      "Early stopping at epoch 21\n",
      "Early stopping at epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:45:39,461] Trial 12 finished with value: 0.06387442648410797 and parameters: {'learning_rate': 0.0017203455861555905, 'lasso_coef': 1.3223299279689656e-05, 'patience': 20}. Best is trial 10 with value: 0.06137522961944342.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 21\n",
      "Early stopping at epoch 21\n",
      "Early stopping at epoch 21\n",
      "Early stopping at epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:45:48,020] Trial 13 finished with value: 0.05800383985042572 and parameters: {'learning_rate': 0.002557182687942291, 'lasso_coef': 1.0217063758505625e-05, 'patience': 20}. Best is trial 13 with value: 0.05800383985042572.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 21\n",
      "Early stopping at epoch 18\n",
      "Early stopping at epoch 18\n",
      "Early stopping at epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:45:56,750] Trial 14 finished with value: 0.11469834558665752 and parameters: {'learning_rate': 0.0006285440298901653, 'lasso_coef': 6.551482958489948e-05, 'patience': 17}. Best is trial 13 with value: 0.05800383985042572.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 18\n",
      "Early stopping at epoch 350\n",
      "Early stopping at epoch 11\n",
      "Early stopping at epoch 11\n",
      "Early stopping at epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:46:02,636] Trial 15 finished with value: 0.08952534273266792 and parameters: {'learning_rate': 0.0032849508021110373, 'lasso_coef': 4.4873284922619077e-05, 'patience': 10}. Best is trial 13 with value: 0.05800383985042572.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 11\n",
      "Early stopping at epoch 124\n",
      "Early stopping at epoch 20\n",
      "Early stopping at epoch 20\n",
      "Early stopping at epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:46:05,729] Trial 16 finished with value: 0.06269191410392523 and parameters: {'learning_rate': 0.021746478708916057, 'lasso_coef': 1.139317111651371e-05, 'patience': 19}. Best is trial 13 with value: 0.05800383985042572.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 20\n",
      "Early stopping at epoch 113\n",
      "Early stopping at epoch 23\n",
      "Early stopping at epoch 36\n",
      "Early stopping at epoch 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:46:09,171] Trial 17 finished with value: 0.8666551500558853 and parameters: {'learning_rate': 0.0021393677766417944, 'lasso_coef': 0.08343573421162252, 'patience': 16}. Best is trial 13 with value: 0.05800383985042572.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 42\n",
      "Early stopping at epoch 133\n",
      "Early stopping at epoch 10\n",
      "Early stopping at epoch 10\n",
      "Early stopping at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:46:11,418] Trial 18 finished with value: 0.1506721630692482 and parameters: {'learning_rate': 0.005097458764190054, 'lasso_coef': 0.00017983033498116815, 'patience': 9}. Best is trial 13 with value: 0.05800383985042572.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10\n",
      "Early stopping at epoch 19\n",
      "Early stopping at epoch 19\n",
      "Early stopping at epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:46:18,618] Trial 19 finished with value: 0.18697901517152787 and parameters: {'learning_rate': 0.0001164732359384199, 'lasso_coef': 3.226797437502225e-05, 'patience': 18}. Best is trial 13 with value: 0.05800383985042572.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 19\n",
      "Early stopping at epoch 26\n",
      "Early stopping at epoch 21\n",
      "Early stopping at epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:46:26,194] Trial 20 finished with value: 0.42780025750398637 and parameters: {'learning_rate': 0.0007371515725763783, 'lasso_coef': 0.0028157672283156024, 'patience': 20}. Best is trial 13 with value: 0.05800383985042572.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 21\n",
      "Early stopping at epoch 131\n",
      "Early stopping at epoch 20\n",
      "Early stopping at epoch 20\n",
      "Early stopping at epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:46:28,878] Trial 21 finished with value: 0.05979943946003914 and parameters: {'learning_rate': 0.018598362815781713, 'lasso_coef': 1.0070031368411097e-05, 'patience': 19}. Best is trial 13 with value: 0.05800383985042572.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 20\n",
      "Early stopping at epoch 87\n",
      "Early stopping at epoch 19\n",
      "Early stopping at epoch 19\n",
      "Early stopping at epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:46:30,899] Trial 22 finished with value: 0.08306158557534218 and parameters: {'learning_rate': 0.03275389202487778, 'lasso_coef': 2.631264493375995e-05, 'patience': 18}. Best is trial 13 with value: 0.05800383985042572.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 19\n",
      "Early stopping at epoch 348\n",
      "Early stopping at epoch 20\n",
      "Early stopping at epoch 21\n",
      "Early stopping at epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:46:36,404] Trial 23 finished with value: 0.1298729658126831 and parameters: {'learning_rate': 0.005511596957508081, 'lasso_coef': 0.0001432751828899532, 'patience': 19}. Best is trial 13 with value: 0.05800383985042572.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 20\n",
      "Early stopping at epoch 17\n",
      "Early stopping at epoch 17\n",
      "Early stopping at epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:46:43,858] Trial 24 finished with value: 0.06473070252686738 and parameters: {'learning_rate': 0.0012053257268082587, 'lasso_coef': 1.0139251755769864e-05, 'patience': 16}. Best is trial 13 with value: 0.05800383985042572.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 17\n",
      "Early stopping at epoch 446\n",
      "Early stopping at epoch 21\n",
      "Early stopping at epoch 21\n",
      "Early stopping at epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:46:50,699] Trial 25 finished with value: 0.07073263693600892 and parameters: {'learning_rate': 0.0028693932329173274, 'lasso_coef': 2.300028009014043e-05, 'patience': 20}. Best is trial 13 with value: 0.05800383985042572.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 21\n",
      "Early stopping at epoch 20\n",
      "Early stopping at epoch 20\n",
      "Early stopping at epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:46:58,210] Trial 26 finished with value: 0.14481886699795724 and parameters: {'learning_rate': 0.000408269227300889, 'lasso_coef': 0.00010854055799375366, 'patience': 19}. Best is trial 13 with value: 0.05800383985042572.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 20\n",
      "Early stopping at epoch 38\n",
      "Early stopping at epoch 21\n",
      "Early stopping at epoch 35\n",
      "Early stopping at epoch 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:47:00,023] Trial 27 finished with value: 0.2299756459891796 and parameters: {'learning_rate': 0.07717029222928128, 'lasso_coef': 0.000283685419378604, 'patience': 16}. Best is trial 13 with value: 0.05800383985042572.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 20\n",
      "Early stopping at epoch 121\n",
      "Early stopping at epoch 15\n",
      "Early stopping at epoch 15\n",
      "Early stopping at epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:47:02,350] Trial 28 finished with value: 0.07745670303702354 and parameters: {'learning_rate': 0.015835412603426652, 'lasso_coef': 2.4210098595267743e-05, 'patience': 14}. Best is trial 13 with value: 0.05800383985042572.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15\n",
      "Early stopping at epoch 243\n",
      "Early stopping at epoch 18\n",
      "Early stopping at epoch 18\n",
      "Early stopping at epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:47:06,636] Trial 29 finished with value: 0.11258915439248085 and parameters: {'learning_rate': 0.008273931183810155, 'lasso_coef': 8.703241007395596e-05, 'patience': 17}. Best is trial 13 with value: 0.05800383985042572.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 18\n",
      "Best hyperparameters: {'learning_rate': 0.002557182687942291, 'lasso_coef': 1.0217063758505625e-05, 'patience': 20}\n",
      "Best value: 0.05800383985042572\n",
      "Epoch [1/500], Train Loss: 0.5492\n",
      "Epoch [2/500], Train Loss: 0.4584\n",
      "Epoch [3/500], Train Loss: 0.4135\n",
      "Epoch [4/500], Train Loss: 0.3794\n",
      "Epoch [5/500], Train Loss: 0.3512\n",
      "Epoch [6/500], Train Loss: 0.3268\n",
      "Epoch [7/500], Train Loss: 0.3071\n",
      "Epoch [8/500], Train Loss: 0.2896\n",
      "Epoch [9/500], Train Loss: 0.2740\n",
      "Epoch [10/500], Train Loss: 0.2611\n",
      "Epoch [11/500], Train Loss: 0.2497\n",
      "Epoch [12/500], Train Loss: 0.2381\n",
      "Epoch [13/500], Train Loss: 0.2279\n",
      "Epoch [14/500], Train Loss: 0.2191\n",
      "Epoch [15/500], Train Loss: 0.2116\n",
      "Epoch [16/500], Train Loss: 0.2043\n",
      "Epoch [17/500], Train Loss: 0.1977\n",
      "Epoch [18/500], Train Loss: 0.1917\n",
      "Epoch [19/500], Train Loss: 0.1854\n",
      "Epoch [20/500], Train Loss: 0.1805\n",
      "Epoch [21/500], Train Loss: 0.1751\n",
      "Epoch [22/500], Train Loss: 0.1710\n",
      "Epoch [23/500], Train Loss: 0.1664\n",
      "Epoch [24/500], Train Loss: 0.1626\n",
      "Epoch [25/500], Train Loss: 0.1589\n",
      "Epoch [26/500], Train Loss: 0.1551\n",
      "Epoch [27/500], Train Loss: 0.1517\n",
      "Epoch [28/500], Train Loss: 0.1488\n",
      "Epoch [29/500], Train Loss: 0.1454\n",
      "Epoch [30/500], Train Loss: 0.1426\n",
      "Epoch [31/500], Train Loss: 0.1395\n",
      "Epoch [32/500], Train Loss: 0.1374\n",
      "Epoch [33/500], Train Loss: 0.1346\n",
      "Epoch [34/500], Train Loss: 0.1320\n",
      "Epoch [35/500], Train Loss: 0.1302\n",
      "Epoch [36/500], Train Loss: 0.1278\n",
      "Epoch [37/500], Train Loss: 0.1255\n",
      "Epoch [38/500], Train Loss: 0.1237\n",
      "Epoch [39/500], Train Loss: 0.1219\n",
      "Epoch [40/500], Train Loss: 0.1199\n",
      "Epoch [41/500], Train Loss: 0.1179\n",
      "Epoch [42/500], Train Loss: 0.1162\n",
      "Epoch [43/500], Train Loss: 0.1145\n",
      "Epoch [44/500], Train Loss: 0.1128\n",
      "Epoch [45/500], Train Loss: 0.1116\n",
      "Epoch [46/500], Train Loss: 0.1101\n",
      "Epoch [47/500], Train Loss: 0.1086\n",
      "Epoch [48/500], Train Loss: 0.1071\n",
      "Epoch [49/500], Train Loss: 0.1060\n",
      "Epoch [50/500], Train Loss: 0.1047\n",
      "Epoch [51/500], Train Loss: 0.1034\n",
      "Epoch [52/500], Train Loss: 0.1022\n",
      "Epoch [53/500], Train Loss: 0.1009\n",
      "Epoch [54/500], Train Loss: 0.1000\n",
      "Epoch [55/500], Train Loss: 0.0986\n",
      "Epoch [56/500], Train Loss: 0.0976\n",
      "Epoch [57/500], Train Loss: 0.0967\n",
      "Epoch [58/500], Train Loss: 0.0956\n",
      "Epoch [59/500], Train Loss: 0.0945\n",
      "Epoch [60/500], Train Loss: 0.0933\n",
      "Epoch [61/500], Train Loss: 0.0923\n",
      "Epoch [62/500], Train Loss: 0.0916\n",
      "Epoch [63/500], Train Loss: 0.0908\n",
      "Epoch [64/500], Train Loss: 0.0899\n",
      "Epoch [65/500], Train Loss: 0.0889\n",
      "Epoch [66/500], Train Loss: 0.0882\n",
      "Epoch [67/500], Train Loss: 0.0874\n",
      "Epoch [68/500], Train Loss: 0.0867\n",
      "Epoch [69/500], Train Loss: 0.0857\n",
      "Epoch [70/500], Train Loss: 0.0852\n",
      "Epoch [71/500], Train Loss: 0.0841\n",
      "Epoch [72/500], Train Loss: 0.0837\n",
      "Epoch [73/500], Train Loss: 0.0828\n",
      "Epoch [74/500], Train Loss: 0.0822\n",
      "Epoch [75/500], Train Loss: 0.0814\n",
      "Epoch [76/500], Train Loss: 0.0811\n",
      "Epoch [77/500], Train Loss: 0.0804\n",
      "Epoch [78/500], Train Loss: 0.0802\n",
      "Epoch [79/500], Train Loss: 0.0792\n",
      "Epoch [80/500], Train Loss: 0.0785\n",
      "Epoch [81/500], Train Loss: 0.0780\n",
      "Epoch [82/500], Train Loss: 0.0772\n",
      "Epoch [83/500], Train Loss: 0.0770\n",
      "Epoch [84/500], Train Loss: 0.0762\n",
      "Epoch [85/500], Train Loss: 0.0757\n",
      "Epoch [86/500], Train Loss: 0.0753\n",
      "Epoch [87/500], Train Loss: 0.0745\n",
      "Epoch [88/500], Train Loss: 0.0741\n",
      "Epoch [89/500], Train Loss: 0.0738\n",
      "Epoch [90/500], Train Loss: 0.0734\n",
      "Epoch [91/500], Train Loss: 0.0729\n",
      "Epoch [92/500], Train Loss: 0.0725\n",
      "Epoch [93/500], Train Loss: 0.0718\n",
      "Epoch [94/500], Train Loss: 0.0714\n",
      "Epoch [95/500], Train Loss: 0.0710\n",
      "Epoch [96/500], Train Loss: 0.0705\n",
      "Epoch [97/500], Train Loss: 0.0701\n",
      "Epoch [98/500], Train Loss: 0.0697\n",
      "Epoch [99/500], Train Loss: 0.0691\n",
      "Epoch [100/500], Train Loss: 0.0687\n",
      "Epoch [101/500], Train Loss: 0.0684\n",
      "Epoch [102/500], Train Loss: 0.0681\n",
      "Epoch [103/500], Train Loss: 0.0678\n",
      "Epoch [104/500], Train Loss: 0.0675\n",
      "Epoch [105/500], Train Loss: 0.0670\n",
      "Epoch [106/500], Train Loss: 0.0665\n",
      "Epoch [107/500], Train Loss: 0.0661\n",
      "Epoch [108/500], Train Loss: 0.0658\n",
      "Epoch [109/500], Train Loss: 0.0655\n",
      "Epoch [110/500], Train Loss: 0.0650\n",
      "Epoch [111/500], Train Loss: 0.0649\n",
      "Epoch [112/500], Train Loss: 0.0647\n",
      "Epoch [113/500], Train Loss: 0.0641\n",
      "Epoch [114/500], Train Loss: 0.0636\n",
      "Epoch [115/500], Train Loss: 0.0633\n",
      "Epoch [116/500], Train Loss: 0.0632\n",
      "Epoch [117/500], Train Loss: 0.0628\n",
      "Epoch [118/500], Train Loss: 0.0628\n",
      "Epoch [119/500], Train Loss: 0.0623\n",
      "Epoch [120/500], Train Loss: 0.0620\n",
      "Epoch [121/500], Train Loss: 0.0616\n",
      "Epoch [122/500], Train Loss: 0.0613\n",
      "Epoch [123/500], Train Loss: 0.0613\n",
      "Epoch [124/500], Train Loss: 0.0608\n",
      "Epoch [125/500], Train Loss: 0.0606\n",
      "Epoch [126/500], Train Loss: 0.0603\n",
      "Epoch [127/500], Train Loss: 0.0600\n",
      "Epoch [128/500], Train Loss: 0.0599\n",
      "Epoch [129/500], Train Loss: 0.0595\n",
      "Epoch [130/500], Train Loss: 0.0592\n",
      "Epoch [131/500], Train Loss: 0.0590\n",
      "Epoch [132/500], Train Loss: 0.0586\n",
      "Epoch [133/500], Train Loss: 0.0586\n",
      "Epoch [134/500], Train Loss: 0.0582\n",
      "Epoch [135/500], Train Loss: 0.0580\n",
      "Epoch [136/500], Train Loss: 0.0579\n",
      "Epoch [137/500], Train Loss: 0.0576\n",
      "Epoch [138/500], Train Loss: 0.0575\n",
      "Epoch [139/500], Train Loss: 0.0571\n",
      "Epoch [140/500], Train Loss: 0.0569\n",
      "Epoch [141/500], Train Loss: 0.0566\n",
      "Epoch [142/500], Train Loss: 0.0564\n",
      "Epoch [143/500], Train Loss: 0.0562\n",
      "Epoch [144/500], Train Loss: 0.0559\n",
      "Epoch [145/500], Train Loss: 0.0558\n",
      "Epoch [146/500], Train Loss: 0.0556\n",
      "Epoch [147/500], Train Loss: 0.0554\n",
      "Epoch [148/500], Train Loss: 0.0551\n",
      "Epoch [149/500], Train Loss: 0.0550\n",
      "Epoch [150/500], Train Loss: 0.0547\n",
      "Epoch [151/500], Train Loss: 0.0546\n",
      "Epoch [152/500], Train Loss: 0.0544\n",
      "Epoch [153/500], Train Loss: 0.0540\n",
      "Epoch [154/500], Train Loss: 0.0542\n",
      "Epoch [155/500], Train Loss: 0.0539\n",
      "Epoch [156/500], Train Loss: 0.0539\n",
      "Epoch [157/500], Train Loss: 0.0534\n",
      "Epoch [158/500], Train Loss: 0.0532\n",
      "Epoch [159/500], Train Loss: 0.0532\n",
      "Epoch [160/500], Train Loss: 0.0530\n",
      "Epoch [161/500], Train Loss: 0.0526\n",
      "Epoch [162/500], Train Loss: 0.0526\n",
      "Epoch [163/500], Train Loss: 0.0524\n",
      "Epoch [164/500], Train Loss: 0.0522\n",
      "Epoch [165/500], Train Loss: 0.0523\n",
      "Epoch [166/500], Train Loss: 0.0520\n",
      "Epoch [167/500], Train Loss: 0.0517\n",
      "Epoch [168/500], Train Loss: 0.0515\n",
      "Epoch [169/500], Train Loss: 0.0513\n",
      "Epoch [170/500], Train Loss: 0.0512\n",
      "Epoch [171/500], Train Loss: 0.0510\n",
      "Epoch [172/500], Train Loss: 0.0510\n",
      "Epoch [173/500], Train Loss: 0.0507\n",
      "Epoch [174/500], Train Loss: 0.0505\n",
      "Epoch [175/500], Train Loss: 0.0505\n",
      "Epoch [176/500], Train Loss: 0.0503\n",
      "Epoch [177/500], Train Loss: 0.0503\n",
      "Epoch [178/500], Train Loss: 0.0499\n",
      "Epoch [179/500], Train Loss: 0.0500\n",
      "Epoch [180/500], Train Loss: 0.0498\n",
      "Epoch [181/500], Train Loss: 0.0496\n",
      "Epoch [182/500], Train Loss: 0.0494\n",
      "Epoch [183/500], Train Loss: 0.0494\n",
      "Epoch [184/500], Train Loss: 0.0491\n",
      "Epoch [185/500], Train Loss: 0.0490\n",
      "Epoch [186/500], Train Loss: 0.0489\n",
      "Epoch [187/500], Train Loss: 0.0489\n",
      "Epoch [188/500], Train Loss: 0.0486\n",
      "Epoch [189/500], Train Loss: 0.0487\n",
      "Epoch [190/500], Train Loss: 0.0485\n",
      "Epoch [191/500], Train Loss: 0.0483\n",
      "Epoch [192/500], Train Loss: 0.0482\n",
      "Epoch [193/500], Train Loss: 0.0479\n",
      "Epoch [194/500], Train Loss: 0.0478\n",
      "Epoch [195/500], Train Loss: 0.0477\n",
      "Epoch [196/500], Train Loss: 0.0477\n",
      "Epoch [197/500], Train Loss: 0.0477\n",
      "Epoch [198/500], Train Loss: 0.0474\n",
      "Epoch [199/500], Train Loss: 0.0473\n",
      "Epoch [200/500], Train Loss: 0.0473\n",
      "Epoch [201/500], Train Loss: 0.0471\n",
      "Epoch [202/500], Train Loss: 0.0470\n",
      "Epoch [203/500], Train Loss: 0.0468\n",
      "Epoch [204/500], Train Loss: 0.0467\n",
      "Epoch [205/500], Train Loss: 0.0466\n",
      "Epoch [206/500], Train Loss: 0.0465\n",
      "Epoch [207/500], Train Loss: 0.0464\n",
      "Epoch [208/500], Train Loss: 0.0462\n",
      "Epoch [209/500], Train Loss: 0.0462\n",
      "Epoch [210/500], Train Loss: 0.0464\n",
      "Epoch [211/500], Train Loss: 0.0461\n",
      "Epoch [212/500], Train Loss: 0.0459\n",
      "Epoch [213/500], Train Loss: 0.0458\n",
      "Epoch [214/500], Train Loss: 0.0456\n",
      "Epoch [215/500], Train Loss: 0.0455\n",
      "Epoch [216/500], Train Loss: 0.0454\n",
      "Epoch [217/500], Train Loss: 0.0454\n",
      "Epoch [218/500], Train Loss: 0.0452\n",
      "Epoch [219/500], Train Loss: 0.0453\n",
      "Epoch [220/500], Train Loss: 0.0451\n",
      "Epoch [221/500], Train Loss: 0.0450\n",
      "Epoch [222/500], Train Loss: 0.0449\n",
      "Epoch [223/500], Train Loss: 0.0447\n",
      "Epoch [224/500], Train Loss: 0.0446\n",
      "Epoch [225/500], Train Loss: 0.0445\n",
      "Epoch [226/500], Train Loss: 0.0444\n",
      "Epoch [227/500], Train Loss: 0.0444\n",
      "Epoch [228/500], Train Loss: 0.0444\n",
      "Epoch [229/500], Train Loss: 0.0443\n",
      "Epoch [230/500], Train Loss: 0.0441\n",
      "Epoch [231/500], Train Loss: 0.0440\n",
      "Epoch [232/500], Train Loss: 0.0440\n",
      "Epoch [233/500], Train Loss: 0.0438\n",
      "Epoch [234/500], Train Loss: 0.0438\n",
      "Epoch [235/500], Train Loss: 0.0437\n",
      "Epoch [236/500], Train Loss: 0.0435\n",
      "Epoch [237/500], Train Loss: 0.0435\n",
      "Epoch [238/500], Train Loss: 0.0435\n",
      "Epoch [239/500], Train Loss: 0.0434\n",
      "Epoch [240/500], Train Loss: 0.0434\n",
      "Epoch [241/500], Train Loss: 0.0432\n",
      "Epoch [242/500], Train Loss: 0.0430\n",
      "Epoch [243/500], Train Loss: 0.0430\n",
      "Epoch [244/500], Train Loss: 0.0428\n",
      "Epoch [245/500], Train Loss: 0.0428\n",
      "Epoch [246/500], Train Loss: 0.0428\n",
      "Epoch [247/500], Train Loss: 0.0428\n",
      "Epoch [248/500], Train Loss: 0.0427\n",
      "Epoch [249/500], Train Loss: 0.0426\n",
      "Epoch [250/500], Train Loss: 0.0424\n",
      "Epoch [251/500], Train Loss: 0.0424\n",
      "Epoch [252/500], Train Loss: 0.0423\n",
      "Epoch [253/500], Train Loss: 0.0422\n",
      "Epoch [254/500], Train Loss: 0.0421\n",
      "Epoch [255/500], Train Loss: 0.0420\n",
      "Epoch [256/500], Train Loss: 0.0421\n",
      "Epoch [257/500], Train Loss: 0.0419\n",
      "Epoch [258/500], Train Loss: 0.0418\n",
      "Epoch [259/500], Train Loss: 0.0417\n",
      "Epoch [260/500], Train Loss: 0.0417\n",
      "Epoch [261/500], Train Loss: 0.0416\n",
      "Epoch [262/500], Train Loss: 0.0415\n",
      "Epoch [263/500], Train Loss: 0.0416\n",
      "Epoch [264/500], Train Loss: 0.0415\n",
      "Epoch [265/500], Train Loss: 0.0414\n",
      "Epoch [266/500], Train Loss: 0.0413\n",
      "Epoch [267/500], Train Loss: 0.0412\n",
      "Epoch [268/500], Train Loss: 0.0411\n",
      "Epoch [269/500], Train Loss: 0.0411\n",
      "Epoch [270/500], Train Loss: 0.0410\n",
      "Epoch [271/500], Train Loss: 0.0409\n",
      "Epoch [272/500], Train Loss: 0.0408\n",
      "Epoch [273/500], Train Loss: 0.0408\n",
      "Epoch [274/500], Train Loss: 0.0407\n",
      "Epoch [275/500], Train Loss: 0.0406\n",
      "Epoch [276/500], Train Loss: 0.0406\n",
      "Epoch [277/500], Train Loss: 0.0405\n",
      "Epoch [278/500], Train Loss: 0.0406\n",
      "Epoch [279/500], Train Loss: 0.0406\n",
      "Epoch [280/500], Train Loss: 0.0404\n",
      "Epoch [281/500], Train Loss: 0.0404\n",
      "Epoch [282/500], Train Loss: 0.0402\n",
      "Epoch [283/500], Train Loss: 0.0402\n",
      "Epoch [284/500], Train Loss: 0.0401\n",
      "Epoch [285/500], Train Loss: 0.0401\n",
      "Epoch [286/500], Train Loss: 0.0401\n",
      "Epoch [287/500], Train Loss: 0.0399\n",
      "Epoch [288/500], Train Loss: 0.0399\n",
      "Epoch [289/500], Train Loss: 0.0398\n",
      "Epoch [290/500], Train Loss: 0.0398\n",
      "Epoch [291/500], Train Loss: 0.0397\n",
      "Epoch [292/500], Train Loss: 0.0398\n",
      "Epoch [293/500], Train Loss: 0.0396\n",
      "Epoch [294/500], Train Loss: 0.0395\n",
      "Epoch [295/500], Train Loss: 0.0395\n",
      "Epoch [296/500], Train Loss: 0.0396\n",
      "Epoch [297/500], Train Loss: 0.0395\n",
      "Epoch [298/500], Train Loss: 0.0394\n",
      "Epoch [299/500], Train Loss: 0.0393\n",
      "Epoch [300/500], Train Loss: 0.0392\n",
      "Epoch [301/500], Train Loss: 0.0392\n",
      "Epoch [302/500], Train Loss: 0.0392\n",
      "Epoch [303/500], Train Loss: 0.0391\n",
      "Epoch [304/500], Train Loss: 0.0391\n",
      "Epoch [305/500], Train Loss: 0.0389\n",
      "Epoch [306/500], Train Loss: 0.0389\n",
      "Epoch [307/500], Train Loss: 0.0390\n",
      "Epoch [308/500], Train Loss: 0.0389\n",
      "Epoch [309/500], Train Loss: 0.0387\n",
      "Epoch [310/500], Train Loss: 0.0387\n",
      "Epoch [311/500], Train Loss: 0.0386\n",
      "Epoch [312/500], Train Loss: 0.0387\n",
      "Epoch [313/500], Train Loss: 0.0387\n",
      "Epoch [314/500], Train Loss: 0.0385\n",
      "Epoch [315/500], Train Loss: 0.0385\n",
      "Epoch [316/500], Train Loss: 0.0383\n",
      "Epoch [317/500], Train Loss: 0.0383\n",
      "Epoch [318/500], Train Loss: 0.0383\n",
      "Epoch [319/500], Train Loss: 0.0385\n",
      "Epoch [320/500], Train Loss: 0.0383\n",
      "Epoch [321/500], Train Loss: 0.0382\n",
      "Epoch [322/500], Train Loss: 0.0381\n",
      "Epoch [323/500], Train Loss: 0.0381\n",
      "Epoch [324/500], Train Loss: 0.0381\n",
      "Epoch [325/500], Train Loss: 0.0380\n",
      "Epoch [326/500], Train Loss: 0.0380\n",
      "Epoch [327/500], Train Loss: 0.0379\n",
      "Epoch [328/500], Train Loss: 0.0379\n",
      "Epoch [329/500], Train Loss: 0.0378\n",
      "Epoch [330/500], Train Loss: 0.0378\n",
      "Epoch [331/500], Train Loss: 0.0377\n",
      "Epoch [332/500], Train Loss: 0.0377\n",
      "Epoch [333/500], Train Loss: 0.0378\n",
      "Epoch [334/500], Train Loss: 0.0376\n",
      "Epoch [335/500], Train Loss: 0.0375\n",
      "Epoch [336/500], Train Loss: 0.0375\n",
      "Epoch [337/500], Train Loss: 0.0376\n",
      "Epoch [338/500], Train Loss: 0.0375\n",
      "Epoch [339/500], Train Loss: 0.0373\n",
      "Epoch [340/500], Train Loss: 0.0375\n",
      "Epoch [341/500], Train Loss: 0.0374\n",
      "Epoch [342/500], Train Loss: 0.0373\n",
      "Epoch [343/500], Train Loss: 0.0372\n",
      "Epoch [344/500], Train Loss: 0.0372\n",
      "Epoch [345/500], Train Loss: 0.0372\n",
      "Epoch [346/500], Train Loss: 0.0371\n",
      "Epoch [347/500], Train Loss: 0.0371\n",
      "Epoch [348/500], Train Loss: 0.0370\n",
      "Epoch [349/500], Train Loss: 0.0370\n",
      "Epoch [350/500], Train Loss: 0.0370\n",
      "Epoch [351/500], Train Loss: 0.0370\n",
      "Epoch [352/500], Train Loss: 0.0370\n",
      "Epoch [353/500], Train Loss: 0.0370\n",
      "Epoch [354/500], Train Loss: 0.0370\n",
      "Epoch [355/500], Train Loss: 0.0369\n",
      "Epoch [356/500], Train Loss: 0.0368\n",
      "Epoch [357/500], Train Loss: 0.0366\n",
      "Epoch [358/500], Train Loss: 0.0366\n",
      "Epoch [359/500], Train Loss: 0.0366\n",
      "Epoch [360/500], Train Loss: 0.0366\n",
      "Epoch [361/500], Train Loss: 0.0366\n",
      "Epoch [362/500], Train Loss: 0.0365\n",
      "Epoch [363/500], Train Loss: 0.0365\n",
      "Epoch [364/500], Train Loss: 0.0365\n",
      "Epoch [365/500], Train Loss: 0.0364\n",
      "Epoch [366/500], Train Loss: 0.0365\n",
      "Epoch [367/500], Train Loss: 0.0364\n",
      "Epoch [368/500], Train Loss: 0.0363\n",
      "Epoch [369/500], Train Loss: 0.0364\n",
      "Epoch [370/500], Train Loss: 0.0364\n",
      "Epoch [371/500], Train Loss: 0.0364\n",
      "Epoch [372/500], Train Loss: 0.0362\n",
      "Epoch [373/500], Train Loss: 0.0362\n",
      "Epoch [374/500], Train Loss: 0.0360\n",
      "Epoch [375/500], Train Loss: 0.0362\n",
      "Epoch [376/500], Train Loss: 0.0360\n",
      "Epoch [377/500], Train Loss: 0.0360\n",
      "Epoch [378/500], Train Loss: 0.0360\n",
      "Epoch [379/500], Train Loss: 0.0359\n",
      "Epoch [380/500], Train Loss: 0.0359\n",
      "Epoch [381/500], Train Loss: 0.0358\n",
      "Epoch [382/500], Train Loss: 0.0359\n",
      "Epoch [383/500], Train Loss: 0.0358\n",
      "Epoch [384/500], Train Loss: 0.0357\n",
      "Epoch [385/500], Train Loss: 0.0357\n",
      "Epoch [386/500], Train Loss: 0.0357\n",
      "Epoch [387/500], Train Loss: 0.0358\n",
      "Epoch [388/500], Train Loss: 0.0357\n",
      "Epoch [389/500], Train Loss: 0.0356\n",
      "Epoch [390/500], Train Loss: 0.0356\n",
      "Epoch [391/500], Train Loss: 0.0356\n",
      "Epoch [392/500], Train Loss: 0.0356\n",
      "Epoch [393/500], Train Loss: 0.0354\n",
      "Epoch [394/500], Train Loss: 0.0355\n",
      "Epoch [395/500], Train Loss: 0.0354\n",
      "Epoch [396/500], Train Loss: 0.0354\n",
      "Epoch [397/500], Train Loss: 0.0353\n",
      "Epoch [398/500], Train Loss: 0.0354\n",
      "Epoch [399/500], Train Loss: 0.0353\n",
      "Epoch [400/500], Train Loss: 0.0353\n",
      "Epoch [401/500], Train Loss: 0.0353\n",
      "Epoch [402/500], Train Loss: 0.0352\n",
      "Epoch [403/500], Train Loss: 0.0352\n",
      "Epoch [404/500], Train Loss: 0.0352\n",
      "Epoch [405/500], Train Loss: 0.0351\n",
      "Epoch [406/500], Train Loss: 0.0350\n",
      "Epoch [407/500], Train Loss: 0.0351\n",
      "Epoch [408/500], Train Loss: 0.0350\n",
      "Epoch [409/500], Train Loss: 0.0350\n",
      "Epoch [410/500], Train Loss: 0.0351\n",
      "Epoch [411/500], Train Loss: 0.0351\n",
      "Epoch [412/500], Train Loss: 0.0350\n",
      "Epoch [413/500], Train Loss: 0.0351\n",
      "Epoch [414/500], Train Loss: 0.0349\n",
      "Epoch [415/500], Train Loss: 0.0349\n",
      "Epoch [416/500], Train Loss: 0.0348\n",
      "Epoch [417/500], Train Loss: 0.0347\n",
      "Epoch [418/500], Train Loss: 0.0348\n",
      "Epoch [419/500], Train Loss: 0.0347\n",
      "Epoch [420/500], Train Loss: 0.0347\n",
      "Epoch [421/500], Train Loss: 0.0348\n",
      "Epoch [422/500], Train Loss: 0.0346\n",
      "Epoch [423/500], Train Loss: 0.0347\n",
      "Epoch [424/500], Train Loss: 0.0346\n",
      "Epoch [425/500], Train Loss: 0.0346\n",
      "Epoch [426/500], Train Loss: 0.0345\n",
      "Epoch [427/500], Train Loss: 0.0345\n",
      "Epoch [428/500], Train Loss: 0.0345\n",
      "Epoch [429/500], Train Loss: 0.0345\n",
      "Epoch [430/500], Train Loss: 0.0346\n",
      "Epoch [431/500], Train Loss: 0.0344\n",
      "Epoch [432/500], Train Loss: 0.0344\n",
      "Epoch [433/500], Train Loss: 0.0344\n",
      "Epoch [434/500], Train Loss: 0.0344\n",
      "Epoch [435/500], Train Loss: 0.0344\n",
      "Epoch [436/500], Train Loss: 0.0343\n",
      "Epoch [437/500], Train Loss: 0.0343\n",
      "Epoch [438/500], Train Loss: 0.0344\n",
      "Epoch [439/500], Train Loss: 0.0343\n",
      "Epoch [440/500], Train Loss: 0.0344\n",
      "Epoch [441/500], Train Loss: 0.0343\n",
      "Epoch [442/500], Train Loss: 0.0342\n",
      "Epoch [443/500], Train Loss: 0.0341\n",
      "Epoch [444/500], Train Loss: 0.0342\n",
      "Epoch [445/500], Train Loss: 0.0343\n",
      "Epoch [446/500], Train Loss: 0.0340\n",
      "Epoch [447/500], Train Loss: 0.0341\n",
      "Epoch [448/500], Train Loss: 0.0341\n",
      "Epoch [449/500], Train Loss: 0.0340\n",
      "Epoch [450/500], Train Loss: 0.0340\n",
      "Epoch [451/500], Train Loss: 0.0340\n",
      "Epoch [452/500], Train Loss: 0.0340\n",
      "Epoch [453/500], Train Loss: 0.0341\n",
      "Epoch [454/500], Train Loss: 0.0338\n",
      "Epoch [455/500], Train Loss: 0.0339\n",
      "Epoch [456/500], Train Loss: 0.0339\n",
      "Epoch [457/500], Train Loss: 0.0339\n",
      "Epoch [458/500], Train Loss: 0.0338\n",
      "Epoch [459/500], Train Loss: 0.0338\n",
      "Epoch [460/500], Train Loss: 0.0337\n",
      "Epoch [461/500], Train Loss: 0.0337\n",
      "Epoch [462/500], Train Loss: 0.0336\n",
      "Epoch [463/500], Train Loss: 0.0337\n",
      "Epoch [464/500], Train Loss: 0.0337\n",
      "Epoch [465/500], Train Loss: 0.0337\n",
      "Epoch [466/500], Train Loss: 0.0337\n",
      "Epoch [467/500], Train Loss: 0.0337\n",
      "Epoch [468/500], Train Loss: 0.0337\n",
      "Epoch [469/500], Train Loss: 0.0336\n",
      "Epoch [470/500], Train Loss: 0.0335\n",
      "Epoch [471/500], Train Loss: 0.0335\n",
      "Epoch [472/500], Train Loss: 0.0336\n",
      "Epoch [473/500], Train Loss: 0.0334\n",
      "Epoch [474/500], Train Loss: 0.0335\n",
      "Epoch [475/500], Train Loss: 0.0334\n",
      "Epoch [476/500], Train Loss: 0.0334\n",
      "Epoch [477/500], Train Loss: 0.0334\n",
      "Epoch [478/500], Train Loss: 0.0334\n",
      "Epoch [479/500], Train Loss: 0.0334\n",
      "Epoch [480/500], Train Loss: 0.0334\n",
      "Epoch [481/500], Train Loss: 0.0333\n",
      "Epoch [482/500], Train Loss: 0.0333\n",
      "Epoch [483/500], Train Loss: 0.0334\n",
      "Epoch [484/500], Train Loss: 0.0333\n",
      "Epoch [485/500], Train Loss: 0.0333\n",
      "Epoch [486/500], Train Loss: 0.0333\n",
      "Epoch [487/500], Train Loss: 0.0332\n",
      "Epoch [488/500], Train Loss: 0.0331\n",
      "Epoch [489/500], Train Loss: 0.0332\n",
      "Epoch [490/500], Train Loss: 0.0333\n",
      "Epoch [491/500], Train Loss: 0.0333\n",
      "Epoch [492/500], Train Loss: 0.0332\n",
      "Epoch [493/500], Train Loss: 0.0332\n",
      "Epoch [494/500], Train Loss: 0.0331\n",
      "Epoch [495/500], Train Loss: 0.0333\n",
      "Epoch [496/500], Train Loss: 0.0332\n",
      "Epoch [497/500], Train Loss: 0.0332\n",
      "Epoch [498/500], Train Loss: 0.0331\n",
      "Epoch [499/500], Train Loss: 0.0330\n",
      "Epoch [500/500], Train Loss: 0.0329\n",
      "Final model saved at: ../../Data/model_results/logistic_regression/models/chr18/final_model_chr18.pth\n",
      "Individual R^2 scores saved at: ../../Data/model_results/logistic_regression/csv_files/chr18/individual_r2_scores_chr18.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:47:14,506] A new study created in memory with name: no-name-2b61e4d0-909b-4996-a152-d363b8aeaa94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual AUC ROC curves saved in: ../../Data/model_results/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../Data/model_results/logistic_regression/csv_files/chr18/performance_metrics.csv\n",
      "Unknown PRS313 SNPs:  14\n",
      "Known PRS313 SNPs:  0\n",
      "23AndMe SNPs with LD to Unknown PRS313 SNPs:  546\n",
      "Early stopping at epoch 200\n",
      "Early stopping at epoch 11\n",
      "Early stopping at epoch 11\n",
      "Early stopping at epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:47:17,193] Trial 0 finished with value: 0.06100585293024778 and parameters: {'learning_rate': 0.0031466767970559726, 'lasso_coef': 1.6667137807557878e-05, 'patience': 10}. Best is trial 0 with value: 0.06100585293024778.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 11\n",
      "Early stopping at epoch 147\n",
      "Early stopping at epoch 26\n",
      "Early stopping at epoch 44\n",
      "Early stopping at epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:47:19,735] Trial 1 finished with value: 0.3325313672423363 and parameters: {'learning_rate': 0.00420026056033055, 'lasso_coef': 0.002912748356184998, 'patience': 11}. Best is trial 0 with value: 0.06100585293024778.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 19\n",
      "Early stopping at epoch 18\n",
      "Early stopping at epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:47:25,560] Trial 2 finished with value: 0.060519037581980226 and parameters: {'learning_rate': 0.0007906022643913437, 'lasso_coef': 1.4042949587596974e-05, 'patience': 17}. Best is trial 2 with value: 0.060519037581980226.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 18\n",
      "Early stopping at epoch 18\n",
      "Early stopping at epoch 56\n",
      "Early stopping at epoch 36\n",
      "Early stopping at epoch 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:47:27,400] Trial 3 finished with value: 1.805228453874588 and parameters: {'learning_rate': 0.0406048723120717, 'lasso_coef': 0.03412557985547225, 'patience': 16}. Best is trial 2 with value: 0.060519037581980226.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 35\n",
      "Early stopping at epoch 17\n",
      "Early stopping at epoch 45\n",
      "Early stopping at epoch 21\n",
      "Early stopping at epoch 22\n",
      "Early stopping at epoch 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:47:28,850] Trial 4 finished with value: 0.4780269116163254 and parameters: {'learning_rate': 0.03440618756253167, 'lasso_coef': 0.0036768432381245565, 'patience': 19}. Best is trial 2 with value: 0.060519037581980226.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 25\n",
      "Early stopping at epoch 251\n",
      "Early stopping at epoch 19\n",
      "Early stopping at epoch 23\n",
      "Early stopping at epoch 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:47:32,314] Trial 5 finished with value: 0.22083605602383613 and parameters: {'learning_rate': 0.004405558926310599, 'lasso_coef': 0.0012490558301248723, 'patience': 18}. Best is trial 2 with value: 0.060519037581980226.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 21\n",
      "Early stopping at epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:47:43,348] Trial 6 finished with value: 0.13493402674794197 and parameters: {'learning_rate': 0.00042240575709737827, 'lasso_coef': 0.0003185193140513053, 'patience': 16}. Best is trial 2 with value: 0.060519037581980226.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 20\n",
      "Early stopping at epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:48:04,266] Trial 7 finished with value: 0.2073843449354172 and parameters: {'learning_rate': 0.00010824126993972152, 'lasso_coef': 0.0005914999164112477, 'patience': 12}. Best is trial 2 with value: 0.060519037581980226.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 13\n",
      "Early stopping at epoch 139\n",
      "Early stopping at epoch 30\n",
      "Early stopping at epoch 39\n",
      "Early stopping at epoch 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:48:07,062] Trial 8 finished with value: 0.43043022602796555 and parameters: {'learning_rate': 0.007950918829636024, 'lasso_coef': 0.004643626190071322, 'patience': 18}. Best is trial 2 with value: 0.060519037581980226.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 22\n",
      "Early stopping at epoch 20\n",
      "Early stopping at epoch 20\n",
      "Early stopping at epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:48:13,240] Trial 9 finished with value: 0.11335986405611038 and parameters: {'learning_rate': 0.000345013809819915, 'lasso_coef': 8.869302611564487e-05, 'patience': 19}. Best is trial 2 with value: 0.060519037581980226.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 20\n",
      "Early stopping at epoch 310\n",
      "Early stopping at epoch 6\n",
      "Early stopping at epoch 6\n",
      "Early stopping at epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:48:16,756] Trial 10 finished with value: 0.07017063181847334 and parameters: {'learning_rate': 0.0010067504464098646, 'lasso_coef': 1.3885017755230874e-05, 'patience': 5}. Best is trial 2 with value: 0.060519037581980226.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 6\n",
      "Early stopping at epoch 341\n",
      "Early stopping at epoch 9\n",
      "Early stopping at epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:48:20,677] Trial 11 finished with value: 0.06046284083276987 and parameters: {'learning_rate': 0.0012489587544580454, 'lasso_coef': 1.1000023991405022e-05, 'patience': 8}. Best is trial 11 with value: 0.06046284083276987.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 9\n",
      "Early stopping at epoch 9\n",
      "Early stopping at epoch 489\n",
      "Early stopping at epoch 8\n",
      "Early stopping at epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:48:26,109] Trial 12 finished with value: 0.08359784614294767 and parameters: {'learning_rate': 0.0009668965015398083, 'lasso_coef': 7.46769933341149e-05, 'patience': 7}. Best is trial 11 with value: 0.06046284083276987.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 8\n",
      "Early stopping at epoch 8\n",
      "Early stopping at epoch 9\n",
      "Early stopping at epoch 9\n",
      "Early stopping at epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:48:31,568] Trial 13 finished with value: 0.0791207704693079 and parameters: {'learning_rate': 0.0007040024075979816, 'lasso_coef': 4.649599515823389e-05, 'patience': 8}. Best is trial 11 with value: 0.06046284083276987.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 9\n",
      "Early stopping at epoch 15\n",
      "Early stopping at epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:48:37,439] Trial 14 finished with value: 0.12814744077622892 and parameters: {'learning_rate': 0.00016292953454661072, 'lasso_coef': 1.2336397012265714e-05, 'patience': 14}. Best is trial 11 with value: 0.06046284083276987.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15\n",
      "Early stopping at epoch 15\n",
      "Early stopping at epoch 485\n",
      "Early stopping at epoch 15\n",
      "Early stopping at epoch 15\n",
      "Early stopping at epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:48:43,075] Trial 15 finished with value: 0.1009226817637682 and parameters: {'learning_rate': 0.002334678955266021, 'lasso_coef': 0.00018741952480218416, 'patience': 14}. Best is trial 11 with value: 0.06046284083276987.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15\n",
      "Early stopping at epoch 83\n",
      "Early stopping at epoch 10\n",
      "Early stopping at epoch 10\n",
      "Early stopping at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:48:44,333] Trial 16 finished with value: 0.07115014810115099 and parameters: {'learning_rate': 0.009616230792529745, 'lasso_coef': 3.7050612512016224e-05, 'patience': 9}. Best is trial 11 with value: 0.06046284083276987.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10\n",
      "Early stopping at epoch 109\n",
      "Early stopping at epoch 32\n",
      "Early stopping at epoch 25\n",
      "Early stopping at epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:48:46,443] Trial 17 finished with value: 0.6735498815774917 and parameters: {'learning_rate': 0.0015352829782875814, 'lasso_coef': 0.08840202965008398, 'patience': 6}. Best is trial 11 with value: 0.06046284083276987.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 20\n",
      "Early stopping at epoch 15\n",
      "Early stopping at epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:48:52,152] Trial 18 finished with value: 0.13739885725080966 and parameters: {'learning_rate': 0.00028761648329616397, 'lasso_coef': 0.00015293177037045835, 'patience': 14}. Best is trial 11 with value: 0.06046284083276987.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15\n",
      "Early stopping at epoch 15\n",
      "Early stopping at epoch 124\n",
      "Early stopping at epoch 17\n",
      "Early stopping at epoch 17\n",
      "Early stopping at epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:48:54,298] Trial 19 finished with value: 0.061886812560260296 and parameters: {'learning_rate': 0.008509045996161441, 'lasso_coef': 2.6628892252222264e-05, 'patience': 16}. Best is trial 11 with value: 0.06046284083276987.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 17\n",
      "Early stopping at epoch 36\n",
      "Early stopping at epoch 23\n",
      "Early stopping at epoch 14\n",
      "Early stopping at epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:48:55,553] Trial 20 finished with value: 1.3528258860111237 and parameters: {'learning_rate': 0.07449814622182958, 'lasso_coef': 0.011760516355739204, 'patience': 12}. Best is trial 11 with value: 0.06046284083276987.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 28\n",
      "Early stopping at epoch 160\n",
      "Early stopping at epoch 11\n",
      "Early stopping at epoch 11\n",
      "Early stopping at epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:48:57,660] Trial 21 finished with value: 0.06606910694390536 and parameters: {'learning_rate': 0.0026036956929321575, 'lasso_coef': 1.3635303591821396e-05, 'patience': 10}. Best is trial 11 with value: 0.06046284083276987.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 11\n",
      "Early stopping at epoch 331\n",
      "Early stopping at epoch 10\n",
      "Early stopping at epoch 10\n",
      "Early stopping at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:49:01,468] Trial 22 finished with value: 0.0668575145304203 and parameters: {'learning_rate': 0.0016825239061926604, 'lasso_coef': 2.9473934958999707e-05, 'patience': 9}. Best is trial 11 with value: 0.06046284083276987.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10\n",
      "Early stopping at epoch 11\n",
      "Early stopping at epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:49:07,083] Trial 23 finished with value: 0.063412044942379 and parameters: {'learning_rate': 0.0006316263423486343, 'lasso_coef': 1.1117456515478986e-05, 'patience': 10}. Best is trial 11 with value: 0.06046284083276987.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 11\n",
      "Early stopping at epoch 11\n",
      "Early stopping at epoch 145\n",
      "Early stopping at epoch 8\n",
      "Early stopping at epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:49:09,029] Trial 24 finished with value: 0.08647089432924986 and parameters: {'learning_rate': 0.004728308224942777, 'lasso_coef': 8.079459986648527e-05, 'patience': 7}. Best is trial 11 with value: 0.06046284083276987.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 8\n",
      "Early stopping at epoch 8\n",
      "Early stopping at epoch 452\n",
      "Early stopping at epoch 9\n",
      "Early stopping at epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:49:14,152] Trial 25 finished with value: 0.06192395724356174 and parameters: {'learning_rate': 0.001253914028501495, 'lasso_coef': 2.343403074837283e-05, 'patience': 8}. Best is trial 11 with value: 0.06046284083276987.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 9\n",
      "Early stopping at epoch 9\n",
      "Early stopping at epoch 68\n",
      "Early stopping at epoch 15\n",
      "Early stopping at epoch 14\n",
      "Early stopping at epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:49:15,474] Trial 26 finished with value: 0.0757237421348691 and parameters: {'learning_rate': 0.02059059149708103, 'lasso_coef': 5.200006434735689e-05, 'patience': 13}. Best is trial 11 with value: 0.06046284083276987.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 14\n",
      "Early stopping at epoch 12\n",
      "Early stopping at epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:49:26,265] Trial 27 finished with value: 0.1368086289614439 and parameters: {'learning_rate': 0.00019505705182717694, 'lasso_coef': 0.0001821345077874043, 'patience': 11}. Best is trial 11 with value: 0.06046284083276987.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 12\n",
      "Early stopping at epoch 21\n",
      "Early stopping at epoch 21\n",
      "Early stopping at epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:49:32,471] Trial 28 finished with value: 0.07585394214838743 and parameters: {'learning_rate': 0.0005000824794896797, 'lasso_coef': 2.2283196109118165e-05, 'patience': 20}. Best is trial 11 with value: 0.06046284083276987.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 21\n",
      "Early stopping at epoch 238\n",
      "Early stopping at epoch 12\n",
      "Early stopping at epoch 13\n",
      "Early stopping at epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:49:35,468] Trial 29 finished with value: 0.23852555602788925 and parameters: {'learning_rate': 0.0027033527042844897, 'lasso_coef': 0.0014838999488969297, 'patience': 11}. Best is trial 11 with value: 0.06046284083276987.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 13\n",
      "Best hyperparameters: {'learning_rate': 0.0012489587544580454, 'lasso_coef': 1.1000023991405022e-05, 'patience': 8}\n",
      "Best value: 0.06046284083276987\n",
      "Epoch [1/500], Train Loss: 0.5733\n",
      "Epoch [2/500], Train Loss: 0.4910\n",
      "Epoch [3/500], Train Loss: 0.4585\n",
      "Epoch [4/500], Train Loss: 0.4346\n",
      "Epoch [5/500], Train Loss: 0.4136\n",
      "Epoch [6/500], Train Loss: 0.3956\n",
      "Epoch [7/500], Train Loss: 0.3788\n",
      "Epoch [8/500], Train Loss: 0.3635\n",
      "Epoch [9/500], Train Loss: 0.3499\n",
      "Epoch [10/500], Train Loss: 0.3375\n",
      "Epoch [11/500], Train Loss: 0.3259\n",
      "Epoch [12/500], Train Loss: 0.3150\n",
      "Epoch [13/500], Train Loss: 0.3049\n",
      "Epoch [14/500], Train Loss: 0.2959\n",
      "Epoch [15/500], Train Loss: 0.2877\n",
      "Epoch [16/500], Train Loss: 0.2796\n",
      "Epoch [17/500], Train Loss: 0.2721\n",
      "Epoch [18/500], Train Loss: 0.2650\n",
      "Epoch [19/500], Train Loss: 0.2582\n",
      "Epoch [20/500], Train Loss: 0.2520\n",
      "Epoch [21/500], Train Loss: 0.2460\n",
      "Epoch [22/500], Train Loss: 0.2407\n",
      "Epoch [23/500], Train Loss: 0.2350\n",
      "Epoch [24/500], Train Loss: 0.2297\n",
      "Epoch [25/500], Train Loss: 0.2256\n",
      "Epoch [26/500], Train Loss: 0.2204\n",
      "Epoch [27/500], Train Loss: 0.2158\n",
      "Epoch [28/500], Train Loss: 0.2117\n",
      "Epoch [29/500], Train Loss: 0.2076\n",
      "Epoch [30/500], Train Loss: 0.2038\n",
      "Epoch [31/500], Train Loss: 0.2004\n",
      "Epoch [32/500], Train Loss: 0.1973\n",
      "Epoch [33/500], Train Loss: 0.1931\n",
      "Epoch [34/500], Train Loss: 0.1902\n",
      "Epoch [35/500], Train Loss: 0.1871\n",
      "Epoch [36/500], Train Loss: 0.1836\n",
      "Epoch [37/500], Train Loss: 0.1808\n",
      "Epoch [38/500], Train Loss: 0.1786\n",
      "Epoch [39/500], Train Loss: 0.1756\n",
      "Epoch [40/500], Train Loss: 0.1731\n",
      "Epoch [41/500], Train Loss: 0.1701\n",
      "Epoch [42/500], Train Loss: 0.1678\n",
      "Epoch [43/500], Train Loss: 0.1653\n",
      "Epoch [44/500], Train Loss: 0.1631\n",
      "Epoch [45/500], Train Loss: 0.1612\n",
      "Epoch [46/500], Train Loss: 0.1591\n",
      "Epoch [47/500], Train Loss: 0.1567\n",
      "Epoch [48/500], Train Loss: 0.1548\n",
      "Epoch [49/500], Train Loss: 0.1524\n",
      "Epoch [50/500], Train Loss: 0.1508\n",
      "Epoch [51/500], Train Loss: 0.1492\n",
      "Epoch [52/500], Train Loss: 0.1469\n",
      "Epoch [53/500], Train Loss: 0.1455\n",
      "Epoch [54/500], Train Loss: 0.1439\n",
      "Epoch [55/500], Train Loss: 0.1425\n",
      "Epoch [56/500], Train Loss: 0.1406\n",
      "Epoch [57/500], Train Loss: 0.1391\n",
      "Epoch [58/500], Train Loss: 0.1375\n",
      "Epoch [59/500], Train Loss: 0.1359\n",
      "Epoch [60/500], Train Loss: 0.1344\n",
      "Epoch [61/500], Train Loss: 0.1333\n",
      "Epoch [62/500], Train Loss: 0.1319\n",
      "Epoch [63/500], Train Loss: 0.1309\n",
      "Epoch [64/500], Train Loss: 0.1290\n",
      "Epoch [65/500], Train Loss: 0.1279\n",
      "Epoch [66/500], Train Loss: 0.1266\n",
      "Epoch [67/500], Train Loss: 0.1250\n",
      "Epoch [68/500], Train Loss: 0.1246\n",
      "Epoch [69/500], Train Loss: 0.1230\n",
      "Epoch [70/500], Train Loss: 0.1219\n",
      "Epoch [71/500], Train Loss: 0.1206\n",
      "Epoch [72/500], Train Loss: 0.1198\n",
      "Epoch [73/500], Train Loss: 0.1189\n",
      "Epoch [74/500], Train Loss: 0.1176\n",
      "Epoch [75/500], Train Loss: 0.1165\n",
      "Epoch [76/500], Train Loss: 0.1157\n",
      "Epoch [77/500], Train Loss: 0.1150\n",
      "Epoch [78/500], Train Loss: 0.1136\n",
      "Epoch [79/500], Train Loss: 0.1126\n",
      "Epoch [80/500], Train Loss: 0.1117\n",
      "Epoch [81/500], Train Loss: 0.1110\n",
      "Epoch [82/500], Train Loss: 0.1098\n",
      "Epoch [83/500], Train Loss: 0.1093\n",
      "Epoch [84/500], Train Loss: 0.1087\n",
      "Epoch [85/500], Train Loss: 0.1073\n",
      "Epoch [86/500], Train Loss: 0.1067\n",
      "Epoch [87/500], Train Loss: 0.1057\n",
      "Epoch [88/500], Train Loss: 0.1049\n",
      "Epoch [89/500], Train Loss: 0.1041\n",
      "Epoch [90/500], Train Loss: 0.1036\n",
      "Epoch [91/500], Train Loss: 0.1027\n",
      "Epoch [92/500], Train Loss: 0.1019\n",
      "Epoch [93/500], Train Loss: 0.1013\n",
      "Epoch [94/500], Train Loss: 0.1006\n",
      "Epoch [95/500], Train Loss: 0.0996\n",
      "Epoch [96/500], Train Loss: 0.0991\n",
      "Epoch [97/500], Train Loss: 0.0981\n",
      "Epoch [98/500], Train Loss: 0.0975\n",
      "Epoch [99/500], Train Loss: 0.0971\n",
      "Epoch [100/500], Train Loss: 0.0965\n",
      "Epoch [101/500], Train Loss: 0.0956\n",
      "Epoch [102/500], Train Loss: 0.0951\n",
      "Epoch [103/500], Train Loss: 0.0944\n",
      "Epoch [104/500], Train Loss: 0.0940\n",
      "Epoch [105/500], Train Loss: 0.0934\n",
      "Epoch [106/500], Train Loss: 0.0929\n",
      "Epoch [107/500], Train Loss: 0.0924\n",
      "Epoch [108/500], Train Loss: 0.0916\n",
      "Epoch [109/500], Train Loss: 0.0912\n",
      "Epoch [110/500], Train Loss: 0.0903\n",
      "Epoch [111/500], Train Loss: 0.0899\n",
      "Epoch [112/500], Train Loss: 0.0894\n",
      "Epoch [113/500], Train Loss: 0.0887\n",
      "Epoch [114/500], Train Loss: 0.0882\n",
      "Epoch [115/500], Train Loss: 0.0879\n",
      "Epoch [116/500], Train Loss: 0.0874\n",
      "Epoch [117/500], Train Loss: 0.0866\n",
      "Epoch [118/500], Train Loss: 0.0865\n",
      "Epoch [119/500], Train Loss: 0.0859\n",
      "Epoch [120/500], Train Loss: 0.0852\n",
      "Epoch [121/500], Train Loss: 0.0851\n",
      "Epoch [122/500], Train Loss: 0.0845\n",
      "Epoch [123/500], Train Loss: 0.0842\n",
      "Epoch [124/500], Train Loss: 0.0836\n",
      "Epoch [125/500], Train Loss: 0.0832\n",
      "Epoch [126/500], Train Loss: 0.0826\n",
      "Epoch [127/500], Train Loss: 0.0821\n",
      "Epoch [128/500], Train Loss: 0.0817\n",
      "Epoch [129/500], Train Loss: 0.0814\n",
      "Epoch [130/500], Train Loss: 0.0809\n",
      "Epoch [131/500], Train Loss: 0.0806\n",
      "Epoch [132/500], Train Loss: 0.0801\n",
      "Epoch [133/500], Train Loss: 0.0800\n",
      "Epoch [134/500], Train Loss: 0.0794\n",
      "Epoch [135/500], Train Loss: 0.0790\n",
      "Epoch [136/500], Train Loss: 0.0788\n",
      "Epoch [137/500], Train Loss: 0.0783\n",
      "Epoch [138/500], Train Loss: 0.0780\n",
      "Epoch [139/500], Train Loss: 0.0774\n",
      "Epoch [140/500], Train Loss: 0.0771\n",
      "Epoch [141/500], Train Loss: 0.0766\n",
      "Epoch [142/500], Train Loss: 0.0762\n",
      "Epoch [143/500], Train Loss: 0.0761\n",
      "Epoch [144/500], Train Loss: 0.0753\n",
      "Epoch [145/500], Train Loss: 0.0756\n",
      "Epoch [146/500], Train Loss: 0.0750\n",
      "Epoch [147/500], Train Loss: 0.0744\n",
      "Epoch [148/500], Train Loss: 0.0742\n",
      "Epoch [149/500], Train Loss: 0.0739\n",
      "Epoch [150/500], Train Loss: 0.0737\n",
      "Epoch [151/500], Train Loss: 0.0733\n",
      "Epoch [152/500], Train Loss: 0.0729\n",
      "Epoch [153/500], Train Loss: 0.0725\n",
      "Epoch [154/500], Train Loss: 0.0724\n",
      "Epoch [155/500], Train Loss: 0.0718\n",
      "Epoch [156/500], Train Loss: 0.0715\n",
      "Epoch [157/500], Train Loss: 0.0713\n",
      "Epoch [158/500], Train Loss: 0.0710\n",
      "Epoch [159/500], Train Loss: 0.0711\n",
      "Epoch [160/500], Train Loss: 0.0704\n",
      "Epoch [161/500], Train Loss: 0.0703\n",
      "Epoch [162/500], Train Loss: 0.0699\n",
      "Epoch [163/500], Train Loss: 0.0695\n",
      "Epoch [164/500], Train Loss: 0.0692\n",
      "Epoch [165/500], Train Loss: 0.0689\n",
      "Epoch [166/500], Train Loss: 0.0689\n",
      "Epoch [167/500], Train Loss: 0.0683\n",
      "Epoch [168/500], Train Loss: 0.0684\n",
      "Epoch [169/500], Train Loss: 0.0680\n",
      "Epoch [170/500], Train Loss: 0.0676\n",
      "Epoch [171/500], Train Loss: 0.0674\n",
      "Epoch [172/500], Train Loss: 0.0671\n",
      "Epoch [173/500], Train Loss: 0.0667\n",
      "Epoch [174/500], Train Loss: 0.0664\n",
      "Epoch [175/500], Train Loss: 0.0663\n",
      "Epoch [176/500], Train Loss: 0.0659\n",
      "Epoch [177/500], Train Loss: 0.0657\n",
      "Epoch [178/500], Train Loss: 0.0657\n",
      "Epoch [179/500], Train Loss: 0.0654\n",
      "Epoch [180/500], Train Loss: 0.0650\n",
      "Epoch [181/500], Train Loss: 0.0647\n",
      "Epoch [182/500], Train Loss: 0.0644\n",
      "Epoch [183/500], Train Loss: 0.0644\n",
      "Epoch [184/500], Train Loss: 0.0642\n",
      "Epoch [185/500], Train Loss: 0.0641\n",
      "Epoch [186/500], Train Loss: 0.0637\n",
      "Epoch [187/500], Train Loss: 0.0636\n",
      "Epoch [188/500], Train Loss: 0.0631\n",
      "Epoch [189/500], Train Loss: 0.0629\n",
      "Epoch [190/500], Train Loss: 0.0623\n",
      "Epoch [191/500], Train Loss: 0.0625\n",
      "Epoch [192/500], Train Loss: 0.0624\n",
      "Epoch [193/500], Train Loss: 0.0619\n",
      "Epoch [194/500], Train Loss: 0.0620\n",
      "Epoch [195/500], Train Loss: 0.0616\n",
      "Epoch [196/500], Train Loss: 0.0613\n",
      "Epoch [197/500], Train Loss: 0.0609\n",
      "Epoch [198/500], Train Loss: 0.0610\n",
      "Epoch [199/500], Train Loss: 0.0608\n",
      "Epoch [200/500], Train Loss: 0.0604\n",
      "Epoch [201/500], Train Loss: 0.0603\n",
      "Epoch [202/500], Train Loss: 0.0599\n",
      "Epoch [203/500], Train Loss: 0.0599\n",
      "Epoch [204/500], Train Loss: 0.0595\n",
      "Epoch [205/500], Train Loss: 0.0593\n",
      "Epoch [206/500], Train Loss: 0.0593\n",
      "Epoch [207/500], Train Loss: 0.0592\n",
      "Epoch [208/500], Train Loss: 0.0589\n",
      "Epoch [209/500], Train Loss: 0.0589\n",
      "Epoch [210/500], Train Loss: 0.0589\n",
      "Epoch [211/500], Train Loss: 0.0585\n",
      "Epoch [212/500], Train Loss: 0.0580\n",
      "Epoch [213/500], Train Loss: 0.0580\n",
      "Epoch [214/500], Train Loss: 0.0579\n",
      "Epoch [215/500], Train Loss: 0.0576\n",
      "Epoch [216/500], Train Loss: 0.0573\n",
      "Epoch [217/500], Train Loss: 0.0572\n",
      "Epoch [218/500], Train Loss: 0.0569\n",
      "Epoch [219/500], Train Loss: 0.0569\n",
      "Epoch [220/500], Train Loss: 0.0568\n",
      "Epoch [221/500], Train Loss: 0.0566\n",
      "Epoch [222/500], Train Loss: 0.0563\n",
      "Epoch [223/500], Train Loss: 0.0563\n",
      "Epoch [224/500], Train Loss: 0.0559\n",
      "Epoch [225/500], Train Loss: 0.0558\n",
      "Epoch [226/500], Train Loss: 0.0557\n",
      "Epoch [227/500], Train Loss: 0.0557\n",
      "Epoch [228/500], Train Loss: 0.0555\n",
      "Epoch [229/500], Train Loss: 0.0553\n",
      "Epoch [230/500], Train Loss: 0.0552\n",
      "Epoch [231/500], Train Loss: 0.0549\n",
      "Epoch [232/500], Train Loss: 0.0547\n",
      "Epoch [233/500], Train Loss: 0.0545\n",
      "Epoch [234/500], Train Loss: 0.0547\n",
      "Epoch [235/500], Train Loss: 0.0543\n",
      "Epoch [236/500], Train Loss: 0.0542\n",
      "Epoch [237/500], Train Loss: 0.0539\n",
      "Epoch [238/500], Train Loss: 0.0540\n",
      "Epoch [239/500], Train Loss: 0.0537\n",
      "Epoch [240/500], Train Loss: 0.0535\n",
      "Epoch [241/500], Train Loss: 0.0535\n",
      "Epoch [242/500], Train Loss: 0.0532\n",
      "Epoch [243/500], Train Loss: 0.0531\n",
      "Epoch [244/500], Train Loss: 0.0529\n",
      "Epoch [245/500], Train Loss: 0.0525\n",
      "Epoch [246/500], Train Loss: 0.0526\n",
      "Epoch [247/500], Train Loss: 0.0524\n",
      "Epoch [248/500], Train Loss: 0.0523\n",
      "Epoch [249/500], Train Loss: 0.0521\n",
      "Epoch [250/500], Train Loss: 0.0521\n",
      "Epoch [251/500], Train Loss: 0.0520\n",
      "Epoch [252/500], Train Loss: 0.0518\n",
      "Epoch [253/500], Train Loss: 0.0516\n",
      "Epoch [254/500], Train Loss: 0.0515\n",
      "Epoch [255/500], Train Loss: 0.0514\n",
      "Epoch [256/500], Train Loss: 0.0511\n",
      "Epoch [257/500], Train Loss: 0.0512\n",
      "Epoch [258/500], Train Loss: 0.0509\n",
      "Epoch [259/500], Train Loss: 0.0509\n",
      "Epoch [260/500], Train Loss: 0.0507\n",
      "Epoch [261/500], Train Loss: 0.0504\n",
      "Epoch [262/500], Train Loss: 0.0504\n",
      "Epoch [263/500], Train Loss: 0.0503\n",
      "Epoch [264/500], Train Loss: 0.0501\n",
      "Epoch [265/500], Train Loss: 0.0501\n",
      "Epoch [266/500], Train Loss: 0.0499\n",
      "Epoch [267/500], Train Loss: 0.0497\n",
      "Epoch [268/500], Train Loss: 0.0496\n",
      "Epoch [269/500], Train Loss: 0.0496\n",
      "Epoch [270/500], Train Loss: 0.0493\n",
      "Epoch [271/500], Train Loss: 0.0493\n",
      "Epoch [272/500], Train Loss: 0.0490\n",
      "Epoch [273/500], Train Loss: 0.0492\n",
      "Epoch [274/500], Train Loss: 0.0488\n",
      "Epoch [275/500], Train Loss: 0.0490\n",
      "Epoch [276/500], Train Loss: 0.0488\n",
      "Epoch [277/500], Train Loss: 0.0487\n",
      "Epoch [278/500], Train Loss: 0.0486\n",
      "Epoch [279/500], Train Loss: 0.0483\n",
      "Epoch [280/500], Train Loss: 0.0483\n",
      "Epoch [281/500], Train Loss: 0.0481\n",
      "Epoch [282/500], Train Loss: 0.0482\n",
      "Epoch [283/500], Train Loss: 0.0481\n",
      "Epoch [284/500], Train Loss: 0.0477\n",
      "Epoch [285/500], Train Loss: 0.0476\n",
      "Epoch [286/500], Train Loss: 0.0477\n",
      "Epoch [287/500], Train Loss: 0.0474\n",
      "Epoch [288/500], Train Loss: 0.0472\n",
      "Epoch [289/500], Train Loss: 0.0472\n",
      "Epoch [290/500], Train Loss: 0.0471\n",
      "Epoch [291/500], Train Loss: 0.0469\n",
      "Epoch [292/500], Train Loss: 0.0468\n",
      "Epoch [293/500], Train Loss: 0.0469\n",
      "Epoch [294/500], Train Loss: 0.0467\n",
      "Epoch [295/500], Train Loss: 0.0466\n",
      "Epoch [296/500], Train Loss: 0.0465\n",
      "Epoch [297/500], Train Loss: 0.0465\n",
      "Epoch [298/500], Train Loss: 0.0463\n",
      "Epoch [299/500], Train Loss: 0.0462\n",
      "Epoch [300/500], Train Loss: 0.0461\n",
      "Epoch [301/500], Train Loss: 0.0461\n",
      "Epoch [302/500], Train Loss: 0.0458\n",
      "Epoch [303/500], Train Loss: 0.0456\n",
      "Epoch [304/500], Train Loss: 0.0456\n",
      "Epoch [305/500], Train Loss: 0.0455\n",
      "Epoch [306/500], Train Loss: 0.0455\n",
      "Epoch [307/500], Train Loss: 0.0454\n",
      "Epoch [308/500], Train Loss: 0.0452\n",
      "Epoch [309/500], Train Loss: 0.0451\n",
      "Epoch [310/500], Train Loss: 0.0450\n",
      "Epoch [311/500], Train Loss: 0.0450\n",
      "Epoch [312/500], Train Loss: 0.0449\n",
      "Epoch [313/500], Train Loss: 0.0447\n",
      "Epoch [314/500], Train Loss: 0.0449\n",
      "Epoch [315/500], Train Loss: 0.0447\n",
      "Epoch [316/500], Train Loss: 0.0444\n",
      "Epoch [317/500], Train Loss: 0.0444\n",
      "Epoch [318/500], Train Loss: 0.0443\n",
      "Epoch [319/500], Train Loss: 0.0444\n",
      "Epoch [320/500], Train Loss: 0.0442\n",
      "Epoch [321/500], Train Loss: 0.0439\n",
      "Epoch [322/500], Train Loss: 0.0440\n",
      "Epoch [323/500], Train Loss: 0.0437\n",
      "Epoch [324/500], Train Loss: 0.0437\n",
      "Epoch [325/500], Train Loss: 0.0438\n",
      "Epoch [326/500], Train Loss: 0.0436\n",
      "Epoch [327/500], Train Loss: 0.0435\n",
      "Epoch [328/500], Train Loss: 0.0434\n",
      "Epoch [329/500], Train Loss: 0.0433\n",
      "Epoch [330/500], Train Loss: 0.0433\n",
      "Epoch [331/500], Train Loss: 0.0432\n",
      "Epoch [332/500], Train Loss: 0.0430\n",
      "Epoch [333/500], Train Loss: 0.0429\n",
      "Epoch [334/500], Train Loss: 0.0429\n",
      "Epoch [335/500], Train Loss: 0.0428\n",
      "Epoch [336/500], Train Loss: 0.0428\n",
      "Epoch [337/500], Train Loss: 0.0427\n",
      "Epoch [338/500], Train Loss: 0.0425\n",
      "Epoch [339/500], Train Loss: 0.0425\n",
      "Epoch [340/500], Train Loss: 0.0425\n",
      "Epoch [341/500], Train Loss: 0.0426\n",
      "Epoch [342/500], Train Loss: 0.0423\n",
      "Epoch [343/500], Train Loss: 0.0422\n",
      "Epoch [344/500], Train Loss: 0.0421\n",
      "Epoch [345/500], Train Loss: 0.0420\n",
      "Epoch [346/500], Train Loss: 0.0419\n",
      "Epoch [347/500], Train Loss: 0.0420\n",
      "Epoch [348/500], Train Loss: 0.0420\n",
      "Epoch [349/500], Train Loss: 0.0418\n",
      "Epoch [350/500], Train Loss: 0.0418\n",
      "Epoch [351/500], Train Loss: 0.0416\n",
      "Epoch [352/500], Train Loss: 0.0416\n",
      "Epoch [353/500], Train Loss: 0.0415\n",
      "Epoch [354/500], Train Loss: 0.0414\n",
      "Epoch [355/500], Train Loss: 0.0413\n",
      "Epoch [356/500], Train Loss: 0.0412\n",
      "Epoch [357/500], Train Loss: 0.0411\n",
      "Epoch [358/500], Train Loss: 0.0410\n",
      "Epoch [359/500], Train Loss: 0.0409\n",
      "Epoch [360/500], Train Loss: 0.0408\n",
      "Epoch [361/500], Train Loss: 0.0409\n",
      "Epoch [362/500], Train Loss: 0.0408\n",
      "Epoch [363/500], Train Loss: 0.0408\n",
      "Epoch [364/500], Train Loss: 0.0407\n",
      "Epoch [365/500], Train Loss: 0.0406\n",
      "Epoch [366/500], Train Loss: 0.0406\n",
      "Epoch [367/500], Train Loss: 0.0402\n",
      "Epoch [368/500], Train Loss: 0.0404\n",
      "Epoch [369/500], Train Loss: 0.0403\n",
      "Epoch [370/500], Train Loss: 0.0403\n",
      "Epoch [371/500], Train Loss: 0.0400\n",
      "Epoch [372/500], Train Loss: 0.0403\n",
      "Epoch [373/500], Train Loss: 0.0402\n",
      "Epoch [374/500], Train Loss: 0.0400\n",
      "Epoch [375/500], Train Loss: 0.0399\n",
      "Epoch [376/500], Train Loss: 0.0398\n",
      "Epoch [377/500], Train Loss: 0.0398\n",
      "Epoch [378/500], Train Loss: 0.0398\n",
      "Epoch [379/500], Train Loss: 0.0395\n",
      "Epoch [380/500], Train Loss: 0.0396\n",
      "Epoch [381/500], Train Loss: 0.0394\n",
      "Epoch [382/500], Train Loss: 0.0394\n",
      "Epoch [383/500], Train Loss: 0.0393\n",
      "Epoch [384/500], Train Loss: 0.0395\n",
      "Epoch [385/500], Train Loss: 0.0391\n",
      "Epoch [386/500], Train Loss: 0.0391\n",
      "Epoch [387/500], Train Loss: 0.0390\n",
      "Epoch [388/500], Train Loss: 0.0390\n",
      "Epoch [389/500], Train Loss: 0.0390\n",
      "Epoch [390/500], Train Loss: 0.0389\n",
      "Epoch [391/500], Train Loss: 0.0388\n",
      "Epoch [392/500], Train Loss: 0.0389\n",
      "Epoch [393/500], Train Loss: 0.0388\n",
      "Epoch [394/500], Train Loss: 0.0387\n",
      "Epoch [395/500], Train Loss: 0.0386\n",
      "Epoch [396/500], Train Loss: 0.0385\n",
      "Epoch [397/500], Train Loss: 0.0386\n",
      "Epoch [398/500], Train Loss: 0.0384\n",
      "Epoch [399/500], Train Loss: 0.0384\n",
      "Epoch [400/500], Train Loss: 0.0384\n",
      "Epoch [401/500], Train Loss: 0.0384\n",
      "Epoch [402/500], Train Loss: 0.0383\n",
      "Epoch [403/500], Train Loss: 0.0381\n",
      "Epoch [404/500], Train Loss: 0.0381\n",
      "Epoch [405/500], Train Loss: 0.0380\n",
      "Epoch [406/500], Train Loss: 0.0381\n",
      "Epoch [407/500], Train Loss: 0.0379\n",
      "Epoch [408/500], Train Loss: 0.0377\n",
      "Epoch [409/500], Train Loss: 0.0379\n",
      "Epoch [410/500], Train Loss: 0.0378\n",
      "Epoch [411/500], Train Loss: 0.0376\n",
      "Epoch [412/500], Train Loss: 0.0377\n",
      "Epoch [413/500], Train Loss: 0.0376\n",
      "Epoch [414/500], Train Loss: 0.0375\n",
      "Epoch [415/500], Train Loss: 0.0374\n",
      "Epoch [416/500], Train Loss: 0.0376\n",
      "Epoch [417/500], Train Loss: 0.0373\n",
      "Epoch [418/500], Train Loss: 0.0373\n",
      "Epoch [419/500], Train Loss: 0.0374\n",
      "Epoch [420/500], Train Loss: 0.0372\n",
      "Epoch [421/500], Train Loss: 0.0371\n",
      "Epoch [422/500], Train Loss: 0.0370\n",
      "Epoch [423/500], Train Loss: 0.0370\n",
      "Epoch [424/500], Train Loss: 0.0370\n",
      "Epoch [425/500], Train Loss: 0.0370\n",
      "Epoch [426/500], Train Loss: 0.0370\n",
      "Epoch [427/500], Train Loss: 0.0368\n",
      "Epoch [428/500], Train Loss: 0.0369\n",
      "Epoch [429/500], Train Loss: 0.0366\n",
      "Epoch [430/500], Train Loss: 0.0367\n",
      "Epoch [431/500], Train Loss: 0.0366\n",
      "Epoch [432/500], Train Loss: 0.0364\n",
      "Epoch [433/500], Train Loss: 0.0365\n",
      "Epoch [434/500], Train Loss: 0.0365\n",
      "Epoch [435/500], Train Loss: 0.0363\n",
      "Epoch [436/500], Train Loss: 0.0363\n",
      "Epoch [437/500], Train Loss: 0.0363\n",
      "Epoch [438/500], Train Loss: 0.0362\n",
      "Epoch [439/500], Train Loss: 0.0361\n",
      "Epoch [440/500], Train Loss: 0.0363\n",
      "Epoch [441/500], Train Loss: 0.0361\n",
      "Epoch [442/500], Train Loss: 0.0361\n",
      "Epoch [443/500], Train Loss: 0.0360\n",
      "Epoch [444/500], Train Loss: 0.0359\n",
      "Epoch [445/500], Train Loss: 0.0357\n",
      "Epoch [446/500], Train Loss: 0.0358\n",
      "Epoch [447/500], Train Loss: 0.0360\n",
      "Epoch [448/500], Train Loss: 0.0359\n",
      "Epoch [449/500], Train Loss: 0.0357\n",
      "Epoch [450/500], Train Loss: 0.0357\n",
      "Epoch [451/500], Train Loss: 0.0357\n",
      "Epoch [452/500], Train Loss: 0.0355\n",
      "Epoch [453/500], Train Loss: 0.0355\n",
      "Epoch [454/500], Train Loss: 0.0356\n",
      "Epoch [455/500], Train Loss: 0.0354\n",
      "Epoch [456/500], Train Loss: 0.0356\n",
      "Epoch [457/500], Train Loss: 0.0355\n",
      "Epoch [458/500], Train Loss: 0.0353\n",
      "Epoch [459/500], Train Loss: 0.0354\n",
      "Epoch [460/500], Train Loss: 0.0353\n",
      "Epoch [461/500], Train Loss: 0.0352\n",
      "Epoch [462/500], Train Loss: 0.0353\n",
      "Epoch [463/500], Train Loss: 0.0351\n",
      "Epoch [464/500], Train Loss: 0.0351\n",
      "Epoch [465/500], Train Loss: 0.0350\n",
      "Epoch [466/500], Train Loss: 0.0350\n",
      "Epoch [467/500], Train Loss: 0.0349\n",
      "Epoch [468/500], Train Loss: 0.0349\n",
      "Epoch [469/500], Train Loss: 0.0350\n",
      "Epoch [470/500], Train Loss: 0.0347\n",
      "Epoch [471/500], Train Loss: 0.0346\n",
      "Epoch [472/500], Train Loss: 0.0347\n",
      "Epoch [473/500], Train Loss: 0.0346\n",
      "Epoch [474/500], Train Loss: 0.0346\n",
      "Epoch [475/500], Train Loss: 0.0346\n",
      "Epoch [476/500], Train Loss: 0.0347\n",
      "Epoch [477/500], Train Loss: 0.0346\n",
      "Epoch [478/500], Train Loss: 0.0345\n",
      "Epoch [479/500], Train Loss: 0.0344\n",
      "Epoch [480/500], Train Loss: 0.0345\n",
      "Epoch [481/500], Train Loss: 0.0342\n",
      "Epoch [482/500], Train Loss: 0.0344\n",
      "Epoch [483/500], Train Loss: 0.0343\n",
      "Epoch [484/500], Train Loss: 0.0344\n",
      "Epoch [485/500], Train Loss: 0.0344\n",
      "Epoch [486/500], Train Loss: 0.0343\n",
      "Epoch [487/500], Train Loss: 0.0341\n",
      "Epoch [488/500], Train Loss: 0.0341\n",
      "Epoch [489/500], Train Loss: 0.0341\n",
      "Epoch [490/500], Train Loss: 0.0339\n",
      "Epoch [491/500], Train Loss: 0.0339\n",
      "Epoch [492/500], Train Loss: 0.0338\n",
      "Epoch [493/500], Train Loss: 0.0340\n",
      "Epoch [494/500], Train Loss: 0.0339\n",
      "Epoch [495/500], Train Loss: 0.0340\n",
      "Epoch [496/500], Train Loss: 0.0337\n",
      "Epoch [497/500], Train Loss: 0.0337\n",
      "Epoch [498/500], Train Loss: 0.0338\n",
      "Epoch [499/500], Train Loss: 0.0337\n",
      "Epoch [500/500], Train Loss: 0.0337\n",
      "Final model saved at: ../../Data/model_results/logistic_regression/models/chr19/final_model_chr19.pth\n",
      "Individual R^2 scores saved at: ../../Data/model_results/logistic_regression/csv_files/chr19/individual_r2_scores_chr19.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:49:41,742] A new study created in memory with name: no-name-971f85c9-81b3-4b27-8f1b-934701dcef08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual AUC ROC curves saved in: ../../Data/model_results/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../Data/model_results/logistic_regression/csv_files/chr19/performance_metrics.csv\n",
      "Unknown PRS313 SNPs:  8\n",
      "Known PRS313 SNPs:  2\n",
      "23AndMe SNPs with LD to Unknown PRS313 SNPs:  150\n",
      "Early stopping at epoch 170\n",
      "Early stopping at epoch 13\n",
      "Early stopping at epoch 13\n",
      "Early stopping at epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:49:43,686] Trial 0 finished with value: 0.10321658737957477 and parameters: {'learning_rate': 0.006607404801870975, 'lasso_coef': 8.259765716660426e-05, 'patience': 11}. Best is trial 0 with value: 0.10321658737957477.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 12\n",
      "Early stopping at epoch 46\n",
      "Early stopping at epoch 20\n",
      "Early stopping at epoch 20\n",
      "Early stopping at epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:49:44,874] Trial 1 finished with value: 0.09008009135723113 and parameters: {'learning_rate': 0.05558895029304008, 'lasso_coef': 1.0918242550317067e-05, 'patience': 19}. Best is trial 1 with value: 0.09008009135723113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 20\n",
      "Early stopping at epoch 143\n",
      "Early stopping at epoch 21\n",
      "Early stopping at epoch 25\n",
      "Early stopping at epoch 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:49:46,998] Trial 2 finished with value: 0.23870069310069084 and parameters: {'learning_rate': 0.008609497071316424, 'lasso_coef': 0.002971064084343805, 'patience': 20}. Best is trial 1 with value: 0.09008009135723113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 29\n",
      "Early stopping at epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:50:00,482] Trial 3 finished with value: 0.15553311221301555 and parameters: {'learning_rate': 0.00015463543348536381, 'lasso_coef': 0.0002967894180873436, 'patience': 13}. Best is trial 1 with value: 0.09008009135723113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 14\n",
      "Early stopping at epoch 217\n",
      "Early stopping at epoch 27\n",
      "Early stopping at epoch 25\n",
      "Early stopping at epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:50:03,180] Trial 4 finished with value: 0.1991207294166088 and parameters: {'learning_rate': 0.007403840524538877, 'lasso_coef': 0.0014554518252338099, 'patience': 18}. Best is trial 1 with value: 0.09008009135723113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 20\n",
      "Early stopping at epoch 47\n",
      "Early stopping at epoch 6\n",
      "Early stopping at epoch 7\n",
      "Early stopping at epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:50:03,854] Trial 5 finished with value: 0.27440817654132843 and parameters: {'learning_rate': 0.01722879263417169, 'lasso_coef': 0.006426258410962235, 'patience': 5}. Best is trial 1 with value: 0.09008009135723113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10\n",
      "Early stopping at epoch 155\n",
      "Early stopping at epoch 12\n",
      "Early stopping at epoch 12\n",
      "Early stopping at epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:50:05,637] Trial 6 finished with value: 0.10074092820286751 and parameters: {'learning_rate': 0.004993051031095147, 'lasso_coef': 6.359631488444077e-05, 'patience': 11}. Best is trial 1 with value: 0.09008009135723113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 12\n",
      "Early stopping at epoch 335\n",
      "Early stopping at epoch 25\n",
      "Early stopping at epoch 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:50:13,733] Trial 7 finished with value: 0.27310377433896066 and parameters: {'learning_rate': 0.0005571003010653208, 'lasso_coef': 0.0884218986183671, 'patience': 12}. Best is trial 1 with value: 0.09008009135723113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 32\n",
      "Early stopping at epoch 10\n",
      "Early stopping at epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:50:18,585] Trial 8 finished with value: 0.1977499671280384 and parameters: {'learning_rate': 0.0008079974116393216, 'lasso_coef': 0.0013564267900933104, 'patience': 8}. Best is trial 1 with value: 0.09008009135723113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 9\n",
      "Early stopping at epoch 9\n",
      "Early stopping at epoch 56\n",
      "Early stopping at epoch 27\n",
      "Early stopping at epoch 66\n",
      "Early stopping at epoch 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:50:20,414] Trial 9 finished with value: 0.28382343500852586 and parameters: {'learning_rate': 0.05486615134206935, 'lasso_coef': 0.004287972505379325, 'patience': 20}. Best is trial 1 with value: 0.09008009135723113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 36\n",
      "Early stopping at epoch 47\n",
      "Early stopping at epoch 18\n",
      "Early stopping at epoch 18\n",
      "Early stopping at epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:50:21,466] Trial 10 finished with value: 0.0981240700930357 and parameters: {'learning_rate': 0.09744873182205314, 'lasso_coef': 1.4491304779095791e-05, 'patience': 17}. Best is trial 1 with value: 0.09008009135723113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 18\n",
      "Early stopping at epoch 50\n",
      "Early stopping at epoch 17\n",
      "Early stopping at epoch 17\n",
      "Early stopping at epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:50:22,513] Trial 11 finished with value: 0.09516208302229642 and parameters: {'learning_rate': 0.09048378078001336, 'lasso_coef': 1.1354957502283344e-05, 'patience': 16}. Best is trial 1 with value: 0.09008009135723113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 18\n",
      "Early stopping at epoch 52\n",
      "Early stopping at epoch 17\n",
      "Early stopping at epoch 17\n",
      "Early stopping at epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:50:23,577] Trial 12 finished with value: 0.08903554640710354 and parameters: {'learning_rate': 0.035403468072068296, 'lasso_coef': 1.2768565997598587e-05, 'patience': 16}. Best is trial 12 with value: 0.08903554640710354.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 17\n",
      "Early stopping at epoch 67\n",
      "Early stopping at epoch 16\n",
      "Early stopping at epoch 16\n",
      "Early stopping at epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:50:24,730] Trial 13 finished with value: 0.09871350396424532 and parameters: {'learning_rate': 0.026848701272146475, 'lasso_coef': 4.498768503929001e-05, 'patience': 15}. Best is trial 12 with value: 0.08903554640710354.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 16\n",
      "Early stopping at epoch 69\n",
      "Early stopping at epoch 15\n",
      "Early stopping at epoch 17\n",
      "Early stopping at epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:50:25,876] Trial 14 finished with value: 0.13132682666182519 and parameters: {'learning_rate': 0.03150265803756505, 'lasso_coef': 0.00026789396409298774, 'patience': 14}. Best is trial 12 with value: 0.08903554640710354.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15\n",
      "Early stopping at epoch 19\n",
      "Early stopping at epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:50:31,037] Trial 15 finished with value: 0.09063189066946506 and parameters: {'learning_rate': 0.0016202918215597985, 'lasso_coef': 2.677950522189289e-05, 'patience': 18}. Best is trial 12 with value: 0.08903554640710354.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 20\n",
      "Early stopping at epoch 19\n",
      "Early stopping at epoch 110\n",
      "Early stopping at epoch 22\n",
      "Early stopping at epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:50:32,911] Trial 16 finished with value: 0.12917991988360883 and parameters: {'learning_rate': 0.015608754436864382, 'lasso_coef': 0.0002699513357488135, 'patience': 19}. Best is trial 12 with value: 0.08903554640710354.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 21\n",
      "Early stopping at epoch 20\n",
      "Early stopping at epoch 278\n",
      "Early stopping at epoch 58\n",
      "Early stopping at epoch 42\n",
      "Early stopping at epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:50:36,727] Trial 17 finished with value: 0.2726046785712242 and parameters: {'learning_rate': 0.002333816056456357, 'lasso_coef': 0.02843662904550877, 'patience': 16}. Best is trial 12 with value: 0.08903554640710354.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 27\n",
      "Early stopping at epoch 31\n",
      "Early stopping at epoch 10\n",
      "Early stopping at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:50:37,365] Trial 18 finished with value: 0.11320972442626953 and parameters: {'learning_rate': 0.039655133437929514, 'lasso_coef': 0.00012467595275598723, 'patience': 9}. Best is trial 12 with value: 0.08903554640710354.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10\n",
      "Early stopping at epoch 10\n",
      "Early stopping at epoch 105\n",
      "Early stopping at epoch 17\n",
      "Early stopping at epoch 16\n",
      "Early stopping at epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:50:38,921] Trial 19 finished with value: 0.09194363579154015 and parameters: {'learning_rate': 0.012545577004637026, 'lasso_coef': 2.5539023571234707e-05, 'patience': 15}. Best is trial 12 with value: 0.08903554640710354.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 16\n",
      "Early stopping at epoch 82\n",
      "Early stopping at epoch 23\n",
      "Early stopping at epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:50:40,387] Trial 20 finished with value: 0.1569675512611866 and parameters: {'learning_rate': 0.055812541121543824, 'lasso_coef': 0.0005073397283032548, 'patience': 18}. Best is trial 12 with value: 0.08903554640710354.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 21\n",
      "Early stopping at epoch 19\n",
      "Early stopping at epoch 20\n",
      "Early stopping at epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:50:45,480] Trial 21 finished with value: 0.08954942412674427 and parameters: {'learning_rate': 0.0019165805066142455, 'lasso_coef': 2.4705928632607438e-05, 'patience': 18}. Best is trial 12 with value: 0.08903554640710354.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 19\n",
      "Early stopping at epoch 19\n",
      "Early stopping at epoch 20\n",
      "Early stopping at epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:50:50,799] Trial 22 finished with value: 0.08937033470720053 and parameters: {'learning_rate': 0.0011182425843249803, 'lasso_coef': 1.1713920462284089e-05, 'patience': 17}. Best is trial 12 with value: 0.08903554640710354.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 18\n",
      "Early stopping at epoch 18\n",
      "Early stopping at epoch 17\n",
      "Early stopping at epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:50:56,105] Trial 23 finished with value: 0.09556480869650841 and parameters: {'learning_rate': 0.0009002989138093772, 'lasso_coef': 2.7421829846183056e-05, 'patience': 16}. Best is trial 12 with value: 0.08903554640710354.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 17\n",
      "Early stopping at epoch 17\n",
      "Early stopping at epoch 15\n",
      "Early stopping at epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:51:05,571] Trial 24 finished with value: 0.1259193394333124 and parameters: {'learning_rate': 0.000256570149498118, 'lasso_coef': 0.00011552994175749572, 'patience': 14}. Best is trial 12 with value: 0.08903554640710354.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15\n",
      "Early stopping at epoch 315\n",
      "Early stopping at epoch 18\n",
      "Early stopping at epoch 18\n",
      "Early stopping at epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:51:09,071] Trial 25 finished with value: 0.09221717864274978 and parameters: {'learning_rate': 0.003359461215478263, 'lasso_coef': 3.362148860830136e-05, 'patience': 17}. Best is trial 12 with value: 0.08903554640710354.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 18\n",
      "Early stopping at epoch 18\n",
      "Early stopping at epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:51:14,134] Trial 26 finished with value: 0.08838827908039093 and parameters: {'learning_rate': 0.001570842809185977, 'lasso_coef': 1.7344440325439046e-05, 'patience': 17}. Best is trial 26 with value: 0.08838827908039093.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 18\n",
      "Early stopping at epoch 18\n",
      "Early stopping at epoch 15\n",
      "Early stopping at epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:51:19,145] Trial 27 finished with value: 0.10261464938521385 and parameters: {'learning_rate': 0.000533571383693736, 'lasso_coef': 1.4560195250750986e-05, 'patience': 14}. Best is trial 26 with value: 0.08838827908039093.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15\n",
      "Early stopping at epoch 15\n",
      "Early stopping at epoch 19\n",
      "Early stopping at epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:51:24,211] Trial 28 finished with value: 0.11261701621115208 and parameters: {'learning_rate': 0.0010352824854580206, 'lasso_coef': 0.0001373364947941393, 'patience': 17}. Best is trial 26 with value: 0.08838827908039093.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 18\n",
      "Early stopping at epoch 18\n",
      "Early stopping at epoch 247\n",
      "Early stopping at epoch 16\n",
      "Early stopping at epoch 16\n",
      "Early stopping at epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:51:26,953] Trial 29 finished with value: 0.1009027749300003 and parameters: {'learning_rate': 0.003873825956720056, 'lasso_coef': 7.257397762310641e-05, 'patience': 15}. Best is trial 26 with value: 0.08838827908039093.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 16\n",
      "Best hyperparameters: {'learning_rate': 0.001570842809185977, 'lasso_coef': 1.7344440325439046e-05, 'patience': 17}\n",
      "Best value: 0.08838827908039093\n",
      "Epoch [1/500], Train Loss: 0.5186\n",
      "Epoch [2/500], Train Loss: 0.3139\n",
      "Epoch [3/500], Train Loss: 0.2716\n",
      "Epoch [4/500], Train Loss: 0.2599\n",
      "Epoch [5/500], Train Loss: 0.2515\n",
      "Epoch [6/500], Train Loss: 0.2455\n",
      "Epoch [7/500], Train Loss: 0.2420\n",
      "Epoch [8/500], Train Loss: 0.2388\n",
      "Epoch [9/500], Train Loss: 0.2354\n",
      "Epoch [10/500], Train Loss: 0.2326\n",
      "Epoch [11/500], Train Loss: 0.2283\n",
      "Epoch [12/500], Train Loss: 0.2261\n",
      "Epoch [13/500], Train Loss: 0.2237\n",
      "Epoch [14/500], Train Loss: 0.2206\n",
      "Epoch [15/500], Train Loss: 0.2177\n",
      "Epoch [16/500], Train Loss: 0.2152\n",
      "Epoch [17/500], Train Loss: 0.2127\n",
      "Epoch [18/500], Train Loss: 0.2111\n",
      "Epoch [19/500], Train Loss: 0.2083\n",
      "Epoch [20/500], Train Loss: 0.2065\n",
      "Epoch [21/500], Train Loss: 0.2041\n",
      "Epoch [22/500], Train Loss: 0.2015\n",
      "Epoch [23/500], Train Loss: 0.1995\n",
      "Epoch [24/500], Train Loss: 0.1967\n",
      "Epoch [25/500], Train Loss: 0.1955\n",
      "Epoch [26/500], Train Loss: 0.1935\n",
      "Epoch [27/500], Train Loss: 0.1920\n",
      "Epoch [28/500], Train Loss: 0.1894\n",
      "Epoch [29/500], Train Loss: 0.1879\n",
      "Epoch [30/500], Train Loss: 0.1864\n",
      "Epoch [31/500], Train Loss: 0.1840\n",
      "Epoch [32/500], Train Loss: 0.1823\n",
      "Epoch [33/500], Train Loss: 0.1806\n",
      "Epoch [34/500], Train Loss: 0.1789\n",
      "Epoch [35/500], Train Loss: 0.1772\n",
      "Epoch [36/500], Train Loss: 0.1756\n",
      "Epoch [37/500], Train Loss: 0.1746\n",
      "Epoch [38/500], Train Loss: 0.1730\n",
      "Epoch [39/500], Train Loss: 0.1712\n",
      "Epoch [40/500], Train Loss: 0.1702\n",
      "Epoch [41/500], Train Loss: 0.1685\n",
      "Epoch [42/500], Train Loss: 0.1673\n",
      "Epoch [43/500], Train Loss: 0.1662\n",
      "Epoch [44/500], Train Loss: 0.1641\n",
      "Epoch [45/500], Train Loss: 0.1632\n",
      "Epoch [46/500], Train Loss: 0.1625\n",
      "Epoch [47/500], Train Loss: 0.1611\n",
      "Epoch [48/500], Train Loss: 0.1601\n",
      "Epoch [49/500], Train Loss: 0.1582\n",
      "Epoch [50/500], Train Loss: 0.1575\n",
      "Epoch [51/500], Train Loss: 0.1560\n",
      "Epoch [52/500], Train Loss: 0.1554\n",
      "Epoch [53/500], Train Loss: 0.1544\n",
      "Epoch [54/500], Train Loss: 0.1529\n",
      "Epoch [55/500], Train Loss: 0.1524\n",
      "Epoch [56/500], Train Loss: 0.1511\n",
      "Epoch [57/500], Train Loss: 0.1501\n",
      "Epoch [58/500], Train Loss: 0.1496\n",
      "Epoch [59/500], Train Loss: 0.1485\n",
      "Epoch [60/500], Train Loss: 0.1481\n",
      "Epoch [61/500], Train Loss: 0.1469\n",
      "Epoch [62/500], Train Loss: 0.1459\n",
      "Epoch [63/500], Train Loss: 0.1452\n",
      "Epoch [64/500], Train Loss: 0.1440\n",
      "Epoch [65/500], Train Loss: 0.1434\n",
      "Epoch [66/500], Train Loss: 0.1422\n",
      "Epoch [67/500], Train Loss: 0.1420\n",
      "Epoch [68/500], Train Loss: 0.1410\n",
      "Epoch [69/500], Train Loss: 0.1404\n",
      "Epoch [70/500], Train Loss: 0.1393\n",
      "Epoch [71/500], Train Loss: 0.1388\n",
      "Epoch [72/500], Train Loss: 0.1378\n",
      "Epoch [73/500], Train Loss: 0.1373\n",
      "Epoch [74/500], Train Loss: 0.1364\n",
      "Epoch [75/500], Train Loss: 0.1360\n",
      "Epoch [76/500], Train Loss: 0.1352\n",
      "Epoch [77/500], Train Loss: 0.1339\n",
      "Epoch [78/500], Train Loss: 0.1340\n",
      "Epoch [79/500], Train Loss: 0.1334\n",
      "Epoch [80/500], Train Loss: 0.1327\n",
      "Epoch [81/500], Train Loss: 0.1323\n",
      "Epoch [82/500], Train Loss: 0.1315\n",
      "Epoch [83/500], Train Loss: 0.1310\n",
      "Epoch [84/500], Train Loss: 0.1307\n",
      "Epoch [85/500], Train Loss: 0.1299\n",
      "Epoch [86/500], Train Loss: 0.1296\n",
      "Epoch [87/500], Train Loss: 0.1289\n",
      "Epoch [88/500], Train Loss: 0.1282\n",
      "Epoch [89/500], Train Loss: 0.1280\n",
      "Epoch [90/500], Train Loss: 0.1273\n",
      "Epoch [91/500], Train Loss: 0.1272\n",
      "Epoch [92/500], Train Loss: 0.1262\n",
      "Epoch [93/500], Train Loss: 0.1262\n",
      "Epoch [94/500], Train Loss: 0.1256\n",
      "Epoch [95/500], Train Loss: 0.1251\n",
      "Epoch [96/500], Train Loss: 0.1242\n",
      "Epoch [97/500], Train Loss: 0.1241\n",
      "Epoch [98/500], Train Loss: 0.1239\n",
      "Epoch [99/500], Train Loss: 0.1229\n",
      "Epoch [100/500], Train Loss: 0.1227\n",
      "Epoch [101/500], Train Loss: 0.1224\n",
      "Epoch [102/500], Train Loss: 0.1215\n",
      "Epoch [103/500], Train Loss: 0.1210\n",
      "Epoch [104/500], Train Loss: 0.1211\n",
      "Epoch [105/500], Train Loss: 0.1209\n",
      "Epoch [106/500], Train Loss: 0.1201\n",
      "Epoch [107/500], Train Loss: 0.1201\n",
      "Epoch [108/500], Train Loss: 0.1193\n",
      "Epoch [109/500], Train Loss: 0.1190\n",
      "Epoch [110/500], Train Loss: 0.1183\n",
      "Epoch [111/500], Train Loss: 0.1180\n",
      "Epoch [112/500], Train Loss: 0.1183\n",
      "Epoch [113/500], Train Loss: 0.1170\n",
      "Epoch [114/500], Train Loss: 0.1174\n",
      "Epoch [115/500], Train Loss: 0.1165\n",
      "Epoch [116/500], Train Loss: 0.1164\n",
      "Epoch [117/500], Train Loss: 0.1160\n",
      "Epoch [118/500], Train Loss: 0.1160\n",
      "Epoch [119/500], Train Loss: 0.1164\n",
      "Epoch [120/500], Train Loss: 0.1156\n",
      "Epoch [121/500], Train Loss: 0.1149\n",
      "Epoch [122/500], Train Loss: 0.1149\n",
      "Epoch [123/500], Train Loss: 0.1146\n",
      "Epoch [124/500], Train Loss: 0.1139\n",
      "Epoch [125/500], Train Loss: 0.1136\n",
      "Epoch [126/500], Train Loss: 0.1132\n",
      "Epoch [127/500], Train Loss: 0.1132\n",
      "Epoch [128/500], Train Loss: 0.1126\n",
      "Epoch [129/500], Train Loss: 0.1124\n",
      "Epoch [130/500], Train Loss: 0.1124\n",
      "Epoch [131/500], Train Loss: 0.1123\n",
      "Epoch [132/500], Train Loss: 0.1117\n",
      "Epoch [133/500], Train Loss: 0.1112\n",
      "Epoch [134/500], Train Loss: 0.1109\n",
      "Epoch [135/500], Train Loss: 0.1113\n",
      "Epoch [136/500], Train Loss: 0.1106\n",
      "Epoch [137/500], Train Loss: 0.1099\n",
      "Epoch [138/500], Train Loss: 0.1100\n",
      "Epoch [139/500], Train Loss: 0.1102\n",
      "Epoch [140/500], Train Loss: 0.1097\n",
      "Epoch [141/500], Train Loss: 0.1096\n",
      "Epoch [142/500], Train Loss: 0.1090\n",
      "Epoch [143/500], Train Loss: 0.1089\n",
      "Epoch [144/500], Train Loss: 0.1087\n",
      "Epoch [145/500], Train Loss: 0.1087\n",
      "Epoch [146/500], Train Loss: 0.1082\n",
      "Epoch [147/500], Train Loss: 0.1081\n",
      "Epoch [148/500], Train Loss: 0.1081\n",
      "Epoch [149/500], Train Loss: 0.1074\n",
      "Epoch [150/500], Train Loss: 0.1071\n",
      "Epoch [151/500], Train Loss: 0.1071\n",
      "Epoch [152/500], Train Loss: 0.1068\n",
      "Epoch [153/500], Train Loss: 0.1071\n",
      "Epoch [154/500], Train Loss: 0.1065\n",
      "Epoch [155/500], Train Loss: 0.1061\n",
      "Epoch [156/500], Train Loss: 0.1060\n",
      "Epoch [157/500], Train Loss: 0.1058\n",
      "Epoch [158/500], Train Loss: 0.1056\n",
      "Epoch [159/500], Train Loss: 0.1053\n",
      "Epoch [160/500], Train Loss: 0.1055\n",
      "Epoch [161/500], Train Loss: 0.1054\n",
      "Epoch [162/500], Train Loss: 0.1049\n",
      "Epoch [163/500], Train Loss: 0.1048\n",
      "Epoch [164/500], Train Loss: 0.1045\n",
      "Epoch [165/500], Train Loss: 0.1042\n",
      "Epoch [166/500], Train Loss: 0.1042\n",
      "Epoch [167/500], Train Loss: 0.1041\n",
      "Epoch [168/500], Train Loss: 0.1039\n",
      "Epoch [169/500], Train Loss: 0.1034\n",
      "Epoch [170/500], Train Loss: 0.1036\n",
      "Epoch [171/500], Train Loss: 0.1036\n",
      "Epoch [172/500], Train Loss: 0.1034\n",
      "Epoch [173/500], Train Loss: 0.1035\n",
      "Epoch [174/500], Train Loss: 0.1028\n",
      "Epoch [175/500], Train Loss: 0.1030\n",
      "Epoch [176/500], Train Loss: 0.1025\n",
      "Epoch [177/500], Train Loss: 0.1030\n",
      "Epoch [178/500], Train Loss: 0.1022\n",
      "Epoch [179/500], Train Loss: 0.1020\n",
      "Epoch [180/500], Train Loss: 0.1018\n",
      "Epoch [181/500], Train Loss: 0.1016\n",
      "Epoch [182/500], Train Loss: 0.1015\n",
      "Epoch [183/500], Train Loss: 0.1017\n",
      "Epoch [184/500], Train Loss: 0.1015\n",
      "Epoch [185/500], Train Loss: 0.1012\n",
      "Epoch [186/500], Train Loss: 0.1015\n",
      "Epoch [187/500], Train Loss: 0.1007\n",
      "Epoch [188/500], Train Loss: 0.1014\n",
      "Epoch [189/500], Train Loss: 0.1014\n",
      "Epoch [190/500], Train Loss: 0.1010\n",
      "Epoch [191/500], Train Loss: 0.1005\n",
      "Epoch [192/500], Train Loss: 0.1001\n",
      "Epoch [193/500], Train Loss: 0.1001\n",
      "Epoch [194/500], Train Loss: 0.1002\n",
      "Epoch [195/500], Train Loss: 0.1004\n",
      "Epoch [196/500], Train Loss: 0.0996\n",
      "Epoch [197/500], Train Loss: 0.0998\n",
      "Epoch [198/500], Train Loss: 0.0996\n",
      "Epoch [199/500], Train Loss: 0.0996\n",
      "Epoch [200/500], Train Loss: 0.0994\n",
      "Epoch [201/500], Train Loss: 0.0993\n",
      "Epoch [202/500], Train Loss: 0.0993\n",
      "Epoch [203/500], Train Loss: 0.0989\n",
      "Epoch [204/500], Train Loss: 0.0990\n",
      "Epoch [205/500], Train Loss: 0.0987\n",
      "Epoch [206/500], Train Loss: 0.0984\n",
      "Epoch [207/500], Train Loss: 0.0985\n",
      "Epoch [208/500], Train Loss: 0.0986\n",
      "Epoch [209/500], Train Loss: 0.0982\n",
      "Epoch [210/500], Train Loss: 0.0983\n",
      "Epoch [211/500], Train Loss: 0.0977\n",
      "Epoch [212/500], Train Loss: 0.0982\n",
      "Epoch [213/500], Train Loss: 0.0979\n",
      "Epoch [214/500], Train Loss: 0.0975\n",
      "Epoch [215/500], Train Loss: 0.0974\n",
      "Epoch [216/500], Train Loss: 0.0975\n",
      "Epoch [217/500], Train Loss: 0.0975\n",
      "Epoch [218/500], Train Loss: 0.0979\n",
      "Epoch [219/500], Train Loss: 0.0974\n",
      "Epoch [220/500], Train Loss: 0.0968\n",
      "Epoch [221/500], Train Loss: 0.0973\n",
      "Epoch [222/500], Train Loss: 0.0971\n",
      "Epoch [223/500], Train Loss: 0.0968\n",
      "Epoch [224/500], Train Loss: 0.0967\n",
      "Epoch [225/500], Train Loss: 0.0967\n",
      "Epoch [226/500], Train Loss: 0.0966\n",
      "Epoch [227/500], Train Loss: 0.0965\n",
      "Epoch [228/500], Train Loss: 0.0966\n",
      "Epoch [229/500], Train Loss: 0.0964\n",
      "Epoch [230/500], Train Loss: 0.0956\n",
      "Epoch [231/500], Train Loss: 0.0957\n",
      "Epoch [232/500], Train Loss: 0.0961\n",
      "Epoch [233/500], Train Loss: 0.0955\n",
      "Epoch [234/500], Train Loss: 0.0961\n",
      "Epoch [235/500], Train Loss: 0.0957\n",
      "Epoch [236/500], Train Loss: 0.0956\n",
      "Epoch [237/500], Train Loss: 0.0959\n",
      "Epoch [238/500], Train Loss: 0.0958\n",
      "Epoch [239/500], Train Loss: 0.0957\n",
      "Epoch [240/500], Train Loss: 0.0956\n",
      "Epoch [241/500], Train Loss: 0.0956\n",
      "Epoch [242/500], Train Loss: 0.0951\n",
      "Epoch [243/500], Train Loss: 0.0950\n",
      "Epoch [244/500], Train Loss: 0.0954\n",
      "Epoch [245/500], Train Loss: 0.0954\n",
      "Epoch [246/500], Train Loss: 0.0951\n",
      "Epoch [247/500], Train Loss: 0.0947\n",
      "Epoch [248/500], Train Loss: 0.0950\n",
      "Epoch [249/500], Train Loss: 0.0950\n",
      "Epoch [250/500], Train Loss: 0.0943\n",
      "Epoch [251/500], Train Loss: 0.0950\n",
      "Epoch [252/500], Train Loss: 0.0945\n",
      "Epoch [253/500], Train Loss: 0.0945\n",
      "Epoch [254/500], Train Loss: 0.0943\n",
      "Epoch [255/500], Train Loss: 0.0943\n",
      "Epoch [256/500], Train Loss: 0.0943\n",
      "Epoch [257/500], Train Loss: 0.0944\n",
      "Epoch [258/500], Train Loss: 0.0942\n",
      "Epoch [259/500], Train Loss: 0.0940\n",
      "Epoch [260/500], Train Loss: 0.0940\n",
      "Epoch [261/500], Train Loss: 0.0938\n",
      "Epoch [262/500], Train Loss: 0.0940\n",
      "Epoch [263/500], Train Loss: 0.0939\n",
      "Epoch [264/500], Train Loss: 0.0938\n",
      "Epoch [265/500], Train Loss: 0.0935\n",
      "Epoch [266/500], Train Loss: 0.0932\n",
      "Epoch [267/500], Train Loss: 0.0934\n",
      "Epoch [268/500], Train Loss: 0.0929\n",
      "Epoch [269/500], Train Loss: 0.0937\n",
      "Epoch [270/500], Train Loss: 0.0932\n",
      "Epoch [271/500], Train Loss: 0.0929\n",
      "Epoch [272/500], Train Loss: 0.0933\n",
      "Epoch [273/500], Train Loss: 0.0933\n",
      "Epoch [274/500], Train Loss: 0.0930\n",
      "Epoch [275/500], Train Loss: 0.0930\n",
      "Epoch [276/500], Train Loss: 0.0930\n",
      "Epoch [277/500], Train Loss: 0.0930\n",
      "Epoch [278/500], Train Loss: 0.0932\n",
      "Epoch [279/500], Train Loss: 0.0929\n",
      "Epoch [280/500], Train Loss: 0.0927\n",
      "Epoch [281/500], Train Loss: 0.0926\n",
      "Epoch [282/500], Train Loss: 0.0926\n",
      "Epoch [283/500], Train Loss: 0.0924\n",
      "Epoch [284/500], Train Loss: 0.0925\n",
      "Epoch [285/500], Train Loss: 0.0926\n",
      "Epoch [286/500], Train Loss: 0.0924\n",
      "Epoch [287/500], Train Loss: 0.0923\n",
      "Epoch [288/500], Train Loss: 0.0920\n",
      "Epoch [289/500], Train Loss: 0.0920\n",
      "Epoch [290/500], Train Loss: 0.0925\n",
      "Epoch [291/500], Train Loss: 0.0922\n",
      "Epoch [292/500], Train Loss: 0.0921\n",
      "Epoch [293/500], Train Loss: 0.0922\n",
      "Epoch [294/500], Train Loss: 0.0919\n",
      "Epoch [295/500], Train Loss: 0.0918\n",
      "Epoch [296/500], Train Loss: 0.0918\n",
      "Epoch [297/500], Train Loss: 0.0918\n",
      "Epoch [298/500], Train Loss: 0.0914\n",
      "Epoch [299/500], Train Loss: 0.0917\n",
      "Epoch [300/500], Train Loss: 0.0917\n",
      "Epoch [301/500], Train Loss: 0.0916\n",
      "Epoch [302/500], Train Loss: 0.0916\n",
      "Epoch [303/500], Train Loss: 0.0913\n",
      "Epoch [304/500], Train Loss: 0.0916\n",
      "Epoch [305/500], Train Loss: 0.0913\n",
      "Epoch [306/500], Train Loss: 0.0913\n",
      "Epoch [307/500], Train Loss: 0.0913\n",
      "Epoch [308/500], Train Loss: 0.0915\n",
      "Epoch [309/500], Train Loss: 0.0910\n",
      "Epoch [310/500], Train Loss: 0.0908\n",
      "Epoch [311/500], Train Loss: 0.0915\n",
      "Epoch [312/500], Train Loss: 0.0908\n",
      "Epoch [313/500], Train Loss: 0.0912\n",
      "Epoch [314/500], Train Loss: 0.0905\n",
      "Epoch [315/500], Train Loss: 0.0913\n",
      "Epoch [316/500], Train Loss: 0.0909\n",
      "Epoch [317/500], Train Loss: 0.0906\n",
      "Epoch [318/500], Train Loss: 0.0909\n",
      "Epoch [319/500], Train Loss: 0.0907\n",
      "Epoch [320/500], Train Loss: 0.0906\n",
      "Epoch [321/500], Train Loss: 0.0911\n",
      "Epoch [322/500], Train Loss: 0.0905\n",
      "Epoch [323/500], Train Loss: 0.0907\n",
      "Epoch [324/500], Train Loss: 0.0908\n",
      "Epoch [325/500], Train Loss: 0.0908\n",
      "Epoch [326/500], Train Loss: 0.0906\n",
      "Epoch [327/500], Train Loss: 0.0905\n",
      "Epoch [328/500], Train Loss: 0.0906\n",
      "Epoch [329/500], Train Loss: 0.0904\n",
      "Epoch [330/500], Train Loss: 0.0905\n",
      "Epoch [331/500], Train Loss: 0.0905\n",
      "Epoch [332/500], Train Loss: 0.0903\n",
      "Epoch [333/500], Train Loss: 0.0902\n",
      "Epoch [334/500], Train Loss: 0.0901\n",
      "Epoch [335/500], Train Loss: 0.0902\n",
      "Epoch [336/500], Train Loss: 0.0899\n",
      "Epoch [337/500], Train Loss: 0.0900\n",
      "Epoch [338/500], Train Loss: 0.0901\n",
      "Epoch [339/500], Train Loss: 0.0897\n",
      "Epoch [340/500], Train Loss: 0.0901\n",
      "Epoch [341/500], Train Loss: 0.0904\n",
      "Epoch [342/500], Train Loss: 0.0901\n",
      "Epoch [343/500], Train Loss: 0.0897\n",
      "Epoch [344/500], Train Loss: 0.0902\n",
      "Epoch [345/500], Train Loss: 0.0897\n",
      "Epoch [346/500], Train Loss: 0.0895\n",
      "Epoch [347/500], Train Loss: 0.0900\n",
      "Epoch [348/500], Train Loss: 0.0897\n",
      "Epoch [349/500], Train Loss: 0.0897\n",
      "Epoch [350/500], Train Loss: 0.0899\n",
      "Epoch [351/500], Train Loss: 0.0897\n",
      "Epoch [352/500], Train Loss: 0.0899\n",
      "Epoch [353/500], Train Loss: 0.0895\n",
      "Epoch [354/500], Train Loss: 0.0896\n",
      "Epoch [355/500], Train Loss: 0.0892\n",
      "Epoch [356/500], Train Loss: 0.0892\n",
      "Epoch [357/500], Train Loss: 0.0895\n",
      "Epoch [358/500], Train Loss: 0.0897\n",
      "Epoch [359/500], Train Loss: 0.0896\n",
      "Epoch [360/500], Train Loss: 0.0895\n",
      "Epoch [361/500], Train Loss: 0.0899\n",
      "Epoch [362/500], Train Loss: 0.0893\n",
      "Epoch [363/500], Train Loss: 0.0892\n",
      "Epoch [364/500], Train Loss: 0.0891\n",
      "Epoch [365/500], Train Loss: 0.0891\n",
      "Epoch [366/500], Train Loss: 0.0891\n",
      "Epoch [367/500], Train Loss: 0.0889\n",
      "Epoch [368/500], Train Loss: 0.0894\n",
      "Epoch [369/500], Train Loss: 0.0886\n",
      "Epoch [370/500], Train Loss: 0.0890\n",
      "Epoch [371/500], Train Loss: 0.0886\n",
      "Epoch [372/500], Train Loss: 0.0892\n",
      "Epoch [373/500], Train Loss: 0.0894\n",
      "Epoch [374/500], Train Loss: 0.0891\n",
      "Epoch [375/500], Train Loss: 0.0887\n",
      "Epoch [376/500], Train Loss: 0.0886\n",
      "Epoch [377/500], Train Loss: 0.0886\n",
      "Epoch [378/500], Train Loss: 0.0893\n",
      "Epoch [379/500], Train Loss: 0.0886\n",
      "Epoch [380/500], Train Loss: 0.0882\n",
      "Epoch [381/500], Train Loss: 0.0890\n",
      "Epoch [382/500], Train Loss: 0.0884\n",
      "Epoch [383/500], Train Loss: 0.0882\n",
      "Epoch [384/500], Train Loss: 0.0889\n",
      "Epoch [385/500], Train Loss: 0.0890\n",
      "Epoch [386/500], Train Loss: 0.0890\n",
      "Epoch [387/500], Train Loss: 0.0886\n",
      "Epoch [388/500], Train Loss: 0.0882\n",
      "Epoch [389/500], Train Loss: 0.0884\n",
      "Epoch [390/500], Train Loss: 0.0881\n",
      "Epoch [391/500], Train Loss: 0.0885\n",
      "Epoch [392/500], Train Loss: 0.0882\n",
      "Epoch [393/500], Train Loss: 0.0885\n",
      "Epoch [394/500], Train Loss: 0.0883\n",
      "Epoch [395/500], Train Loss: 0.0883\n",
      "Epoch [396/500], Train Loss: 0.0881\n",
      "Epoch [397/500], Train Loss: 0.0882\n",
      "Epoch [398/500], Train Loss: 0.0883\n",
      "Epoch [399/500], Train Loss: 0.0889\n",
      "Epoch [400/500], Train Loss: 0.0882\n",
      "Epoch [401/500], Train Loss: 0.0879\n",
      "Epoch [402/500], Train Loss: 0.0880\n",
      "Epoch [403/500], Train Loss: 0.0880\n",
      "Epoch [404/500], Train Loss: 0.0879\n",
      "Epoch [405/500], Train Loss: 0.0884\n",
      "Epoch [406/500], Train Loss: 0.0883\n",
      "Epoch [407/500], Train Loss: 0.0882\n",
      "Epoch [408/500], Train Loss: 0.0878\n",
      "Epoch [409/500], Train Loss: 0.0874\n",
      "Epoch [410/500], Train Loss: 0.0878\n",
      "Epoch [411/500], Train Loss: 0.0876\n",
      "Epoch [412/500], Train Loss: 0.0884\n",
      "Epoch [413/500], Train Loss: 0.0880\n",
      "Epoch [414/500], Train Loss: 0.0883\n",
      "Epoch [415/500], Train Loss: 0.0881\n",
      "Epoch [416/500], Train Loss: 0.0880\n",
      "Epoch [417/500], Train Loss: 0.0879\n",
      "Epoch [418/500], Train Loss: 0.0876\n",
      "Epoch [419/500], Train Loss: 0.0874\n",
      "Epoch [420/500], Train Loss: 0.0882\n",
      "Epoch [421/500], Train Loss: 0.0880\n",
      "Epoch [422/500], Train Loss: 0.0873\n",
      "Epoch [423/500], Train Loss: 0.0877\n",
      "Epoch [424/500], Train Loss: 0.0877\n",
      "Epoch [425/500], Train Loss: 0.0875\n",
      "Epoch [426/500], Train Loss: 0.0872\n",
      "Epoch [427/500], Train Loss: 0.0878\n",
      "Epoch [428/500], Train Loss: 0.0878\n",
      "Epoch [429/500], Train Loss: 0.0874\n",
      "Epoch [430/500], Train Loss: 0.0874\n",
      "Epoch [431/500], Train Loss: 0.0874\n",
      "Epoch [432/500], Train Loss: 0.0876\n",
      "Epoch [433/500], Train Loss: 0.0875\n",
      "Epoch [434/500], Train Loss: 0.0877\n",
      "Epoch [435/500], Train Loss: 0.0874\n",
      "Epoch [436/500], Train Loss: 0.0876\n",
      "Epoch [437/500], Train Loss: 0.0874\n",
      "Epoch [438/500], Train Loss: 0.0874\n",
      "Epoch [439/500], Train Loss: 0.0875\n",
      "Epoch [440/500], Train Loss: 0.0876\n",
      "Epoch [441/500], Train Loss: 0.0873\n",
      "Epoch [442/500], Train Loss: 0.0877\n",
      "Epoch [443/500], Train Loss: 0.0876\n",
      "Early stopping at epoch 443\n",
      "Final model saved at: ../../Data/model_results/logistic_regression/models/chr20/final_model_chr20.pth\n",
      "Individual R^2 scores saved at: ../../Data/model_results/logistic_regression/csv_files/chr20/individual_r2_scores_chr20.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:51:31,728] A new study created in memory with name: no-name-eabf5d7b-4f74-4ae9-8290-af0eac1d7a6a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual AUC ROC curves saved in: ../../Data/model_results/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../Data/model_results/logistic_regression/csv_files/chr20/performance_metrics.csv\n",
      "Unknown PRS313 SNPs:  8\n",
      "Known PRS313 SNPs:  4\n",
      "23AndMe SNPs with LD to Unknown PRS313 SNPs:  162\n",
      "Early stopping at epoch 192\n",
      "Early stopping at epoch 55\n",
      "Early stopping at epoch 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:51:34,811] Trial 0 finished with value: 0.3943843603134155 and parameters: {'learning_rate': 0.0020445644100678876, 'lasso_coef': 0.07851189659243829, 'patience': 13}. Best is trial 0 with value: 0.3943843603134155.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 42\n",
      "Early stopping at epoch 18\n",
      "Early stopping at epoch 17\n",
      "Early stopping at epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:51:40,117] Trial 1 finished with value: 0.04389124484732747 and parameters: {'learning_rate': 0.005154028168754169, 'lasso_coef': 8.24838598004499e-05, 'patience': 16}. Best is trial 1 with value: 0.04389124484732747.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 18\n",
      "Early stopping at epoch 17\n",
      "Early stopping at epoch 23\n",
      "Early stopping at epoch 20\n",
      "Early stopping at epoch 25\n",
      "Early stopping at epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:51:41,086] Trial 2 finished with value: 0.4259891673922539 and parameters: {'learning_rate': 0.040679070466001235, 'lasso_coef': 0.013221183338502808, 'patience': 11}. Best is trial 1 with value: 0.04389124484732747.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 22\n",
      "Early stopping at epoch 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:51:51,020] Trial 3 finished with value: 0.37317647635936735 and parameters: {'learning_rate': 0.0003778911021843866, 'lasso_coef': 0.046746369120327806, 'patience': 19}. Best is trial 1 with value: 0.04389124484732747.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 60\n",
      "Early stopping at epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:52:09,071] Trial 4 finished with value: 0.2356407694518566 and parameters: {'learning_rate': 0.0002088314999487781, 'lasso_coef': 0.0032933020406268588, 'patience': 18}. Best is trial 1 with value: 0.04389124484732747.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 35\n",
      "Early stopping at epoch 292\n",
      "Early stopping at epoch 13\n",
      "Early stopping at epoch 12\n",
      "Early stopping at epoch 11\n",
      "Early stopping at epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:52:12,248] Trial 5 finished with value: 0.15779474526643752 and parameters: {'learning_rate': 0.0029421641059896243, 'lasso_coef': 0.001596041101344343, 'patience': 10}. Best is trial 1 with value: 0.04389124484732747.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 486\n",
      "Early stopping at epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:52:21,395] Trial 6 finished with value: 0.3554616317152977 and parameters: {'learning_rate': 0.0004446677153936035, 'lasso_coef': 0.017853018576051226, 'patience': 11}. Best is trial 1 with value: 0.04389124484732747.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 20\n",
      "Early stopping at epoch 14\n",
      "Early stopping at epoch 162\n",
      "Early stopping at epoch 179\n",
      "Early stopping at epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:52:24,814] Trial 7 finished with value: 0.30187795907258985 and parameters: {'learning_rate': 0.0020687555350935786, 'lasso_coef': 0.008063048462764528, 'patience': 14}. Best is trial 1 with value: 0.04389124484732747.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15\n",
      "Early stopping at epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:52:46,748] Trial 8 finished with value: 0.15537501648068427 and parameters: {'learning_rate': 0.00016393613319692793, 'lasso_coef': 0.0007241192578415522, 'patience': 11}. Best is trial 1 with value: 0.04389124484732747.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 145\n",
      "Early stopping at epoch 100\n",
      "Early stopping at epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:52:49,397] Trial 9 finished with value: 0.33140636533498763 and parameters: {'learning_rate': 0.0024041189429780093, 'lasso_coef': 0.011345814522917344, 'patience': 15}. Best is trial 1 with value: 0.04389124484732747.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 18\n",
      "Early stopping at epoch 16\n",
      "Early stopping at epoch 63\n",
      "Early stopping at epoch 6\n",
      "Early stopping at epoch 6\n",
      "Early stopping at epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:52:50,183] Trial 10 finished with value: 0.04420601595193148 and parameters: {'learning_rate': 0.026530205314956836, 'lasso_coef': 4.8979617732567294e-05, 'patience': 5}. Best is trial 1 with value: 0.04389124484732747.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 6\n",
      "Early stopping at epoch 48\n",
      "Early stopping at epoch 6\n",
      "Early stopping at epoch 6\n",
      "Early stopping at epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:52:50,838] Trial 11 finished with value: 0.045548121258616445 and parameters: {'learning_rate': 0.029606134082046903, 'lasso_coef': 4.796058358323588e-05, 'patience': 5}. Best is trial 1 with value: 0.04389124484732747.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 6\n",
      "Early stopping at epoch 134\n",
      "Early stopping at epoch 6\n",
      "Early stopping at epoch 6\n",
      "Early stopping at epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:52:52,263] Trial 12 finished with value: 0.03755166744813323 and parameters: {'learning_rate': 0.009227704848516552, 'lasso_coef': 1.9836170360607817e-05, 'patience': 5}. Best is trial 12 with value: 0.03755166744813323.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 6\n",
      "Early stopping at epoch 127\n",
      "Early stopping at epoch 9\n",
      "Early stopping at epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:52:53,766] Trial 13 finished with value: 0.034383214777335525 and parameters: {'learning_rate': 0.011046014486216076, 'lasso_coef': 1.3006619912515894e-05, 'patience': 8}. Best is trial 13 with value: 0.034383214777335525.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 9\n",
      "Early stopping at epoch 9\n",
      "Early stopping at epoch 155\n",
      "Early stopping at epoch 8\n",
      "Early stopping at epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:52:55,505] Trial 14 finished with value: 0.03346250429749489 and parameters: {'learning_rate': 0.009967365376004055, 'lasso_coef': 1.3575795654471141e-05, 'patience': 7}. Best is trial 14 with value: 0.03346250429749489.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 8\n",
      "Early stopping at epoch 8\n",
      "Early stopping at epoch 136\n",
      "Early stopping at epoch 9\n",
      "Early stopping at epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:52:57,043] Trial 15 finished with value: 0.03397833709605038 and parameters: {'learning_rate': 0.009418656815817765, 'lasso_coef': 1.0271130495995688e-05, 'patience': 8}. Best is trial 14 with value: 0.03346250429749489.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10\n",
      "Early stopping at epoch 9\n",
      "Early stopping at epoch 51\n",
      "Early stopping at epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:52:57,927] Trial 16 finished with value: 0.06303845420479774 and parameters: {'learning_rate': 0.09562985431539069, 'lasso_coef': 0.00015399497013035345, 'patience': 8}. Best is trial 14 with value: 0.03346250429749489.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 21\n",
      "Early stopping at epoch 11\n",
      "Early stopping at epoch 9\n",
      "Early stopping at epoch 484\n",
      "Early stopping at epoch 9\n",
      "Early stopping at epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:53:06,710] Trial 17 finished with value: 0.08903785571455955 and parameters: {'learning_rate': 0.0010102776665490724, 'lasso_coef': 0.00046216470234275176, 'patience': 8}. Best is trial 14 with value: 0.03346250429749489.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 9\n",
      "Early stopping at epoch 140\n",
      "Early stopping at epoch 8\n",
      "Early stopping at epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:53:08,231] Trial 18 finished with value: 0.03369177635759115 and parameters: {'learning_rate': 0.00985264845577104, 'lasso_coef': 1.0470280748063025e-05, 'patience': 7}. Best is trial 14 with value: 0.03346250429749489.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 8\n",
      "Early stopping at epoch 8\n",
      "Early stopping at epoch 115\n",
      "Early stopping at epoch 8\n",
      "Early stopping at epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:53:09,615] Trial 19 finished with value: 0.06523853875696659 and parameters: {'learning_rate': 0.016523466662563383, 'lasso_coef': 0.00021200627443914604, 'patience': 7}. Best is trial 14 with value: 0.03346250429749489.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 11\n",
      "Early stopping at epoch 9\n",
      "Early stopping at epoch 32\n",
      "Early stopping at epoch 7\n",
      "Early stopping at epoch 7\n",
      "Early stopping at epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:53:10,156] Trial 20 finished with value: 0.040556170092895624 and parameters: {'learning_rate': 0.07251464808015887, 'lasso_coef': 2.8544221765267625e-05, 'patience': 6}. Best is trial 14 with value: 0.03346250429749489.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 7\n",
      "Early stopping at epoch 274\n",
      "Early stopping at epoch 10\n",
      "Early stopping at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:53:12,968] Trial 21 finished with value: 0.03154926891438663 and parameters: {'learning_rate': 0.005894874703421142, 'lasso_coef': 1.143453242553534e-05, 'patience': 9}. Best is trial 21 with value: 0.03154926891438663.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10\n",
      "Early stopping at epoch 10\n",
      "Early stopping at epoch 267\n",
      "Early stopping at epoch 11\n",
      "Early stopping at epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:53:15,831] Trial 22 finished with value: 0.03626437466591596 and parameters: {'learning_rate': 0.007177054509337489, 'lasso_coef': 2.9781367190075453e-05, 'patience': 10}. Best is trial 21 with value: 0.03154926891438663.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 11\n",
      "Early stopping at epoch 11\n",
      "Early stopping at epoch 385\n",
      "Early stopping at epoch 10\n",
      "Early stopping at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:53:19,598] Trial 23 finished with value: 0.04746190793812275 and parameters: {'learning_rate': 0.004770038018310529, 'lasso_coef': 9.38996807744264e-05, 'patience': 9}. Best is trial 21 with value: 0.03154926891438663.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10\n",
      "Early stopping at epoch 10\n",
      "Early stopping at epoch 111\n",
      "Early stopping at epoch 8\n",
      "Early stopping at epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:53:20,887] Trial 24 finished with value: 0.03209313447587192 and parameters: {'learning_rate': 0.01531805594184999, 'lasso_coef': 1.0082295089039571e-05, 'patience': 7}. Best is trial 21 with value: 0.03154926891438663.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 9\n",
      "Early stopping at epoch 8\n",
      "Early stopping at epoch 83\n",
      "Early stopping at epoch 7\n",
      "Early stopping at epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:53:22,010] Trial 25 finished with value: 0.077314954996109 and parameters: {'learning_rate': 0.017940882114708647, 'lasso_coef': 0.0003181884514770589, 'patience': 6}. Best is trial 21 with value: 0.03154926891438663.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 9\n",
      "Early stopping at epoch 7\n",
      "Early stopping at epoch 10\n",
      "Early stopping at epoch 10\n",
      "Early stopping at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:53:26,937] Trial 26 finished with value: 0.043225500546395776 and parameters: {'learning_rate': 0.0011321491047840706, 'lasso_coef': 2.3135184203366222e-05, 'patience': 9}. Best is trial 21 with value: 0.03154926891438663.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10\n",
      "Early stopping at epoch 429\n",
      "Early stopping at epoch 13\n",
      "Early stopping at epoch 13\n",
      "Early stopping at epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:53:31,218] Trial 27 finished with value: 0.04087431412190199 and parameters: {'learning_rate': 0.004821611952618483, 'lasso_coef': 5.8768949824598046e-05, 'patience': 12}. Best is trial 21 with value: 0.03154926891438663.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 13\n",
      "Early stopping at epoch 118\n",
      "Early stopping at epoch 8\n",
      "Early stopping at epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:53:32,552] Trial 28 finished with value: 0.05608807392418384 and parameters: {'learning_rate': 0.017530869475218182, 'lasso_coef': 0.00013319391064628444, 'patience': 7}. Best is trial 21 with value: 0.03154926891438663.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 8\n",
      "Early stopping at epoch 8\n",
      "Early stopping at epoch 14\n",
      "Early stopping at epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:53:37,430] Trial 29 finished with value: 0.03931445791386068 and parameters: {'learning_rate': 0.0013014408674641416, 'lasso_coef': 1.802968648480145e-05, 'patience': 13}. Best is trial 21 with value: 0.03154926891438663.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 14\n",
      "Early stopping at epoch 14\n",
      "Best hyperparameters: {'learning_rate': 0.005894874703421142, 'lasso_coef': 1.143453242553534e-05, 'patience': 9}\n",
      "Best value: 0.03154926891438663\n",
      "Epoch [1/500], Train Loss: 0.4445\n",
      "Epoch [2/500], Train Loss: 0.3372\n",
      "Epoch [3/500], Train Loss: 0.2986\n",
      "Epoch [4/500], Train Loss: 0.2711\n",
      "Epoch [5/500], Train Loss: 0.2483\n",
      "Epoch [6/500], Train Loss: 0.2299\n",
      "Epoch [7/500], Train Loss: 0.2141\n",
      "Epoch [8/500], Train Loss: 0.2000\n",
      "Epoch [9/500], Train Loss: 0.1882\n",
      "Epoch [10/500], Train Loss: 0.1784\n",
      "Epoch [11/500], Train Loss: 0.1688\n",
      "Epoch [12/500], Train Loss: 0.1601\n",
      "Epoch [13/500], Train Loss: 0.1530\n",
      "Epoch [14/500], Train Loss: 0.1467\n",
      "Epoch [15/500], Train Loss: 0.1403\n",
      "Epoch [16/500], Train Loss: 0.1348\n",
      "Epoch [17/500], Train Loss: 0.1294\n",
      "Epoch [18/500], Train Loss: 0.1249\n",
      "Epoch [19/500], Train Loss: 0.1204\n",
      "Epoch [20/500], Train Loss: 0.1163\n",
      "Epoch [21/500], Train Loss: 0.1130\n",
      "Epoch [22/500], Train Loss: 0.1092\n",
      "Epoch [23/500], Train Loss: 0.1057\n",
      "Epoch [24/500], Train Loss: 0.1027\n",
      "Epoch [25/500], Train Loss: 0.0999\n",
      "Epoch [26/500], Train Loss: 0.0973\n",
      "Epoch [27/500], Train Loss: 0.0943\n",
      "Epoch [28/500], Train Loss: 0.0920\n",
      "Epoch [29/500], Train Loss: 0.0899\n",
      "Epoch [30/500], Train Loss: 0.0875\n",
      "Epoch [31/500], Train Loss: 0.0855\n",
      "Epoch [32/500], Train Loss: 0.0838\n",
      "Epoch [33/500], Train Loss: 0.0816\n",
      "Epoch [34/500], Train Loss: 0.0803\n",
      "Epoch [35/500], Train Loss: 0.0786\n",
      "Epoch [36/500], Train Loss: 0.0770\n",
      "Epoch [37/500], Train Loss: 0.0757\n",
      "Epoch [38/500], Train Loss: 0.0739\n",
      "Epoch [39/500], Train Loss: 0.0723\n",
      "Epoch [40/500], Train Loss: 0.0712\n",
      "Epoch [41/500], Train Loss: 0.0702\n",
      "Epoch [42/500], Train Loss: 0.0695\n",
      "Epoch [43/500], Train Loss: 0.0674\n",
      "Epoch [44/500], Train Loss: 0.0666\n",
      "Epoch [45/500], Train Loss: 0.0658\n",
      "Epoch [46/500], Train Loss: 0.0647\n",
      "Epoch [47/500], Train Loss: 0.0636\n",
      "Epoch [48/500], Train Loss: 0.0630\n",
      "Epoch [49/500], Train Loss: 0.0618\n",
      "Epoch [50/500], Train Loss: 0.0607\n",
      "Epoch [51/500], Train Loss: 0.0598\n",
      "Epoch [52/500], Train Loss: 0.0590\n",
      "Epoch [53/500], Train Loss: 0.0582\n",
      "Epoch [54/500], Train Loss: 0.0578\n",
      "Epoch [55/500], Train Loss: 0.0566\n",
      "Epoch [56/500], Train Loss: 0.0558\n",
      "Epoch [57/500], Train Loss: 0.0555\n",
      "Epoch [58/500], Train Loss: 0.0548\n",
      "Epoch [59/500], Train Loss: 0.0540\n",
      "Epoch [60/500], Train Loss: 0.0532\n",
      "Epoch [61/500], Train Loss: 0.0531\n",
      "Epoch [62/500], Train Loss: 0.0521\n",
      "Epoch [63/500], Train Loss: 0.0520\n",
      "Epoch [64/500], Train Loss: 0.0511\n",
      "Epoch [65/500], Train Loss: 0.0506\n",
      "Epoch [66/500], Train Loss: 0.0501\n",
      "Epoch [67/500], Train Loss: 0.0495\n",
      "Epoch [68/500], Train Loss: 0.0491\n",
      "Epoch [69/500], Train Loss: 0.0485\n",
      "Epoch [70/500], Train Loss: 0.0483\n",
      "Epoch [71/500], Train Loss: 0.0476\n",
      "Epoch [72/500], Train Loss: 0.0471\n",
      "Epoch [73/500], Train Loss: 0.0469\n",
      "Epoch [74/500], Train Loss: 0.0464\n",
      "Epoch [75/500], Train Loss: 0.0458\n",
      "Epoch [76/500], Train Loss: 0.0457\n",
      "Epoch [77/500], Train Loss: 0.0451\n",
      "Epoch [78/500], Train Loss: 0.0446\n",
      "Epoch [79/500], Train Loss: 0.0442\n",
      "Epoch [80/500], Train Loss: 0.0438\n",
      "Epoch [81/500], Train Loss: 0.0436\n",
      "Epoch [82/500], Train Loss: 0.0435\n",
      "Epoch [83/500], Train Loss: 0.0431\n",
      "Epoch [84/500], Train Loss: 0.0427\n",
      "Epoch [85/500], Train Loss: 0.0422\n",
      "Epoch [86/500], Train Loss: 0.0417\n",
      "Epoch [87/500], Train Loss: 0.0415\n",
      "Epoch [88/500], Train Loss: 0.0412\n",
      "Epoch [89/500], Train Loss: 0.0410\n",
      "Epoch [90/500], Train Loss: 0.0411\n",
      "Epoch [91/500], Train Loss: 0.0406\n",
      "Epoch [92/500], Train Loss: 0.0399\n",
      "Epoch [93/500], Train Loss: 0.0400\n",
      "Epoch [94/500], Train Loss: 0.0397\n",
      "Epoch [95/500], Train Loss: 0.0395\n",
      "Epoch [96/500], Train Loss: 0.0389\n",
      "Epoch [97/500], Train Loss: 0.0389\n",
      "Epoch [98/500], Train Loss: 0.0385\n",
      "Epoch [99/500], Train Loss: 0.0386\n",
      "Epoch [100/500], Train Loss: 0.0383\n",
      "Epoch [101/500], Train Loss: 0.0380\n",
      "Epoch [102/500], Train Loss: 0.0379\n",
      "Epoch [103/500], Train Loss: 0.0374\n",
      "Epoch [104/500], Train Loss: 0.0371\n",
      "Epoch [105/500], Train Loss: 0.0369\n",
      "Epoch [106/500], Train Loss: 0.0366\n",
      "Epoch [107/500], Train Loss: 0.0366\n",
      "Epoch [108/500], Train Loss: 0.0363\n",
      "Epoch [109/500], Train Loss: 0.0362\n",
      "Epoch [110/500], Train Loss: 0.0359\n",
      "Epoch [111/500], Train Loss: 0.0357\n",
      "Epoch [112/500], Train Loss: 0.0357\n",
      "Epoch [113/500], Train Loss: 0.0354\n",
      "Epoch [114/500], Train Loss: 0.0352\n",
      "Epoch [115/500], Train Loss: 0.0348\n",
      "Epoch [116/500], Train Loss: 0.0350\n",
      "Epoch [117/500], Train Loss: 0.0346\n",
      "Epoch [118/500], Train Loss: 0.0345\n",
      "Epoch [119/500], Train Loss: 0.0343\n",
      "Epoch [120/500], Train Loss: 0.0341\n",
      "Epoch [121/500], Train Loss: 0.0339\n",
      "Epoch [122/500], Train Loss: 0.0341\n",
      "Epoch [123/500], Train Loss: 0.0334\n",
      "Epoch [124/500], Train Loss: 0.0337\n",
      "Epoch [125/500], Train Loss: 0.0332\n",
      "Epoch [126/500], Train Loss: 0.0335\n",
      "Epoch [127/500], Train Loss: 0.0330\n",
      "Epoch [128/500], Train Loss: 0.0329\n",
      "Epoch [129/500], Train Loss: 0.0327\n",
      "Epoch [130/500], Train Loss: 0.0327\n",
      "Epoch [131/500], Train Loss: 0.0325\n",
      "Epoch [132/500], Train Loss: 0.0323\n",
      "Epoch [133/500], Train Loss: 0.0322\n",
      "Epoch [134/500], Train Loss: 0.0324\n",
      "Epoch [135/500], Train Loss: 0.0321\n",
      "Epoch [136/500], Train Loss: 0.0321\n",
      "Epoch [137/500], Train Loss: 0.0318\n",
      "Epoch [138/500], Train Loss: 0.0316\n",
      "Epoch [139/500], Train Loss: 0.0312\n",
      "Epoch [140/500], Train Loss: 0.0312\n",
      "Epoch [141/500], Train Loss: 0.0311\n",
      "Epoch [142/500], Train Loss: 0.0311\n",
      "Epoch [143/500], Train Loss: 0.0311\n",
      "Epoch [144/500], Train Loss: 0.0312\n",
      "Epoch [145/500], Train Loss: 0.0307\n",
      "Epoch [146/500], Train Loss: 0.0308\n",
      "Epoch [147/500], Train Loss: 0.0306\n",
      "Epoch [148/500], Train Loss: 0.0308\n",
      "Epoch [149/500], Train Loss: 0.0305\n",
      "Epoch [150/500], Train Loss: 0.0302\n",
      "Epoch [151/500], Train Loss: 0.0301\n",
      "Epoch [152/500], Train Loss: 0.0299\n",
      "Epoch [153/500], Train Loss: 0.0300\n",
      "Epoch [154/500], Train Loss: 0.0298\n",
      "Epoch [155/500], Train Loss: 0.0298\n",
      "Epoch [156/500], Train Loss: 0.0296\n",
      "Epoch [157/500], Train Loss: 0.0297\n",
      "Epoch [158/500], Train Loss: 0.0294\n",
      "Epoch [159/500], Train Loss: 0.0294\n",
      "Epoch [160/500], Train Loss: 0.0293\n",
      "Epoch [161/500], Train Loss: 0.0293\n",
      "Epoch [162/500], Train Loss: 0.0290\n",
      "Epoch [163/500], Train Loss: 0.0289\n",
      "Epoch [164/500], Train Loss: 0.0289\n",
      "Epoch [165/500], Train Loss: 0.0292\n",
      "Epoch [166/500], Train Loss: 0.0291\n",
      "Epoch [167/500], Train Loss: 0.0291\n",
      "Epoch [168/500], Train Loss: 0.0286\n",
      "Epoch [169/500], Train Loss: 0.0284\n",
      "Epoch [170/500], Train Loss: 0.0283\n",
      "Epoch [171/500], Train Loss: 0.0283\n",
      "Epoch [172/500], Train Loss: 0.0280\n",
      "Epoch [173/500], Train Loss: 0.0284\n",
      "Epoch [174/500], Train Loss: 0.0279\n",
      "Epoch [175/500], Train Loss: 0.0278\n",
      "Epoch [176/500], Train Loss: 0.0278\n",
      "Epoch [177/500], Train Loss: 0.0277\n",
      "Epoch [178/500], Train Loss: 0.0276\n",
      "Epoch [179/500], Train Loss: 0.0278\n",
      "Epoch [180/500], Train Loss: 0.0276\n",
      "Epoch [181/500], Train Loss: 0.0276\n",
      "Epoch [182/500], Train Loss: 0.0274\n",
      "Epoch [183/500], Train Loss: 0.0277\n",
      "Epoch [184/500], Train Loss: 0.0274\n",
      "Epoch [185/500], Train Loss: 0.0275\n",
      "Epoch [186/500], Train Loss: 0.0273\n",
      "Epoch [187/500], Train Loss: 0.0272\n",
      "Epoch [188/500], Train Loss: 0.0273\n",
      "Epoch [189/500], Train Loss: 0.0268\n",
      "Epoch [190/500], Train Loss: 0.0271\n",
      "Epoch [191/500], Train Loss: 0.0268\n",
      "Epoch [192/500], Train Loss: 0.0266\n",
      "Epoch [193/500], Train Loss: 0.0271\n",
      "Epoch [194/500], Train Loss: 0.0269\n",
      "Epoch [195/500], Train Loss: 0.0267\n",
      "Epoch [196/500], Train Loss: 0.0268\n",
      "Epoch [197/500], Train Loss: 0.0264\n",
      "Epoch [198/500], Train Loss: 0.0266\n",
      "Epoch [199/500], Train Loss: 0.0268\n",
      "Epoch [200/500], Train Loss: 0.0262\n",
      "Epoch [201/500], Train Loss: 0.0264\n",
      "Epoch [202/500], Train Loss: 0.0261\n",
      "Epoch [203/500], Train Loss: 0.0262\n",
      "Epoch [204/500], Train Loss: 0.0259\n",
      "Epoch [205/500], Train Loss: 0.0260\n",
      "Epoch [206/500], Train Loss: 0.0262\n",
      "Epoch [207/500], Train Loss: 0.0259\n",
      "Epoch [208/500], Train Loss: 0.0258\n",
      "Epoch [209/500], Train Loss: 0.0260\n",
      "Epoch [210/500], Train Loss: 0.0259\n",
      "Epoch [211/500], Train Loss: 0.0257\n",
      "Epoch [212/500], Train Loss: 0.0258\n",
      "Epoch [213/500], Train Loss: 0.0257\n",
      "Epoch [214/500], Train Loss: 0.0254\n",
      "Epoch [215/500], Train Loss: 0.0258\n",
      "Epoch [216/500], Train Loss: 0.0262\n",
      "Epoch [217/500], Train Loss: 0.0254\n",
      "Epoch [218/500], Train Loss: 0.0253\n",
      "Epoch [219/500], Train Loss: 0.0254\n",
      "Epoch [220/500], Train Loss: 0.0254\n",
      "Epoch [221/500], Train Loss: 0.0254\n",
      "Epoch [222/500], Train Loss: 0.0250\n",
      "Epoch [223/500], Train Loss: 0.0256\n",
      "Epoch [224/500], Train Loss: 0.0250\n",
      "Epoch [225/500], Train Loss: 0.0252\n",
      "Epoch [226/500], Train Loss: 0.0251\n",
      "Epoch [227/500], Train Loss: 0.0252\n",
      "Epoch [228/500], Train Loss: 0.0250\n",
      "Epoch [229/500], Train Loss: 0.0248\n",
      "Epoch [230/500], Train Loss: 0.0248\n",
      "Epoch [231/500], Train Loss: 0.0247\n",
      "Epoch [232/500], Train Loss: 0.0249\n",
      "Epoch [233/500], Train Loss: 0.0247\n",
      "Epoch [234/500], Train Loss: 0.0247\n",
      "Epoch [235/500], Train Loss: 0.0252\n",
      "Epoch [236/500], Train Loss: 0.0245\n",
      "Epoch [237/500], Train Loss: 0.0247\n",
      "Epoch [238/500], Train Loss: 0.0244\n",
      "Epoch [239/500], Train Loss: 0.0246\n",
      "Epoch [240/500], Train Loss: 0.0246\n",
      "Epoch [241/500], Train Loss: 0.0243\n",
      "Epoch [242/500], Train Loss: 0.0243\n",
      "Epoch [243/500], Train Loss: 0.0243\n",
      "Epoch [244/500], Train Loss: 0.0244\n",
      "Epoch [245/500], Train Loss: 0.0242\n",
      "Epoch [246/500], Train Loss: 0.0241\n",
      "Epoch [247/500], Train Loss: 0.0239\n",
      "Epoch [248/500], Train Loss: 0.0242\n",
      "Epoch [249/500], Train Loss: 0.0242\n",
      "Epoch [250/500], Train Loss: 0.0240\n",
      "Epoch [251/500], Train Loss: 0.0242\n",
      "Epoch [252/500], Train Loss: 0.0239\n",
      "Epoch [253/500], Train Loss: 0.0240\n",
      "Epoch [254/500], Train Loss: 0.0241\n",
      "Epoch [255/500], Train Loss: 0.0240\n",
      "Epoch [256/500], Train Loss: 0.0238\n",
      "Epoch [257/500], Train Loss: 0.0238\n",
      "Epoch [258/500], Train Loss: 0.0238\n",
      "Epoch [259/500], Train Loss: 0.0238\n",
      "Epoch [260/500], Train Loss: 0.0238\n",
      "Epoch [261/500], Train Loss: 0.0236\n",
      "Epoch [262/500], Train Loss: 0.0235\n",
      "Epoch [263/500], Train Loss: 0.0239\n",
      "Epoch [264/500], Train Loss: 0.0237\n",
      "Epoch [265/500], Train Loss: 0.0235\n",
      "Epoch [266/500], Train Loss: 0.0235\n",
      "Epoch [267/500], Train Loss: 0.0233\n",
      "Epoch [268/500], Train Loss: 0.0234\n",
      "Epoch [269/500], Train Loss: 0.0234\n",
      "Epoch [270/500], Train Loss: 0.0232\n",
      "Epoch [271/500], Train Loss: 0.0233\n",
      "Epoch [272/500], Train Loss: 0.0234\n",
      "Epoch [273/500], Train Loss: 0.0233\n",
      "Epoch [274/500], Train Loss: 0.0235\n",
      "Epoch [275/500], Train Loss: 0.0234\n",
      "Epoch [276/500], Train Loss: 0.0232\n",
      "Epoch [277/500], Train Loss: 0.0233\n",
      "Epoch [278/500], Train Loss: 0.0231\n",
      "Epoch [279/500], Train Loss: 0.0232\n",
      "Epoch [280/500], Train Loss: 0.0233\n",
      "Epoch [281/500], Train Loss: 0.0233\n",
      "Epoch [282/500], Train Loss: 0.0231\n",
      "Epoch [283/500], Train Loss: 0.0231\n",
      "Epoch [284/500], Train Loss: 0.0230\n",
      "Epoch [285/500], Train Loss: 0.0229\n",
      "Epoch [286/500], Train Loss: 0.0232\n",
      "Epoch [287/500], Train Loss: 0.0230\n",
      "Epoch [288/500], Train Loss: 0.0232\n",
      "Epoch [289/500], Train Loss: 0.0230\n",
      "Epoch [290/500], Train Loss: 0.0230\n",
      "Epoch [291/500], Train Loss: 0.0230\n",
      "Epoch [292/500], Train Loss: 0.0227\n",
      "Epoch [293/500], Train Loss: 0.0229\n",
      "Epoch [294/500], Train Loss: 0.0227\n",
      "Epoch [295/500], Train Loss: 0.0227\n",
      "Epoch [296/500], Train Loss: 0.0228\n",
      "Epoch [297/500], Train Loss: 0.0226\n",
      "Epoch [298/500], Train Loss: 0.0226\n",
      "Epoch [299/500], Train Loss: 0.0226\n",
      "Epoch [300/500], Train Loss: 0.0229\n",
      "Epoch [301/500], Train Loss: 0.0227\n",
      "Epoch [302/500], Train Loss: 0.0225\n",
      "Epoch [303/500], Train Loss: 0.0225\n",
      "Epoch [304/500], Train Loss: 0.0225\n",
      "Epoch [305/500], Train Loss: 0.0224\n",
      "Epoch [306/500], Train Loss: 0.0225\n",
      "Epoch [307/500], Train Loss: 0.0227\n",
      "Epoch [308/500], Train Loss: 0.0228\n",
      "Epoch [309/500], Train Loss: 0.0222\n",
      "Epoch [310/500], Train Loss: 0.0224\n",
      "Epoch [311/500], Train Loss: 0.0224\n",
      "Epoch [312/500], Train Loss: 0.0224\n",
      "Epoch [313/500], Train Loss: 0.0223\n",
      "Epoch [314/500], Train Loss: 0.0223\n",
      "Epoch [315/500], Train Loss: 0.0223\n",
      "Epoch [316/500], Train Loss: 0.0228\n",
      "Epoch [317/500], Train Loss: 0.0224\n",
      "Epoch [318/500], Train Loss: 0.0223\n",
      "Early stopping at epoch 318\n",
      "Final model saved at: ../../Data/model_results/logistic_regression/models/chr21/final_model_chr21.pth\n",
      "Individual R^2 scores saved at: ../../Data/model_results/logistic_regression/csv_files/chr21/individual_r2_scores_chr21.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:53:40,836] A new study created in memory with name: no-name-de8709ec-71b9-440d-893b-396db94dae54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual AUC ROC curves saved in: ../../Data/model_results/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../Data/model_results/logistic_regression/csv_files/chr21/performance_metrics.csv\n",
      "Unknown PRS313 SNPs:  22\n",
      "Known PRS313 SNPs:  6\n",
      "23AndMe SNPs with LD to Unknown PRS313 SNPs:  498\n",
      "Early stopping at epoch 160\n",
      "Early stopping at epoch 14\n",
      "Early stopping at epoch 14\n",
      "Early stopping at epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:53:43,325] Trial 0 finished with value: 0.13146005123853682 and parameters: {'learning_rate': 0.006463973937660458, 'lasso_coef': 7.359548709910162e-05, 'patience': 13}. Best is trial 0 with value: 0.13146005123853682.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 14\n",
      "Early stopping at epoch 90\n",
      "Early stopping at epoch 26\n",
      "Early stopping at epoch 39\n",
      "Early stopping at epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:53:45,654] Trial 1 finished with value: 0.6940975278615952 and parameters: {'learning_rate': 0.007482689100728751, 'lasso_coef': 0.035360155616472724, 'patience': 12}. Best is trial 0 with value: 0.13146005123853682.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 20\n",
      "Early stopping at epoch 92\n",
      "Early stopping at epoch 11\n",
      "Early stopping at epoch 21\n",
      "Early stopping at epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:53:47,479] Trial 2 finished with value: 0.6439183801412582 and parameters: {'learning_rate': 0.032014597713476166, 'lasso_coef': 0.006705774736941332, 'patience': 10}. Best is trial 0 with value: 0.13146005123853682.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15\n",
      "Early stopping at epoch 18\n",
      "Early stopping at epoch 18\n",
      "Early stopping at epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:53:54,051] Trial 3 finished with value: 0.14071499109268187 and parameters: {'learning_rate': 0.00026874054236024645, 'lasso_coef': 2.294835901688995e-05, 'patience': 17}. Best is trial 0 with value: 0.13146005123853682.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 18\n",
      "Early stopping at epoch 198\n",
      "Early stopping at epoch 32\n",
      "Early stopping at epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:53:57,113] Trial 4 finished with value: 0.31591111421585083 and parameters: {'learning_rate': 0.002836279580938689, 'lasso_coef': 0.0029303636266376464, 'patience': 8}. Best is trial 0 with value: 0.13146005123853682.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 9\n",
      "Early stopping at epoch 10\n",
      "Early stopping at epoch 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:54:14,740] Trial 5 finished with value: 0.23384588584303856 and parameters: {'learning_rate': 0.00032353152046862003, 'lasso_coef': 0.0008975589873132763, 'patience': 18}. Best is trial 0 with value: 0.13146005123853682.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 25\n",
      "Early stopping at epoch 80\n",
      "Early stopping at epoch 26\n",
      "Early stopping at epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:54:16,398] Trial 6 finished with value: 0.40377490371465685 and parameters: {'learning_rate': 0.0052979132728656194, 'lasso_coef': 0.006728446232932388, 'patience': 6}. Best is trial 0 with value: 0.13146005123853682.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 16\n",
      "Early stopping at epoch 9\n",
      "Early stopping at epoch 418\n",
      "Early stopping at epoch 11\n",
      "Early stopping at epoch 11\n",
      "Early stopping at epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:54:21,703] Trial 7 finished with value: 0.12502430230379105 and parameters: {'learning_rate': 0.0018403561802387697, 'lasso_coef': 6.320078622763998e-05, 'patience': 10}. Best is trial 7 with value: 0.12502430230379105.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 11\n",
      "Early stopping at epoch 82\n",
      "Early stopping at epoch 7\n",
      "Early stopping at epoch 6\n",
      "Early stopping at epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:54:23,165] Trial 8 finished with value: 0.42332849502563474 and parameters: {'learning_rate': 0.005879304378639864, 'lasso_coef': 0.007915939125068694, 'patience': 5}. Best is trial 7 with value: 0.12502430230379105.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:54:52,531] Trial 9 finished with value: 0.2932612746953964 and parameters: {'learning_rate': 0.00010038998078478525, 'lasso_coef': 0.0012605030039342326, 'patience': 16}. Best is trial 7 with value: 0.12502430230379105.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 21\n",
      "Early stopping at epoch 21\n",
      "Early stopping at epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:55:00,133] Trial 10 finished with value: 0.15315149649977683 and parameters: {'learning_rate': 0.0013406566705293377, 'lasso_coef': 0.00017700045778053022, 'patience': 20}. Best is trial 7 with value: 0.12502430230379105.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 22\n",
      "Early stopping at epoch 67\n",
      "Early stopping at epoch 14\n",
      "Early stopping at epoch 14\n",
      "Early stopping at epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:55:01,597] Trial 11 finished with value: 0.11601618304848671 and parameters: {'learning_rate': 0.03128014655534097, 'lasso_coef': 3.0995900694677306e-05, 'patience': 13}. Best is trial 11 with value: 0.11601618304848671.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 14\n",
      "Early stopping at epoch 39\n",
      "Early stopping at epoch 14\n",
      "Early stopping at epoch 14\n",
      "Early stopping at epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:55:02,706] Trial 12 finished with value: 0.11157129891216755 and parameters: {'learning_rate': 0.07756839683539618, 'lasso_coef': 1.4789603630511473e-05, 'patience': 13}. Best is trial 12 with value: 0.11157129891216755.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 14\n",
      "Early stopping at epoch 57\n",
      "Early stopping at epoch 15\n",
      "Early stopping at epoch 15\n",
      "Early stopping at epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:55:04,053] Trial 13 finished with value: 0.111756382137537 and parameters: {'learning_rate': 0.07949921430607382, 'lasso_coef': 1.2782567551370922e-05, 'patience': 14}. Best is trial 12 with value: 0.11157129891216755.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15\n",
      "Early stopping at epoch 53\n",
      "Early stopping at epoch 16\n",
      "Early stopping at epoch 16\n",
      "Early stopping at epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:55:05,400] Trial 14 finished with value: 0.11196014657616615 and parameters: {'learning_rate': 0.09437413694676125, 'lasso_coef': 1.0544829875194315e-05, 'patience': 15}. Best is trial 12 with value: 0.11157129891216755.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 16\n",
      "Early stopping at epoch 43\n",
      "Early stopping at epoch 38\n",
      "Early stopping at epoch 31\n",
      "Early stopping at epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:55:07,061] Trial 15 finished with value: 0.2111651949584484 and parameters: {'learning_rate': 0.0647561719695327, 'lasso_coef': 0.0003121418993860712, 'patience': 14}. Best is trial 12 with value: 0.11157129891216755.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15\n",
      "Early stopping at epoch 45\n",
      "Early stopping at epoch 12\n",
      "Early stopping at epoch 12\n",
      "Early stopping at epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:55:08,166] Trial 16 finished with value: 0.10483918450772763 and parameters: {'learning_rate': 0.019978570775143697, 'lasso_coef': 1.494620620312452e-05, 'patience': 11}. Best is trial 16 with value: 0.10483918450772763.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 12\n",
      "Early stopping at epoch 99\n",
      "Early stopping at epoch 12\n",
      "Early stopping at epoch 13\n",
      "Early stopping at epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:55:09,866] Trial 17 finished with value: 0.16328383311629296 and parameters: {'learning_rate': 0.018080877438595648, 'lasso_coef': 0.00019978343968939972, 'patience': 11}. Best is trial 16 with value: 0.10483918450772763.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 12\n",
      "Early stopping at epoch 41\n",
      "Early stopping at epoch 18\n",
      "Early stopping at epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:55:11,144] Trial 18 finished with value: 2.2618600130081177 and parameters: {'learning_rate': 0.01628450622192895, 'lasso_coef': 0.09823963851626424, 'patience': 8}. Best is trial 16 with value: 0.10483918450772763.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 23\n",
      "Early stopping at epoch 13\n",
      "Early stopping at epoch 53\n",
      "Early stopping at epoch 10\n",
      "Early stopping at epoch 10\n",
      "Early stopping at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:55:12,228] Trial 19 finished with value: 0.14471927881240845 and parameters: {'learning_rate': 0.03247018187482522, 'lasso_coef': 8.066633387984332e-05, 'patience': 9}. Best is trial 16 with value: 0.10483918450772763.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10\n",
      "Early stopping at epoch 100\n",
      "Early stopping at epoch 12\n",
      "Early stopping at epoch 12\n",
      "Early stopping at epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:55:13,977] Trial 20 finished with value: 0.11347190402448178 and parameters: {'learning_rate': 0.013203814729308318, 'lasso_coef': 3.2567199506904095e-05, 'patience': 11}. Best is trial 16 with value: 0.10483918450772763.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 12\n",
      "Early stopping at epoch 39\n",
      "Early stopping at epoch 16\n",
      "Early stopping at epoch 16\n",
      "Early stopping at epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:55:15,180] Trial 21 finished with value: 0.11226344145834447 and parameters: {'learning_rate': 0.05993446916141463, 'lasso_coef': 1.5193415016360477e-05, 'patience': 15}. Best is trial 16 with value: 0.10483918450772763.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 16\n",
      "Early stopping at epoch 57\n",
      "Early stopping at epoch 13\n",
      "Early stopping at epoch 13\n",
      "Early stopping at epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:55:16,441] Trial 22 finished with value: 0.11016089469194412 and parameters: {'learning_rate': 0.09897213957093876, 'lasso_coef': 1.067393748086209e-05, 'patience': 12}. Best is trial 16 with value: 0.10483918450772763.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 13\n",
      "Early stopping at epoch 41\n",
      "Early stopping at epoch 13\n",
      "Early stopping at epoch 14\n",
      "Early stopping at epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:55:17,538] Trial 23 finished with value: 0.12677502892911435 and parameters: {'learning_rate': 0.04005139005775305, 'lasso_coef': 3.6807739742303297e-05, 'patience': 12}. Best is trial 16 with value: 0.10483918450772763.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 13\n",
      "Early stopping at epoch 68\n",
      "Early stopping at epoch 12\n",
      "Early stopping at epoch 12\n",
      "Early stopping at epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:55:18,875] Trial 24 finished with value: 0.11135490238666534 and parameters: {'learning_rate': 0.09749883073253046, 'lasso_coef': 1.066106261060926e-05, 'patience': 11}. Best is trial 16 with value: 0.10483918450772763.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 12\n",
      "Early stopping at epoch 36\n",
      "Early stopping at epoch 8\n",
      "Early stopping at epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:55:19,660] Trial 25 finished with value: 0.10184641219675541 and parameters: {'learning_rate': 0.049203396615788146, 'lasso_coef': 1.0119257827823962e-05, 'patience': 7}. Best is trial 25 with value: 0.10184641219675541.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 8\n",
      "Early stopping at epoch 8\n",
      "Early stopping at epoch 56\n",
      "Early stopping at epoch 8\n",
      "Early stopping at epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:55:20,674] Trial 26 finished with value: 0.16014210507273674 and parameters: {'learning_rate': 0.019646057617749003, 'lasso_coef': 0.00014781560410358308, 'patience': 7}. Best is trial 25 with value: 0.10184641219675541.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 8\n",
      "Early stopping at epoch 8\n",
      "Early stopping at epoch 108\n",
      "Early stopping at epoch 10\n",
      "Early stopping at epoch 10\n",
      "Early stopping at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:55:22,381] Trial 27 finished with value: 0.12327408380806446 and parameters: {'learning_rate': 0.011009243048414065, 'lasso_coef': 4.859911995735738e-05, 'patience': 9}. Best is trial 25 with value: 0.10184641219675541.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10\n",
      "Early stopping at epoch 42\n",
      "Early stopping at epoch 10\n",
      "Early stopping at epoch 22\n",
      "Early stopping at epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:55:23,500] Trial 28 finished with value: 0.22448972538113593 and parameters: {'learning_rate': 0.0486829518267503, 'lasso_coef': 0.0004572684394916112, 'patience': 6}. Best is trial 25 with value: 0.10184641219675541.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 8\n",
      "Early stopping at epoch 37\n",
      "Early stopping at epoch 6\n",
      "Early stopping at epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:55:24,254] Trial 29 finished with value: 0.15745203122496604 and parameters: {'learning_rate': 0.026921853285385215, 'lasso_coef': 0.00011281836327027214, 'patience': 5}. Best is trial 25 with value: 0.10184641219675541.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 8\n",
      "Early stopping at epoch 6\n",
      "Best hyperparameters: {'learning_rate': 0.049203396615788146, 'lasso_coef': 1.0119257827823962e-05, 'patience': 7}\n",
      "Best value: 0.10184641219675541\n",
      "Epoch [1/500], Train Loss: 0.7135\n",
      "Epoch [2/500], Train Loss: 0.3198\n",
      "Epoch [3/500], Train Loss: 0.2397\n",
      "Epoch [4/500], Train Loss: 0.2110\n",
      "Epoch [5/500], Train Loss: 0.1908\n",
      "Epoch [6/500], Train Loss: 0.1744\n",
      "Epoch [7/500], Train Loss: 0.1679\n",
      "Epoch [8/500], Train Loss: 0.1590\n",
      "Epoch [9/500], Train Loss: 0.1552\n",
      "Epoch [10/500], Train Loss: 0.1512\n",
      "Epoch [11/500], Train Loss: 0.1493\n",
      "Epoch [12/500], Train Loss: 0.1465\n",
      "Epoch [13/500], Train Loss: 0.1450\n",
      "Epoch [14/500], Train Loss: 0.1444\n",
      "Epoch [15/500], Train Loss: 0.1398\n",
      "Epoch [16/500], Train Loss: 0.1429\n",
      "Epoch [17/500], Train Loss: 0.1466\n",
      "Epoch [18/500], Train Loss: 0.1430\n",
      "Epoch [19/500], Train Loss: 0.1263\n",
      "Epoch [20/500], Train Loss: 0.1093\n",
      "Epoch [21/500], Train Loss: 0.0993\n",
      "Epoch [22/500], Train Loss: 0.0961\n",
      "Epoch [23/500], Train Loss: 0.0978\n",
      "Epoch [24/500], Train Loss: 0.0949\n",
      "Epoch [25/500], Train Loss: 0.0948\n",
      "Epoch [26/500], Train Loss: 0.0953\n",
      "Epoch [27/500], Train Loss: 0.0959\n",
      "Epoch [28/500], Train Loss: 0.0920\n",
      "Epoch [29/500], Train Loss: 0.0957\n",
      "Epoch [30/500], Train Loss: 0.0976\n",
      "Epoch [31/500], Train Loss: 0.0955\n",
      "Epoch [32/500], Train Loss: 0.0955\n",
      "Epoch [33/500], Train Loss: 0.0937\n",
      "Epoch [34/500], Train Loss: 0.0940\n",
      "Epoch [35/500], Train Loss: 0.0901\n",
      "Epoch [36/500], Train Loss: 0.0904\n",
      "Epoch [37/500], Train Loss: 0.0929\n",
      "Epoch [38/500], Train Loss: 0.0922\n",
      "Epoch [39/500], Train Loss: 0.0917\n",
      "Epoch [40/500], Train Loss: 0.0923\n",
      "Epoch [41/500], Train Loss: 0.0902\n",
      "Epoch [42/500], Train Loss: 0.0889\n",
      "Epoch [43/500], Train Loss: 0.0876\n",
      "Epoch [44/500], Train Loss: 0.0883\n",
      "Epoch [45/500], Train Loss: 0.0876\n",
      "Epoch [46/500], Train Loss: 0.0887\n",
      "Epoch [47/500], Train Loss: 0.0900\n",
      "Epoch [48/500], Train Loss: 0.0945\n",
      "Epoch [49/500], Train Loss: 0.0942\n",
      "Epoch [50/500], Train Loss: 0.0949\n",
      "Epoch [51/500], Train Loss: 0.0951\n",
      "Epoch [52/500], Train Loss: 0.0909\n",
      "Early stopping at epoch 52\n",
      "Final model saved at: ../../Data/model_results/logistic_regression/models/chr22/final_model_chr22.pth\n",
      "Individual R^2 scores saved at: ../../Data/model_results/logistic_regression/csv_files/chr22/individual_r2_scores_chr22.csv\n",
      "Skipping SNP chr22_29121087_A_G_PRS313_Unknown_paternal due to insufficient data\n",
      "Individual AUC ROC curves saved in: ../../Data/model_results/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../Data/model_results/logistic_regression/csv_files/chr22/performance_metrics.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:1133: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "from sklearn.metrics import r2_score as sklearn_r2_score\n",
    "import optuna\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Load and preprocess the data\n",
    "data_directory = '../../Data/Filtered_split_training_data/'\n",
    "start = 1\n",
    "\n",
    "# Initialize lists to store the performance metrics for each chromosome\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "false_positive_rates = []\n",
    "auc_rocs = []\n",
    "r2_scores = []\n",
    "\n",
    "\n",
    "# Create folders for saving files\n",
    "output_folder = \"../../Data/model_results/logistic_regression/\"\n",
    "model_folder = output_folder + \"models/\"\n",
    "csv_folder = output_folder + \"csv_files/\"\n",
    "curve_folder = output_folder + \"roc_curves/\"\n",
    "\n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "os.makedirs(csv_folder, exist_ok=True)\n",
    "os.makedirs(curve_folder, exist_ok=True)\n",
    "\n",
    "for chromosome_number in range(start, 23):\n",
    "\n",
    "    # Create subfolders for the current chromosome\n",
    "    chr_model_folder = model_folder + f\"chr{chromosome_number}/\"\n",
    "    chr_csv_folder = csv_folder + f\"chr{chromosome_number}/\"\n",
    "    chr_curve_folder = curve_folder + f\"chr{chromosome_number}/\"\n",
    "\n",
    "    os.makedirs(chr_model_folder, exist_ok=True)\n",
    "    os.makedirs(chr_csv_folder, exist_ok=True)\n",
    "    os.makedirs(chr_curve_folder, exist_ok=True)\n",
    "\n",
    "    file_name = data_directory + f\"23AndMe_PRS313_merged_chr{chromosome_number}_matching_split.parquet\"\n",
    "    data = pd.read_parquet(file_name)\n",
    "    print(\"Unknown PRS313 SNPs: \", data[[col for col in data.columns if \"PRS313_Unknown\" in col]].shape[1])\n",
    "    print(\"Known PRS313 SNPs: \", data[[col for col in data.columns if \"PRS313_Known\" in col]].shape[1])\n",
    "    print(\"23AndMe SNPs with LD to Unknown PRS313 SNPs: \", data[[col for col in data.columns if \"PRS313_\" not in col]].shape[1])\n",
    "\n",
    "    # Split the data into features and target\n",
    "    X = torch.tensor(data.filter(regex='^(?!.*Unknown)').values, dtype=torch.float32)\n",
    "    y = torch.tensor(data.filter(regex='Unknown').values, dtype=torch.float32)\n",
    "\n",
    "    # Split the data into train-validation and test sets\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Define the logistic regression model with lasso regularization\n",
    "    class LogisticRegression(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim, lasso_coef=0.0):\n",
    "            super(LogisticRegression, self).__init__()\n",
    "            self.linear = nn.Linear(input_dim, output_dim)\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "            self.lasso_coef = lasso_coef\n",
    "\n",
    "        def forward(self, x):\n",
    "            out = self.linear(x)\n",
    "            out = self.sigmoid(out)\n",
    "            return out\n",
    "\n",
    "        def lasso_loss(self):\n",
    "            return self.lasso_coef * torch.norm(self.linear.weight, p=1)\n",
    "\n",
    "    # Set the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Set the hyperparameters for tuning\n",
    "    input_dim = X_train_val.shape[1]\n",
    "    output_dim = y_train_val.shape[1]\n",
    "    num_epochs = 500\n",
    "    batch_size = 128\n",
    "\n",
    "    # Define the objective function for Optuna with cross-validation and early stopping\n",
    "    def objective(trial):\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-1, log=True)\n",
    "        lasso_coef = trial.suggest_float('lasso_coef', 1e-5, 1e-1, log=True)\n",
    "        patience = trial.suggest_int('patience', 5, 20)\n",
    "\n",
    "        model = LogisticRegression(input_dim, output_dim, lasso_coef).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        criterion = nn.BCELoss()\n",
    "\n",
    "        kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        fold_losses = []\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train_val)):\n",
    "            X_train, X_val = X_train_val[train_idx], X_train_val[val_idx]\n",
    "            y_train, y_val = y_train_val[train_idx], y_train_val[val_idx]\n",
    "\n",
    "            train_dataset = TensorDataset(X_train, y_train)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "            best_val_loss = float('inf')\n",
    "            counter = 0\n",
    "\n",
    "            for epoch in range(num_epochs):\n",
    "                train_loss = 0.0\n",
    "                for batch_X, batch_y in train_loader:\n",
    "                    batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "                    outputs = model(batch_X)\n",
    "                    loss = criterion(outputs, batch_y) + model.lasso_loss()\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    train_loss += loss.item()\n",
    "\n",
    "                train_loss /= len(train_loader)\n",
    "\n",
    "                val_dataset = TensorDataset(X_val, y_val)\n",
    "                val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    val_loss = 0.0\n",
    "                    for batch_X, batch_y in val_loader:\n",
    "                        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                        outputs = model(batch_X)\n",
    "                        loss = criterion(outputs, batch_y) + model.lasso_loss()\n",
    "                        val_loss += loss.item()\n",
    "\n",
    "                    val_loss /= len(val_loader)\n",
    "\n",
    "                    if val_loss < best_val_loss:\n",
    "                        best_val_loss = val_loss\n",
    "                        counter = 0\n",
    "                    else:\n",
    "                        counter += 1\n",
    "\n",
    "                    if counter >= patience:\n",
    "                        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                        break\n",
    "\n",
    "            fold_losses.append(best_val_loss)\n",
    "\n",
    "        return np.mean(fold_losses)\n",
    "\n",
    "    # Create an Optuna study and optimize the hyperparameters\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=30)\n",
    "\n",
    "    # Print the best hyperparameters and best value\n",
    "    print(\"Best hyperparameters:\", study.best_params)\n",
    "    print(\"Best value:\", study.best_value)\n",
    "\n",
    "    # Train the final model with the best hyperparameters and early stopping\n",
    "    best_learning_rate = study.best_params['learning_rate']\n",
    "    best_lasso_coef = study.best_params['lasso_coef']\n",
    "    best_patience = study.best_params['patience']\n",
    "\n",
    "    model = LogisticRegression(input_dim, output_dim, best_lasso_coef).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_learning_rate)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_val, y_train_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    best_train_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y) + model.lasso_loss()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "        if train_loss < best_train_loss:\n",
    "            best_train_loss = train_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        if counter >= best_patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    # Save the final model\n",
    "    model_save_path = chr_model_folder + f'final_model_chr{chromosome_number}.pth'\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"Final model saved at: {model_save_path}\")\n",
    "\n",
    "    # Evaluate the final model on the test set\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test.to(device))\n",
    "        test_preds = (test_outputs > 0.5).float()\n",
    "        test_accuracy = float(((test_preds > 0.5) == y_test).float().mean())\n",
    "        test_precision = precision_score(y_test.cpu().numpy(), test_preds.cpu().numpy(), average='micro')\n",
    "        test_recall = recall_score(y_test.cpu().numpy(), test_preds.cpu().numpy(), average='micro')\n",
    "        test_f1 = f1_score(y_test.cpu().numpy(), test_preds.cpu().numpy(), average='micro')\n",
    "        test_roc_auc = roc_auc_score(y_test.cpu().numpy(), test_outputs.cpu().numpy(), average='micro')\n",
    "        test_r2 = sklearn_r2_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "\n",
    "        # Calculate false positive rate\n",
    "        cm = confusion_matrix(y_test.cpu().numpy().ravel(), test_preds.cpu().numpy().ravel())\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        test_fpr = fp / (fp + tn)\n",
    "\n",
    "        # Append performance metrics to the lists\n",
    "        accuracies.append(test_accuracy)\n",
    "        precisions.append(test_precision)\n",
    "        recalls.append(test_recall)\n",
    "        false_positive_rates.append(test_fpr)\n",
    "        auc_rocs.append(test_roc_auc)\n",
    "        r2_scores.append(test_r2)\n",
    "\n",
    "        # Calculate individual R^2 scores for each SNP\n",
    "        individual_r2_scores = sklearn_r2_score(y_test.cpu().numpy(), test_outputs.cpu().numpy(), multioutput='raw_values')\n",
    "\n",
    "        # Get the names of the SNPs from the original dataframe\n",
    "        snp_names = data.filter(regex='Unknown').columns\n",
    "\n",
    "        # Save individual R^2 scores to a CSV file\n",
    "        csv_file = chr_csv_folder + f'individual_r2_scores_chr{chromosome_number}.csv'\n",
    "\n",
    "        with open(csv_file, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['SNP', 'R2 Score'])\n",
    "            for snp, r2_score in zip(snp_names, individual_r2_scores):\n",
    "                writer.writerow([snp, r2_score])\n",
    "\n",
    "        print(f\"Individual R^2 scores saved at: {csv_file}\")\n",
    "\n",
    "        # Save individual AUC ROC curves for each SNP\n",
    "        for i, snp in enumerate(snp_names):\n",
    "            try: \n",
    "                fpr, tpr, _ = roc_curve(y_test.cpu().numpy()[:, i], test_outputs.cpu().numpy()[:, i])\n",
    "                plt.figure()\n",
    "                plt.plot(fpr, tpr, label=f'AUC ROC = {roc_auc_score(y_test.cpu().numpy()[:, i], test_outputs.cpu().numpy()[:, i]):.4f}')\n",
    "                plt.xlabel('False Positive Rate')\n",
    "                plt.ylabel('True Positive Rate')\n",
    "                plt.title(f'AUC ROC Curve - {snp}')\n",
    "                plt.legend()\n",
    "                \n",
    "                curve_file = chr_curve_folder + f'auc_roc_curve_{snp}_chr{chromosome_number}.png'\n",
    "                plt.savefig(curve_file)\n",
    "                plt.close()\n",
    "            except ValueError:\n",
    "                # Save a placeholder image if there is insufficient data\n",
    "                plt.figure()\n",
    "                plt.axis('off')\n",
    "                plt.text(0.5, 0.5, \"Insufficient data for ROC curve\", ha='center', va='center')\n",
    "                curve_file = chr_curve_folder + f'auc_roc_curve_{snp}_chr{chromosome_number}.png'\n",
    "                plt.savefig(curve_file)\n",
    "                plt.close()\n",
    "\n",
    "                print(f\"Skipping SNP {snp} due to insufficient data\")\n",
    "\n",
    "\n",
    "        print(f\"Individual AUC ROC curves saved in: {curve_folder}\")\n",
    "\n",
    "        # Create a DataFrame to store the performance metrics for each chromosome\n",
    "        performance_df = pd.DataFrame({\n",
    "            'Chromosome': list(range(start, chromosome_number + 1)),\n",
    "            'Accuracy': accuracies,\n",
    "            'Precision': precisions,\n",
    "            'Recall': recalls,\n",
    "            'False Positive Rate': false_positive_rates,\n",
    "            'AUC ROC': auc_rocs,\n",
    "            'R2 Score': r2_scores\n",
    "        })\n",
    "\n",
    "        # Save the performance metrics to a CSV file\n",
    "        performance_csv_file = csv_folder + 'performance_metrics.csv'\n",
    "        performance_df.to_csv(performance_csv_file, index=False)\n",
    "        print(f\"Performance metrics saved at: {performance_csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def imputation_quality_score(imputed_data, true_data):\n",
    "    \"\"\"\n",
    "    Calculates the Imputation Quality Score (IQS) between imputed data and true data.\n",
    "    \n",
    "    Args:\n",
    "        imputed_data (numpy.ndarray): The imputed data matrix.\n",
    "        true_data (numpy.ndarray): The true data matrix (with missing values).\n",
    "        \n",
    "    Returns:\n",
    "        float: The Imputation Quality Score (IQS).\n",
    "    \"\"\"\n",
    "    # Get the indices of missing values in the true data\n",
    "    missing_indices = np.isnan(true_data)\n",
    "    \n",
    "    # Extract the imputed values and true values at the missing indices\n",
    "    imputed_values = imputed_data[missing_indices]\n",
    "    true_values = true_data[missing_indices]\n",
    "    \n",
    "    # Calculate the sum of squared differences between imputed and true values\n",
    "    squared_diff_imputed = np.sum((imputed_values - true_values) ** 2)\n",
    "    \n",
    "    # Calculate the mean of the true values\n",
    "    mean_true = np.nanmean(true_data)\n",
    "    \n",
    "    # Calculate the sum of squared differences between mean and true values\n",
    "    squared_diff_mean = np.sum((mean_true - true_values) ** 2)\n",
    "    \n",
    "    # Calculate the Imputation Quality Score (IQS)\n",
    "    iqs = 1 - (squared_diff_imputed / squared_diff_mean)\n",
    "    \n",
    "    return iqs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
