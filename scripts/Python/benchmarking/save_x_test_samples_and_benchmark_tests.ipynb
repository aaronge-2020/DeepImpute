{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test sample IDs saved to ../../../Data/model_results_unphased_all_PRS/linear_regression/csv_files/X_test_sample_ids.txt\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load and preprocess the data\n",
    "data_directory = '../../../Data/Filtered_unphased_training_data_union_final/'\n",
    "output_folder = \"../../../Data/model_results_unphased_all_PRS/linear_regression/\"\n",
    "csv_folder = output_folder + \"csv_files/\"\n",
    "os.makedirs(csv_folder, exist_ok=True)\n",
    "\n",
    "file_name = data_directory + \"23AndMe_PRS313_merged_chr2_matching_combined.parquet\"\n",
    "data = pd.read_parquet(file_name)\n",
    "\n",
    "# Split the data into features and target\n",
    "X = data.filter(regex='^(?!.*PRS313_)')\n",
    "y = data.filter(regex='PRS313_')\n",
    "sample_ids = data.index.values  # Assuming sample IDs are in the index\n",
    "\n",
    "# Save the y dataframe\n",
    "y.to_csv(data_directory + \"true_labels_y_test.csv\")\n",
    "\n",
    "# Split the data into train-validation and test sets\n",
    "X_train_val, X_test, y_train_val, y_test, sample_train_val, sample_test = train_test_split(\n",
    "    X, y, sample_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save sample IDs of X_test to a file\n",
    "x_test_sample_ids_file = csv_folder + \"X_test_sample_ids.txt\"\n",
    "np.savetxt(x_test_sample_ids_file, sample_test, fmt='%s')\n",
    "\n",
    "print(f\"X_test sample IDs saved to {x_test_sample_ids_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Test Set Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "data_directory = '../../../Data/Filtered_unphased_training_data_union_final/'\n",
    "output_dir = \"../../../Data/y_test_labels_unphased/\"\n",
    "output_dir_benchmark = \"../../../scripts/Python/benchmarking/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Initialize empty dataframes to store all y_test values\n",
    "all_y_test = pd.DataFrame()\n",
    "all_missing_values_y_test = pd.DataFrame()\n",
    "all_simple_imputation_values_y_test = pd.DataFrame()\n",
    "\n",
    "# Load the MAF values\n",
    "maf_df = pd.read_csv(\"../../../Data/MAF_calculations/23AndMe_PRS313_all_chromosomes_MAF.csv\")\n",
    "maf_dict = maf_df.set_index('SNP')['MAF'].to_dict()\n",
    "\n",
    "for i in range(1, 23):\n",
    "    file_name = data_directory + f\"23AndMe_PRS313_merged_chr{i}_matching_combined.parquet\"\n",
    "    data = pd.read_parquet(file_name)\n",
    "\n",
    "    # Split the data into features and target\n",
    "    X = data.filter(regex='^(?!.*PRS313_)')\n",
    "    y = data.filter(regex='PRS313_')\n",
    "    sample_ids = data.index.values  # Assuming sample IDs are in the index\n",
    "\n",
    "    # Split the data into train-validation and test sets\n",
    "    X_train_val, X_test, y_train_val, y_test, sample_train_val, sample_test = train_test_split(\n",
    "        X, y, sample_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Convert y_test to a dataframe with sample_ids as a column\n",
    "    y_test['sample_id'] = sample_test\n",
    "    y_test.set_index('sample_id', inplace=True)\n",
    "\n",
    "    # Create missing_values_y_test\n",
    "    missing_values_y_test = y_test.copy()\n",
    "    missing_values_y_test.loc[:, missing_values_y_test.columns.str.contains('PRS313_Unknown')] = 0\n",
    "\n",
    "    # Create simple_imputation_values_y_test\n",
    "    simple_imputation_values_y_test = y_test.copy()\n",
    "    for col in simple_imputation_values_y_test.columns:\n",
    "        if \"PRS313_Unknown\" in col:\n",
    "            maf_value = maf_dict.get(col, 0)  # Default to 0 if SNP not found in MAF dictionary\n",
    "            simple_imputation_values_y_test[col] = simple_imputation_values_y_test[col].apply(\n",
    "                lambda x: 2 * maf_value if \"PRS313_Unknown\" in col else x\n",
    "            )\n",
    "\n",
    "    # Merge y_test with all_y_test\n",
    "    if all_y_test.empty:\n",
    "        all_y_test = y_test\n",
    "        all_missing_values_y_test = missing_values_y_test\n",
    "        all_simple_imputation_values_y_test = simple_imputation_values_y_test\n",
    "    else:\n",
    "        all_y_test = all_y_test.join(y_test, how='outer')\n",
    "        all_missing_values_y_test = all_missing_values_y_test.join(missing_values_y_test, how='outer')\n",
    "        all_simple_imputation_values_y_test = all_simple_imputation_values_y_test.join(simple_imputation_values_y_test, how='outer')\n",
    "\n",
    "    # Save the individual y dataframe\n",
    "    y_test.to_csv(output_dir + f\"chr{i}_true_labels_y_test.csv\")\n",
    "    # missing_values_y_test.to_csv(output_dir + f\"chr{i}_missing_values_y_test.csv\")\n",
    "    # simple_imputation_values_y_test.to_csv(output_dir + f\"chr{i}_simple_imputation_values_y_test.csv\")\n",
    "\n",
    "# Save all y_test values into single CSV files\n",
    "all_y_test.to_csv(output_dir_benchmark + \"all_chr_true_labels_y_test.csv\")\n",
    "all_missing_values_y_test.to_csv(output_dir_benchmark + \"all_chr_missing_values_y_test.csv\")\n",
    "all_simple_imputation_values_y_test.to_csv(output_dir_benchmark + \"all_chr_simple_imputation_values_y_test.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the X_train_val SNPs from 23andME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Split the data into train-validation and test sets\u001b[39;00m\n\u001b[1;32m     31\u001b[0m X_train_val, X_test, y_train_val, y_test, sample_train_val, sample_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m     32\u001b[0m     X, y, sample_ids, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m snp_ids \u001b[38;5;241m=\u001b[39m extract_snp_ids(X_train_val\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Save the y dataframe\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# X_train_val.to_csv(output_dir + f\"chr{i}_true_labels_y_test.csv\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Split the data into train-validation and test sets\u001b[39;00m\n\u001b[1;32m     31\u001b[0m X_train_val, X_test, y_train_val, y_test, sample_train_val, sample_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m     32\u001b[0m     X, y, sample_ids, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m snp_ids \u001b[38;5;241m=\u001b[39m extract_snp_ids(X_train_val\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Save the y dataframe\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# X_train_val.to_csv(output_dir + f\"chr{i}_true_labels_y_test.csv\")\u001b[39;00m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "data_directory = '../../../Data/Filtered_unphased_training_data_union_final/'\n",
    "\n",
    "output_dir = \"../../../Data/y_test_labels_unphased/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "# Extract SNP IDs from column names\n",
    "def extract_snp_ids(column_names):\n",
    "    snp_ids = []\n",
    "    for col in column_names:\n",
    "        match = \n",
    "        if match:\n",
    "            snp_ids.append(col)\n",
    "    return snp_ids\n",
    "\n",
    "for i in range(1, 23):\n",
    "    file_name = data_directory + \\\n",
    "        f\"23AndMe_PRS313_merged_chr{i}_matching_combined.parquet\"\n",
    "    data = pd.read_parquet(file_name)\n",
    "\n",
    "    # Split the data into features and target\n",
    "    X = data.filter(regex='^(?!.*PRS313_)')\n",
    "    y = data.filter(regex='PRS313_')\n",
    "    sample_ids = data.index.values  # Assuming sample IDs are in the index\n",
    "\n",
    "    # Split the data into train-validation and test sets\n",
    "    X_train_val, X_test, y_train_val, y_test, sample_train_val, sample_test = train_test_split(\n",
    "        X, y, sample_ids, test_size=0.2, random_state=42)\n",
    "    snp_ids = extract_snp_ids(X_train_val.columns)\n",
    "\n",
    "    # Save the y dataframe\n",
    "    # X_train_val.to_csv(output_dir + f\"chr{i}_true_labels_y_test.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
