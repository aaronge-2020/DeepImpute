{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use LD Proxy API to find the relevant positions for each of the PRS313 SNPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "\n",
    "\n",
    "# API key\n",
    "api_key = \"ac16be4ad92d\"\n",
    "\n",
    "# Base URL for LDProxy API\n",
    "base_url = \"https://ldlink.nih.gov/LDlinkRest/ldproxy\"\n",
    "\n",
    "population = \"ALL\"\n",
    "\n",
    "window = 1000000\n",
    "\n",
    "r2_threshold = 0.01\n",
    "\n",
    "# Read the chromosome positions from the text file\n",
    "PRS313_LD = pd.read_excel(\"../../Data/PRS313_with_23andMe.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Iterate over each position\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m PRS313_LD:\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43min_23andMe\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m      8\u001b[0m          \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Extract chromosome and position from the line\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a folder to store the CSV files\n",
    "output_folder = \"../ld_variants\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterate over each position\n",
    "for sample in PRS313_LD:\n",
    "    if sample[\"in_23andMe\"] == True:\n",
    "         continue\n",
    "\n",
    "    # Extract chromosome and position from the line\n",
    "    chrom = sample[\"Chromosome\"]\n",
    "    pos = sample[\"Positionb\"]\n",
    "    \n",
    "    # Construct the API request URL\n",
    "    url = f\"{base_url}?var={chrom}:{pos}&pop={population}&r2_d=r2&window={window}&genome_build=grch37&token={api_key}\"\n",
    "    \n",
    "    # Send the API request\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "            # Create a StringIO object from the data\n",
    "            data_io = io.StringIO(response.text)\n",
    "\n",
    "            # Read the data into a DataFrame using read_csv\n",
    "            df = pd.read_csv(data_io, sep='\\\\t')\n",
    "            \n",
    "            # # Filter variants with high LD scores (e.g., R2 >= 0.8)\n",
    "            # high_ld_variants = df[df[\"R2\"].astype(float) >= 0.8]\n",
    "            \n",
    "            # Generate a unique filename for the CSV file\n",
    "            output_file = os.path.join(output_folder, f\"{chrom}_{pos}.csv\")\n",
    "            \n",
    "            # Save the high LD variants to a CSV file\n",
    "            df.to_csv(output_file, index=False)\n",
    "            \n",
    "            print(f\"Saved high LD variants for {chrom}:{pos} to {output_file}\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data for {chrom}:{pos}. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find overlap b/w 23AndMe and LD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping LD for 12_14413931.csv because it is a known position. Column added to training data\n",
      "chr12_115796577_A_G_PRS313_Unknown\n",
      "Processed 12_115796577.csv\n",
      "Found 27 matching columns\n",
      "Skipping LD for 12_115835836.csv because it is a known position. Column added to training data\n",
      "Skipping LD for 12_120832146.csv because it is a known position. Column added to training data\n",
      "chr12_103097887_C_T_PRS313_Unknown\n",
      "Processed 12_103097887.csv\n",
      "Found 60 matching columns\n",
      "Skipping LD for 12_96027759.csv because it is a known position. Column added to training data\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 76\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m coord \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m alleles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;66;03m# Got rid of allele matching, because of coding differences. GC--> C (in PRS313) is coded as (C/-) in dbSNP and LDProxy\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m         mask \u001b[38;5;241m=\u001b[39m (\u001b[43mvariants\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCoord\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcoord\u001b[49m)\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;66;03m# mask = (variants['Coord'] == coord) & (variants['Alleles'] == alleles)\u001b[39;00m\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m     80\u001b[0m             \u001b[38;5;66;03m# Only append column if it has not been added before\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/ops/common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/arraylike.py:40\u001b[0m, in \u001b[0;36mOpsMixin.__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__eq__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meq\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/series.py:5799\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   5796\u001b[0m lvalues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m   5797\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m extract_array(other, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 5799\u001b[0m res_values \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomparison_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5801\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(res_values, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:346\u001b[0m, in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invalid_comparison(lvalues, rvalues, op)\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m lvalues\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rvalues, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 346\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43mcomp_method_OBJECT_ARRAY\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m _na_arithmetic_op(lvalues, rvalues, op, is_cmp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:131\u001b[0m, in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n\u001b[1;32m    129\u001b[0m     result \u001b[38;5;241m=\u001b[39m libops\u001b[38;5;241m.\u001b[39mvec_compare(x\u001b[38;5;241m.\u001b[39mravel(), y\u001b[38;5;241m.\u001b[39mravel(), op)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 131\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mlibops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscalar_compare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "directory = \"../../Data/ld_variants\"\n",
    "\n",
    "# Create a folder to store the CSV files\n",
    "output_folder = \"../../Data/ld_variants\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "def extract_coord_alleles(col_name):\n",
    "    match = re.search(r'chr(\\d+)_(\\d+)_([ACGT]+)_([ACGT,]+)', col_name)\n",
    "    if match:\n",
    "        chr_num, position, ref_allele, alt_alleles = match.groups()\n",
    "        return f'chr{chr_num}:{position}', f'({ref_allele}/{alt_alleles})'\n",
    "    return None, None\n",
    "\n",
    "# Create an empty list to store the matching variants across all chromosomes\n",
    "matching_variants_all = []\n",
    "\n",
    "training_data_folder = \"../../Data/Raw_training_data_23andMe_union/\"\n",
    "folder = \"../../Data/Filtered_unphased_training_data_union/\"\n",
    "\n",
    "\n",
    "# Iterate over each chromosome folder\n",
    "for chrom_folder in os.listdir(output_folder):\n",
    "    # Check if the folder is a chromosome folder\n",
    "    if chrom_folder.startswith(\"chr\"):\n",
    "        # Extract the chromosome number\n",
    "        chrom = chrom_folder[3:]\n",
    "\n",
    "        files_folder = os.path.join(output_folder, chrom_folder)\n",
    "        \n",
    "        # Load the training data for the chromosome\n",
    "        training_data = pd.read_parquet(f\"{training_data_folder}23AndMe_PRS313_merged_chr{chrom}.parquet\")\n",
    "        \n",
    "        # Create empty lists to store the matching columns and variants for the chromosome\n",
    "        matching_columns = []\n",
    "        matching_variants_chrom = []\n",
    "        not_found_snps_chrom = []\n",
    "\n",
    "        # Read all the files in the folder\n",
    "        for filename in os.listdir(files_folder):\n",
    "\n",
    "            # Check if the file is a CSV file\n",
    "            if filename.endswith(\".csv\"):\n",
    "\n",
    "                # Check if the position is an unknown position or a known position \n",
    "                position = filename.split('.')[0]\n",
    "\n",
    "                # Find the column name with the position\n",
    "                position_column = training_data.columns[training_data.columns.str.contains(position)]\n",
    "\n",
    "                if (position_column[0].split(\"_\")[-1] == \"Known\"):\n",
    "                    # If the position is a known position, skip the file, because we don't need LD proxies for known positions\n",
    "                    print(f\"Skipping LD for {filename} because it is a known position. Column added to training data\")\n",
    "\n",
    "                    if position_column[0] not in matching_columns:\n",
    "\n",
    "                        # Add the matching column to the list\n",
    "                        matching_columns.append(position_column[0])\n",
    "\n",
    "                    continue\n",
    "\n",
    "\n",
    "                # Load the variants for the file\n",
    "                variants = pd.read_csv(os.path.join(files_folder, filename))\n",
    "                \n",
    "                # Find the matching columns in the training data\n",
    "                for column in training_data.columns:\n",
    "                    coord, alleles = extract_coord_alleles(column)\n",
    "                    if coord is not None and alleles is not None:\n",
    "                        try:\n",
    "                            # Got rid of allele matching, because of coding differences. GC--> C (in PRS313) is coded as (C/-) in dbSNP and LDProxy\n",
    "                            mask = (variants['Coord'] == coord)\n",
    "                            # mask = (variants['Coord'] == coord) & (variants['Alleles'] == alleles)\n",
    "                                \n",
    "                            if mask.any():\n",
    "                                # Only append column if it has not been added before\n",
    "                                if column not in matching_columns:\n",
    "\n",
    "                                    # Check column added is an unknown position\n",
    "                                    if (column.split(\"_\")[-1] == \"Unknown\"):\n",
    "                                        print(column)\n",
    "\n",
    "\n",
    "                                    matching_columns.append(column)\n",
    "                                    matching_variants_chrom.append(variants[mask])\n",
    "                        except KeyError:\n",
    "                            error_position = filename.split('.')[0]\n",
    "                            not_found_snps_chrom.append(error_position)\n",
    "                            print(f\"SNP {error_position} not found in dbSNP and cannot be proxied using LDProxy\")\n",
    "\n",
    "                            # Find the columns in training_data.columns with positions within +/- 500K BP of the error_position\n",
    "                            counter_error_added = 0\n",
    "                            for col in training_data.columns:\n",
    "                                coord, _ = extract_coord_alleles(col)\n",
    "                                if coord is not None:\n",
    "                                    col_position = int(coord.split(':')[1])\n",
    "                                    error_bp = int(error_position.split('_')[1])\n",
    "                                    if abs(col_position - error_bp) <= 500000 and col not in matching_columns:\n",
    "                                        counter_error_added += 1\n",
    "                                        matching_columns.append(col)\n",
    "\n",
    "                            print(f\"Added {counter_error_added} columns to matching_columns for missing data position: {error_position}\")\n",
    "                            \n",
    "                            break\n",
    "\n",
    "                print(f\"Processed {filename}\")\n",
    "                print(f\"Found {len(matching_columns)} matching columns\")\n",
    "                \n",
    "\n",
    "        # Get the matching columns from the training data\n",
    "        matching_data = training_data[matching_columns]\n",
    "\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        save_path = f\"{folder}/23AndMe_PRS313_merged_chr{chrom}_matching.parquet\"\n",
    "        # Save the matching data for the chromosome\n",
    "        matching_data.to_parquet(save_path)\n",
    "\n",
    "        # Concatenate the matching variants for the chromosome into a single dataframe\n",
    "        matching_variants_chrom_df = pd.concat(matching_variants_chrom, ignore_index=True)\n",
    "        \n",
    "        # Append the matching variants for the chromosome to the overall list\n",
    "        matching_variants_all.append(matching_variants_chrom_df)\n",
    "\n",
    "        print(f\"Saved to file {save_path}\")\n",
    "        print(f\"Found {len(matching_columns)} matching columns\")\n",
    "        print(f\"Found {len(matching_variants_chrom_df)} matching variants\")\n",
    "\n",
    "# Concatenate the matching variants from all chromosomes into a single dataframe\n",
    "matching_variants_all_df = pd.concat(matching_variants_all, ignore_index=True)\n",
    "\n",
    "# Save the dataframe with matching variants across all chromosomes\n",
    "matching_variants_all_df.to_csv(f\"{folder}/23AndMe_matching_variants.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Reference and Alternate allele information to the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "matching_variants_all_df = pd.read_csv(f\"{folder}/23AndMe_matching_variants.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "# Directory containing the VCF files\n",
    "vcf_directory = \"../../Data/23AndMePositionsUnion\"\n",
    "\n",
    "# Read in the 23andMe_matching_variants.csv file\n",
    "with open(f\"{folder}/23andMe_matching_variants.csv\", \"r\") as csv_file:\n",
    "    reader = csv.DictReader(csv_file)\n",
    "    data = list(reader)\n",
    "\n",
    "# Process each row in the CSV\n",
    "for row in data:\n",
    "    # Extract the chromosome and position from the Coord column\n",
    "    chrom, pos = row[\"Coord\"].split(\":\")\n",
    "    pos = pos.replace(\",\", \"\")  # Remove any commas from the position\n",
    "    \n",
    "    # Construct the path to the corresponding VCF file\n",
    "    vcf_file = os.path.join(vcf_directory, chrom, f\"{chrom}_pos{pos}.vcf\")\n",
    "    \n",
    "    # Check if the VCF file exists\n",
    "    if os.path.isfile(vcf_file):\n",
    "        # Read the VCF file and extract the reference and alternate alleles\n",
    "        with open(vcf_file, \"r\") as file:\n",
    "            for line in file:\n",
    "                if not line.startswith(\"#\"):\n",
    "                    fields = line.strip().split(\"\\t\")\n",
    "                    ref = fields[3]\n",
    "                    alt = fields[4].split(\",\")  # Split alternate alleles by comma\n",
    "                    break\n",
    "        \n",
    "        # Update the row with the reference and alternate alleles\n",
    "        row[\"Ref\"] = ref\n",
    "        row[\"Alt\"] = \",\".join(alt)  # Join alternate alleles with comma\n",
    "    else:\n",
    "        # If the VCF file doesn't exist, set the reference and alternate alleles to empty strings\n",
    "        row[\"Ref\"] = \"\"\n",
    "        row[\"Alt\"] = \"\"\n",
    "\n",
    "# Write the updated data to a new CSV file\n",
    "with open(f\"{folder}/23andMe_matching_variants_updated.csv\", \"w\", newline=\"\") as csv_file:\n",
    "    fieldnames = data[0].keys()\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Final Training Data with +/- 500K BP Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4138\n",
      "3266\n",
      "3217\n",
      "2468\n",
      "5178\n",
      "3592\n",
      "2290\n",
      "2740\n",
      "1953\n",
      "2854\n",
      "3720\n",
      "2333\n",
      "861\n",
      "806\n",
      "1252\n",
      "3042\n",
      "1565\n",
      "1537\n",
      "1847\n",
      "1127\n",
      "575\n",
      "2007\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "window_size = 250000\n",
    "pattern = re.compile(r\"chr\\d+_(\\d+)_\")\n",
    "\n",
    "# Create an empty list to store the matching variants across all chromosomes\n",
    "matching_variants_all = []\n",
    "\n",
    "for chrom in range(1, 23):\n",
    "    # Load the training data for the chromosome\n",
    "    training_data = pd.read_parquet(f\"../../Data/Raw_training_data/23AndMe_PRS313_merged_chr{chrom}.parquet\")\n",
    "    \n",
    "    # Get all columns with \"PRS313\" in the name\n",
    "    prs313_unknown_columns = [col for col in training_data.columns if \"PRS313_Unknown\" in col]\n",
    "    prs313_unknown_positions = [int(pattern.search(col).group(1)) for col in prs313_unknown_columns]\n",
    "    prs313_unknown_positions_set = set(prs313_unknown_positions)\n",
    "    \n",
    "    # Get all columns in training_data that contain a number within +/- 500k of the PRS313_Unknown position\n",
    "    filtered_columns = [col for col in training_data.columns if any(abs(int(pattern.search(col).group(1)) - pos) <= window_size for pos in prs313_unknown_positions_set)]\n",
    "    \n",
    "    training_data_filtered = training_data[filtered_columns]\n",
    "\n",
    "    print(len(filtered_columns))\n",
    "\n",
    "    # Save the filtered training data for the chromosome\n",
    "    # training_data_filtered.to_parquet(f\"../../Data/500k_window_filtered_data/23AndMe_PRS313_merged_chr{chrom}_filtered.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2504, 2290)\n",
      "(2504, 444)\n"
     ]
    }
   ],
   "source": [
    "chrom = 7\n",
    "\n",
    "training_data_window = pd.read_parquet(f\"../../Data/500k_window_filtered_data/23AndMe_PRS313_merged_chr{chrom}_filtered.parquet\")\n",
    "print(training_data_window.shape)\n",
    "\n",
    "training_data_ld_proxy = pd.read_parquet(f\"../../Data/filtered_training_data/23AndMe_PRS313_merged_chr{chrom}_matching.parquet\")\n",
    "print(training_data_ld_proxy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['chr1_88177403_G_A', 'chr1_88127152_T_C', 'chr1_88208135_G_A',\n",
       "       'chr1_88109828_G_A', 'chr1_88086894_C_A', 'chr1_88091734_C_T',\n",
       "       'chr1_88116467_A_C', 'chr1_88220810_AC_A', 'chr1_88073752_G_T',\n",
       "       'chr1_88120134_T_C',\n",
       "       ...\n",
       "       'chr1_172366806_A_G', 'chr1_172419651_T_G', 'chr1_172316842_G_A',\n",
       "       'chr1_171934790_G_A', 'chr1_172632057_A_G', 'chr1_172627498_C_T',\n",
       "       'chr1_172464519_T_G', 'chr1_172328767_T_TA_PRS313_Unknown',\n",
       "       'chr1_121280485_A_G', 'chr1_121137155_A_G'],\n",
       "      dtype='object', length=1132)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'chr1_88177403_G_A', 'chr1_88127152_T_C', 'chr1_88208135_G_A',\n",
    "       'chr1_88109828_G_A', 'chr1_88086894_C_A', 'chr1_88091734_C_T',\n",
    "       'chr1_88116467_A_C', 'chr1_88220810_AC_A', 'chr1_88073752_G_T',\n",
    "       'chr1_88120134_T_C',\n",
    "       ...\n",
    "       'chr1_172366806_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
