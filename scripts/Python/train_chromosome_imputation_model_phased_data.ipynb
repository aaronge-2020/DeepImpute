{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_iqs_phased(true_haplotypes, imputed_haplotypes, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calculate the Imputation Quality Score (IQS) for phased haplotypes.\n",
    "\n",
    "    Args:\n",
    "        true_haplotypes (numpy.ndarray): 2D array of true haplotypes, where each haplotype is represented by binary values (0 or 1).\n",
    "        imputed_haplotypes (numpy.ndarray): 2D array of imputed haplotypes, where each haplotype is represented by floating-point values between 0 and 1.\n",
    "        threshold (float): Threshold for converting imputed probabilities to binary values (default: 0.5).\n",
    "\n",
    "    Returns:\n",
    "        float: Imputation Quality Score (IQS).\n",
    "    \"\"\"\n",
    "    # Check if the shapes of true and imputed haplotypes are the same\n",
    "    if true_haplotypes.shape != imputed_haplotypes.shape:\n",
    "        raise ValueError(\"Shape of true haplotypes and imputed haplotypes must be the same.\")\n",
    "\n",
    "    # Convert imputed probabilities to binary values based on the threshold\n",
    "    imputed_binary = (imputed_haplotypes >= threshold).astype(int)\n",
    "\n",
    "    # Create a contingency table\n",
    "    contingency_table = np.zeros((2, 2), dtype=int)\n",
    "\n",
    "    # Fill the contingency table\n",
    "    for true_hap, imputed_hap in zip(true_haplotypes, imputed_binary):\n",
    "        for true_allele, imputed_allele in zip(true_hap, imputed_hap):\n",
    "            contingency_table[int(true_allele), int(imputed_allele)] += 1\n",
    "\n",
    "    # Calculate the total number of alleles\n",
    "    total_alleles = np.sum(contingency_table)\n",
    "\n",
    "    # Calculate the observed agreement (Po)\n",
    "    po = np.sum(np.diag(contingency_table)) / total_alleles\n",
    "\n",
    "    # Calculate the expected agreement by chance (Pc)\n",
    "    true_counts = np.sum(contingency_table, axis=1)\n",
    "    imputed_counts = np.sum(contingency_table, axis=0)\n",
    "    pc = np.sum(true_counts * imputed_counts) / (total_alleles ** 2)\n",
    "\n",
    "    # Calculate the Imputation Quality Score (IQS)\n",
    "    iqs = (po - pc) / (1 - pc)\n",
    "\n",
    "    return iqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown PRS313 SNPs:  40\n",
      "Known PRS313 SNPs:  20\n",
      "23AndMe SNPs with LD to Unknown PRS313 SNPs:  1700\n",
      "Total SNPs used for Training:  1720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-04-29 15:15:02,169] Trial 7 finished with value: 0.568806591629982 and parameters: {'learning_rate': 0.0518680673246504, 'l1_coef': 0.00013206670528191153, 'patience': 10, 'batch_size': 256}. Best is trial 5 with value: 0.16208273768424988.\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-04-29 15:15:05,578] Trial 6 finished with value: 0.2744054410606623 and parameters: {'learning_rate': 0.04874113112792734, 'l1_coef': 3.238419629592629e-05, 'patience': 14, 'batch_size': 128}. Best is trial 5 with value: 0.16208273768424988.\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-04-29 15:15:16,943] Trial 10 finished with value: 0.4736872106790543 and parameters: {'learning_rate': 0.005274404223933004, 'l1_coef': 0.002037460959334297, 'patience': 14, 'batch_size': 256}. Best is trial 5 with value: 0.16208273768424988.\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-04-29 15:15:49,856] Trial 11 finished with value: 0.10575263649225235 and parameters: {'learning_rate': 0.05537115049140719, 'l1_coef': 1.810919133702422e-05, 'patience': 20, 'batch_size': 64}. Best is trial 11 with value: 0.10575263649225235.\n",
      "[I 2024-04-29 15:15:51,858] Trial 9 finished with value: 0.49470812976360323 and parameters: {'learning_rate': 0.0003852878299907987, 'l1_coef': 0.0023589865851102445, 'patience': 5, 'batch_size': 256}. Best is trial 11 with value: 0.10575263649225235.\n",
      "[I 2024-04-29 15:15:57,151] Trial 12 finished with value: 0.10858490742169893 and parameters: {'learning_rate': 0.00334526305150122, 'l1_coef': 1.902039288555638e-05, 'patience': 10, 'batch_size': 32}. Best is trial 11 with value: 0.10575263649225235.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "from sklearn.metrics import r2_score as sklearn_r2_score\n",
    "import optuna\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# Load and preprocess the data\n",
    "data_directory = '../../Data/Filtered_phased_training_data/'\n",
    "start = 1\n",
    "\n",
    "# Initialize lists to store the performance metrics for each chromosome\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "false_positive_rates = []\n",
    "auc_rocs = []\n",
    "r2_scores = []\n",
    "iqs_scores = []\n",
    "\n",
    "# Create folders for saving files\n",
    "output_folder = \"../../Data/model_results/logistic_regression/\"\n",
    "model_folder = output_folder + \"models/\"\n",
    "csv_folder = output_folder + \"csv_files/\"\n",
    "curve_folder = output_folder + \"roc_curves/\"\n",
    "\n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "os.makedirs(csv_folder, exist_ok=True)\n",
    "os.makedirs(curve_folder, exist_ok=True)\n",
    "\n",
    "for chromosome_number in range(start, 23):\n",
    "    # Create subfolders for the current chromosome\n",
    "    chr_model_folder = model_folder + f\"chr{chromosome_number}/\"\n",
    "    chr_csv_folder = csv_folder + f\"chr{chromosome_number}/\"\n",
    "    chr_curve_folder = curve_folder + f\"chr{chromosome_number}/\"\n",
    "\n",
    "    os.makedirs(chr_model_folder, exist_ok=True)\n",
    "    os.makedirs(chr_csv_folder, exist_ok=True)\n",
    "    os.makedirs(chr_curve_folder, exist_ok=True)\n",
    "\n",
    "    file_name = data_directory + f\"23AndMe_PRS313_merged_chr{chromosome_number}_matching_split.parquet\"\n",
    "    data = pd.read_parquet(file_name)\n",
    "\n",
    "\n",
    "    # Split the data into features and target\n",
    "    X = torch.tensor(data.filter(regex='^(?!.*Unknown)').values, dtype=torch.float32)\n",
    "    y = torch.tensor(data.filter(regex='Unknown').values, dtype=torch.float32)\n",
    "\n",
    "    print(\"Unknown PRS313 SNPs: \", y.shape[1])\n",
    "    print(\"Known PRS313 SNPs: \", data[[col for col in data.columns if \"PRS313_Known\" in col]].shape[1])\n",
    "    print(\"23AndMe SNPs with LD to Unknown PRS313 SNPs: \", data[[col for col in data.columns if \"PRS313_\" not in col]].shape[1])\n",
    "    print(\"Total SNPs used for Training: \", X.shape[1])\n",
    "\n",
    "    # Split the data into train-validation and test sets\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Define the logistic regression model with lasso regularization\n",
    "    class LogisticRegression(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim, l1_coef=0.0):\n",
    "            super(LogisticRegression, self).__init__()\n",
    "            self.linear = nn.Linear(input_dim, output_dim)\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "            self.l1_coef = l1_coef\n",
    "\n",
    "        def forward(self, x):\n",
    "            out = self.linear(x)\n",
    "            out = self.sigmoid(out)\n",
    "            return out\n",
    "\n",
    "        def l1_loss(self):\n",
    "            return self.l1_coef * torch.norm(self.linear.weight, p=1)\n",
    "        \n",
    "    # Set the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Set the hyperparameters for tuning\n",
    "    input_dim = X_train_val.shape[1]\n",
    "    output_dim = y_train_val.shape[1]\n",
    "    num_epochs = 500\n",
    "    batch_size = 128\n",
    "\n",
    "    # Define the objective function for Optuna with cross-validation and early stopping\n",
    "    def objective(trial):\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-1, log=True)\n",
    "        l1_coef = trial.suggest_float('l1_coef', 1e-5, 1e-1, log=True)\n",
    "        patience = trial.suggest_int('patience', 5, 20)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "\n",
    "        model = LogisticRegression(input_dim, output_dim, l1_coef).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        criterion = nn.BCELoss()\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=False)\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        fold_losses = []\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_val, y_train_val.argmax(dim=1))):\n",
    "            X_train, X_val = X_train_val[train_idx], X_train_val[val_idx]\n",
    "            y_train, y_val = y_train_val[train_idx], y_train_val[val_idx]\n",
    "\n",
    "            train_dataset = TensorDataset(X_train, y_train)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "            best_val_loss = float('inf')\n",
    "            counter = 0\n",
    "\n",
    "            for epoch in range(num_epochs):\n",
    "                train_loss = 0.0\n",
    "                for batch_X, batch_y in train_loader:\n",
    "                    batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "                    outputs = model(batch_X)\n",
    "                    loss = criterion(outputs, batch_y) + model.l1_loss()\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    train_loss += loss.item()\n",
    "\n",
    "                train_loss /= len(train_loader)\n",
    "\n",
    "                val_dataset = TensorDataset(X_val, y_val)\n",
    "                val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    val_loss = 0.0\n",
    "                    for batch_X, batch_y in val_loader:\n",
    "                        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                        outputs = model(batch_X)\n",
    "                        loss = criterion(outputs, batch_y) + model.l1_loss()\n",
    "                        val_loss += loss.item()\n",
    "\n",
    "                    val_loss /= len(val_loader)\n",
    "                    scheduler.step(val_loss)\n",
    "\n",
    "                    if val_loss < best_val_loss:\n",
    "                        best_val_loss = val_loss\n",
    "                        counter = 0\n",
    "                    else:\n",
    "                        counter += 1\n",
    "\n",
    "                    if counter >= patience:\n",
    "                        # print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                        break\n",
    "\n",
    "            fold_losses.append(best_val_loss)\n",
    "\n",
    "        return np.mean(fold_losses)\n",
    "\n",
    "    # Create the \"optuna_studies\" folder if it doesn't exist\n",
    "    os.makedirs(\"optuna_studies\", exist_ok=True)\n",
    "\n",
    "    # Create an Optuna study and optimize the hyperparameters\n",
    "    study_name = f\"chr{chromosome_number}_study\"\n",
    "    storage_name = f\"sqlite:///optuna_studies/{study_name}.db\"\n",
    "\n",
    "    # Check if the study exists\n",
    "\n",
    "    current_dir = os.getcwd()\n",
    "    study_exists = os.path.exists(current_dir + f\"/optuna_studies/{study_name}.db\")\n",
    "    \n",
    "    if study_exists:\n",
    "        # Load the existing study\n",
    "        study = optuna.load_study(study_name=study_name, storage=storage_name)\n",
    "    else:\n",
    "        # Create a new study\n",
    "        study = optuna.create_study(direction='minimize', study_name=study_name, storage=storage_name)\n",
    "\n",
    "    study.optimize(objective, n_trials=50, n_jobs=-1)\n",
    "\n",
    "    # Print the best hyperparameters and best value\n",
    "    print(f\"Chr {chromosome_number} - Best hyperparameters: {study.best_params}\")\n",
    "    print(f\"Chr {chromosome_number} - Best value: {study.best_value:.4f}\")\n",
    "\n",
    "    # Train the final model with the best hyperparameters and early stopping\n",
    "    best_learning_rate = study.best_params['learning_rate']\n",
    "    best_l1_coef = study.best_params['l1_coef']\n",
    "    best_patience = study.best_params['patience']\n",
    "    best_batch_size = study.best_params['batch_size']\n",
    "\n",
    "    model = LogisticRegression(input_dim, output_dim, best_l1_coef).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_learning_rate)\n",
    "    criterion = nn.BCELoss()\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=False)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_val, y_train_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=best_batch_size, shuffle=True)\n",
    "\n",
    "    best_train_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y) + model.l1_loss()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "        if train_loss < best_train_loss:\n",
    "            best_train_loss = train_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        if counter >= best_patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "        scheduler.step(train_loss)\n",
    "\n",
    "    # Save the final model\n",
    "    model_save_path = chr_model_folder + f'final_model_chr{chromosome_number}.pth'\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"Final model saved at: {model_save_path}\")\n",
    "\n",
    "    # Evaluate the final model on the test set\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test.to(device))\n",
    "        test_preds = (test_outputs > 0.5).float()\n",
    "        test_accuracy = float(((test_preds > 0.5) == y_test).float().mean())\n",
    "        test_precision = precision_score(y_test.cpu().numpy(), test_preds.cpu().numpy(), average='micro')\n",
    "        test_recall = recall_score(y_test.cpu().numpy(), test_preds.cpu().numpy(), average='micro')\n",
    "        test_f1 = f1_score(y_test.cpu().numpy(), test_preds.cpu().numpy(), average='micro')\n",
    "        test_roc_auc = roc_auc_score(y_test.cpu().numpy(), test_outputs.cpu().numpy(), average='micro')\n",
    "        test_r2 = sklearn_r2_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "        test_iqs = calculate_iqs_phased(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "\n",
    "        # Calculate false positive rate\n",
    "        cm = confusion_matrix(y_test.cpu().numpy().ravel(), test_preds.cpu().numpy().ravel())\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        test_fpr = fp / (fp + tn)\n",
    "\n",
    "        # Append performance metrics to the lists\n",
    "        accuracies.append(test_accuracy)\n",
    "        precisions.append(test_precision)\n",
    "        recalls.append(test_recall)\n",
    "        false_positive_rates.append(test_fpr)\n",
    "        auc_rocs.append(test_roc_auc)\n",
    "        r2_scores.append(test_r2)\n",
    "        iqs_scores.append(test_iqs)\n",
    "\n",
    "        # Calculate individual R^2 scores for each SNP\n",
    "        individual_r2_scores = sklearn_r2_score(y_test.cpu().numpy(), test_outputs.cpu().numpy(), multioutput='raw_values')\n",
    "\n",
    "        # Calculate individual IQS scores for each SNP\n",
    "        individual_iqs_scores = np.array([calculate_iqs_phased(y_test.cpu().numpy()[:, i].reshape(-1, 1), test_outputs.cpu().numpy()[:, i].reshape(-1, 1)) for i in range(y_test.shape[1])])\n",
    "\n",
    "        # Get the names of the SNPs from the original dataframe\n",
    "        snp_names = data.filter(regex='Unknown').columns\n",
    "\n",
    "        # Save individual R^2 scores to a CSV file\n",
    "        csv_file = chr_csv_folder + f'individual_r2_scores_chr{chromosome_number}.csv'\n",
    "\n",
    "        with open(csv_file, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['SNP', 'R2 Score'])\n",
    "            for snp, r2_score in zip(snp_names, individual_r2_scores):\n",
    "                writer.writerow([snp, r2_score])\n",
    "\n",
    "        print(f\"Individual R^2 scores saved at: {csv_file}\")\n",
    "\n",
    "        # Save individual IQS scores to a CSV file\n",
    "        iqs_csv_file = chr_csv_folder + f'individual_iqs_scores_chr{chromosome_number}.csv'\n",
    "\n",
    "        with open(iqs_csv_file, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['SNP', 'IQS Score'])\n",
    "            for snp, iqs_score in zip(snp_names, individual_iqs_scores):\n",
    "                writer.writerow([snp, iqs_score])\n",
    "\n",
    "        print(f\"Individual IQS scores saved at: {iqs_csv_file}\")\n",
    "\n",
    "        # Save individual AUC ROC curves for each SNP\n",
    "        for i, snp in enumerate(snp_names):\n",
    "            try: \n",
    "                fpr, tpr, _ = roc_curve(y_test.cpu().numpy()[:, i], test_outputs.cpu().numpy()[:, i])\n",
    "                plt.figure()\n",
    "                plt.plot(fpr, tpr, label=f'AUC ROC = {roc_auc_score(y_test.cpu().numpy()[:, i], test_outputs.cpu().numpy()[:, i]):.4f}')\n",
    "                plt.xlabel('False Positive Rate')\n",
    "                plt.ylabel('True Positive Rate')\n",
    "                plt.title(f'AUC ROC Curve - {snp}')\n",
    "                plt.legend()\n",
    "                \n",
    "                curve_file = chr_curve_folder + f'auc_roc_curve_{snp}_chr{chromosome_number}.png'\n",
    "                plt.savefig(curve_file)\n",
    "                plt.close()\n",
    "            except ValueError:\n",
    "                # Save a placeholder image if there is insufficient data\n",
    "                plt.figure()\n",
    "                plt.axis('off')\n",
    "                plt.text(0.5, 0.5, \"Insufficient data for ROC curve\", ha='center', va='center')\n",
    "                curve_file = chr_curve_folder + f'auc_roc_curve_{snp}_chr{chromosome_number}.png'\n",
    "                plt.savefig(curve_file)\n",
    "                plt.close()\n",
    "\n",
    "                print(f\"Skipping SNP {snp} due to insufficient data\")\n",
    "\n",
    "\n",
    "        print(f\"Individual AUC ROC curves saved in: {curve_folder}\")\n",
    "\n",
    "        # Create a DataFrame to store the performance metrics for each chromosome\n",
    "        performance_df = pd.DataFrame({\n",
    "            'Chromosome': list(range(start, chromosome_number + 1)),\n",
    "            'Accuracy': accuracies,\n",
    "            'Precision': precisions,\n",
    "            'Recall': recalls,\n",
    "            'False Positive Rate': false_positive_rates,\n",
    "            'AUC ROC': auc_rocs,\n",
    "            'R2 Score': r2_scores,\n",
    "            'IQS Score': iqs_scores\n",
    "        })\n",
    "\n",
    "        # Save the performance metrics to a CSV file\n",
    "        performance_csv_file = csv_folder + 'performance_metrics.csv'\n",
    "        performance_df.to_csv(performance_csv_file, index=False)\n",
    "        print(f\"Performance metrics saved at: {performance_csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_iqs_unphased(true_genotypes, imputed_genotypes, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calculate the Imputation Quality Score (IQS) for unphased genotypes.\n",
    "\n",
    "    Args:\n",
    "        true_genotypes (numpy.ndarray): 2D array of true genotypes, where each genotype is represented by values 0, 1, or 2.\n",
    "        imputed_genotypes (numpy.ndarray): 2D array of imputed genotypes, where each genotype is represented by floating-point values between 0 and 2.\n",
    "        threshold (float): Threshold for converting imputed probabilities to discrete genotypes (default: 0.5).\n",
    "\n",
    "    Returns:\n",
    "        float: Imputation Quality Score (IQS).\n",
    "    \"\"\"\n",
    "    # Check if the shapes of true and imputed genotypes are the same\n",
    "    if true_genotypes.shape != imputed_genotypes.shape:\n",
    "        raise ValueError(\"Shape of true genotypes and imputed genotypes must be the same.\")\n",
    "\n",
    "    # Convert imputed probabilities to discrete genotypes based on the threshold\n",
    "    imputed_discrete = np.round(imputed_genotypes).astype(int)\n",
    "\n",
    "    # Create a contingency table\n",
    "    contingency_table = np.zeros((3, 3), dtype=int)\n",
    "\n",
    "    # Fill the contingency table\n",
    "    for true_geno, imputed_geno in zip(true_genotypes, imputed_discrete):\n",
    "        for true_allele, imputed_allele in zip(true_geno, imputed_geno):\n",
    "            contingency_table[int(true_allele), int(imputed_allele)] += 1\n",
    "\n",
    "    # Calculate the total number of alleles\n",
    "    total_alleles = np.sum(contingency_table)\n",
    "\n",
    "    # Calculate the observed agreement (Po)\n",
    "    po = np.sum(np.diag(contingency_table)) / total_alleles\n",
    "\n",
    "    # Calculate the expected agreement by chance (Pc)\n",
    "    true_counts = np.sum(contingency_table, axis=1)\n",
    "    imputed_counts = np.sum(contingency_table, axis=0)\n",
    "    pc = np.sum(true_counts * imputed_counts) / (total_alleles ** 2)\n",
    "\n",
    "    # Calculate the Imputation Quality Score (IQS)\n",
    "    iqs = (po - pc) / (1 - pc)\n",
    "\n",
    "    return iqs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
