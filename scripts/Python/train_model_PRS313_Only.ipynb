{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('concatenated_snps_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chr2:192381934:C:T_maternal</th>\n",
       "      <th>chr2:192381934:C:T_paternal</th>\n",
       "      <th>chr8:129199566:G:A_maternal</th>\n",
       "      <th>chr8:129199566:G:A_paternal</th>\n",
       "      <th>chr8:29509616:A:C_maternal</th>\n",
       "      <th>chr8:29509616:A:C_paternal</th>\n",
       "      <th>chr2:121089731:T:C_maternal</th>\n",
       "      <th>chr2:121089731:T:C_paternal</th>\n",
       "      <th>chr15:75750383:T:C_maternal</th>\n",
       "      <th>chr15:75750383:T:C_paternal</th>\n",
       "      <th>...</th>\n",
       "      <th>chr5:176134882:T:A,C_maternal</th>\n",
       "      <th>chr5:176134882:T:A,C_paternal</th>\n",
       "      <th>chr5:52679539:C:CA,CAA_maternal</th>\n",
       "      <th>chr5:52679539:C:CA,CAA_paternal</th>\n",
       "      <th>chr7:91459189:A:AT,ATT_maternal</th>\n",
       "      <th>chr7:91459189:A:AT,ATT_paternal</th>\n",
       "      <th>chr10:22861490:A:C,T_maternal</th>\n",
       "      <th>chr10:22861490:A:C,T_paternal</th>\n",
       "      <th>chr22:38583315:AAAAG:AAAAGAAAG,AAAAGAAAGAAAG,A_maternal</th>\n",
       "      <th>chr22:38583315:AAAAG:AAAAGAAAG,AAAAGAAAGAAAG,A_paternal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>HG00096</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HG00097</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HG00099</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HG00100</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HG00101</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NA21137</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NA21141</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NA21142</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NA21143</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NA21144</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2504 rows × 626 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         chr2:192381934:C:T_maternal  chr2:192381934:C:T_paternal  \\\n",
       "HG00096                         True                         True   \n",
       "HG00097                         True                         True   \n",
       "HG00099                         True                         True   \n",
       "HG00100                        False                         True   \n",
       "HG00101                         True                         True   \n",
       "...                              ...                          ...   \n",
       "NA21137                         True                         True   \n",
       "NA21141                         True                         True   \n",
       "NA21142                         True                        False   \n",
       "NA21143                         True                         True   \n",
       "NA21144                         True                         True   \n",
       "\n",
       "         chr8:129199566:G:A_maternal  chr8:129199566:G:A_paternal  \\\n",
       "HG00096                        False                         True   \n",
       "HG00097                         True                        False   \n",
       "HG00099                        False                        False   \n",
       "HG00100                        False                        False   \n",
       "HG00101                        False                        False   \n",
       "...                              ...                          ...   \n",
       "NA21137                        False                        False   \n",
       "NA21141                        False                        False   \n",
       "NA21142                         True                        False   \n",
       "NA21143                        False                        False   \n",
       "NA21144                        False                        False   \n",
       "\n",
       "         chr8:29509616:A:C_maternal  chr8:29509616:A:C_paternal  \\\n",
       "HG00096                        True                        True   \n",
       "HG00097                        True                        True   \n",
       "HG00099                       False                       False   \n",
       "HG00100                       False                        True   \n",
       "HG00101                        True                        True   \n",
       "...                             ...                         ...   \n",
       "NA21137                        True                        True   \n",
       "NA21141                       False                       False   \n",
       "NA21142                       False                        True   \n",
       "NA21143                        True                        True   \n",
       "NA21144                       False                        True   \n",
       "\n",
       "         chr2:121089731:T:C_maternal  chr2:121089731:T:C_paternal  \\\n",
       "HG00096                         True                        False   \n",
       "HG00097                        False                        False   \n",
       "HG00099                        False                         True   \n",
       "HG00100                        False                        False   \n",
       "HG00101                         True                        False   \n",
       "...                              ...                          ...   \n",
       "NA21137                        False                        False   \n",
       "NA21141                        False                        False   \n",
       "NA21142                        False                        False   \n",
       "NA21143                        False                        False   \n",
       "NA21144                        False                        False   \n",
       "\n",
       "         chr15:75750383:T:C_maternal  chr15:75750383:T:C_paternal  ...  \\\n",
       "HG00096                         True                        False  ...   \n",
       "HG00097                        False                        False  ...   \n",
       "HG00099                        False                        False  ...   \n",
       "HG00100                        False                        False  ...   \n",
       "HG00101                         True                         True  ...   \n",
       "...                              ...                          ...  ...   \n",
       "NA21137                        False                        False  ...   \n",
       "NA21141                        False                         True  ...   \n",
       "NA21142                         True                        False  ...   \n",
       "NA21143                         True                         True  ...   \n",
       "NA21144                        False                         True  ...   \n",
       "\n",
       "         chr5:176134882:T:A,C_maternal  chr5:176134882:T:A,C_paternal  \\\n",
       "HG00096                          False                           True   \n",
       "HG00097                           True                          False   \n",
       "HG00099                          False                          False   \n",
       "HG00100                           True                           True   \n",
       "HG00101                           True                          False   \n",
       "...                                ...                            ...   \n",
       "NA21137                           True                          False   \n",
       "NA21141                           True                           True   \n",
       "NA21142                           True                           True   \n",
       "NA21143                          False                          False   \n",
       "NA21144                           True                           True   \n",
       "\n",
       "         chr5:52679539:C:CA,CAA_maternal  chr5:52679539:C:CA,CAA_paternal  \\\n",
       "HG00096                            False                            False   \n",
       "HG00097                            False                            False   \n",
       "HG00099                            False                            False   \n",
       "HG00100                            False                            False   \n",
       "HG00101                            False                            False   \n",
       "...                                  ...                              ...   \n",
       "NA21137                            False                            False   \n",
       "NA21141                            False                            False   \n",
       "NA21142                            False                            False   \n",
       "NA21143                             True                            False   \n",
       "NA21144                            False                            False   \n",
       "\n",
       "         chr7:91459189:A:AT,ATT_maternal  chr7:91459189:A:AT,ATT_paternal  \\\n",
       "HG00096                             True                            False   \n",
       "HG00097                             True                            False   \n",
       "HG00099                             True                             True   \n",
       "HG00100                            False                             True   \n",
       "HG00101                            False                            False   \n",
       "...                                  ...                              ...   \n",
       "NA21137                            False                            False   \n",
       "NA21141                             True                             True   \n",
       "NA21142                            False                             True   \n",
       "NA21143                             True                            False   \n",
       "NA21144                             True                            False   \n",
       "\n",
       "         chr10:22861490:A:C,T_maternal  chr10:22861490:A:C,T_paternal  \\\n",
       "HG00096                           True                           True   \n",
       "HG00097                           True                           True   \n",
       "HG00099                           True                           True   \n",
       "HG00100                          False                           True   \n",
       "HG00101                           True                           True   \n",
       "...                                ...                            ...   \n",
       "NA21137                           True                           True   \n",
       "NA21141                           True                           True   \n",
       "NA21142                           True                           True   \n",
       "NA21143                          False                           True   \n",
       "NA21144                           True                           True   \n",
       "\n",
       "         chr22:38583315:AAAAG:AAAAGAAAG,AAAAGAAAGAAAG,A_maternal  \\\n",
       "HG00096                                              False         \n",
       "HG00097                                              False         \n",
       "HG00099                                               True         \n",
       "HG00100                                              False         \n",
       "HG00101                                              False         \n",
       "...                                                    ...         \n",
       "NA21137                                               True         \n",
       "NA21141                                              False         \n",
       "NA21142                                              False         \n",
       "NA21143                                               True         \n",
       "NA21144                                               True         \n",
       "\n",
       "         chr22:38583315:AAAAG:AAAAGAAAG,AAAAGAAAGAAAG,A_paternal  \n",
       "HG00096                                              False        \n",
       "HG00097                                              False        \n",
       "HG00099                                               True        \n",
       "HG00100                                              False        \n",
       "HG00101                                              False        \n",
       "...                                                    ...        \n",
       "NA21137                                              False        \n",
       "NA21141                                               True        \n",
       "NA21142                                               True        \n",
       "NA21143                                              False        \n",
       "NA21144                                              False        \n",
       "\n",
       "[2504 rows x 626 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "# Define the transformer model architecture\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_heads, dropout):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Linear(input_size, hidden_size)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_size,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_size * 4,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.output = nn.Linear(hidden_size, input_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.output(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 0.6891, Train Acc: 0.3341, Train Prec: 0.3341, Train Rec: 1.0000, Val Loss: 0.6715, Val Acc: 0.3361, Val Prec: 0.3361, Val Rec: 1.0000\n",
      "Epoch [2/50], Train Loss: 0.6717, Train Acc: 0.3341, Train Prec: 0.3341, Train Rec: 1.0000, Val Loss: 0.6712, Val Acc: 0.3361, Val Prec: 0.3361, Val Rec: 1.0000\n",
      "Epoch [3/50], Train Loss: 0.6714, Train Acc: 0.3341, Train Prec: 0.3341, Train Rec: 1.0000, Val Loss: 0.6710, Val Acc: 0.3361, Val Prec: 0.3361, Val Rec: 1.0000\n",
      "Epoch [4/50], Train Loss: 0.6712, Train Acc: 0.3341, Train Prec: 0.3341, Train Rec: 1.0000, Val Loss: 0.6708, Val Acc: 0.3361, Val Prec: 0.3361, Val Rec: 1.0000\n",
      "Epoch [5/50], Train Loss: 0.6713, Train Acc: 0.3341, Train Prec: 0.3341, Train Rec: 1.0000, Val Loss: 0.6709, Val Acc: 0.3361, Val Prec: 0.3361, Val Rec: 1.0000\n",
      "Epoch [6/50], Train Loss: 0.6712, Train Acc: 0.3341, Train Prec: 0.3341, Train Rec: 1.0000, Val Loss: 0.6708, Val Acc: 0.3361, Val Prec: 0.3361, Val Rec: 1.0000\n",
      "Epoch [7/50], Train Loss: 0.6711, Train Acc: 0.3341, Train Prec: 0.3341, Train Rec: 1.0000, Val Loss: 0.6708, Val Acc: 0.3361, Val Prec: 0.3361, Val Rec: 1.0000\n",
      "Epoch [8/50], Train Loss: 0.6712, Train Acc: 0.3341, Train Prec: 0.3341, Train Rec: 1.0000, Val Loss: 0.6708, Val Acc: 0.3361, Val Prec: 0.3361, Val Rec: 1.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# Calculate average loss\u001b[39;00m\n\u001b[1;32m     52\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 53\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     55\u001b[0m train_preds\u001b[38;5;241m.\u001b[39mextend((output[batch_mask]\u001b[38;5;241m.\u001b[39msigmoid() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten())\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    155\u001b[0m         group,\n\u001b[1;32m    156\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m         state_steps)\n\u001b[0;32m--> 163\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 311\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/adam.py:384\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    381\u001b[0m     param \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(param)\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m--> 384\u001b[0m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlerp_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set hyperparameters\n",
    "input_size = 626\n",
    "hidden_size = 256\n",
    "num_layers = 3\n",
    "num_heads = 8\n",
    "dropout = 0.1\n",
    "batch_size = 32\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "masking_percentage = 0.2\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "# Prepare the data\n",
    "def mask_data(data, masking_percentage):\n",
    "    mask = torch.rand(data.shape) < masking_percentage\n",
    "    masked_data = data.clone()\n",
    "    masked_data[mask] = -1  # Set masked values to -1 or any other appropriate value\n",
    "    return masked_data, mask, data\n",
    "\n",
    "# Assuming you have the data frame loaded as 'df'\n",
    "data = torch.tensor(df.values, dtype=torch.float32)\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_masked_data, train_mask, train_data = mask_data(train_data, masking_percentage)\n",
    "val_masked_data, val_mask, val_data = mask_data(val_data, masking_percentage)\n",
    "\n",
    "train_dataset = TensorDataset(train_masked_data, train_mask, train_data)\n",
    "val_dataset = TensorDataset(val_masked_data, val_mask, val_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = TransformerModel(input_size, hidden_size, num_layers, num_heads, dropout)\n",
    "criterion = nn.BCEWithLogitsLoss(reduction='none')  # Use BCEWithLogitsLoss for numerical stability\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_preds = []\n",
    "    train_targets = []\n",
    "    for batch_masked_data, batch_mask, batch_data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_masked_data)\n",
    "        loss = criterion(output[batch_mask], batch_data[batch_mask])\n",
    "        loss = loss.mean()  # Calculate average loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        train_preds.extend((output[batch_mask].sigmoid() > 0.5).cpu().numpy().flatten())\n",
    "        train_targets.extend(batch_data[batch_mask].cpu().numpy().flatten())\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_preds = []\n",
    "    val_targets = []\n",
    "    with torch.no_grad():\n",
    "        for batch_masked_data, batch_mask, batch_data in val_loader:\n",
    "            output = model(batch_masked_data)\n",
    "            loss = criterion(output[batch_mask], batch_data[batch_mask])\n",
    "            loss = loss.mean()  # Calculate average loss\n",
    "            val_loss += loss.item()\n",
    "            val_preds.extend((output[batch_mask].sigmoid() > 0.5).cpu().numpy().flatten())\n",
    "            val_targets.extend(batch_data[batch_mask].cpu().numpy().flatten())\n",
    "    \n",
    "    train_acc = accuracy_score(train_targets, train_preds)\n",
    "    train_prec = precision_score(train_targets, train_preds)\n",
    "    train_rec = recall_score(train_targets, train_preds)\n",
    "    \n",
    "    val_acc = accuracy_score(val_targets, val_preds)\n",
    "    val_prec = precision_score(val_targets, val_preds)\n",
    "    val_rec = recall_score(val_targets, val_preds)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "          f\"Train Loss: {train_loss / len(train_loader):.4f}, \"\n",
    "          f\"Train Acc: {train_acc:.4f}, \"\n",
    "          f\"Train Prec: {train_prec:.4f}, \"\n",
    "          f\"Train Rec: {train_rec:.4f}, \"\n",
    "          f\"Val Loss: {val_loss / len(val_loader):.4f}, \"\n",
    "          f\"Val Acc: {val_acc:.4f}, \"\n",
    "          f\"Val Prec: {val_prec:.4f}, \"\n",
    "          f\"Val Rec: {val_rec:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"genomic_imputation_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 0.6727, Train Acc: 0.6952, Train Prec: 0.5314, Train Rec: 0.7418, Val Loss: 0.6688, Val Acc: 0.7024, Val Prec: 0.5415, Val Rec: 0.7465, Learning Rate: 0.001000\n",
      "Epoch [2/50], Train Loss: 0.6667, Train Acc: 0.7033, Train Prec: 0.5394, Train Rec: 0.7643, Val Loss: 0.6682, Val Acc: 0.7140, Val Prec: 0.5581, Val Rec: 0.7159, Learning Rate: 0.001000\n",
      "Epoch [3/50], Train Loss: 0.6622, Train Acc: 0.7054, Train Prec: 0.5421, Train Rec: 0.7615, Val Loss: 0.6698, Val Acc: 0.6893, Val Prec: 0.5259, Val Rec: 0.7654, Learning Rate: 0.001000\n",
      "Epoch [4/50], Train Loss: 0.6572, Train Acc: 0.7002, Train Prec: 0.5357, Train Rec: 0.7683, Val Loss: 0.6705, Val Acc: 0.6843, Val Prec: 0.5205, Val Rec: 0.7704, Learning Rate: 0.001000\n",
      "Epoch [5/50], Train Loss: 0.6524, Train Acc: 0.6914, Train Prec: 0.5255, Train Rec: 0.7845, Val Loss: 0.6701, Val Acc: 0.6963, Val Prec: 0.5345, Val Rec: 0.7461, Learning Rate: 0.001000\n",
      "Epoch [6/50], Train Loss: 0.6480, Train Acc: 0.6878, Train Prec: 0.5216, Train Rec: 0.7893, Val Loss: 0.6712, Val Acc: 0.6843, Val Prec: 0.5206, Val Rec: 0.7658, Learning Rate: 0.001000\n",
      "Epoch [7/50], Train Loss: 0.6442, Train Acc: 0.6817, Train Prec: 0.5152, Train Rec: 0.7977, Val Loss: 0.6721, Val Acc: 0.6747, Val Prec: 0.5104, Val Rec: 0.7849, Learning Rate: 0.001000\n",
      "Epoch 00008: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch [8/50], Train Loss: 0.6406, Train Acc: 0.6777, Train Prec: 0.5112, Train Rec: 0.8043, Val Loss: 0.6725, Val Acc: 0.6757, Val Prec: 0.5115, Val Rec: 0.7812, Learning Rate: 0.000500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m output \u001b[38;5;241m=\u001b[39m model(batch_masked_data)\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output[batch_mask], batch_data[batch_mask])\n\u001b[0;32m---> 20\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calculate average loss\u001b[39;00m\n\u001b[1;32m     21\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load the trained model and keep training it with a learning rate scheduler\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = TransformerModel(input_size, hidden_size, num_layers, num_heads, dropout)\n",
    "model.load_state_dict(torch.load(\"genomic_imputation_model.pth\"))\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_preds = []\n",
    "    train_targets = []\n",
    "    for batch_masked_data, batch_mask, batch_data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_masked_data)\n",
    "        loss = criterion(output[batch_mask], batch_data[batch_mask])\n",
    "        loss = loss.mean()  # Calculate average loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        train_preds.extend((output[batch_mask].sigmoid() > 0.5).cpu().numpy().flatten())\n",
    "        train_targets.extend(batch_data[batch_mask].cpu().numpy().flatten())\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_preds = []\n",
    "    val_targets = []\n",
    "    with torch.no_grad():\n",
    "        for batch_masked_data, batch_mask, batch_data in val_loader:\n",
    "            output = model(batch_masked_data)\n",
    "            loss = criterion(output[batch_mask], batch_data[batch_mask])\n",
    "            loss = loss.mean()  # Calculate average loss\n",
    "            val_loss += loss.item()\n",
    "            val_preds.extend((output[batch_mask].sigmoid() > 0.5).cpu().numpy().flatten())\n",
    "            val_targets.extend(batch_data[batch_mask].cpu().numpy().flatten())\n",
    "    \n",
    "    # Update the learning rate based on the validation loss\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    train_acc = accuracy_score(train_targets, train_preds)\n",
    "    train_prec = precision_score(train_targets, train_preds)\n",
    "    train_rec = recall_score(train_targets, train_preds)\n",
    "    \n",
    "    val_acc = accuracy_score(val_targets, val_preds)\n",
    "    val_prec = precision_score(val_targets, val_preds)\n",
    "    val_rec = recall_score(val_targets, val_preds)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "          f\"Train Loss: {train_loss / len(train_loader):.4f}, \"\n",
    "          f\"Train Acc: {train_acc:.4f}, \"\n",
    "          f\"Train Prec: {train_prec:.4f}, \"\n",
    "          f\"Train Rec: {train_rec:.4f}, \"\n",
    "          f\"Val Loss: {val_loss / len(val_loader):.4f}, \"\n",
    "          f\"Val Acc: {val_acc:.4f}, \"\n",
    "          f\"Val Prec: {val_prec:.4f}, \"\n",
    "          f\"Val Rec: {val_rec:.4f}, \"\n",
    "          f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"genomic_imputation_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
