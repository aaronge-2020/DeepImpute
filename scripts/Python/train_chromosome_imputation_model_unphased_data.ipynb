{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Model Unphased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_iqs_unphased(true_genotypes, imputed_genotypes):\n",
    "    \"\"\"\n",
    "    Calculate the Imputation Quality Score (IQS) for ununphased genotypes.\n",
    "\n",
    "    Args:\n",
    "        true_genotypes (numpy.ndarray): 2D array of true genotypes, where each genotype is represented by values 0, 1, or 2.\n",
    "        imputed_genotypes (numpy.ndarray): 2D array of imputed genotypes, where each genotype is represented by floating-point values between 0 and 2.\n",
    "        threshold (float): Threshold for converting imputed probabilities to discrete genotypes (default: 0.5).\n",
    "\n",
    "    Returns:\n",
    "        float: Imputation Quality Score (IQS).\n",
    "    \"\"\"\n",
    "    # Check if the shapes of true and imputed genotypes are the same\n",
    "    if true_genotypes.shape != imputed_genotypes.shape:\n",
    "        raise ValueError(\"Shape of true genotypes and imputed genotypes must be the same.\")\n",
    "\n",
    "    # Convert imputed probabilities to discrete genotypes based on the threshold\n",
    "    imputed_discrete = np.round(imputed_genotypes).astype(int)\n",
    "\n",
    "    # Create a contingency table\n",
    "    contingency_table = np.zeros((3, 3), dtype=int)\n",
    "\n",
    "    # Fill the contingency table\n",
    "    for true_geno, imputed_geno in zip(true_genotypes, imputed_discrete):\n",
    "        for true_allele, imputed_allele in zip(true_geno, imputed_geno):\n",
    "            contingency_table[int(true_allele), int(imputed_allele)] += 1\n",
    "\n",
    "    # Calculate the total number of alleles\n",
    "    total_alleles = np.sum(contingency_table)\n",
    "\n",
    "    # Calculate the observed agreement (Po)\n",
    "    po = np.sum(np.diag(contingency_table)) / total_alleles\n",
    "\n",
    "    # Calculate the expected agreement by chance (Pc)\n",
    "    true_counts = np.sum(contingency_table, axis=1)\n",
    "    imputed_counts = np.sum(contingency_table, axis=0)\n",
    "    pc = np.sum(true_counts * imputed_counts) / (total_alleles ** 2)\n",
    "\n",
    "    # Calculate the Imputation Quality Score (IQS)\n",
    "    iqs = (po - pc) / (1 - pc)\n",
    "\n",
    "    return iqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALL PRS Masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total SNPs:  929\n",
      "PRS313 SNPs:  30\n",
      "Total SNPs used for Training:  899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-05-02 07:59:38,093] Trial 100 finished with value: 0.23691181402940015 and parameters: {'learning_rate': 4.175004197951897e-05, 'l1_coef': 1.1197476595644615e-06, 'patience': 21, 'batch_size': 32}. Best is trial 76 with value: 0.1756902786401602.\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-05-02 07:59:38,783] Trial 102 finished with value: 0.24792250990867615 and parameters: {'learning_rate': 3.5937947724836175e-05, 'l1_coef': 6.900078419255583e-07, 'patience': 22, 'batch_size': 32}. Best is trial 76 with value: 0.1756902786401602.\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-05-02 07:59:39,996] Trial 104 finished with value: 0.24107947257848888 and parameters: {'learning_rate': 3.9284474248021434e-05, 'l1_coef': 9.966446372987957e-07, 'patience': 22, 'batch_size': 32}. Best is trial 76 with value: 0.1756902786401602.\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-05-02 07:59:41,785] Trial 97 finished with value: 0.2441617878583761 and parameters: {'learning_rate': 3.861362904102172e-05, 'l1_coef': 1.6494019459883064e-06, 'patience': 23, 'batch_size': 32}. Best is trial 76 with value: 0.1756902786401602.\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-05-02 07:59:42,331] Trial 101 finished with value: 0.2229064024411715 and parameters: {'learning_rate': 4.8474886779099516e-05, 'l1_coef': 4.588368916455555e-07, 'patience': 23, 'batch_size': 32}. Best is trial 76 with value: 0.1756902786401602.\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-05-02 07:59:42,995] Trial 99 finished with value: 0.225984033712974 and parameters: {'learning_rate': 4.830740124452633e-05, 'l1_coef': 1.6080698482166888e-06, 'patience': 23, 'batch_size': 32}. Best is trial 76 with value: 0.1756902786401602.\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-05-02 07:59:43,517] Trial 103 finished with value: 0.24057220243490657 and parameters: {'learning_rate': 4.001667573831486e-05, 'l1_coef': 1.3356597605172918e-06, 'patience': 23, 'batch_size': 32}. Best is trial 76 with value: 0.1756902786401602.\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-05-02 07:59:44,967] Trial 106 finished with value: 0.28343565395245185 and parameters: {'learning_rate': 2.393811830139805e-05, 'l1_coef': 6.802425399704015e-07, 'patience': 23, 'batch_size': 32}. Best is trial 76 with value: 0.1756902786401602.\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-05-02 07:59:46,164] Trial 98 finished with value: 0.23496008102710428 and parameters: {'learning_rate': 4.196571816852534e-05, 'l1_coef': 4.878711167068667e-07, 'patience': 24, 'batch_size': 32}. Best is trial 76 with value: 0.1756902786401602.\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-05-02 07:59:52,688] Trial 105 finished with value: 0.23697798389654895 and parameters: {'learning_rate': 4.122426271914926e-05, 'l1_coef': 8.916546978108885e-07, 'patience': 27, 'batch_size': 32}. Best is trial 76 with value: 0.1756902786401602.\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-05-02 08:00:47,866] Trial 116 finished with value: 0.17657428968411226 and parameters: {'learning_rate': 0.009832714508082789, 'l1_coef': 5.667830301578693e-06, 'patience': 16, 'batch_size': 32}. Best is trial 76 with value: 0.1756902786401602.\n",
      "[I 2024-05-02 08:00:56,129] Trial 114 finished with value: 35.212163103543794 and parameters: {'learning_rate': 0.31375386842837594, 'l1_coef': 4.65719609187585e-06, 'patience': 16, 'batch_size': 32}. Best is trial 76 with value: 0.1756902786401602.\n",
      "[I 2024-05-02 08:01:05,563] Trial 112 finished with value: 24.58834903423603 and parameters: {'learning_rate': 0.14867624397257503, 'l1_coef': 3.9359259334718106e-09, 'patience': 26, 'batch_size': 32}. Best is trial 76 with value: 0.1756902786401602.\n",
      "[I 2024-05-02 08:01:08,957] Trial 107 finished with value: 27.030426113422102 and parameters: {'learning_rate': 0.20615095102335224, 'l1_coef': 2.6836992695016227e-06, 'patience': 25, 'batch_size': 32}. Best is trial 76 with value: 0.1756902786401602.\n",
      "[I 2024-05-02 08:01:44,423] Trial 115 finished with value: 28.745114898681642 and parameters: {'learning_rate': 0.17284541592042285, 'l1_coef': 3.8617578395789674e-06, 'patience': 29, 'batch_size': 32}. Best is trial 76 with value: 0.1756902786401602.\n",
      "[I 2024-05-02 08:02:58,897] Trial 109 finished with value: 0.21312162853204283 and parameters: {'learning_rate': 6.694145017629747e-05, 'l1_coef': 4.3382677050301625e-06, 'patience': 30, 'batch_size': 32}. Best is trial 76 with value: 0.1756902786401602.\n",
      "[I 2024-05-02 08:03:37,858] Trial 111 finished with value: 0.27782012086648206 and parameters: {'learning_rate': 1.9124696934050792e-05, 'l1_coef': 4.874116216143633e-06, 'patience': 16, 'batch_size': 32}. Best is trial 76 with value: 0.1756902786401602.\n",
      "[I 2024-05-02 08:03:44,767] Trial 110 finished with value: 0.26954387311752026 and parameters: {'learning_rate': 1.7985656642558187e-05, 'l1_coef': 2.212164649948527e-07, 'patience': 16, 'batch_size': 32}. Best is trial 76 with value: 0.1756902786401602.\n",
      "[I 2024-05-02 08:03:47,875] Trial 108 finished with value: 0.3797455319991479 and parameters: {'learning_rate': 4.399743861057262e-06, 'l1_coef': 3.950273411272919e-06, 'patience': 24, 'batch_size': 32}. Best is trial 76 with value: 0.1756902786401602.\n",
      "[I 2024-05-02 08:03:48,478] Trial 113 finished with value: 0.30405054367505585 and parameters: {'learning_rate': 1.198593786841062e-05, 'l1_coef': 4.8881256477373255e-06, 'patience': 27, 'batch_size': 32}. Best is trial 76 with value: 0.1756902786401602.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 1 - Best hyperparameters: {'learning_rate': 0.010820235563222399, 'l1_coef': 5.083034754078524e-06, 'patience': 16, 'batch_size': 32}\n",
      "Chr 1 - Best value: 0.1757\n",
      "Final model saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/models_unphased/chr1/final_model_chr1.pth\n",
      "Individual R^2 scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr1/individual_r2_scores_chr1.csv\n",
      "Individual IQS scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr1/individual_iqs_scores_chr1.csv\n",
      "Individual AUC ROC curves saved in: ../../Data/model_results_unphased_all_PRS/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/performance_metrics.csv\n",
      "Total SNPs:  629\n",
      "PRS313 SNPs:  21\n",
      "Total SNPs used for Training:  608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 08:04:17,352] Trial 53 finished with value: 25.5994615994967 and parameters: {'learning_rate': 0.28022605039386095, 'l1_coef': 4.410745733646335e-07, 'patience': 3, 'batch_size': 32}. Best is trial 51 with value: 0.13727086541744377.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "from sklearn.metrics import r2_score as sklearn_r2_score\n",
    "import optuna\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# Load and preprocess the data\n",
    "data_directory = '../../Data/Filtered_unphased_training_data_union/'\n",
    "start = 1\n",
    "\n",
    "# Initialize lists to store the performance metrics for each chromosome\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "false_positive_rates = []\n",
    "auc_rocs = []\n",
    "r2_scores = []\n",
    "iqs_scores = []\n",
    "\n",
    "# Create folders for saving files\n",
    "output_folder = \"../../Data/model_results_unphased_all_PRS/logistic_regression/\"\n",
    "model_folder = output_folder + \"models_unphased/\"\n",
    "csv_folder = output_folder + \"csv_files/\"\n",
    "curve_folder = output_folder + \"roc_curves/\"\n",
    "\n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "os.makedirs(csv_folder, exist_ok=True)\n",
    "os.makedirs(curve_folder, exist_ok=True)\n",
    "\n",
    "for chromosome_number in range(start, 23):\n",
    "    # Create subfolders for the current chromosome\n",
    "    chr_model_folder = model_folder + f\"chr{chromosome_number}/\"\n",
    "    chr_csv_folder = csv_folder + f\"chr{chromosome_number}/\"\n",
    "    chr_curve_folder = curve_folder + f\"chr{chromosome_number}/\"\n",
    "\n",
    "    os.makedirs(chr_model_folder, exist_ok=True)\n",
    "    os.makedirs(chr_csv_folder, exist_ok=True)\n",
    "    os.makedirs(chr_curve_folder, exist_ok=True)\n",
    "\n",
    "    file_name = data_directory + f\"23AndMe_PRS313_merged_chr{chromosome_number}_matching_combined.parquet\"\n",
    "    data = pd.read_parquet(file_name)\n",
    "\n",
    "\n",
    "    # Split the data into features and target\n",
    "    X = torch.tensor(data.filter(regex='^(?!.*PRS313_)').values, dtype=torch.float32)\n",
    "    y = torch.tensor(data.filter(regex='PRS313_').values, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    print(\"Total SNPs: \", data.shape[1])\n",
    "    print(\"PRS313 SNPs: \", y.shape[1])\n",
    "    # print(\"Known PRS313 SNPs: \", data[[col for col in data.columns if \"PRS313_Known\" in col]].shape[1])\n",
    "    # print(\"23AndMe SNPs with LD to Unknown PRS313 SNPs: \", data[[col for col in data.columns if \"PRS313_\" not in col]].shape[1])\n",
    "    print(\"Total SNPs used for Training: \", X.shape[1])\n",
    "\n",
    "    # Split the data into train-validation and test sets\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Define the logistic regression model with lasso regularization\n",
    "    class LogisticRegression(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim, l1_coef=0.0):\n",
    "            super(LogisticRegression, self).__init__()\n",
    "            self.linear = nn.Linear(input_dim, output_dim)\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "            self.l1_coef = l1_coef\n",
    "\n",
    "        def forward(self, x):\n",
    "            out = self.linear(x)\n",
    "            out = self.sigmoid(out)\n",
    "            return out\n",
    "\n",
    "        def l1_loss(self):\n",
    "            return self.l1_coef * torch.norm(self.linear.weight, p=1)\n",
    "        \n",
    "    # Set the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Set the hyperparameters for tuning\n",
    "    input_dim = X_train_val.shape[1]\n",
    "    output_dim = y_train_val.shape[1]\n",
    "    num_epochs = 500\n",
    "    batch_size = 128\n",
    "\n",
    "    # Define the objective function for Optuna with cross-validation and early stopping\n",
    "    def objective(trial):\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-6, 1e-1, log=True)\n",
    "        l1_coef = trial.suggest_float('l1_coef', 1e-10, 1e-1, log=True)\n",
    "        patience = trial.suggest_int('patience', 5, 20)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "\n",
    "        model = LogisticRegression(input_dim, output_dim, l1_coef).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        criterion = nn.BCELoss()\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=False)\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        fold_losses = []\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_val, y_train_val.argmax(dim=1))):\n",
    "            X_train, X_val = X_train_val[train_idx], X_train_val[val_idx]\n",
    "            y_train, y_val = y_train_val[train_idx], y_train_val[val_idx]\n",
    "\n",
    "            train_dataset = TensorDataset(X_train, y_train)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "            best_val_loss = float('inf')\n",
    "            counter = 0\n",
    "\n",
    "            for epoch in range(num_epochs):\n",
    "                train_loss = 0.0\n",
    "                for batch_X, batch_y in train_loader:\n",
    "                    batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "                    outputs = model(batch_X)\n",
    "                    loss = criterion(outputs, batch_y) + model.l1_loss()\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    train_loss += loss.item()\n",
    "\n",
    "                train_loss /= len(train_loader)\n",
    "\n",
    "                val_dataset = TensorDataset(X_val, y_val)\n",
    "                val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    val_loss = 0.0\n",
    "                    for batch_X, batch_y in val_loader:\n",
    "                        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                        outputs = model(batch_X)\n",
    "                        loss = criterion(outputs, batch_y) + model.l1_loss()\n",
    "                        val_loss += loss.item()\n",
    "\n",
    "                    val_loss /= len(val_loader)\n",
    "                    scheduler.step(val_loss)\n",
    "\n",
    "                    if val_loss < best_val_loss:\n",
    "                        best_val_loss = val_loss\n",
    "                        counter = 0\n",
    "                    else:\n",
    "                        counter += 1\n",
    "\n",
    "                    if counter >= patience:\n",
    "                        # print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                        break\n",
    "\n",
    "            fold_losses.append(best_val_loss)\n",
    "\n",
    "        return np.mean(fold_losses)\n",
    "\n",
    "    # Create the \"optuna_studies\" folder if it doesn't exist\n",
    "    os.makedirs(\"optuna_studies\", exist_ok=True)\n",
    "\n",
    "    # Create an Optuna study and optimize the hyperparameters\n",
    "    study_name = f\"unphased_full_23andMe_chr{chromosome_number}_study\"\n",
    "    storage_name = f\"sqlite:///optuna_studies/{study_name}.db\"\n",
    "\n",
    "    # Check if the study exists\n",
    "\n",
    "    current_dir = os.getcwd()\n",
    "    study_exists = os.path.exists(current_dir + f\"/optuna_studies/{study_name}.db\")\n",
    "    \n",
    "    if study_exists:\n",
    "        # Load the existing study\n",
    "        study = optuna.load_study(study_name=study_name, storage=storage_name)\n",
    "    else:\n",
    "        # Create a new study\n",
    "        study = optuna.create_study(direction='minimize', study_name=study_name, storage=storage_name)\n",
    "\n",
    "    study.optimize(objective, n_trials=1, n_jobs=-1)\n",
    "\n",
    "    # Print the best hyperparameters and best value\n",
    "    print(f\"Chr {chromosome_number} - Best hyperparameters: {study.best_params}\")\n",
    "    print(f\"Chr {chromosome_number} - Best value: {study.best_value:.4f}\")\n",
    "\n",
    "    # Train the final model with the best hyperparameters and early stopping\n",
    "    best_learning_rate = study.best_params['learning_rate']\n",
    "    best_l1_coef = study.best_params['l1_coef']\n",
    "    best_patience = study.best_params['patience']\n",
    "    best_batch_size = study.best_params['batch_size']\n",
    "\n",
    "    model = LogisticRegression(input_dim, output_dim, best_l1_coef).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_learning_rate)\n",
    "    criterion = nn.BCELoss()\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=False)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_val, y_train_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=best_batch_size, shuffle=True)\n",
    "\n",
    "    best_train_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y) + model.l1_loss()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        # print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "        if train_loss < best_train_loss:\n",
    "            best_train_loss = train_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        if counter >= best_patience:\n",
    "            # print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "        scheduler.step(train_loss)\n",
    "\n",
    "    # Save the final model\n",
    "    model_save_path = chr_model_folder + f'final_model_chr{chromosome_number}.pth'\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"Final model saved at: {model_save_path}\")\n",
    "\n",
    "    # Evaluate the final model on the test set\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test.to(device))\n",
    "        test_preds = (test_outputs > 0.5).float()\n",
    "        test_accuracy = float(((test_preds > 0.5) == y_test).float().mean())\n",
    "        test_precision = precision_score(y_test.cpu().numpy(), test_preds.cpu().numpy(), average='micro')\n",
    "        test_recall = recall_score(y_test.cpu().numpy(), test_preds.cpu().numpy(), average='micro')\n",
    "        test_f1 = f1_score(y_test.cpu().numpy(), test_preds.cpu().numpy(), average='micro')\n",
    "        test_roc_auc = roc_auc_score(y_test.cpu().numpy(), test_outputs.cpu().numpy(), average='micro')\n",
    "        test_r2 = sklearn_r2_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "        test_iqs = calculate_iqs_unphased(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "\n",
    "        # Calculate false positive rate\n",
    "        cm = confusion_matrix(y_test.cpu().numpy().ravel(), test_preds.cpu().numpy().ravel())\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        test_fpr = fp / (fp + tn)\n",
    "\n",
    "        # Append performance metrics to the lists\n",
    "        accuracies.append(test_accuracy)\n",
    "        precisions.append(test_precision)\n",
    "        recalls.append(test_recall)\n",
    "        false_positive_rates.append(test_fpr)\n",
    "        auc_rocs.append(test_roc_auc)\n",
    "        r2_scores.append(test_r2)\n",
    "        iqs_scores.append(test_iqs)\n",
    "\n",
    "        # Calculate individual R^2 scores for each SNP\n",
    "        individual_r2_scores = sklearn_r2_score(y_test.cpu().numpy(), test_outputs.cpu().numpy(), multioutput='raw_values')\n",
    "\n",
    "        # Calculate individual IQS scores for each SNP\n",
    "        individual_iqs_scores = np.array([calculate_iqs_unphased(y_test.cpu().numpy()[:, i].reshape(-1, 1), test_outputs.cpu().numpy()[:, i].reshape(-1, 1)) for i in range(y_test.shape[1])])\n",
    "\n",
    "        # Get the names of the SNPs from the original dataframe\n",
    "        snp_names = data.filter(regex='PRS').columns\n",
    "\n",
    "        # Save individual R^2 scores to a CSV file\n",
    "        csv_file = chr_csv_folder + f'individual_r2_scores_chr{chromosome_number}.csv'\n",
    "\n",
    "        with open(csv_file, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['SNP', 'R2 Score'])\n",
    "            for snp, r2_score in zip(snp_names, individual_r2_scores):\n",
    "                writer.writerow([snp, r2_score])\n",
    "\n",
    "        print(f\"Individual R^2 scores saved at: {csv_file}\")\n",
    "\n",
    "        # Save individual IQS scores to a CSV file\n",
    "        iqs_csv_file = chr_csv_folder + f'individual_iqs_scores_chr{chromosome_number}.csv'\n",
    "\n",
    "        with open(iqs_csv_file, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['SNP', 'IQS Score'])\n",
    "            for snp, iqs_score in zip(snp_names, individual_iqs_scores):\n",
    "                writer.writerow([snp, iqs_score])\n",
    "\n",
    "        print(f\"Individual IQS scores saved at: {iqs_csv_file}\")\n",
    "\n",
    "        # Save individual AUC ROC curves for each SNP\n",
    "        for i, snp in enumerate(snp_names):\n",
    "            try: \n",
    "                fpr, tpr, _ = roc_curve(y_test.cpu().numpy()[:, i], test_outputs.cpu().numpy()[:, i])\n",
    "                plt.figure()\n",
    "                plt.plot(fpr, tpr, label=f'AUC ROC = {roc_auc_score(y_test.cpu().numpy()[:, i], test_outputs.cpu().numpy()[:, i]):.4f}')\n",
    "                plt.xlabel('False Positive Rate')\n",
    "                plt.ylabel('True Positive Rate')\n",
    "                plt.title(f'AUC ROC Curve - {snp}')\n",
    "                plt.legend()\n",
    "                \n",
    "                curve_file = chr_curve_folder + f'auc_roc_curve_{snp}_chr{chromosome_number}.png'\n",
    "                plt.savefig(curve_file)\n",
    "                plt.close()\n",
    "            except ValueError:\n",
    "                # Save a placeholder image if there is insufficient data\n",
    "                plt.figure()\n",
    "                plt.axis('off')\n",
    "                plt.text(0.5, 0.5, \"Insufficient data for ROC curve\", ha='center', va='center')\n",
    "                curve_file = chr_curve_folder + f'auc_roc_curve_{snp}_chr{chromosome_number}.png'\n",
    "                plt.savefig(curve_file)\n",
    "                plt.close()\n",
    "\n",
    "                print(f\"Skipping SNP {snp} due to insufficient data\")\n",
    "\n",
    "\n",
    "        print(f\"Individual AUC ROC curves saved in: {curve_folder}\")\n",
    "\n",
    "        # Create a DataFrame to store the performance metrics for each chromosome\n",
    "        performance_df = pd.DataFrame({\n",
    "            'Chromosome': list(range(start, chromosome_number + 1)),\n",
    "            'Accuracy': accuracies,\n",
    "            'Precision': precisions,\n",
    "            'Recall': recalls,\n",
    "            'False Positive Rate': false_positive_rates,\n",
    "            'AUC ROC': auc_rocs,\n",
    "            'R2 Score': r2_scores,\n",
    "            'IQS Score': iqs_scores\n",
    "        })\n",
    "\n",
    "        # Save the performance metrics to a CSV file\n",
    "        performance_csv_file = csv_folder + 'performance_metrics.csv'\n",
    "        performance_df.to_csv(performance_csv_file, index=False)\n",
    "        print(f\"Performance metrics saved at: {performance_csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRS313 SNPs saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/prs313_snps.csv\n",
      "Total number of PRS313 SNPs: 313\n"
     ]
    }
   ],
   "source": [
    "# Loop through all the training datasets and document the PRS313 SNPs in each dataset. Save this to a CSV file.\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data_directory = '../../Data/Filtered_unphased_training_data_union/'\n",
    "output_folder = \"../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/\"\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Initialize a list to store the PRS313 SNPs in each dataset\n",
    "prs313_snps = []\n",
    "\n",
    "for chromosome_number in range(1, 23):\n",
    "    file_name = data_directory + \\\n",
    "        f\"23AndMe_PRS313_merged_chr{chromosome_number}_matching_combined.parquet\"\n",
    "    data = pd.read_parquet(file_name)\n",
    "\n",
    "    prs313_snps.append(data.filter(regex='PRS313_').columns)\n",
    "\n",
    "# Create a DataFrame to store the PRS313 SNPs in each dataset\n",
    "prs313_df = pd.DataFrame({\n",
    "    'Chromosome': list(range(1, 23)),\n",
    "    'PRS313 SNPs': prs313_snps,\n",
    "    \"Number of PRS313 SNPs\": [len(snps) for snps in prs313_snps]\n",
    "})\n",
    "\n",
    "# Save the PRS313 SNPs to a CSV file\n",
    "prs313_csv_file = output_folder + 'prs313_snps.csv'\n",
    "prs313_df.to_csv(prs313_csv_file, index=False)\n",
    "print(f\"PRS313 SNPs saved at: {prs313_csv_file}\")\n",
    "\n",
    "# Print the total number of PRS313 SNPs in all datasets\n",
    "total_prs313_snps = sum(prs313_df[\"Number of PRS313 SNPs\"])\n",
    "print(f\"Total number of PRS313 SNPs: {total_prs313_snps}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chromosome\n",
       "1     30\n",
       "2     21\n",
       "3     16\n",
       "4     11\n",
       "5     34\n",
       "6     20\n",
       "7     14\n",
       "8     21\n",
       "9     14\n",
       "10    18\n",
       "11    19\n",
       "12    17\n",
       "13     5\n",
       "14     8\n",
       "15     7\n",
       "16    14\n",
       "17     9\n",
       "18     9\n",
       "19     7\n",
       "20     4\n",
       "21     4\n",
       "22    11\n",
       "Name: SNPa, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the PRS313 xlsx\n",
    "prs313_file = \"../../Data/PRS313.xlsx\"\n",
    "\n",
    "# Load the PRS313 data\n",
    "prs313_data = pd.read_excel(prs313_file)\n",
    "\n",
    "# Get the number of PRS313 SNPs per chromosome\n",
    "prs313_snps_per_chromosome = prs313_data.groupby(\"Chromosome\")[\"SNPa\"].count()\n",
    "prs313_snps_per_chromosome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     30\n",
       "1     21\n",
       "2     16\n",
       "3     11\n",
       "4     34\n",
       "5     20\n",
       "6     14\n",
       "7     21\n",
       "8     14\n",
       "9     18\n",
       "10    19\n",
       "11    17\n",
       "12     5\n",
       "13     8\n",
       "14     7\n",
       "15    14\n",
       "16     9\n",
       "17     9\n",
       "18     7\n",
       "19     4\n",
       "20     5\n",
       "21    11\n",
       "Name: Number of PRS313 SNPs, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prs313_df[\"Number of PRS313 SNPs\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
