{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Model Unphased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_iqs_unphased(true_genotypes, imputed_genotypes):\n",
    "    \"\"\"\n",
    "    Calculate the Imputation Quality Score (IQS) for ununphased genotypes.\n",
    "\n",
    "    Args:\n",
    "        true_genotypes (numpy.ndarray): 2D array of true genotypes, where each genotype is represented by values 0, 1, or 2.\n",
    "        imputed_genotypes (numpy.ndarray): 2D array of imputed genotypes, where each genotype is represented by floating-point values between 0 and 2.\n",
    "        threshold (float): Threshold for converting imputed probabilities to discrete genotypes (default: 0.5).\n",
    "\n",
    "    Returns:\n",
    "        float: Imputation Quality Score (IQS).\n",
    "    \"\"\"\n",
    "    # Check if the shapes of true and imputed genotypes are the same\n",
    "    if true_genotypes.shape != imputed_genotypes.shape:\n",
    "        raise ValueError(\"Shape of true genotypes and imputed genotypes must be the same.\")\n",
    "\n",
    "    # Convert imputed probabilities to discrete genotypes based on the threshold\n",
    "    imputed_discrete = np.round(imputed_genotypes).astype(int)\n",
    "\n",
    "    # Create a contingency table\n",
    "    contingency_table = np.zeros((3, 3), dtype=int)\n",
    "\n",
    "    # Fill the contingency table\n",
    "    for true_geno, imputed_geno in zip(true_genotypes, imputed_discrete):\n",
    "        for true_allele, imputed_allele in zip(true_geno, imputed_geno):\n",
    "            contingency_table[int(true_allele), int(imputed_allele)] += 1\n",
    "\n",
    "    # Calculate the total number of alleles\n",
    "    total_alleles = np.sum(contingency_table)\n",
    "\n",
    "    # Calculate the observed agreement (Po)\n",
    "    po = np.sum(np.diag(contingency_table)) / total_alleles\n",
    "\n",
    "    # Calculate the expected agreement by chance (Pc)\n",
    "    true_counts = np.sum(contingency_table, axis=1)\n",
    "    imputed_counts = np.sum(contingency_table, axis=0)\n",
    "    pc = np.sum(true_counts * imputed_counts) / (total_alleles ** 2)\n",
    "\n",
    "    # Calculate the Imputation Quality Score (IQS)\n",
    "    iqs = (po - pc) / (1 - pc)\n",
    "\n",
    "    return iqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown PRS313 SNPs:  30\n",
      "Known PRS313 SNPs:  10\n",
      "23AndMe SNPs with LD to Unknown PRS313 SNPs:  850\n",
      "Total SNPs used for Training:  850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-04-30 10:11:23,397] Trial 136 finished with value: 0.24072570720544229 and parameters: {'learning_rate': 0.06244280451709654, 'l1_coef': 1.2869733065346573e-05, 'patience': 16, 'batch_size': 32}. Best is trial 32 with value: 0.08649516483912101.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 1 - Best hyperparameters: {'learning_rate': 0.01823591759267881, 'l1_coef': 1.0049346589997416e-05, 'patience': 9, 'batch_size': 32}\n",
      "Chr 1 - Best value: 0.0865\n",
      "Epoch [1/500], Train Loss: 0.6092\n",
      "Epoch [2/500], Train Loss: 0.3647\n",
      "Epoch [3/500], Train Loss: 0.3238\n",
      "Epoch [4/500], Train Loss: 0.2944\n",
      "Epoch [5/500], Train Loss: 0.2686\n",
      "Epoch [6/500], Train Loss: 0.2754\n",
      "Epoch [7/500], Train Loss: 0.2642\n",
      "Epoch [8/500], Train Loss: 0.2579\n",
      "Epoch [9/500], Train Loss: 0.2536\n",
      "Epoch [10/500], Train Loss: 0.2070\n",
      "Epoch [11/500], Train Loss: 0.1987\n",
      "Epoch [12/500], Train Loss: 0.1979\n",
      "Epoch [13/500], Train Loss: 0.1946\n",
      "Epoch [14/500], Train Loss: 0.1905\n",
      "Epoch [15/500], Train Loss: 0.1923\n",
      "Epoch [16/500], Train Loss: 0.1934\n",
      "Epoch [17/500], Train Loss: 0.1864\n",
      "Epoch [18/500], Train Loss: 0.1898\n",
      "Epoch [19/500], Train Loss: 0.1831\n",
      "Epoch [20/500], Train Loss: 0.1852\n",
      "Epoch [21/500], Train Loss: 0.1829\n",
      "Epoch [22/500], Train Loss: 0.1796\n",
      "Epoch [23/500], Train Loss: 0.1805\n",
      "Epoch [24/500], Train Loss: 0.1769\n",
      "Epoch [25/500], Train Loss: 0.1837\n",
      "Epoch [26/500], Train Loss: 0.1865\n",
      "Epoch [27/500], Train Loss: 0.1865\n",
      "Epoch [28/500], Train Loss: 0.1764\n",
      "Epoch [29/500], Train Loss: 0.1771\n",
      "Epoch [30/500], Train Loss: 0.1722\n",
      "Epoch [31/500], Train Loss: 0.1750\n",
      "Epoch [32/500], Train Loss: 0.1754\n",
      "Epoch [33/500], Train Loss: 0.1842\n",
      "Epoch [34/500], Train Loss: 0.2021\n",
      "Epoch [35/500], Train Loss: 0.2200\n",
      "Epoch [36/500], Train Loss: 0.2208\n",
      "Epoch [37/500], Train Loss: 0.1869\n",
      "Epoch [38/500], Train Loss: 0.1811\n",
      "Epoch [39/500], Train Loss: 0.1800\n",
      "Early stopping at epoch 39\n",
      "Final model saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/models_unphased/chr1/final_model_chr1.pth\n",
      "Individual R^2 scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr1/individual_r2_scores_chr1.csv\n",
      "Individual IQS scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr1/individual_iqs_scores_chr1.csv\n",
      "Individual AUC ROC curves saved in: ../../Data/model_results_unphased_all_PRS/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/performance_metrics.csv\n",
      "Unknown PRS313 SNPs:  21\n",
      "Known PRS313 SNPs:  4\n",
      "23AndMe SNPs with LD to Unknown PRS313 SNPs:  596\n",
      "Total SNPs used for Training:  596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-30 10:11:28,262] Trial 53 finished with value: 8.442827844619751 and parameters: {'learning_rate': 0.08866024484499739, 'l1_coef': 1.9153174210294036e-05, 'patience': 7, 'batch_size': 128}. Best is trial 50 with value: 0.11959010179226218.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 2 - Best hyperparameters: {'learning_rate': 0.008236314117298912, 'l1_coef': 1.0089572204728918e-05, 'patience': 10, 'batch_size': 32}\n",
      "Chr 2 - Best value: 0.1196\n",
      "Epoch [1/500], Train Loss: 0.4278\n",
      "Epoch [2/500], Train Loss: 0.3145\n",
      "Epoch [3/500], Train Loss: 0.2757\n",
      "Epoch [4/500], Train Loss: 0.2468\n",
      "Epoch [5/500], Train Loss: 0.2339\n",
      "Epoch [6/500], Train Loss: 0.2202\n",
      "Epoch [7/500], Train Loss: 0.2112\n",
      "Epoch [8/500], Train Loss: 0.2029\n",
      "Epoch [9/500], Train Loss: 0.1988\n",
      "Epoch [10/500], Train Loss: 0.1932\n",
      "Epoch [11/500], Train Loss: 0.1907\n",
      "Epoch [12/500], Train Loss: 0.1849\n",
      "Epoch [13/500], Train Loss: 0.1821\n",
      "Epoch [14/500], Train Loss: 0.1792\n",
      "Epoch [15/500], Train Loss: 0.1770\n",
      "Epoch [16/500], Train Loss: 0.1747\n",
      "Epoch [17/500], Train Loss: 0.1721\n",
      "Epoch [18/500], Train Loss: 0.1711\n",
      "Epoch [19/500], Train Loss: 0.1684\n",
      "Epoch [20/500], Train Loss: 0.1653\n",
      "Epoch [21/500], Train Loss: 0.1664\n",
      "Epoch [22/500], Train Loss: 0.1672\n",
      "Epoch [23/500], Train Loss: 0.1641\n",
      "Epoch [24/500], Train Loss: 0.1623\n",
      "Epoch [25/500], Train Loss: 0.1602\n",
      "Epoch [26/500], Train Loss: 0.1620\n",
      "Epoch [27/500], Train Loss: 0.1601\n",
      "Epoch [28/500], Train Loss: 0.1617\n",
      "Epoch [29/500], Train Loss: 0.1602\n",
      "Epoch [30/500], Train Loss: 0.1578\n",
      "Epoch [31/500], Train Loss: 0.1577\n",
      "Epoch [32/500], Train Loss: 0.1595\n",
      "Epoch [33/500], Train Loss: 0.1544\n",
      "Epoch [34/500], Train Loss: 0.1556\n",
      "Epoch [35/500], Train Loss: 0.1548\n",
      "Epoch [36/500], Train Loss: 0.1528\n",
      "Epoch [37/500], Train Loss: 0.1541\n",
      "Epoch [38/500], Train Loss: 0.1531\n",
      "Epoch [39/500], Train Loss: 0.1531\n",
      "Epoch [40/500], Train Loss: 0.1513\n",
      "Epoch [41/500], Train Loss: 0.1527\n",
      "Epoch [42/500], Train Loss: 0.1522\n",
      "Epoch [43/500], Train Loss: 0.1518\n",
      "Epoch [44/500], Train Loss: 0.1519\n",
      "Epoch [45/500], Train Loss: 0.1511\n",
      "Epoch [46/500], Train Loss: 0.1510\n",
      "Epoch [47/500], Train Loss: 0.1500\n",
      "Epoch [48/500], Train Loss: 0.1502\n",
      "Epoch [49/500], Train Loss: 0.1528\n",
      "Epoch [50/500], Train Loss: 0.1487\n",
      "Epoch [51/500], Train Loss: 0.1482\n",
      "Epoch [52/500], Train Loss: 0.1497\n",
      "Epoch [53/500], Train Loss: 0.1489\n",
      "Epoch [54/500], Train Loss: 0.1490\n",
      "Epoch [55/500], Train Loss: 0.1490\n",
      "Epoch [56/500], Train Loss: 0.1490\n",
      "Epoch [57/500], Train Loss: 0.1462\n",
      "Epoch [58/500], Train Loss: 0.1472\n",
      "Epoch [59/500], Train Loss: 0.1493\n",
      "Epoch [60/500], Train Loss: 0.1460\n",
      "Epoch [61/500], Train Loss: 0.1467\n",
      "Epoch [62/500], Train Loss: 0.1457\n",
      "Epoch [63/500], Train Loss: 0.1457\n",
      "Epoch [64/500], Train Loss: 0.1463\n",
      "Epoch [65/500], Train Loss: 0.1476\n",
      "Epoch [66/500], Train Loss: 0.1473\n",
      "Epoch [67/500], Train Loss: 0.1465\n",
      "Epoch [68/500], Train Loss: 0.1483\n",
      "Epoch [69/500], Train Loss: 0.1442\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 209\u001b[0m\n\u001b[1;32m    206\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_y) \u001b[38;5;241m+\u001b[39m model\u001b[38;5;241m.\u001b[39ml1_loss()\n\u001b[1;32m    208\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 209\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    212\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "from sklearn.metrics import r2_score as sklearn_r2_score\n",
    "import optuna\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# Load and preprocess the data\n",
    "data_directory = '../../Data/Filtered_unphased_training_data/'\n",
    "start = 1\n",
    "\n",
    "# Initialize lists to store the performance metrics for each chromosome\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "false_positive_rates = []\n",
    "auc_rocs = []\n",
    "r2_scores = []\n",
    "iqs_scores = []\n",
    "\n",
    "# Create folders for saving files\n",
    "output_folder = \"../../Data/model_results_unphased/logistic_regression/\"\n",
    "model_folder = output_folder + \"models_unphased/\"\n",
    "csv_folder = output_folder + \"csv_files/\"\n",
    "curve_folder = output_folder + \"roc_curves/\"\n",
    "\n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "os.makedirs(csv_folder, exist_ok=True)\n",
    "os.makedirs(curve_folder, exist_ok=True)\n",
    "\n",
    "for chromosome_number in range(start, 23):\n",
    "    # Create subfolders for the current chromosome\n",
    "    chr_model_folder = model_folder + f\"chr{chromosome_number}/\"\n",
    "    chr_csv_folder = csv_folder + f\"chr{chromosome_number}/\"\n",
    "    chr_curve_folder = curve_folder + f\"chr{chromosome_number}/\"\n",
    "\n",
    "    os.makedirs(chr_model_folder, exist_ok=True)\n",
    "    os.makedirs(chr_csv_folder, exist_ok=True)\n",
    "    os.makedirs(chr_curve_folder, exist_ok=True)\n",
    "\n",
    "    file_name = data_directory + f\"23AndMe_PRS313_merged_chr{chromosome_number}_matching_combined.parquet\"\n",
    "    data = pd.read_parquet(file_name)\n",
    "\n",
    "\n",
    "    # Split the data into features and target\n",
    "    X = torch.tensor(data.filter(regex='^(?!.*Unknown)').values, dtype=torch.float32)\n",
    "    y = torch.tensor(data.filter(regex='Unknown').values, dtype=torch.float32)\n",
    "\n",
    "    print(\"Unknown PRS313 SNPs: \", y.shape[1])\n",
    "    print(\"Known PRS313 SNPs: \", data[[col for col in data.columns if \"PRS313_Known\" in col]].shape[1])\n",
    "    print(\"23AndMe SNPs with LD to Unknown PRS313 SNPs: \", data[[col for col in data.columns if \"PRS313_\" not in col]].shape[1])\n",
    "    print(\"Total SNPs used for Training: \", X.shape[1])\n",
    "\n",
    "    # Split the data into train-validation and test sets\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Define the logistic regression model with lasso regularization\n",
    "    class LogisticRegression(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim, l1_coef=0.0):\n",
    "            super(LogisticRegression, self).__init__()\n",
    "            self.linear = nn.Linear(input_dim, output_dim)\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "            self.l1_coef = l1_coef\n",
    "\n",
    "        def forward(self, x):\n",
    "            out = self.linear(x)\n",
    "            out = self.sigmoid(out)\n",
    "            return out\n",
    "\n",
    "        def l1_loss(self):\n",
    "            return self.l1_coef * torch.norm(self.linear.weight, p=1)\n",
    "        \n",
    "    # Set the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Set the hyperparameters for tuning\n",
    "    input_dim = X_train_val.shape[1]\n",
    "    output_dim = y_train_val.shape[1]\n",
    "    num_epochs = 500\n",
    "    batch_size = 128\n",
    "\n",
    "    # Define the objective function for Optuna with cross-validation and early stopping\n",
    "    def objective(trial):\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-1, log=True)\n",
    "        l1_coef = trial.suggest_float('l1_coef', 1e-5, 1e-1, log=True)\n",
    "        patience = trial.suggest_int('patience', 5, 20)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "\n",
    "        model = LogisticRegression(input_dim, output_dim, l1_coef).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        criterion = nn.BCELoss()\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=False)\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        fold_losses = []\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_val, y_train_val.argmax(dim=1))):\n",
    "            X_train, X_val = X_train_val[train_idx], X_train_val[val_idx]\n",
    "            y_train, y_val = y_train_val[train_idx], y_train_val[val_idx]\n",
    "\n",
    "            train_dataset = TensorDataset(X_train, y_train)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "            best_val_loss = float('inf')\n",
    "            counter = 0\n",
    "\n",
    "            for epoch in range(num_epochs):\n",
    "                train_loss = 0.0\n",
    "                for batch_X, batch_y in train_loader:\n",
    "                    batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "                    outputs = model(batch_X)\n",
    "                    loss = criterion(outputs, batch_y) + model.l1_loss()\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    train_loss += loss.item()\n",
    "\n",
    "                train_loss /= len(train_loader)\n",
    "\n",
    "                val_dataset = TensorDataset(X_val, y_val)\n",
    "                val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    val_loss = 0.0\n",
    "                    for batch_X, batch_y in val_loader:\n",
    "                        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                        outputs = model(batch_X)\n",
    "                        loss = criterion(outputs, batch_y) + model.l1_loss()\n",
    "                        val_loss += loss.item()\n",
    "\n",
    "                    val_loss /= len(val_loader)\n",
    "                    scheduler.step(val_loss)\n",
    "\n",
    "                    if val_loss < best_val_loss:\n",
    "                        best_val_loss = val_loss\n",
    "                        counter = 0\n",
    "                    else:\n",
    "                        counter += 1\n",
    "\n",
    "                    if counter >= patience:\n",
    "                        # print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                        break\n",
    "\n",
    "            fold_losses.append(best_val_loss)\n",
    "\n",
    "        return np.mean(fold_losses)\n",
    "\n",
    "    # Create the \"optuna_studies\" folder if it doesn't exist\n",
    "    os.makedirs(\"optuna_studies\", exist_ok=True)\n",
    "\n",
    "    # Create an Optuna study and optimize the hyperparameters\n",
    "    study_name = f\"chr{chromosome_number}_study\"\n",
    "    storage_name = f\"sqlite:///optuna_studies/{study_name}.db\"\n",
    "\n",
    "    # Check if the study exists\n",
    "\n",
    "    current_dir = os.getcwd()\n",
    "    study_exists = os.path.exists(current_dir + f\"/optuna_studies/{study_name}.db\")\n",
    "    \n",
    "    if study_exists:\n",
    "        # Load the existing study\n",
    "        study = optuna.load_study(study_name=study_name, storage=storage_name)\n",
    "    else:\n",
    "        # Create a new study\n",
    "        study = optuna.create_study(direction='minimize', study_name=study_name, storage=storage_name)\n",
    "\n",
    "    study.optimize(objective, n_trials=25, n_jobs=-1)\n",
    "\n",
    "    # Print the best hyperparameters and best value\n",
    "    print(f\"Chr {chromosome_number} - Best hyperparameters: {study.best_params}\")\n",
    "    print(f\"Chr {chromosome_number} - Best value: {study.best_value:.4f}\")\n",
    "\n",
    "    # Train the final model with the best hyperparameters and early stopping\n",
    "    best_learning_rate = study.best_params['learning_rate']\n",
    "    best_l1_coef = study.best_params['l1_coef']\n",
    "    best_patience = study.best_params['patience']\n",
    "    best_batch_size = study.best_params['batch_size']\n",
    "\n",
    "    model = LogisticRegression(input_dim, output_dim, best_l1_coef).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_learning_rate)\n",
    "    criterion = nn.BCELoss()\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=False)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_val, y_train_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=best_batch_size, shuffle=True)\n",
    "\n",
    "    best_train_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y) + model.l1_loss()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "        if train_loss < best_train_loss:\n",
    "            best_train_loss = train_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        if counter >= best_patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "        scheduler.step(train_loss)\n",
    "\n",
    "    # Save the final model\n",
    "    model_save_path = chr_model_folder + f'final_model_chr{chromosome_number}.pth'\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"Final model saved at: {model_save_path}\")\n",
    "\n",
    "    # Evaluate the final model on the test set\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test.to(device))\n",
    "        test_preds = (test_outputs > 0.5).float()\n",
    "        test_accuracy = float(((test_preds > 0.5) == y_test).float().mean())\n",
    "        test_precision = precision_score(y_test.cpu().numpy(), test_preds.cpu().numpy(), average='micro')\n",
    "        test_recall = recall_score(y_test.cpu().numpy(), test_preds.cpu().numpy(), average='micro')\n",
    "        test_f1 = f1_score(y_test.cpu().numpy(), test_preds.cpu().numpy(), average='micro')\n",
    "        test_roc_auc = roc_auc_score(y_test.cpu().numpy(), test_outputs.cpu().numpy(), average='micro')\n",
    "        test_r2 = sklearn_r2_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "        test_iqs = calculate_iqs_unphased(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "\n",
    "        # Calculate false positive rate\n",
    "        cm = confusion_matrix(y_test.cpu().numpy().ravel(), test_preds.cpu().numpy().ravel())\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        test_fpr = fp / (fp + tn)\n",
    "\n",
    "        # Append performance metrics to the lists\n",
    "        accuracies.append(test_accuracy)\n",
    "        precisions.append(test_precision)\n",
    "        recalls.append(test_recall)\n",
    "        false_positive_rates.append(test_fpr)\n",
    "        auc_rocs.append(test_roc_auc)\n",
    "        r2_scores.append(test_r2)\n",
    "        iqs_scores.append(test_iqs)\n",
    "\n",
    "        # Calculate individual R^2 scores for each SNP\n",
    "        individual_r2_scores = sklearn_r2_score(y_test.cpu().numpy(), test_outputs.cpu().numpy(), multioutput='raw_values')\n",
    "\n",
    "        # Calculate individual IQS scores for each SNP\n",
    "        individual_iqs_scores = np.array([calculate_iqs_unphased(y_test.cpu().numpy()[:, i].reshape(-1, 1), test_outputs.cpu().numpy()[:, i].reshape(-1, 1)) for i in range(y_test.shape[1])])\n",
    "\n",
    "        # Get the names of the SNPs from the original dataframe\n",
    "        snp_names = data.filter(regex='Unknown').columns\n",
    "\n",
    "        # Save individual R^2 scores to a CSV file\n",
    "        csv_file = chr_csv_folder + f'individual_r2_scores_chr{chromosome_number}.csv'\n",
    "\n",
    "        with open(csv_file, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['SNP', 'R2 Score'])\n",
    "            for snp, r2_score in zip(snp_names, individual_r2_scores):\n",
    "                writer.writerow([snp, r2_score])\n",
    "\n",
    "        print(f\"Individual R^2 scores saved at: {csv_file}\")\n",
    "\n",
    "        # Save individual IQS scores to a CSV file\n",
    "        iqs_csv_file = chr_csv_folder + f'individual_iqs_scores_chr{chromosome_number}.csv'\n",
    "\n",
    "        with open(iqs_csv_file, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['SNP', 'IQS Score'])\n",
    "            for snp, iqs_score in zip(snp_names, individual_iqs_scores):\n",
    "                writer.writerow([snp, iqs_score])\n",
    "\n",
    "        print(f\"Individual IQS scores saved at: {iqs_csv_file}\")\n",
    "\n",
    "        # Save individual AUC ROC curves for each SNP\n",
    "        for i, snp in enumerate(snp_names):\n",
    "            try: \n",
    "                fpr, tpr, _ = roc_curve(y_test.cpu().numpy()[:, i], test_outputs.cpu().numpy()[:, i])\n",
    "                plt.figure()\n",
    "                plt.plot(fpr, tpr, label=f'AUC ROC = {roc_auc_score(y_test.cpu().numpy()[:, i], test_outputs.cpu().numpy()[:, i]):.4f}')\n",
    "                plt.xlabel('False Positive Rate')\n",
    "                plt.ylabel('True Positive Rate')\n",
    "                plt.title(f'AUC ROC Curve - {snp}')\n",
    "                plt.legend()\n",
    "                \n",
    "                curve_file = chr_curve_folder + f'auc_roc_curve_{snp}_chr{chromosome_number}.png'\n",
    "                plt.savefig(curve_file)\n",
    "                plt.close()\n",
    "            except ValueError:\n",
    "                # Save a placeholder image if there is insufficient data\n",
    "                plt.figure()\n",
    "                plt.axis('off')\n",
    "                plt.text(0.5, 0.5, \"Insufficient data for ROC curve\", ha='center', va='center')\n",
    "                curve_file = chr_curve_folder + f'auc_roc_curve_{snp}_chr{chromosome_number}.png'\n",
    "                plt.savefig(curve_file)\n",
    "                plt.close()\n",
    "\n",
    "                print(f\"Skipping SNP {snp} due to insufficient data\")\n",
    "\n",
    "\n",
    "        print(f\"Individual AUC ROC curves saved in: {curve_folder}\")\n",
    "\n",
    "        # Create a DataFrame to store the performance metrics for each chromosome\n",
    "        performance_df = pd.DataFrame({\n",
    "            'Chromosome': list(range(start, chromosome_number + 1)),\n",
    "            'Accuracy': accuracies,\n",
    "            'Precision': precisions,\n",
    "            'Recall': recalls,\n",
    "            'False Positive Rate': false_positive_rates,\n",
    "            'AUC ROC': auc_rocs,\n",
    "            'R2 Score': r2_scores,\n",
    "            'IQS Score': iqs_scores\n",
    "        })\n",
    "\n",
    "        # Save the performance metrics to a CSV file\n",
    "        performance_csv_file = csv_folder + 'performance_metrics.csv'\n",
    "        performance_df.to_csv(performance_csv_file, index=False)\n",
    "        print(f\"Performance metrics saved at: {performance_csv_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALL PRS Masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total SNPs:  880\n",
      "PRS313 SNPs:  30\n",
      "Total SNPs used for Training:  850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-04-30 10:13:55,609] Trial 138 finished with value: 0.18939957183140976 and parameters: {'learning_rate': 0.024010253494540893, 'l1_coef': 1.3182032660444333e-05, 'patience': 7, 'batch_size': 32}. Best is trial 32 with value: 0.08649516483912101.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 1 - Best hyperparameters: {'learning_rate': 0.01823591759267881, 'l1_coef': 1.0049346589997416e-05, 'patience': 9, 'batch_size': 32}\n",
      "Chr 1 - Best value: 0.0865\n",
      "Epoch [1/500], Train Loss: 0.5700\n",
      "Epoch [2/500], Train Loss: 0.3623\n",
      "Epoch [3/500], Train Loss: 0.3167\n",
      "Epoch [4/500], Train Loss: 0.2940\n",
      "Epoch [5/500], Train Loss: 0.2642\n",
      "Epoch [6/500], Train Loss: 0.2285\n",
      "Epoch [7/500], Train Loss: 0.2209\n",
      "Epoch [8/500], Train Loss: 0.2134\n",
      "Epoch [9/500], Train Loss: 0.2108\n",
      "Epoch [10/500], Train Loss: 0.2026\n",
      "Epoch [11/500], Train Loss: 0.1973\n",
      "Epoch [12/500], Train Loss: 0.1928\n",
      "Epoch [13/500], Train Loss: 0.1903\n",
      "Epoch [14/500], Train Loss: 0.1890\n",
      "Epoch [15/500], Train Loss: 0.1922\n",
      "Epoch [16/500], Train Loss: 0.1922\n",
      "Epoch [17/500], Train Loss: 0.1944\n",
      "Epoch [18/500], Train Loss: 0.1883\n",
      "Epoch [19/500], Train Loss: 0.1875\n",
      "Epoch [20/500], Train Loss: 0.1876\n",
      "Epoch [21/500], Train Loss: 0.1903\n",
      "Epoch [22/500], Train Loss: 0.1797\n",
      "Epoch [23/500], Train Loss: 0.1770\n",
      "Epoch [24/500], Train Loss: 0.1848\n",
      "Epoch [25/500], Train Loss: 0.1880\n",
      "Epoch [26/500], Train Loss: 0.1787\n",
      "Epoch [27/500], Train Loss: 0.1780\n",
      "Epoch [28/500], Train Loss: 0.1794\n",
      "Epoch [29/500], Train Loss: 0.1779\n",
      "Epoch [30/500], Train Loss: 0.1436\n",
      "Epoch [31/500], Train Loss: 0.1381\n",
      "Epoch [32/500], Train Loss: 0.1369\n",
      "Epoch [33/500], Train Loss: 0.1362\n",
      "Epoch [34/500], Train Loss: 0.1354\n",
      "Epoch [35/500], Train Loss: 0.1356\n",
      "Epoch [36/500], Train Loss: 0.1353\n",
      "Epoch [37/500], Train Loss: 0.1346\n",
      "Epoch [38/500], Train Loss: 0.1345\n",
      "Epoch [39/500], Train Loss: 0.1345\n",
      "Epoch [40/500], Train Loss: 0.1345\n",
      "Epoch [41/500], Train Loss: 0.1338\n",
      "Epoch [42/500], Train Loss: 0.1338\n",
      "Epoch [43/500], Train Loss: 0.1340\n",
      "Epoch [44/500], Train Loss: 0.1338\n",
      "Epoch [45/500], Train Loss: 0.1334\n",
      "Epoch [46/500], Train Loss: 0.1335\n",
      "Epoch [47/500], Train Loss: 0.1340\n",
      "Epoch [48/500], Train Loss: 0.1335\n",
      "Epoch [49/500], Train Loss: 0.1328\n",
      "Epoch [50/500], Train Loss: 0.1332\n",
      "Epoch [51/500], Train Loss: 0.1335\n",
      "Epoch [52/500], Train Loss: 0.1335\n",
      "Epoch [53/500], Train Loss: 0.1337\n",
      "Epoch [54/500], Train Loss: 0.1329\n",
      "Epoch [55/500], Train Loss: 0.1333\n",
      "Epoch [56/500], Train Loss: 0.1297\n",
      "Epoch [57/500], Train Loss: 0.1293\n",
      "Epoch [58/500], Train Loss: 0.1293\n",
      "Epoch [59/500], Train Loss: 0.1293\n",
      "Epoch [60/500], Train Loss: 0.1292\n",
      "Epoch [61/500], Train Loss: 0.1293\n",
      "Epoch [62/500], Train Loss: 0.1292\n",
      "Epoch [63/500], Train Loss: 0.1292\n",
      "Epoch [64/500], Train Loss: 0.1293\n",
      "Epoch [65/500], Train Loss: 0.1292\n",
      "Epoch [66/500], Train Loss: 0.1293\n",
      "Epoch [67/500], Train Loss: 0.1288\n",
      "Epoch [68/500], Train Loss: 0.1287\n",
      "Epoch [69/500], Train Loss: 0.1287\n",
      "Epoch [70/500], Train Loss: 0.1286\n",
      "Epoch [71/500], Train Loss: 0.1286\n",
      "Epoch [72/500], Train Loss: 0.1287\n",
      "Epoch [73/500], Train Loss: 0.1287\n",
      "Epoch [74/500], Train Loss: 0.1286\n",
      "Epoch [75/500], Train Loss: 0.1288\n",
      "Epoch [76/500], Train Loss: 0.1287\n",
      "Epoch [77/500], Train Loss: 0.1287\n",
      "Epoch [78/500], Train Loss: 0.1286\n",
      "Epoch [79/500], Train Loss: 0.1286\n",
      "Epoch [80/500], Train Loss: 0.1285\n",
      "Epoch [81/500], Train Loss: 0.1286\n",
      "Epoch [82/500], Train Loss: 0.1286\n",
      "Epoch [83/500], Train Loss: 0.1287\n",
      "Epoch [84/500], Train Loss: 0.1286\n",
      "Epoch [85/500], Train Loss: 0.1286\n",
      "Epoch [86/500], Train Loss: 0.1286\n",
      "Epoch [87/500], Train Loss: 0.1287\n",
      "Epoch [88/500], Train Loss: 0.1286\n",
      "Epoch [89/500], Train Loss: 0.1287\n",
      "Early stopping at epoch 89\n",
      "Final model saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/models_unphased/chr1/final_model_chr1.pth\n",
      "Individual R^2 scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr1/individual_r2_scores_chr1.csv\n",
      "Individual IQS scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr1/individual_iqs_scores_chr1.csv\n",
      "Individual AUC ROC curves saved in: ../../Data/model_results_unphased_all_PRS/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/performance_metrics.csv\n",
      "Total SNPs:  617\n",
      "PRS313 SNPs:  21\n",
      "Total SNPs used for Training:  596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-30 10:14:02,317] Trial 54 finished with value: 0.17957539604260372 and parameters: {'learning_rate': 0.019514807663622482, 'l1_coef': 1.9827302273015474e-05, 'patience': 9, 'batch_size': 32}. Best is trial 50 with value: 0.11959010179226218.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 2 - Best hyperparameters: {'learning_rate': 0.008236314117298912, 'l1_coef': 1.0089572204728918e-05, 'patience': 10, 'batch_size': 32}\n",
      "Chr 2 - Best value: 0.1196\n",
      "Epoch [1/500], Train Loss: 0.4341\n",
      "Epoch [2/500], Train Loss: 0.3183\n",
      "Epoch [3/500], Train Loss: 0.2745\n",
      "Epoch [4/500], Train Loss: 0.2505\n",
      "Epoch [5/500], Train Loss: 0.2330\n",
      "Epoch [6/500], Train Loss: 0.2221\n",
      "Epoch [7/500], Train Loss: 0.2127\n",
      "Epoch [8/500], Train Loss: 0.2058\n",
      "Epoch [9/500], Train Loss: 0.1977\n",
      "Epoch [10/500], Train Loss: 0.1920\n",
      "Epoch [11/500], Train Loss: 0.1893\n",
      "Epoch [12/500], Train Loss: 0.1855\n",
      "Epoch [13/500], Train Loss: 0.1825\n",
      "Epoch [14/500], Train Loss: 0.1787\n",
      "Epoch [15/500], Train Loss: 0.1767\n",
      "Epoch [16/500], Train Loss: 0.1767\n",
      "Epoch [17/500], Train Loss: 0.1718\n",
      "Epoch [18/500], Train Loss: 0.1712\n",
      "Epoch [19/500], Train Loss: 0.1713\n",
      "Epoch [20/500], Train Loss: 0.1701\n",
      "Epoch [21/500], Train Loss: 0.1661\n",
      "Epoch [22/500], Train Loss: 0.1652\n",
      "Epoch [23/500], Train Loss: 0.1638\n",
      "Epoch [24/500], Train Loss: 0.1626\n",
      "Epoch [25/500], Train Loss: 0.1611\n",
      "Epoch [26/500], Train Loss: 0.1602\n",
      "Epoch [27/500], Train Loss: 0.1607\n",
      "Epoch [28/500], Train Loss: 0.1614\n",
      "Epoch [29/500], Train Loss: 0.1601\n",
      "Epoch [30/500], Train Loss: 0.1603\n",
      "Epoch [31/500], Train Loss: 0.1587\n",
      "Epoch [32/500], Train Loss: 0.1594\n",
      "Epoch [33/500], Train Loss: 0.1580\n",
      "Epoch [34/500], Train Loss: 0.1570\n",
      "Epoch [35/500], Train Loss: 0.1555\n",
      "Epoch [36/500], Train Loss: 0.1522\n",
      "Epoch [37/500], Train Loss: 0.1546\n",
      "Epoch [38/500], Train Loss: 0.1545\n",
      "Epoch [39/500], Train Loss: 0.1523\n",
      "Epoch [40/500], Train Loss: 0.1511\n",
      "Epoch [41/500], Train Loss: 0.1530\n",
      "Epoch [42/500], Train Loss: 0.1511\n",
      "Epoch [43/500], Train Loss: 0.1488\n",
      "Epoch [44/500], Train Loss: 0.1495\n",
      "Epoch [45/500], Train Loss: 0.1513\n",
      "Epoch [46/500], Train Loss: 0.1498\n",
      "Epoch [47/500], Train Loss: 0.1513\n",
      "Epoch [48/500], Train Loss: 0.1484\n",
      "Epoch [49/500], Train Loss: 0.1500\n",
      "Epoch [50/500], Train Loss: 0.1501\n",
      "Epoch [51/500], Train Loss: 0.1476\n",
      "Epoch [52/500], Train Loss: 0.1467\n",
      "Epoch [53/500], Train Loss: 0.1482\n",
      "Epoch [54/500], Train Loss: 0.1514\n",
      "Epoch [55/500], Train Loss: 0.1476\n",
      "Epoch [56/500], Train Loss: 0.1464\n",
      "Epoch [57/500], Train Loss: 0.1506\n",
      "Epoch [58/500], Train Loss: 0.1486\n",
      "Epoch [59/500], Train Loss: 0.1497\n",
      "Epoch [60/500], Train Loss: 0.1477\n",
      "Epoch [61/500], Train Loss: 0.1454\n",
      "Epoch [62/500], Train Loss: 0.1466\n",
      "Epoch [63/500], Train Loss: 0.1446\n",
      "Epoch [64/500], Train Loss: 0.1447\n",
      "Epoch [65/500], Train Loss: 0.1463\n",
      "Epoch [66/500], Train Loss: 0.1456\n",
      "Epoch [67/500], Train Loss: 0.1469\n",
      "Epoch [68/500], Train Loss: 0.1456\n",
      "Epoch [69/500], Train Loss: 0.1461\n",
      "Epoch [70/500], Train Loss: 0.1320\n",
      "Epoch [71/500], Train Loss: 0.1308\n",
      "Epoch [72/500], Train Loss: 0.1305\n",
      "Epoch [73/500], Train Loss: 0.1300\n",
      "Epoch [74/500], Train Loss: 0.1303\n",
      "Epoch [75/500], Train Loss: 0.1298\n",
      "Epoch [76/500], Train Loss: 0.1297\n",
      "Epoch [77/500], Train Loss: 0.1300\n",
      "Epoch [78/500], Train Loss: 0.1300\n",
      "Epoch [79/500], Train Loss: 0.1298\n",
      "Epoch [80/500], Train Loss: 0.1297\n",
      "Epoch [81/500], Train Loss: 0.1297\n",
      "Epoch [82/500], Train Loss: 0.1300\n",
      "Epoch [83/500], Train Loss: 0.1297\n",
      "Epoch [84/500], Train Loss: 0.1301\n",
      "Epoch [85/500], Train Loss: 0.1297\n",
      "Epoch [86/500], Train Loss: 0.1297\n",
      "Epoch [87/500], Train Loss: 0.1298\n",
      "Epoch [88/500], Train Loss: 0.1299\n",
      "Epoch [89/500], Train Loss: 0.1296\n",
      "Epoch [90/500], Train Loss: 0.1300\n",
      "Epoch [91/500], Train Loss: 0.1295\n",
      "Epoch [92/500], Train Loss: 0.1297\n",
      "Epoch [93/500], Train Loss: 0.1298\n",
      "Epoch [94/500], Train Loss: 0.1298\n",
      "Epoch [95/500], Train Loss: 0.1296\n",
      "Epoch [96/500], Train Loss: 0.1299\n",
      "Epoch [97/500], Train Loss: 0.1296\n",
      "Epoch [98/500], Train Loss: 0.1283\n",
      "Epoch [99/500], Train Loss: 0.1281\n",
      "Epoch [100/500], Train Loss: 0.1278\n",
      "Epoch [101/500], Train Loss: 0.1279\n",
      "Epoch [102/500], Train Loss: 0.1279\n",
      "Epoch [103/500], Train Loss: 0.1278\n",
      "Epoch [104/500], Train Loss: 0.1280\n",
      "Epoch [105/500], Train Loss: 0.1279\n",
      "Epoch [106/500], Train Loss: 0.1279\n",
      "Epoch [107/500], Train Loss: 0.1275\n",
      "Epoch [108/500], Train Loss: 0.1277\n",
      "Epoch [109/500], Train Loss: 0.1276\n",
      "Epoch [110/500], Train Loss: 0.1276\n",
      "Epoch [111/500], Train Loss: 0.1277\n",
      "Epoch [112/500], Train Loss: 0.1279\n",
      "Epoch [113/500], Train Loss: 0.1277\n",
      "Epoch [114/500], Train Loss: 0.1277\n",
      "Epoch [115/500], Train Loss: 0.1275\n",
      "Epoch [116/500], Train Loss: 0.1277\n",
      "Epoch [117/500], Train Loss: 0.1275\n",
      "Epoch [118/500], Train Loss: 0.1275\n",
      "Epoch [119/500], Train Loss: 0.1275\n",
      "Epoch [120/500], Train Loss: 0.1276\n",
      "Epoch [121/500], Train Loss: 0.1277\n",
      "Epoch [122/500], Train Loss: 0.1277\n",
      "Epoch [123/500], Train Loss: 0.1276\n",
      "Epoch [124/500], Train Loss: 0.1276\n",
      "Epoch [125/500], Train Loss: 0.1276\n",
      "Epoch [126/500], Train Loss: 0.1277\n",
      "Epoch [127/500], Train Loss: 0.1277\n",
      "Early stopping at epoch 127\n",
      "Final model saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/models_unphased/chr2/final_model_chr2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yg/982c10113957_2gb06y92y35sx509h/T/ipykernel_81857/73971118.py:42: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  iqs = (po - pc) / (1 - pc)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual R^2 scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr2/individual_r2_scores_chr2.csv\n",
      "Individual IQS scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr2/individual_iqs_scores_chr2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:1124: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping SNP chr2_217955896_GA_G_PRS313_Unknown_combined due to insufficient data\n",
      "Individual AUC ROC curves saved in: ../../Data/model_results_unphased_all_PRS/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/performance_metrics.csv\n",
      "Total SNPs:  947\n",
      "PRS313 SNPs:  16\n",
      "Total SNPs used for Training:  931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-04-30 10:14:09,398] Trial 52 finished with value: 0.1572665859013796 and parameters: {'learning_rate': 0.0011689403608019387, 'l1_coef': 1.751091597056413e-05, 'patience': 16, 'batch_size': 128}. Best is trial 11 with value: 0.04525891537112849.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 3 - Best hyperparameters: {'learning_rate': 0.01168727761615579, 'l1_coef': 1.1084312581961695e-05, 'patience': 15, 'batch_size': 64}\n",
      "Chr 3 - Best value: 0.0453\n",
      "Epoch [1/500], Train Loss: 0.5049\n",
      "Epoch [2/500], Train Loss: 0.2854\n",
      "Epoch [3/500], Train Loss: 0.2288\n",
      "Epoch [4/500], Train Loss: 0.2009\n",
      "Epoch [5/500], Train Loss: 0.1854\n",
      "Epoch [6/500], Train Loss: 0.1680\n",
      "Epoch [7/500], Train Loss: 0.1581\n",
      "Epoch [8/500], Train Loss: 0.1535\n",
      "Epoch [9/500], Train Loss: 0.1421\n",
      "Epoch [10/500], Train Loss: 0.1342\n",
      "Epoch [11/500], Train Loss: 0.1327\n",
      "Epoch [12/500], Train Loss: 0.1274\n",
      "Epoch [13/500], Train Loss: 0.1213\n",
      "Epoch [14/500], Train Loss: 0.1199\n",
      "Epoch [15/500], Train Loss: 0.1184\n",
      "Epoch [16/500], Train Loss: 0.1148\n",
      "Epoch [17/500], Train Loss: 0.1140\n",
      "Epoch [18/500], Train Loss: 0.1125\n",
      "Epoch [19/500], Train Loss: 0.1081\n",
      "Epoch [20/500], Train Loss: 0.1035\n",
      "Epoch [21/500], Train Loss: 0.1030\n",
      "Epoch [22/500], Train Loss: 0.1058\n",
      "Epoch [23/500], Train Loss: 0.1004\n",
      "Epoch [24/500], Train Loss: 0.0990\n",
      "Epoch [25/500], Train Loss: 0.0987\n",
      "Epoch [26/500], Train Loss: 0.0996\n",
      "Epoch [27/500], Train Loss: 0.0963\n",
      "Epoch [28/500], Train Loss: 0.0948\n",
      "Epoch [29/500], Train Loss: 0.0937\n",
      "Epoch [30/500], Train Loss: 0.0926\n",
      "Epoch [31/500], Train Loss: 0.0964\n",
      "Epoch [32/500], Train Loss: 0.0938\n",
      "Epoch [33/500], Train Loss: 0.0901\n",
      "Epoch [34/500], Train Loss: 0.0890\n",
      "Epoch [35/500], Train Loss: 0.0873\n",
      "Epoch [36/500], Train Loss: 0.0861\n",
      "Epoch [37/500], Train Loss: 0.0880\n",
      "Epoch [38/500], Train Loss: 0.0866\n",
      "Epoch [39/500], Train Loss: 0.0857\n",
      "Epoch [40/500], Train Loss: 0.0860\n",
      "Epoch [41/500], Train Loss: 0.0848\n",
      "Epoch [42/500], Train Loss: 0.0851\n",
      "Epoch [43/500], Train Loss: 0.0850\n",
      "Epoch [44/500], Train Loss: 0.0834\n",
      "Epoch [45/500], Train Loss: 0.0839\n",
      "Epoch [46/500], Train Loss: 0.0868\n",
      "Epoch [47/500], Train Loss: 0.0815\n",
      "Epoch [48/500], Train Loss: 0.0812\n",
      "Epoch [49/500], Train Loss: 0.0815\n",
      "Epoch [50/500], Train Loss: 0.0813\n",
      "Epoch [51/500], Train Loss: 0.0817\n",
      "Epoch [52/500], Train Loss: 0.0805\n",
      "Epoch [53/500], Train Loss: 0.0818\n",
      "Epoch [54/500], Train Loss: 0.0785\n",
      "Epoch [55/500], Train Loss: 0.0807\n",
      "Epoch [56/500], Train Loss: 0.0809\n",
      "Epoch [57/500], Train Loss: 0.0798\n",
      "Epoch [58/500], Train Loss: 0.0827\n",
      "Epoch [59/500], Train Loss: 0.0812\n",
      "Epoch [60/500], Train Loss: 0.0806\n",
      "Epoch [61/500], Train Loss: 0.0728\n",
      "Epoch [62/500], Train Loss: 0.0714\n",
      "Epoch [63/500], Train Loss: 0.0720\n",
      "Epoch [64/500], Train Loss: 0.0714\n",
      "Epoch [65/500], Train Loss: 0.0711\n",
      "Epoch [66/500], Train Loss: 0.0715\n",
      "Epoch [67/500], Train Loss: 0.0713\n",
      "Epoch [68/500], Train Loss: 0.0713\n",
      "Epoch [69/500], Train Loss: 0.0710\n",
      "Epoch [70/500], Train Loss: 0.0709\n",
      "Epoch [71/500], Train Loss: 0.0707\n",
      "Epoch [72/500], Train Loss: 0.0713\n",
      "Epoch [73/500], Train Loss: 0.0706\n",
      "Epoch [74/500], Train Loss: 0.0707\n",
      "Epoch [75/500], Train Loss: 0.0706\n",
      "Epoch [76/500], Train Loss: 0.0706\n",
      "Epoch [77/500], Train Loss: 0.0708\n",
      "Epoch [78/500], Train Loss: 0.0705\n",
      "Epoch [79/500], Train Loss: 0.0706\n",
      "Epoch [80/500], Train Loss: 0.0708\n",
      "Epoch [81/500], Train Loss: 0.0709\n",
      "Epoch [82/500], Train Loss: 0.0702\n",
      "Epoch [83/500], Train Loss: 0.0707\n",
      "Epoch [84/500], Train Loss: 0.0703\n",
      "Epoch [85/500], Train Loss: 0.0709\n",
      "Epoch [86/500], Train Loss: 0.0702\n",
      "Epoch [87/500], Train Loss: 0.0707\n",
      "Epoch [88/500], Train Loss: 0.0706\n",
      "Epoch [89/500], Train Loss: 0.0703\n",
      "Epoch [90/500], Train Loss: 0.0705\n",
      "Epoch [91/500], Train Loss: 0.0703\n",
      "Epoch [92/500], Train Loss: 0.0701\n",
      "Epoch [93/500], Train Loss: 0.0701\n",
      "Epoch [94/500], Train Loss: 0.0704\n",
      "Epoch [95/500], Train Loss: 0.0703\n",
      "Epoch [96/500], Train Loss: 0.0702\n",
      "Epoch [97/500], Train Loss: 0.0704\n",
      "Epoch [98/500], Train Loss: 0.0705\n",
      "Epoch [99/500], Train Loss: 0.0697\n",
      "Epoch [100/500], Train Loss: 0.0692\n",
      "Epoch [101/500], Train Loss: 0.0695\n",
      "Epoch [102/500], Train Loss: 0.0692\n",
      "Epoch [103/500], Train Loss: 0.0696\n",
      "Epoch [104/500], Train Loss: 0.0697\n",
      "Epoch [105/500], Train Loss: 0.0695\n",
      "Epoch [106/500], Train Loss: 0.0694\n",
      "Epoch [107/500], Train Loss: 0.0696\n",
      "Epoch [108/500], Train Loss: 0.0694\n",
      "Epoch [109/500], Train Loss: 0.0693\n",
      "Epoch [110/500], Train Loss: 0.0692\n",
      "Epoch [111/500], Train Loss: 0.0692\n",
      "Epoch [112/500], Train Loss: 0.0693\n",
      "Epoch [113/500], Train Loss: 0.0693\n",
      "Epoch [114/500], Train Loss: 0.0696\n",
      "Epoch [115/500], Train Loss: 0.0693\n",
      "Epoch [116/500], Train Loss: 0.0693\n",
      "Epoch [117/500], Train Loss: 0.0694\n",
      "Early stopping at epoch 117\n",
      "Final model saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/models_unphased/chr3/final_model_chr3.pth\n",
      "Individual R^2 scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr3/individual_r2_scores_chr3.csv\n",
      "Individual IQS scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr3/individual_iqs_scores_chr3.csv\n",
      "Individual AUC ROC curves saved in: ../../Data/model_results_unphased_all_PRS/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/performance_metrics.csv\n",
      "Total SNPs:  1034\n",
      "PRS313 SNPs:  11\n",
      "Total SNPs used for Training:  1023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-04-30 10:14:16,702] Trial 52 finished with value: 0.15921089889911505 and parameters: {'learning_rate': 0.0011741904885725096, 'l1_coef': 6.940710723028401e-05, 'patience': 17, 'batch_size': 32}. Best is trial 6 with value: 0.05639927589467593.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 4 - Best hyperparameters: {'learning_rate': 0.010022005750497007, 'l1_coef': 1.0999251330619296e-05, 'patience': 13, 'batch_size': 64}\n",
      "Chr 4 - Best value: 0.0564\n",
      "Epoch [1/500], Train Loss: 0.5581\n",
      "Epoch [2/500], Train Loss: 0.3583\n",
      "Epoch [3/500], Train Loss: 0.2870\n",
      "Epoch [4/500], Train Loss: 0.2443\n",
      "Epoch [5/500], Train Loss: 0.2133\n",
      "Epoch [6/500], Train Loss: 0.1965\n",
      "Epoch [7/500], Train Loss: 0.1809\n",
      "Epoch [8/500], Train Loss: 0.1709\n",
      "Epoch [9/500], Train Loss: 0.1611\n",
      "Epoch [10/500], Train Loss: 0.1525\n",
      "Epoch [11/500], Train Loss: 0.1429\n",
      "Epoch [12/500], Train Loss: 0.1360\n",
      "Epoch [13/500], Train Loss: 0.1312\n",
      "Epoch [14/500], Train Loss: 0.1269\n",
      "Epoch [15/500], Train Loss: 0.1247\n",
      "Epoch [16/500], Train Loss: 0.1187\n",
      "Epoch [17/500], Train Loss: 0.1143\n",
      "Epoch [18/500], Train Loss: 0.1102\n",
      "Epoch [19/500], Train Loss: 0.1097\n",
      "Epoch [20/500], Train Loss: 0.1051\n",
      "Epoch [21/500], Train Loss: 0.1052\n",
      "Epoch [22/500], Train Loss: 0.1000\n",
      "Epoch [23/500], Train Loss: 0.0959\n",
      "Epoch [24/500], Train Loss: 0.0953\n",
      "Epoch [25/500], Train Loss: 0.0944\n",
      "Epoch [26/500], Train Loss: 0.0931\n",
      "Epoch [27/500], Train Loss: 0.0899\n",
      "Epoch [28/500], Train Loss: 0.0883\n",
      "Epoch [29/500], Train Loss: 0.0901\n",
      "Epoch [30/500], Train Loss: 0.0873\n",
      "Epoch [31/500], Train Loss: 0.0851\n",
      "Epoch [32/500], Train Loss: 0.0833\n",
      "Epoch [33/500], Train Loss: 0.0816\n",
      "Epoch [34/500], Train Loss: 0.0806\n",
      "Epoch [35/500], Train Loss: 0.0799\n",
      "Epoch [36/500], Train Loss: 0.0777\n",
      "Epoch [37/500], Train Loss: 0.0764\n",
      "Epoch [38/500], Train Loss: 0.0759\n",
      "Epoch [39/500], Train Loss: 0.0750\n",
      "Epoch [40/500], Train Loss: 0.0771\n",
      "Epoch [41/500], Train Loss: 0.0732\n",
      "Epoch [42/500], Train Loss: 0.0739\n",
      "Epoch [43/500], Train Loss: 0.0727\n",
      "Epoch [44/500], Train Loss: 0.0730\n",
      "Epoch [45/500], Train Loss: 0.0706\n",
      "Epoch [46/500], Train Loss: 0.0717\n",
      "Epoch [47/500], Train Loss: 0.0708\n",
      "Epoch [48/500], Train Loss: 0.0703\n",
      "Epoch [49/500], Train Loss: 0.0696\n",
      "Epoch [50/500], Train Loss: 0.0701\n",
      "Epoch [51/500], Train Loss: 0.0675\n",
      "Epoch [52/500], Train Loss: 0.0694\n",
      "Epoch [53/500], Train Loss: 0.0679\n",
      "Epoch [54/500], Train Loss: 0.0691\n",
      "Epoch [55/500], Train Loss: 0.0652\n",
      "Epoch [56/500], Train Loss: 0.0669\n",
      "Epoch [57/500], Train Loss: 0.0654\n",
      "Epoch [58/500], Train Loss: 0.0635\n",
      "Epoch [59/500], Train Loss: 0.0630\n",
      "Epoch [60/500], Train Loss: 0.0638\n",
      "Epoch [61/500], Train Loss: 0.0641\n",
      "Epoch [62/500], Train Loss: 0.0631\n",
      "Epoch [63/500], Train Loss: 0.0625\n",
      "Epoch [64/500], Train Loss: 0.0619\n",
      "Epoch [65/500], Train Loss: 0.0630\n",
      "Epoch [66/500], Train Loss: 0.0617\n",
      "Epoch [67/500], Train Loss: 0.0604\n",
      "Epoch [68/500], Train Loss: 0.0607\n",
      "Epoch [69/500], Train Loss: 0.0604\n",
      "Epoch [70/500], Train Loss: 0.0601\n",
      "Epoch [71/500], Train Loss: 0.0603\n",
      "Epoch [72/500], Train Loss: 0.0604\n",
      "Epoch [73/500], Train Loss: 0.0606\n",
      "Epoch [74/500], Train Loss: 0.0607\n",
      "Epoch [75/500], Train Loss: 0.0576\n",
      "Epoch [76/500], Train Loss: 0.0584\n",
      "Epoch [77/500], Train Loss: 0.0585\n",
      "Epoch [78/500], Train Loss: 0.0576\n",
      "Epoch [79/500], Train Loss: 0.0573\n",
      "Epoch [80/500], Train Loss: 0.0568\n",
      "Epoch [81/500], Train Loss: 0.0573\n",
      "Epoch [82/500], Train Loss: 0.0561\n",
      "Epoch [83/500], Train Loss: 0.0571\n",
      "Epoch [84/500], Train Loss: 0.0576\n",
      "Epoch [85/500], Train Loss: 0.0568\n",
      "Epoch [86/500], Train Loss: 0.0560\n",
      "Epoch [87/500], Train Loss: 0.0544\n",
      "Epoch [88/500], Train Loss: 0.0588\n",
      "Epoch [89/500], Train Loss: 0.0559\n",
      "Epoch [90/500], Train Loss: 0.0552\n",
      "Epoch [91/500], Train Loss: 0.0540\n",
      "Epoch [92/500], Train Loss: 0.0553\n",
      "Epoch [93/500], Train Loss: 0.0556\n",
      "Epoch [94/500], Train Loss: 0.0566\n",
      "Epoch [95/500], Train Loss: 0.0570\n",
      "Epoch [96/500], Train Loss: 0.0539\n",
      "Epoch [97/500], Train Loss: 0.0547\n",
      "Epoch [98/500], Train Loss: 0.0546\n",
      "Epoch [99/500], Train Loss: 0.0540\n",
      "Epoch [100/500], Train Loss: 0.0523\n",
      "Epoch [101/500], Train Loss: 0.0530\n",
      "Epoch [102/500], Train Loss: 0.0535\n",
      "Epoch [103/500], Train Loss: 0.0527\n",
      "Epoch [104/500], Train Loss: 0.0522\n",
      "Epoch [105/500], Train Loss: 0.0527\n",
      "Epoch [106/500], Train Loss: 0.0531\n",
      "Epoch [107/500], Train Loss: 0.0539\n",
      "Epoch [108/500], Train Loss: 0.0533\n",
      "Epoch [109/500], Train Loss: 0.0530\n",
      "Epoch [110/500], Train Loss: 0.0518\n",
      "Epoch [111/500], Train Loss: 0.0514\n",
      "Epoch [112/500], Train Loss: 0.0522\n",
      "Epoch [113/500], Train Loss: 0.0517\n",
      "Epoch [114/500], Train Loss: 0.0522\n",
      "Epoch [115/500], Train Loss: 0.0524\n",
      "Epoch [116/500], Train Loss: 0.0514\n",
      "Epoch [117/500], Train Loss: 0.0526\n",
      "Epoch [118/500], Train Loss: 0.0477\n",
      "Epoch [119/500], Train Loss: 0.0474\n",
      "Epoch [120/500], Train Loss: 0.0473\n",
      "Epoch [121/500], Train Loss: 0.0471\n",
      "Epoch [122/500], Train Loss: 0.0471\n",
      "Epoch [123/500], Train Loss: 0.0471\n",
      "Epoch [124/500], Train Loss: 0.0469\n",
      "Epoch [125/500], Train Loss: 0.0471\n",
      "Epoch [126/500], Train Loss: 0.0469\n",
      "Epoch [127/500], Train Loss: 0.0468\n",
      "Epoch [128/500], Train Loss: 0.0469\n",
      "Epoch [129/500], Train Loss: 0.0467\n",
      "Epoch [130/500], Train Loss: 0.0468\n",
      "Epoch [131/500], Train Loss: 0.0468\n",
      "Epoch [132/500], Train Loss: 0.0468\n",
      "Epoch [133/500], Train Loss: 0.0468\n",
      "Epoch [134/500], Train Loss: 0.0467\n",
      "Epoch [135/500], Train Loss: 0.0472\n",
      "Epoch [136/500], Train Loss: 0.0468\n",
      "Epoch [137/500], Train Loss: 0.0467\n",
      "Epoch [138/500], Train Loss: 0.0468\n",
      "Epoch [139/500], Train Loss: 0.0467\n",
      "Epoch [140/500], Train Loss: 0.0467\n",
      "Epoch [141/500], Train Loss: 0.0464\n",
      "Epoch [142/500], Train Loss: 0.0466\n",
      "Epoch [143/500], Train Loss: 0.0462\n",
      "Epoch [144/500], Train Loss: 0.0462\n",
      "Epoch [145/500], Train Loss: 0.0463\n",
      "Epoch [146/500], Train Loss: 0.0464\n",
      "Epoch [147/500], Train Loss: 0.0462\n",
      "Epoch [148/500], Train Loss: 0.0461\n",
      "Epoch [149/500], Train Loss: 0.0465\n",
      "Epoch [150/500], Train Loss: 0.0462\n",
      "Epoch [151/500], Train Loss: 0.0462\n",
      "Epoch [152/500], Train Loss: 0.0463\n",
      "Epoch [153/500], Train Loss: 0.0463\n",
      "Epoch [154/500], Train Loss: 0.0462\n",
      "Epoch [155/500], Train Loss: 0.0463\n",
      "Epoch [156/500], Train Loss: 0.0461\n",
      "Epoch [157/500], Train Loss: 0.0461\n",
      "Epoch [158/500], Train Loss: 0.0461\n",
      "Epoch [159/500], Train Loss: 0.0460\n",
      "Epoch [160/500], Train Loss: 0.0461\n",
      "Epoch [161/500], Train Loss: 0.0464\n",
      "Epoch [162/500], Train Loss: 0.0463\n",
      "Epoch [163/500], Train Loss: 0.0462\n",
      "Epoch [164/500], Train Loss: 0.0462\n",
      "Epoch [165/500], Train Loss: 0.0464\n",
      "Epoch [166/500], Train Loss: 0.0462\n",
      "Epoch [167/500], Train Loss: 0.0462\n",
      "Epoch [168/500], Train Loss: 0.0461\n",
      "Epoch [169/500], Train Loss: 0.0462\n",
      "Epoch [170/500], Train Loss: 0.0462\n",
      "Epoch [171/500], Train Loss: 0.0470\n",
      "Epoch [172/500], Train Loss: 0.0463\n",
      "Early stopping at epoch 172\n",
      "Final model saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/models_unphased/chr4/final_model_chr4.pth\n",
      "Individual R^2 scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr4/individual_r2_scores_chr4.csv\n",
      "Individual IQS scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr4/individual_iqs_scores_chr4.csv\n",
      "Individual AUC ROC curves saved in: ../../Data/model_results_unphased_all_PRS/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/performance_metrics.csv\n",
      "Total SNPs:  1334\n",
      "PRS313 SNPs:  34\n",
      "Total SNPs used for Training:  1300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-04-30 10:14:23,896] Trial 52 finished with value: 0.17206094094685148 and parameters: {'learning_rate': 0.0016840216950022215, 'l1_coef': 1.1820460713931181e-05, 'patience': 7, 'batch_size': 64}. Best is trial 34 with value: 0.07268482106072562.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 5 - Best hyperparameters: {'learning_rate': 0.010757867162206202, 'l1_coef': 1.0440017068267564e-05, 'patience': 9, 'batch_size': 64}\n",
      "Chr 5 - Best value: 0.0727\n",
      "Epoch [1/500], Train Loss: 0.6085\n",
      "Epoch [2/500], Train Loss: 0.3667\n",
      "Epoch [3/500], Train Loss: 0.3008\n",
      "Epoch [4/500], Train Loss: 0.2636\n",
      "Epoch [5/500], Train Loss: 0.2407\n",
      "Epoch [6/500], Train Loss: 0.2259\n",
      "Epoch [7/500], Train Loss: 0.2077\n",
      "Epoch [8/500], Train Loss: 0.1954\n",
      "Epoch [9/500], Train Loss: 0.1863\n",
      "Epoch [10/500], Train Loss: 0.1785\n",
      "Epoch [11/500], Train Loss: 0.1731\n",
      "Epoch [12/500], Train Loss: 0.1693\n",
      "Epoch [13/500], Train Loss: 0.1611\n",
      "Epoch [14/500], Train Loss: 0.1574\n",
      "Epoch [15/500], Train Loss: 0.1541\n",
      "Epoch [16/500], Train Loss: 0.1515\n",
      "Epoch [17/500], Train Loss: 0.1498\n",
      "Epoch [18/500], Train Loss: 0.1456\n",
      "Epoch [19/500], Train Loss: 0.1405\n",
      "Epoch [20/500], Train Loss: 0.1392\n",
      "Epoch [21/500], Train Loss: 0.1400\n",
      "Epoch [22/500], Train Loss: 0.1350\n",
      "Epoch [23/500], Train Loss: 0.1319\n",
      "Epoch [24/500], Train Loss: 0.1315\n",
      "Epoch [25/500], Train Loss: 0.1330\n",
      "Epoch [26/500], Train Loss: 0.1293\n",
      "Epoch [27/500], Train Loss: 0.1271\n",
      "Epoch [28/500], Train Loss: 0.1273\n",
      "Epoch [29/500], Train Loss: 0.1255\n",
      "Epoch [30/500], Train Loss: 0.1221\n",
      "Epoch [31/500], Train Loss: 0.1219\n",
      "Epoch [32/500], Train Loss: 0.1210\n",
      "Epoch [33/500], Train Loss: 0.1205\n",
      "Epoch [34/500], Train Loss: 0.1184\n",
      "Epoch [35/500], Train Loss: 0.1180\n",
      "Epoch [36/500], Train Loss: 0.1169\n",
      "Epoch [37/500], Train Loss: 0.1166\n",
      "Epoch [38/500], Train Loss: 0.1156\n",
      "Epoch [39/500], Train Loss: 0.1156\n",
      "Epoch [40/500], Train Loss: 0.1147\n",
      "Epoch [41/500], Train Loss: 0.1143\n",
      "Epoch [42/500], Train Loss: 0.1144\n",
      "Epoch [43/500], Train Loss: 0.1131\n",
      "Epoch [44/500], Train Loss: 0.1139\n",
      "Epoch [45/500], Train Loss: 0.1128\n",
      "Epoch [46/500], Train Loss: 0.1121\n",
      "Epoch [47/500], Train Loss: 0.1107\n",
      "Epoch [48/500], Train Loss: 0.1095\n",
      "Epoch [49/500], Train Loss: 0.1096\n",
      "Epoch [50/500], Train Loss: 0.1100\n",
      "Epoch [51/500], Train Loss: 0.1101\n",
      "Epoch [52/500], Train Loss: 0.1086\n",
      "Epoch [53/500], Train Loss: 0.1102\n",
      "Epoch [54/500], Train Loss: 0.1088\n",
      "Epoch [55/500], Train Loss: 0.1084\n",
      "Epoch [56/500], Train Loss: 0.1067\n",
      "Epoch [57/500], Train Loss: 0.1081\n",
      "Epoch [58/500], Train Loss: 0.1069\n",
      "Epoch [59/500], Train Loss: 0.1061\n",
      "Epoch [60/500], Train Loss: 0.1066\n",
      "Epoch [61/500], Train Loss: 0.1072\n",
      "Epoch [62/500], Train Loss: 0.1058\n",
      "Epoch [63/500], Train Loss: 0.1073\n",
      "Epoch [64/500], Train Loss: 0.1081\n",
      "Epoch [65/500], Train Loss: 0.1087\n",
      "Epoch [66/500], Train Loss: 0.1047\n",
      "Epoch [67/500], Train Loss: 0.1058\n",
      "Epoch [68/500], Train Loss: 0.1061\n",
      "Epoch [69/500], Train Loss: 0.1049\n",
      "Epoch [70/500], Train Loss: 0.1051\n",
      "Epoch [71/500], Train Loss: 0.1030\n",
      "Epoch [72/500], Train Loss: 0.1041\n",
      "Epoch [73/500], Train Loss: 0.1035\n",
      "Epoch [74/500], Train Loss: 0.1031\n",
      "Epoch [75/500], Train Loss: 0.1045\n",
      "Epoch [76/500], Train Loss: 0.1036\n",
      "Epoch [77/500], Train Loss: 0.1034\n",
      "Epoch [78/500], Train Loss: 0.0947\n",
      "Epoch [79/500], Train Loss: 0.0926\n",
      "Epoch [80/500], Train Loss: 0.0923\n",
      "Epoch [81/500], Train Loss: 0.0919\n",
      "Epoch [82/500], Train Loss: 0.0918\n",
      "Epoch [83/500], Train Loss: 0.0916\n",
      "Epoch [84/500], Train Loss: 0.0918\n",
      "Epoch [85/500], Train Loss: 0.0917\n",
      "Epoch [86/500], Train Loss: 0.0918\n",
      "Epoch [87/500], Train Loss: 0.0914\n",
      "Epoch [88/500], Train Loss: 0.0911\n",
      "Epoch [89/500], Train Loss: 0.0911\n",
      "Epoch [90/500], Train Loss: 0.0914\n",
      "Epoch [91/500], Train Loss: 0.0913\n",
      "Epoch [92/500], Train Loss: 0.0911\n",
      "Epoch [93/500], Train Loss: 0.0910\n",
      "Epoch [94/500], Train Loss: 0.0910\n",
      "Epoch [95/500], Train Loss: 0.0912\n",
      "Epoch [96/500], Train Loss: 0.0910\n",
      "Epoch [97/500], Train Loss: 0.0912\n",
      "Epoch [98/500], Train Loss: 0.0911\n",
      "Epoch [99/500], Train Loss: 0.0910\n",
      "Epoch [100/500], Train Loss: 0.0910\n",
      "Epoch [101/500], Train Loss: 0.0909\n",
      "Epoch [102/500], Train Loss: 0.0909\n",
      "Epoch [103/500], Train Loss: 0.0908\n",
      "Epoch [104/500], Train Loss: 0.0907\n",
      "Epoch [105/500], Train Loss: 0.0907\n",
      "Epoch [106/500], Train Loss: 0.0910\n",
      "Epoch [107/500], Train Loss: 0.0908\n",
      "Epoch [108/500], Train Loss: 0.0908\n",
      "Epoch [109/500], Train Loss: 0.0908\n",
      "Epoch [110/500], Train Loss: 0.0907\n",
      "Epoch [111/500], Train Loss: 0.0908\n",
      "Epoch [112/500], Train Loss: 0.0906\n",
      "Epoch [113/500], Train Loss: 0.0908\n",
      "Epoch [114/500], Train Loss: 0.0906\n",
      "Epoch [115/500], Train Loss: 0.0907\n",
      "Epoch [116/500], Train Loss: 0.0906\n",
      "Epoch [117/500], Train Loss: 0.0907\n",
      "Epoch [118/500], Train Loss: 0.0906\n",
      "Epoch [119/500], Train Loss: 0.0898\n",
      "Epoch [120/500], Train Loss: 0.0897\n",
      "Epoch [121/500], Train Loss: 0.0894\n",
      "Epoch [122/500], Train Loss: 0.0895\n",
      "Epoch [123/500], Train Loss: 0.0894\n",
      "Epoch [124/500], Train Loss: 0.0895\n",
      "Epoch [125/500], Train Loss: 0.0895\n",
      "Epoch [126/500], Train Loss: 0.0894\n",
      "Epoch [127/500], Train Loss: 0.0895\n",
      "Epoch [128/500], Train Loss: 0.0893\n",
      "Epoch [129/500], Train Loss: 0.0894\n",
      "Epoch [130/500], Train Loss: 0.0891\n",
      "Epoch [131/500], Train Loss: 0.0894\n",
      "Epoch [132/500], Train Loss: 0.0893\n",
      "Epoch [133/500], Train Loss: 0.0894\n",
      "Epoch [134/500], Train Loss: 0.0895\n",
      "Epoch [135/500], Train Loss: 0.0894\n",
      "Epoch [136/500], Train Loss: 0.0892\n",
      "Epoch [137/500], Train Loss: 0.0892\n",
      "Epoch [138/500], Train Loss: 0.0893\n",
      "Epoch [139/500], Train Loss: 0.0893\n",
      "Early stopping at epoch 139\n",
      "Final model saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/models_unphased/chr5/final_model_chr5.pth\n",
      "Individual R^2 scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr5/individual_r2_scores_chr5.csv\n",
      "Individual IQS scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr5/individual_iqs_scores_chr5.csv\n",
      "Individual AUC ROC curves saved in: ../../Data/model_results_unphased_all_PRS/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/performance_metrics.csv\n",
      "Total SNPs:  632\n",
      "PRS313 SNPs:  20\n",
      "Total SNPs used for Training:  612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-04-30 10:14:37,414] Trial 52 finished with value: 0.1935613960027695 and parameters: {'learning_rate': 0.00022440703253433187, 'l1_coef': 3.637667356288873e-05, 'patience': 11, 'batch_size': 64}. Best is trial 47 with value: 0.14764729972396579.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 6 - Best hyperparameters: {'learning_rate': 0.0005159914294373423, 'l1_coef': 1.0687801532020404e-05, 'patience': 8, 'batch_size': 64}\n",
      "Chr 6 - Best value: 0.1476\n",
      "Epoch [1/500], Train Loss: 0.5457\n",
      "Epoch [2/500], Train Loss: 0.4890\n",
      "Epoch [3/500], Train Loss: 0.4712\n",
      "Epoch [4/500], Train Loss: 0.4564\n",
      "Epoch [5/500], Train Loss: 0.4434\n",
      "Epoch [6/500], Train Loss: 0.4314\n",
      "Epoch [7/500], Train Loss: 0.4214\n",
      "Epoch [8/500], Train Loss: 0.4116\n",
      "Epoch [9/500], Train Loss: 0.4025\n",
      "Epoch [10/500], Train Loss: 0.3952\n",
      "Epoch [11/500], Train Loss: 0.3869\n",
      "Epoch [12/500], Train Loss: 0.3794\n",
      "Epoch [13/500], Train Loss: 0.3720\n",
      "Epoch [14/500], Train Loss: 0.3654\n",
      "Epoch [15/500], Train Loss: 0.3590\n",
      "Epoch [16/500], Train Loss: 0.3525\n",
      "Epoch [17/500], Train Loss: 0.3469\n",
      "Epoch [18/500], Train Loss: 0.3417\n",
      "Epoch [19/500], Train Loss: 0.3358\n",
      "Epoch [20/500], Train Loss: 0.3316\n",
      "Epoch [21/500], Train Loss: 0.3265\n",
      "Epoch [22/500], Train Loss: 0.3224\n",
      "Epoch [23/500], Train Loss: 0.3187\n",
      "Epoch [24/500], Train Loss: 0.3148\n",
      "Epoch [25/500], Train Loss: 0.3099\n",
      "Epoch [26/500], Train Loss: 0.3059\n",
      "Epoch [27/500], Train Loss: 0.3031\n",
      "Epoch [28/500], Train Loss: 0.2994\n",
      "Epoch [29/500], Train Loss: 0.2958\n",
      "Epoch [30/500], Train Loss: 0.2918\n",
      "Epoch [31/500], Train Loss: 0.2898\n",
      "Epoch [32/500], Train Loss: 0.2864\n",
      "Epoch [33/500], Train Loss: 0.2837\n",
      "Epoch [34/500], Train Loss: 0.2813\n",
      "Epoch [35/500], Train Loss: 0.2785\n",
      "Epoch [36/500], Train Loss: 0.2756\n",
      "Epoch [37/500], Train Loss: 0.2733\n",
      "Epoch [38/500], Train Loss: 0.2704\n",
      "Epoch [39/500], Train Loss: 0.2683\n",
      "Epoch [40/500], Train Loss: 0.2658\n",
      "Epoch [41/500], Train Loss: 0.2637\n",
      "Epoch [42/500], Train Loss: 0.2614\n",
      "Epoch [43/500], Train Loss: 0.2600\n",
      "Epoch [44/500], Train Loss: 0.2575\n",
      "Epoch [45/500], Train Loss: 0.2554\n",
      "Epoch [46/500], Train Loss: 0.2537\n",
      "Epoch [47/500], Train Loss: 0.2514\n",
      "Epoch [48/500], Train Loss: 0.2503\n",
      "Epoch [49/500], Train Loss: 0.2474\n",
      "Epoch [50/500], Train Loss: 0.2465\n",
      "Epoch [51/500], Train Loss: 0.2445\n",
      "Epoch [52/500], Train Loss: 0.2430\n",
      "Epoch [53/500], Train Loss: 0.2407\n",
      "Epoch [54/500], Train Loss: 0.2392\n",
      "Epoch [55/500], Train Loss: 0.2377\n",
      "Epoch [56/500], Train Loss: 0.2363\n",
      "Epoch [57/500], Train Loss: 0.2351\n",
      "Epoch [58/500], Train Loss: 0.2331\n",
      "Epoch [59/500], Train Loss: 0.2322\n",
      "Epoch [60/500], Train Loss: 0.2311\n",
      "Epoch [61/500], Train Loss: 0.2299\n",
      "Epoch [62/500], Train Loss: 0.2280\n",
      "Epoch [63/500], Train Loss: 0.2267\n",
      "Epoch [64/500], Train Loss: 0.2256\n",
      "Epoch [65/500], Train Loss: 0.2243\n",
      "Epoch [66/500], Train Loss: 0.2230\n",
      "Epoch [67/500], Train Loss: 0.2222\n",
      "Epoch [68/500], Train Loss: 0.2214\n",
      "Epoch [69/500], Train Loss: 0.2198\n",
      "Epoch [70/500], Train Loss: 0.2191\n",
      "Epoch [71/500], Train Loss: 0.2178\n",
      "Epoch [72/500], Train Loss: 0.2160\n",
      "Epoch [73/500], Train Loss: 0.2157\n",
      "Epoch [74/500], Train Loss: 0.2150\n",
      "Epoch [75/500], Train Loss: 0.2137\n",
      "Epoch [76/500], Train Loss: 0.2126\n",
      "Epoch [77/500], Train Loss: 0.2121\n",
      "Epoch [78/500], Train Loss: 0.2100\n",
      "Epoch [79/500], Train Loss: 0.2094\n",
      "Epoch [80/500], Train Loss: 0.2093\n",
      "Epoch [81/500], Train Loss: 0.2079\n",
      "Epoch [82/500], Train Loss: 0.2067\n",
      "Epoch [83/500], Train Loss: 0.2059\n",
      "Epoch [84/500], Train Loss: 0.2053\n",
      "Epoch [85/500], Train Loss: 0.2043\n",
      "Epoch [86/500], Train Loss: 0.2034\n",
      "Epoch [87/500], Train Loss: 0.2034\n",
      "Epoch [88/500], Train Loss: 0.2012\n",
      "Epoch [89/500], Train Loss: 0.2009\n",
      "Epoch [90/500], Train Loss: 0.2003\n",
      "Epoch [91/500], Train Loss: 0.1996\n",
      "Epoch [92/500], Train Loss: 0.1987\n",
      "Epoch [93/500], Train Loss: 0.1975\n",
      "Epoch [94/500], Train Loss: 0.1969\n",
      "Epoch [95/500], Train Loss: 0.1961\n",
      "Epoch [96/500], Train Loss: 0.1955\n",
      "Epoch [97/500], Train Loss: 0.1952\n",
      "Epoch [98/500], Train Loss: 0.1943\n",
      "Epoch [99/500], Train Loss: 0.1939\n",
      "Epoch [100/500], Train Loss: 0.1922\n",
      "Epoch [101/500], Train Loss: 0.1926\n",
      "Epoch [102/500], Train Loss: 0.1911\n",
      "Epoch [103/500], Train Loss: 0.1906\n",
      "Epoch [104/500], Train Loss: 0.1902\n",
      "Epoch [105/500], Train Loss: 0.1896\n",
      "Epoch [106/500], Train Loss: 0.1886\n",
      "Epoch [107/500], Train Loss: 0.1886\n",
      "Epoch [108/500], Train Loss: 0.1877\n",
      "Epoch [109/500], Train Loss: 0.1871\n",
      "Epoch [110/500], Train Loss: 0.1865\n",
      "Epoch [111/500], Train Loss: 0.1859\n",
      "Epoch [112/500], Train Loss: 0.1856\n",
      "Epoch [113/500], Train Loss: 0.1851\n",
      "Epoch [114/500], Train Loss: 0.1842\n",
      "Epoch [115/500], Train Loss: 0.1845\n",
      "Epoch [116/500], Train Loss: 0.1829\n",
      "Epoch [117/500], Train Loss: 0.1833\n",
      "Epoch [118/500], Train Loss: 0.1828\n",
      "Epoch [119/500], Train Loss: 0.1818\n",
      "Epoch [120/500], Train Loss: 0.1807\n",
      "Epoch [121/500], Train Loss: 0.1794\n",
      "Epoch [122/500], Train Loss: 0.1805\n",
      "Epoch [123/500], Train Loss: 0.1795\n",
      "Epoch [124/500], Train Loss: 0.1788\n",
      "Epoch [125/500], Train Loss: 0.1786\n",
      "Epoch [126/500], Train Loss: 0.1782\n",
      "Epoch [127/500], Train Loss: 0.1770\n",
      "Epoch [128/500], Train Loss: 0.1766\n",
      "Epoch [129/500], Train Loss: 0.1773\n",
      "Epoch [130/500], Train Loss: 0.1766\n",
      "Epoch [131/500], Train Loss: 0.1755\n",
      "Epoch [132/500], Train Loss: 0.1748\n",
      "Epoch [133/500], Train Loss: 0.1743\n",
      "Epoch [134/500], Train Loss: 0.1743\n",
      "Epoch [135/500], Train Loss: 0.1734\n",
      "Epoch [136/500], Train Loss: 0.1737\n",
      "Epoch [137/500], Train Loss: 0.1729\n",
      "Epoch [138/500], Train Loss: 0.1726\n",
      "Epoch [139/500], Train Loss: 0.1718\n",
      "Epoch [140/500], Train Loss: 0.1712\n",
      "Epoch [141/500], Train Loss: 0.1716\n",
      "Epoch [142/500], Train Loss: 0.1711\n",
      "Epoch [143/500], Train Loss: 0.1706\n",
      "Epoch [144/500], Train Loss: 0.1709\n",
      "Epoch [145/500], Train Loss: 0.1703\n",
      "Epoch [146/500], Train Loss: 0.1695\n",
      "Epoch [147/500], Train Loss: 0.1691\n",
      "Epoch [148/500], Train Loss: 0.1687\n",
      "Epoch [149/500], Train Loss: 0.1683\n",
      "Epoch [150/500], Train Loss: 0.1676\n",
      "Epoch [151/500], Train Loss: 0.1672\n",
      "Epoch [152/500], Train Loss: 0.1668\n",
      "Epoch [153/500], Train Loss: 0.1666\n",
      "Epoch [154/500], Train Loss: 0.1663\n",
      "Epoch [155/500], Train Loss: 0.1662\n",
      "Epoch [156/500], Train Loss: 0.1656\n",
      "Epoch [157/500], Train Loss: 0.1652\n",
      "Epoch [158/500], Train Loss: 0.1655\n",
      "Epoch [159/500], Train Loss: 0.1648\n",
      "Epoch [160/500], Train Loss: 0.1637\n",
      "Epoch [161/500], Train Loss: 0.1643\n",
      "Epoch [162/500], Train Loss: 0.1635\n",
      "Epoch [163/500], Train Loss: 0.1635\n",
      "Epoch [164/500], Train Loss: 0.1632\n",
      "Epoch [165/500], Train Loss: 0.1634\n",
      "Epoch [166/500], Train Loss: 0.1629\n",
      "Epoch [167/500], Train Loss: 0.1621\n",
      "Epoch [168/500], Train Loss: 0.1616\n",
      "Epoch [169/500], Train Loss: 0.1621\n",
      "Epoch [170/500], Train Loss: 0.1609\n",
      "Epoch [171/500], Train Loss: 0.1609\n",
      "Epoch [172/500], Train Loss: 0.1602\n",
      "Epoch [173/500], Train Loss: 0.1600\n",
      "Epoch [174/500], Train Loss: 0.1598\n",
      "Epoch [175/500], Train Loss: 0.1599\n",
      "Epoch [176/500], Train Loss: 0.1593\n",
      "Epoch [177/500], Train Loss: 0.1595\n",
      "Epoch [178/500], Train Loss: 0.1582\n",
      "Epoch [179/500], Train Loss: 0.1582\n",
      "Epoch [180/500], Train Loss: 0.1580\n",
      "Epoch [181/500], Train Loss: 0.1578\n",
      "Epoch [182/500], Train Loss: 0.1579\n",
      "Epoch [183/500], Train Loss: 0.1576\n",
      "Epoch [184/500], Train Loss: 0.1575\n",
      "Epoch [185/500], Train Loss: 0.1564\n",
      "Epoch [186/500], Train Loss: 0.1574\n",
      "Epoch [187/500], Train Loss: 0.1569\n",
      "Epoch [188/500], Train Loss: 0.1562\n",
      "Epoch [189/500], Train Loss: 0.1566\n",
      "Epoch [190/500], Train Loss: 0.1554\n",
      "Epoch [191/500], Train Loss: 0.1553\n",
      "Epoch [192/500], Train Loss: 0.1550\n",
      "Epoch [193/500], Train Loss: 0.1553\n",
      "Epoch [194/500], Train Loss: 0.1550\n",
      "Epoch [195/500], Train Loss: 0.1544\n",
      "Epoch [196/500], Train Loss: 0.1544\n",
      "Epoch [197/500], Train Loss: 0.1544\n",
      "Epoch [198/500], Train Loss: 0.1536\n",
      "Epoch [199/500], Train Loss: 0.1535\n",
      "Epoch [200/500], Train Loss: 0.1535\n",
      "Epoch [201/500], Train Loss: 0.1530\n",
      "Epoch [202/500], Train Loss: 0.1529\n",
      "Epoch [203/500], Train Loss: 0.1521\n",
      "Epoch [204/500], Train Loss: 0.1525\n",
      "Epoch [205/500], Train Loss: 0.1522\n",
      "Epoch [206/500], Train Loss: 0.1518\n",
      "Epoch [207/500], Train Loss: 0.1517\n",
      "Epoch [208/500], Train Loss: 0.1522\n",
      "Epoch [209/500], Train Loss: 0.1513\n",
      "Epoch [210/500], Train Loss: 0.1512\n",
      "Epoch [211/500], Train Loss: 0.1510\n",
      "Epoch [212/500], Train Loss: 0.1507\n",
      "Epoch [213/500], Train Loss: 0.1501\n",
      "Epoch [214/500], Train Loss: 0.1506\n",
      "Epoch [215/500], Train Loss: 0.1502\n",
      "Epoch [216/500], Train Loss: 0.1503\n",
      "Epoch [217/500], Train Loss: 0.1491\n",
      "Epoch [218/500], Train Loss: 0.1490\n",
      "Epoch [219/500], Train Loss: 0.1493\n",
      "Epoch [220/500], Train Loss: 0.1486\n",
      "Epoch [221/500], Train Loss: 0.1492\n",
      "Epoch [222/500], Train Loss: 0.1483\n",
      "Epoch [223/500], Train Loss: 0.1491\n",
      "Epoch [224/500], Train Loss: 0.1478\n",
      "Epoch [225/500], Train Loss: 0.1490\n",
      "Epoch [226/500], Train Loss: 0.1481\n",
      "Epoch [227/500], Train Loss: 0.1476\n",
      "Epoch [228/500], Train Loss: 0.1473\n",
      "Epoch [229/500], Train Loss: 0.1477\n",
      "Epoch [230/500], Train Loss: 0.1474\n",
      "Epoch [231/500], Train Loss: 0.1477\n",
      "Epoch [232/500], Train Loss: 0.1473\n",
      "Epoch [233/500], Train Loss: 0.1463\n",
      "Epoch [234/500], Train Loss: 0.1468\n",
      "Epoch [235/500], Train Loss: 0.1458\n",
      "Epoch [236/500], Train Loss: 0.1466\n",
      "Epoch [237/500], Train Loss: 0.1460\n",
      "Epoch [238/500], Train Loss: 0.1458\n",
      "Epoch [239/500], Train Loss: 0.1450\n",
      "Epoch [240/500], Train Loss: 0.1456\n",
      "Epoch [241/500], Train Loss: 0.1458\n",
      "Epoch [242/500], Train Loss: 0.1450\n",
      "Epoch [243/500], Train Loss: 0.1460\n",
      "Epoch [244/500], Train Loss: 0.1446\n",
      "Epoch [245/500], Train Loss: 0.1445\n",
      "Epoch [246/500], Train Loss: 0.1442\n",
      "Epoch [247/500], Train Loss: 0.1444\n",
      "Epoch [248/500], Train Loss: 0.1445\n",
      "Epoch [249/500], Train Loss: 0.1440\n",
      "Epoch [250/500], Train Loss: 0.1437\n",
      "Epoch [251/500], Train Loss: 0.1437\n",
      "Epoch [252/500], Train Loss: 0.1434\n",
      "Epoch [253/500], Train Loss: 0.1437\n",
      "Epoch [254/500], Train Loss: 0.1431\n",
      "Epoch [255/500], Train Loss: 0.1431\n",
      "Epoch [256/500], Train Loss: 0.1431\n",
      "Epoch [257/500], Train Loss: 0.1432\n",
      "Epoch [258/500], Train Loss: 0.1429\n",
      "Epoch [259/500], Train Loss: 0.1420\n",
      "Epoch [260/500], Train Loss: 0.1420\n",
      "Epoch [261/500], Train Loss: 0.1422\n",
      "Epoch [262/500], Train Loss: 0.1422\n",
      "Epoch [263/500], Train Loss: 0.1420\n",
      "Epoch [264/500], Train Loss: 0.1414\n",
      "Epoch [265/500], Train Loss: 0.1416\n",
      "Epoch [266/500], Train Loss: 0.1414\n",
      "Epoch [267/500], Train Loss: 0.1413\n",
      "Epoch [268/500], Train Loss: 0.1413\n",
      "Epoch [269/500], Train Loss: 0.1413\n",
      "Epoch [270/500], Train Loss: 0.1405\n",
      "Epoch [271/500], Train Loss: 0.1414\n",
      "Epoch [272/500], Train Loss: 0.1405\n",
      "Epoch [273/500], Train Loss: 0.1409\n",
      "Epoch [274/500], Train Loss: 0.1400\n",
      "Epoch [275/500], Train Loss: 0.1404\n",
      "Epoch [276/500], Train Loss: 0.1402\n",
      "Epoch [277/500], Train Loss: 0.1401\n",
      "Epoch [278/500], Train Loss: 0.1400\n",
      "Epoch [279/500], Train Loss: 0.1394\n",
      "Epoch [280/500], Train Loss: 0.1395\n",
      "Epoch [281/500], Train Loss: 0.1393\n",
      "Epoch [282/500], Train Loss: 0.1399\n",
      "Epoch [283/500], Train Loss: 0.1400\n",
      "Epoch [284/500], Train Loss: 0.1395\n",
      "Epoch [285/500], Train Loss: 0.1388\n",
      "Epoch [286/500], Train Loss: 0.1387\n",
      "Epoch [287/500], Train Loss: 0.1396\n",
      "Epoch [288/500], Train Loss: 0.1393\n",
      "Epoch [289/500], Train Loss: 0.1392\n",
      "Epoch [290/500], Train Loss: 0.1391\n",
      "Epoch [291/500], Train Loss: 0.1382\n",
      "Epoch [292/500], Train Loss: 0.1385\n",
      "Epoch [293/500], Train Loss: 0.1379\n",
      "Epoch [294/500], Train Loss: 0.1378\n",
      "Epoch [295/500], Train Loss: 0.1377\n",
      "Epoch [296/500], Train Loss: 0.1377\n",
      "Epoch [297/500], Train Loss: 0.1377\n",
      "Epoch [298/500], Train Loss: 0.1376\n",
      "Epoch [299/500], Train Loss: 0.1376\n",
      "Epoch [300/500], Train Loss: 0.1374\n",
      "Epoch [301/500], Train Loss: 0.1377\n",
      "Epoch [302/500], Train Loss: 0.1374\n",
      "Epoch [303/500], Train Loss: 0.1369\n",
      "Epoch [304/500], Train Loss: 0.1373\n",
      "Epoch [305/500], Train Loss: 0.1368\n",
      "Epoch [306/500], Train Loss: 0.1370\n",
      "Epoch [307/500], Train Loss: 0.1366\n",
      "Epoch [308/500], Train Loss: 0.1369\n",
      "Epoch [309/500], Train Loss: 0.1362\n",
      "Epoch [310/500], Train Loss: 0.1369\n",
      "Epoch [311/500], Train Loss: 0.1366\n",
      "Epoch [312/500], Train Loss: 0.1362\n",
      "Epoch [313/500], Train Loss: 0.1360\n",
      "Epoch [314/500], Train Loss: 0.1363\n",
      "Epoch [315/500], Train Loss: 0.1361\n",
      "Epoch [316/500], Train Loss: 0.1355\n",
      "Epoch [317/500], Train Loss: 0.1358\n",
      "Epoch [318/500], Train Loss: 0.1354\n",
      "Epoch [319/500], Train Loss: 0.1359\n",
      "Epoch [320/500], Train Loss: 0.1356\n",
      "Epoch [321/500], Train Loss: 0.1351\n",
      "Epoch [322/500], Train Loss: 0.1355\n",
      "Epoch [323/500], Train Loss: 0.1353\n",
      "Epoch [324/500], Train Loss: 0.1350\n",
      "Epoch [325/500], Train Loss: 0.1350\n",
      "Epoch [326/500], Train Loss: 0.1351\n",
      "Epoch [327/500], Train Loss: 0.1349\n",
      "Epoch [328/500], Train Loss: 0.1351\n",
      "Epoch [329/500], Train Loss: 0.1347\n",
      "Epoch [330/500], Train Loss: 0.1343\n",
      "Epoch [331/500], Train Loss: 0.1347\n",
      "Epoch [332/500], Train Loss: 0.1339\n",
      "Epoch [333/500], Train Loss: 0.1344\n",
      "Epoch [334/500], Train Loss: 0.1344\n",
      "Epoch [335/500], Train Loss: 0.1345\n",
      "Epoch [336/500], Train Loss: 0.1337\n",
      "Epoch [337/500], Train Loss: 0.1337\n",
      "Epoch [338/500], Train Loss: 0.1339\n",
      "Epoch [339/500], Train Loss: 0.1340\n",
      "Epoch [340/500], Train Loss: 0.1334\n",
      "Epoch [341/500], Train Loss: 0.1334\n",
      "Epoch [342/500], Train Loss: 0.1335\n",
      "Epoch [343/500], Train Loss: 0.1332\n",
      "Epoch [344/500], Train Loss: 0.1333\n",
      "Epoch [345/500], Train Loss: 0.1332\n",
      "Epoch [346/500], Train Loss: 0.1333\n",
      "Epoch [347/500], Train Loss: 0.1328\n",
      "Epoch [348/500], Train Loss: 0.1326\n",
      "Epoch [349/500], Train Loss: 0.1329\n",
      "Epoch [350/500], Train Loss: 0.1331\n",
      "Epoch [351/500], Train Loss: 0.1326\n",
      "Epoch [352/500], Train Loss: 0.1334\n",
      "Epoch [353/500], Train Loss: 0.1331\n",
      "Epoch [354/500], Train Loss: 0.1324\n",
      "Epoch [355/500], Train Loss: 0.1324\n",
      "Epoch [356/500], Train Loss: 0.1322\n",
      "Epoch [357/500], Train Loss: 0.1326\n",
      "Epoch [358/500], Train Loss: 0.1319\n",
      "Epoch [359/500], Train Loss: 0.1323\n",
      "Epoch [360/500], Train Loss: 0.1326\n",
      "Epoch [361/500], Train Loss: 0.1321\n",
      "Epoch [362/500], Train Loss: 0.1318\n",
      "Epoch [363/500], Train Loss: 0.1317\n",
      "Epoch [364/500], Train Loss: 0.1318\n",
      "Epoch [365/500], Train Loss: 0.1318\n",
      "Epoch [366/500], Train Loss: 0.1315\n",
      "Epoch [367/500], Train Loss: 0.1316\n",
      "Epoch [368/500], Train Loss: 0.1311\n",
      "Epoch [369/500], Train Loss: 0.1312\n",
      "Epoch [370/500], Train Loss: 0.1312\n",
      "Epoch [371/500], Train Loss: 0.1311\n",
      "Epoch [372/500], Train Loss: 0.1309\n",
      "Epoch [373/500], Train Loss: 0.1311\n",
      "Epoch [374/500], Train Loss: 0.1313\n",
      "Epoch [375/500], Train Loss: 0.1311\n",
      "Epoch [376/500], Train Loss: 0.1314\n",
      "Epoch [377/500], Train Loss: 0.1301\n",
      "Epoch [378/500], Train Loss: 0.1308\n",
      "Epoch [379/500], Train Loss: 0.1302\n",
      "Epoch [380/500], Train Loss: 0.1304\n",
      "Epoch [381/500], Train Loss: 0.1306\n",
      "Epoch [382/500], Train Loss: 0.1305\n",
      "Epoch [383/500], Train Loss: 0.1303\n",
      "Epoch [384/500], Train Loss: 0.1300\n",
      "Epoch [385/500], Train Loss: 0.1296\n",
      "Epoch [386/500], Train Loss: 0.1294\n",
      "Epoch [387/500], Train Loss: 0.1296\n",
      "Epoch [388/500], Train Loss: 0.1294\n",
      "Epoch [389/500], Train Loss: 0.1298\n",
      "Epoch [390/500], Train Loss: 0.1294\n",
      "Epoch [391/500], Train Loss: 0.1296\n",
      "Epoch [392/500], Train Loss: 0.1295\n",
      "Epoch [393/500], Train Loss: 0.1294\n",
      "Epoch [394/500], Train Loss: 0.1297\n",
      "Epoch [395/500], Train Loss: 0.1293\n",
      "Epoch [396/500], Train Loss: 0.1294\n",
      "Epoch [397/500], Train Loss: 0.1295\n",
      "Epoch [398/500], Train Loss: 0.1302\n",
      "Epoch [399/500], Train Loss: 0.1294\n",
      "Epoch [400/500], Train Loss: 0.1299\n",
      "Epoch [401/500], Train Loss: 0.1299\n",
      "Epoch [402/500], Train Loss: 0.1295\n",
      "Epoch [403/500], Train Loss: 0.1293\n",
      "Early stopping at epoch 403\n",
      "Final model saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/models_unphased/chr6/final_model_chr6.pth\n",
      "Individual R^2 scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr6/individual_r2_scores_chr6.csv\n",
      "Individual IQS scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr6/individual_iqs_scores_chr6.csv\n",
      "Individual AUC ROC curves saved in: ../../Data/model_results_unphased_all_PRS/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/performance_metrics.csv\n",
      "Total SNPs:  487\n",
      "PRS313 SNPs:  14\n",
      "Total SNPs used for Training:  473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-04-30 10:14:51,258] Trial 51 finished with value: 0.23524432549109825 and parameters: {'learning_rate': 0.00030856565095044357, 'l1_coef': 3.202290357425731e-05, 'patience': 13, 'batch_size': 32}. Best is trial 22 with value: 0.10665186093403742.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 7 - Best hyperparameters: {'learning_rate': 0.00022929815822353796, 'l1_coef': 1.1733499979155006e-05, 'patience': 12, 'batch_size': 32}\n",
      "Chr 7 - Best value: 0.1067\n",
      "Epoch [1/500], Train Loss: 0.6075\n",
      "Epoch [2/500], Train Loss: 0.5603\n",
      "Epoch [3/500], Train Loss: 0.5434\n",
      "Epoch [4/500], Train Loss: 0.5298\n",
      "Epoch [5/500], Train Loss: 0.5181\n",
      "Epoch [6/500], Train Loss: 0.5089\n",
      "Epoch [7/500], Train Loss: 0.5002\n",
      "Epoch [8/500], Train Loss: 0.4922\n",
      "Epoch [9/500], Train Loss: 0.4848\n",
      "Epoch [10/500], Train Loss: 0.4786\n",
      "Epoch [11/500], Train Loss: 0.4722\n",
      "Epoch [12/500], Train Loss: 0.4665\n",
      "Epoch [13/500], Train Loss: 0.4609\n",
      "Epoch [14/500], Train Loss: 0.4560\n",
      "Epoch [15/500], Train Loss: 0.4508\n",
      "Epoch [16/500], Train Loss: 0.4457\n",
      "Epoch [17/500], Train Loss: 0.4414\n",
      "Epoch [18/500], Train Loss: 0.4371\n",
      "Epoch [19/500], Train Loss: 0.4328\n",
      "Epoch [20/500], Train Loss: 0.4288\n",
      "Epoch [21/500], Train Loss: 0.4244\n",
      "Epoch [22/500], Train Loss: 0.4206\n",
      "Epoch [23/500], Train Loss: 0.4171\n",
      "Epoch [24/500], Train Loss: 0.4133\n",
      "Epoch [25/500], Train Loss: 0.4100\n",
      "Epoch [26/500], Train Loss: 0.4062\n",
      "Epoch [27/500], Train Loss: 0.4032\n",
      "Epoch [28/500], Train Loss: 0.3997\n",
      "Epoch [29/500], Train Loss: 0.3963\n",
      "Epoch [30/500], Train Loss: 0.3934\n",
      "Epoch [31/500], Train Loss: 0.3905\n",
      "Epoch [32/500], Train Loss: 0.3875\n",
      "Epoch [33/500], Train Loss: 0.3847\n",
      "Epoch [34/500], Train Loss: 0.3818\n",
      "Epoch [35/500], Train Loss: 0.3792\n",
      "Epoch [36/500], Train Loss: 0.3765\n",
      "Epoch [37/500], Train Loss: 0.3741\n",
      "Epoch [38/500], Train Loss: 0.3712\n",
      "Epoch [39/500], Train Loss: 0.3689\n",
      "Epoch [40/500], Train Loss: 0.3664\n",
      "Epoch [41/500], Train Loss: 0.3641\n",
      "Epoch [42/500], Train Loss: 0.3619\n",
      "Epoch [43/500], Train Loss: 0.3595\n",
      "Epoch [44/500], Train Loss: 0.3572\n",
      "Epoch [45/500], Train Loss: 0.3549\n",
      "Epoch [46/500], Train Loss: 0.3529\n",
      "Epoch [47/500], Train Loss: 0.3509\n",
      "Epoch [48/500], Train Loss: 0.3486\n",
      "Epoch [49/500], Train Loss: 0.3468\n",
      "Epoch [50/500], Train Loss: 0.3448\n",
      "Epoch [51/500], Train Loss: 0.3429\n",
      "Epoch [52/500], Train Loss: 0.3412\n",
      "Epoch [53/500], Train Loss: 0.3391\n",
      "Epoch [54/500], Train Loss: 0.3372\n",
      "Epoch [55/500], Train Loss: 0.3355\n",
      "Epoch [56/500], Train Loss: 0.3339\n",
      "Epoch [57/500], Train Loss: 0.3320\n",
      "Epoch [58/500], Train Loss: 0.3307\n",
      "Epoch [59/500], Train Loss: 0.3290\n",
      "Epoch [60/500], Train Loss: 0.3273\n",
      "Epoch [61/500], Train Loss: 0.3253\n",
      "Epoch [62/500], Train Loss: 0.3238\n",
      "Epoch [63/500], Train Loss: 0.3226\n",
      "Epoch [64/500], Train Loss: 0.3207\n",
      "Epoch [65/500], Train Loss: 0.3195\n",
      "Epoch [66/500], Train Loss: 0.3180\n",
      "Epoch [67/500], Train Loss: 0.3162\n",
      "Epoch [68/500], Train Loss: 0.3149\n",
      "Epoch [69/500], Train Loss: 0.3139\n",
      "Epoch [70/500], Train Loss: 0.3123\n",
      "Epoch [71/500], Train Loss: 0.3109\n",
      "Epoch [72/500], Train Loss: 0.3095\n",
      "Epoch [73/500], Train Loss: 0.3084\n",
      "Epoch [74/500], Train Loss: 0.3069\n",
      "Epoch [75/500], Train Loss: 0.3058\n",
      "Epoch [76/500], Train Loss: 0.3045\n",
      "Epoch [77/500], Train Loss: 0.3031\n",
      "Epoch [78/500], Train Loss: 0.3020\n",
      "Epoch [79/500], Train Loss: 0.3005\n",
      "Epoch [80/500], Train Loss: 0.2995\n",
      "Epoch [81/500], Train Loss: 0.2986\n",
      "Epoch [82/500], Train Loss: 0.2973\n",
      "Epoch [83/500], Train Loss: 0.2965\n",
      "Epoch [84/500], Train Loss: 0.2951\n",
      "Epoch [85/500], Train Loss: 0.2939\n",
      "Epoch [86/500], Train Loss: 0.2926\n",
      "Epoch [87/500], Train Loss: 0.2919\n",
      "Epoch [88/500], Train Loss: 0.2906\n",
      "Epoch [89/500], Train Loss: 0.2897\n",
      "Epoch [90/500], Train Loss: 0.2889\n",
      "Epoch [91/500], Train Loss: 0.2879\n",
      "Epoch [92/500], Train Loss: 0.2869\n",
      "Epoch [93/500], Train Loss: 0.2856\n",
      "Epoch [94/500], Train Loss: 0.2849\n",
      "Epoch [95/500], Train Loss: 0.2838\n",
      "Epoch [96/500], Train Loss: 0.2829\n",
      "Epoch [97/500], Train Loss: 0.2818\n",
      "Epoch [98/500], Train Loss: 0.2808\n",
      "Epoch [99/500], Train Loss: 0.2803\n",
      "Epoch [100/500], Train Loss: 0.2790\n",
      "Epoch [101/500], Train Loss: 0.2783\n",
      "Epoch [102/500], Train Loss: 0.2774\n",
      "Epoch [103/500], Train Loss: 0.2765\n",
      "Epoch [104/500], Train Loss: 0.2755\n",
      "Epoch [105/500], Train Loss: 0.2749\n",
      "Epoch [106/500], Train Loss: 0.2742\n",
      "Epoch [107/500], Train Loss: 0.2731\n",
      "Epoch [108/500], Train Loss: 0.2722\n",
      "Epoch [109/500], Train Loss: 0.2714\n",
      "Epoch [110/500], Train Loss: 0.2708\n",
      "Epoch [111/500], Train Loss: 0.2699\n",
      "Epoch [112/500], Train Loss: 0.2694\n",
      "Epoch [113/500], Train Loss: 0.2681\n",
      "Epoch [114/500], Train Loss: 0.2677\n",
      "Epoch [115/500], Train Loss: 0.2669\n",
      "Epoch [116/500], Train Loss: 0.2662\n",
      "Epoch [117/500], Train Loss: 0.2655\n",
      "Epoch [118/500], Train Loss: 0.2649\n",
      "Epoch [119/500], Train Loss: 0.2642\n",
      "Epoch [120/500], Train Loss: 0.2632\n",
      "Epoch [121/500], Train Loss: 0.2626\n",
      "Epoch [122/500], Train Loss: 0.2618\n",
      "Epoch [123/500], Train Loss: 0.2614\n",
      "Epoch [124/500], Train Loss: 0.2607\n",
      "Epoch [125/500], Train Loss: 0.2598\n",
      "Epoch [126/500], Train Loss: 0.2595\n",
      "Epoch [127/500], Train Loss: 0.2585\n",
      "Epoch [128/500], Train Loss: 0.2579\n",
      "Epoch [129/500], Train Loss: 0.2577\n",
      "Epoch [130/500], Train Loss: 0.2569\n",
      "Epoch [131/500], Train Loss: 0.2561\n",
      "Epoch [132/500], Train Loss: 0.2555\n",
      "Epoch [133/500], Train Loss: 0.2551\n",
      "Epoch [134/500], Train Loss: 0.2541\n",
      "Epoch [135/500], Train Loss: 0.2537\n",
      "Epoch [136/500], Train Loss: 0.2531\n",
      "Epoch [137/500], Train Loss: 0.2526\n",
      "Epoch [138/500], Train Loss: 0.2520\n",
      "Epoch [139/500], Train Loss: 0.2515\n",
      "Epoch [140/500], Train Loss: 0.2509\n",
      "Epoch [141/500], Train Loss: 0.2503\n",
      "Epoch [142/500], Train Loss: 0.2500\n",
      "Epoch [143/500], Train Loss: 0.2492\n",
      "Epoch [144/500], Train Loss: 0.2487\n",
      "Epoch [145/500], Train Loss: 0.2480\n",
      "Epoch [146/500], Train Loss: 0.2478\n",
      "Epoch [147/500], Train Loss: 0.2474\n",
      "Epoch [148/500], Train Loss: 0.2467\n",
      "Epoch [149/500], Train Loss: 0.2459\n",
      "Epoch [150/500], Train Loss: 0.2454\n",
      "Epoch [151/500], Train Loss: 0.2450\n",
      "Epoch [152/500], Train Loss: 0.2445\n",
      "Epoch [153/500], Train Loss: 0.2442\n",
      "Epoch [154/500], Train Loss: 0.2436\n",
      "Epoch [155/500], Train Loss: 0.2430\n",
      "Epoch [156/500], Train Loss: 0.2425\n",
      "Epoch [157/500], Train Loss: 0.2421\n",
      "Epoch [158/500], Train Loss: 0.2418\n",
      "Epoch [159/500], Train Loss: 0.2414\n",
      "Epoch [160/500], Train Loss: 0.2409\n",
      "Epoch [161/500], Train Loss: 0.2403\n",
      "Epoch [162/500], Train Loss: 0.2398\n",
      "Epoch [163/500], Train Loss: 0.2395\n",
      "Epoch [164/500], Train Loss: 0.2390\n",
      "Epoch [165/500], Train Loss: 0.2384\n",
      "Epoch [166/500], Train Loss: 0.2381\n",
      "Epoch [167/500], Train Loss: 0.2378\n",
      "Epoch [168/500], Train Loss: 0.2371\n",
      "Epoch [169/500], Train Loss: 0.2369\n",
      "Epoch [170/500], Train Loss: 0.2363\n",
      "Epoch [171/500], Train Loss: 0.2362\n",
      "Epoch [172/500], Train Loss: 0.2358\n",
      "Epoch [173/500], Train Loss: 0.2355\n",
      "Epoch [174/500], Train Loss: 0.2346\n",
      "Epoch [175/500], Train Loss: 0.2344\n",
      "Epoch [176/500], Train Loss: 0.2341\n",
      "Epoch [177/500], Train Loss: 0.2337\n",
      "Epoch [178/500], Train Loss: 0.2332\n",
      "Epoch [179/500], Train Loss: 0.2326\n",
      "Epoch [180/500], Train Loss: 0.2325\n",
      "Epoch [181/500], Train Loss: 0.2323\n",
      "Epoch [182/500], Train Loss: 0.2316\n",
      "Epoch [183/500], Train Loss: 0.2314\n",
      "Epoch [184/500], Train Loss: 0.2310\n",
      "Epoch [185/500], Train Loss: 0.2307\n",
      "Epoch [186/500], Train Loss: 0.2303\n",
      "Epoch [187/500], Train Loss: 0.2300\n",
      "Epoch [188/500], Train Loss: 0.2296\n",
      "Epoch [189/500], Train Loss: 0.2292\n",
      "Epoch [190/500], Train Loss: 0.2293\n",
      "Epoch [191/500], Train Loss: 0.2288\n",
      "Epoch [192/500], Train Loss: 0.2284\n",
      "Epoch [193/500], Train Loss: 0.2279\n",
      "Epoch [194/500], Train Loss: 0.2276\n",
      "Epoch [195/500], Train Loss: 0.2274\n",
      "Epoch [196/500], Train Loss: 0.2269\n",
      "Epoch [197/500], Train Loss: 0.2265\n",
      "Epoch [198/500], Train Loss: 0.2263\n",
      "Epoch [199/500], Train Loss: 0.2262\n",
      "Epoch [200/500], Train Loss: 0.2257\n",
      "Epoch [201/500], Train Loss: 0.2254\n",
      "Epoch [202/500], Train Loss: 0.2252\n",
      "Epoch [203/500], Train Loss: 0.2246\n",
      "Epoch [204/500], Train Loss: 0.2244\n",
      "Epoch [205/500], Train Loss: 0.2241\n",
      "Epoch [206/500], Train Loss: 0.2237\n",
      "Epoch [207/500], Train Loss: 0.2232\n",
      "Epoch [208/500], Train Loss: 0.2234\n",
      "Epoch [209/500], Train Loss: 0.2230\n",
      "Epoch [210/500], Train Loss: 0.2225\n",
      "Epoch [211/500], Train Loss: 0.2226\n",
      "Epoch [212/500], Train Loss: 0.2223\n",
      "Epoch [213/500], Train Loss: 0.2217\n",
      "Epoch [214/500], Train Loss: 0.2216\n",
      "Epoch [215/500], Train Loss: 0.2213\n",
      "Epoch [216/500], Train Loss: 0.2209\n",
      "Epoch [217/500], Train Loss: 0.2208\n",
      "Epoch [218/500], Train Loss: 0.2202\n",
      "Epoch [219/500], Train Loss: 0.2201\n",
      "Epoch [220/500], Train Loss: 0.2198\n",
      "Epoch [221/500], Train Loss: 0.2196\n",
      "Epoch [222/500], Train Loss: 0.2192\n",
      "Epoch [223/500], Train Loss: 0.2190\n",
      "Epoch [224/500], Train Loss: 0.2191\n",
      "Epoch [225/500], Train Loss: 0.2186\n",
      "Epoch [226/500], Train Loss: 0.2182\n",
      "Epoch [227/500], Train Loss: 0.2181\n",
      "Epoch [228/500], Train Loss: 0.2178\n",
      "Epoch [229/500], Train Loss: 0.2178\n",
      "Epoch [230/500], Train Loss: 0.2172\n",
      "Epoch [231/500], Train Loss: 0.2170\n",
      "Epoch [232/500], Train Loss: 0.2169\n",
      "Epoch [233/500], Train Loss: 0.2166\n",
      "Epoch [234/500], Train Loss: 0.2163\n",
      "Epoch [235/500], Train Loss: 0.2161\n",
      "Epoch [236/500], Train Loss: 0.2158\n",
      "Epoch [237/500], Train Loss: 0.2157\n",
      "Epoch [238/500], Train Loss: 0.2155\n",
      "Epoch [239/500], Train Loss: 0.2153\n",
      "Epoch [240/500], Train Loss: 0.2154\n",
      "Epoch [241/500], Train Loss: 0.2148\n",
      "Epoch [242/500], Train Loss: 0.2144\n",
      "Epoch [243/500], Train Loss: 0.2144\n",
      "Epoch [244/500], Train Loss: 0.2141\n",
      "Epoch [245/500], Train Loss: 0.2140\n",
      "Epoch [246/500], Train Loss: 0.2137\n",
      "Epoch [247/500], Train Loss: 0.2133\n",
      "Epoch [248/500], Train Loss: 0.2133\n",
      "Epoch [249/500], Train Loss: 0.2129\n",
      "Epoch [250/500], Train Loss: 0.2127\n",
      "Epoch [251/500], Train Loss: 0.2132\n",
      "Epoch [252/500], Train Loss: 0.2124\n",
      "Epoch [253/500], Train Loss: 0.2122\n",
      "Epoch [254/500], Train Loss: 0.2119\n",
      "Epoch [255/500], Train Loss: 0.2119\n",
      "Epoch [256/500], Train Loss: 0.2115\n",
      "Epoch [257/500], Train Loss: 0.2115\n",
      "Epoch [258/500], Train Loss: 0.2115\n",
      "Epoch [259/500], Train Loss: 0.2110\n",
      "Epoch [260/500], Train Loss: 0.2108\n",
      "Epoch [261/500], Train Loss: 0.2105\n",
      "Epoch [262/500], Train Loss: 0.2104\n",
      "Epoch [263/500], Train Loss: 0.2103\n",
      "Epoch [264/500], Train Loss: 0.2102\n",
      "Epoch [265/500], Train Loss: 0.2100\n",
      "Epoch [266/500], Train Loss: 0.2097\n",
      "Epoch [267/500], Train Loss: 0.2096\n",
      "Epoch [268/500], Train Loss: 0.2094\n",
      "Epoch [269/500], Train Loss: 0.2089\n",
      "Epoch [270/500], Train Loss: 0.2092\n",
      "Epoch [271/500], Train Loss: 0.2088\n",
      "Epoch [272/500], Train Loss: 0.2084\n",
      "Epoch [273/500], Train Loss: 0.2083\n",
      "Epoch [274/500], Train Loss: 0.2084\n",
      "Epoch [275/500], Train Loss: 0.2081\n",
      "Epoch [276/500], Train Loss: 0.2080\n",
      "Epoch [277/500], Train Loss: 0.2078\n",
      "Epoch [278/500], Train Loss: 0.2075\n",
      "Epoch [279/500], Train Loss: 0.2071\n",
      "Epoch [280/500], Train Loss: 0.2070\n",
      "Epoch [281/500], Train Loss: 0.2069\n",
      "Epoch [282/500], Train Loss: 0.2068\n",
      "Epoch [283/500], Train Loss: 0.2066\n",
      "Epoch [284/500], Train Loss: 0.2066\n",
      "Epoch [285/500], Train Loss: 0.2063\n",
      "Epoch [286/500], Train Loss: 0.2063\n",
      "Epoch [287/500], Train Loss: 0.2059\n",
      "Epoch [288/500], Train Loss: 0.2060\n",
      "Epoch [289/500], Train Loss: 0.2056\n",
      "Epoch [290/500], Train Loss: 0.2054\n",
      "Epoch [291/500], Train Loss: 0.2054\n",
      "Epoch [292/500], Train Loss: 0.2055\n",
      "Epoch [293/500], Train Loss: 0.2053\n",
      "Epoch [294/500], Train Loss: 0.2049\n",
      "Epoch [295/500], Train Loss: 0.2046\n",
      "Epoch [296/500], Train Loss: 0.2046\n",
      "Epoch [297/500], Train Loss: 0.2046\n",
      "Epoch [298/500], Train Loss: 0.2045\n",
      "Epoch [299/500], Train Loss: 0.2044\n",
      "Epoch [300/500], Train Loss: 0.2040\n",
      "Epoch [301/500], Train Loss: 0.2037\n",
      "Epoch [302/500], Train Loss: 0.2037\n",
      "Epoch [303/500], Train Loss: 0.2036\n",
      "Epoch [304/500], Train Loss: 0.2034\n",
      "Epoch [305/500], Train Loss: 0.2032\n",
      "Epoch [306/500], Train Loss: 0.2032\n",
      "Epoch [307/500], Train Loss: 0.2029\n",
      "Epoch [308/500], Train Loss: 0.2027\n",
      "Epoch [309/500], Train Loss: 0.2025\n",
      "Epoch [310/500], Train Loss: 0.2026\n",
      "Epoch [311/500], Train Loss: 0.2026\n",
      "Epoch [312/500], Train Loss: 0.2022\n",
      "Epoch [313/500], Train Loss: 0.2020\n",
      "Epoch [314/500], Train Loss: 0.2018\n",
      "Epoch [315/500], Train Loss: 0.2017\n",
      "Epoch [316/500], Train Loss: 0.2017\n",
      "Epoch [317/500], Train Loss: 0.2015\n",
      "Epoch [318/500], Train Loss: 0.2014\n",
      "Epoch [319/500], Train Loss: 0.2014\n",
      "Epoch [320/500], Train Loss: 0.2012\n",
      "Epoch [321/500], Train Loss: 0.2010\n",
      "Epoch [322/500], Train Loss: 0.2009\n",
      "Epoch [323/500], Train Loss: 0.2007\n",
      "Epoch [324/500], Train Loss: 0.2008\n",
      "Epoch [325/500], Train Loss: 0.2007\n",
      "Epoch [326/500], Train Loss: 0.2004\n",
      "Epoch [327/500], Train Loss: 0.2003\n",
      "Epoch [328/500], Train Loss: 0.2002\n",
      "Epoch [329/500], Train Loss: 0.2000\n",
      "Epoch [330/500], Train Loss: 0.2001\n",
      "Epoch [331/500], Train Loss: 0.1997\n",
      "Epoch [332/500], Train Loss: 0.1997\n",
      "Epoch [333/500], Train Loss: 0.1995\n",
      "Epoch [334/500], Train Loss: 0.1994\n",
      "Epoch [335/500], Train Loss: 0.1993\n",
      "Epoch [336/500], Train Loss: 0.1991\n",
      "Epoch [337/500], Train Loss: 0.1991\n",
      "Epoch [338/500], Train Loss: 0.1991\n",
      "Epoch [339/500], Train Loss: 0.1986\n",
      "Epoch [340/500], Train Loss: 0.1987\n",
      "Epoch [341/500], Train Loss: 0.1983\n",
      "Epoch [342/500], Train Loss: 0.1984\n",
      "Epoch [343/500], Train Loss: 0.1982\n",
      "Epoch [344/500], Train Loss: 0.1983\n",
      "Epoch [345/500], Train Loss: 0.1982\n",
      "Epoch [346/500], Train Loss: 0.1980\n",
      "Epoch [347/500], Train Loss: 0.1977\n",
      "Epoch [348/500], Train Loss: 0.1977\n",
      "Epoch [349/500], Train Loss: 0.1976\n",
      "Epoch [350/500], Train Loss: 0.1974\n",
      "Epoch [351/500], Train Loss: 0.1973\n",
      "Epoch [352/500], Train Loss: 0.1975\n",
      "Epoch [353/500], Train Loss: 0.1970\n",
      "Epoch [354/500], Train Loss: 0.1973\n",
      "Epoch [355/500], Train Loss: 0.1971\n",
      "Epoch [356/500], Train Loss: 0.1971\n",
      "Epoch [357/500], Train Loss: 0.1968\n",
      "Epoch [358/500], Train Loss: 0.1966\n",
      "Epoch [359/500], Train Loss: 0.1966\n",
      "Epoch [360/500], Train Loss: 0.1964\n",
      "Epoch [361/500], Train Loss: 0.1964\n",
      "Epoch [362/500], Train Loss: 0.1964\n",
      "Epoch [363/500], Train Loss: 0.1965\n",
      "Epoch [364/500], Train Loss: 0.1959\n",
      "Epoch [365/500], Train Loss: 0.1957\n",
      "Epoch [366/500], Train Loss: 0.1959\n",
      "Epoch [367/500], Train Loss: 0.1959\n",
      "Epoch [368/500], Train Loss: 0.1956\n",
      "Epoch [369/500], Train Loss: 0.1956\n",
      "Epoch [370/500], Train Loss: 0.1954\n",
      "Epoch [371/500], Train Loss: 0.1952\n",
      "Epoch [372/500], Train Loss: 0.1955\n",
      "Epoch [373/500], Train Loss: 0.1953\n",
      "Epoch [374/500], Train Loss: 0.1949\n",
      "Epoch [375/500], Train Loss: 0.1950\n",
      "Epoch [376/500], Train Loss: 0.1948\n",
      "Epoch [377/500], Train Loss: 0.1948\n",
      "Epoch [378/500], Train Loss: 0.1948\n",
      "Epoch [379/500], Train Loss: 0.1945\n",
      "Epoch [380/500], Train Loss: 0.1944\n",
      "Epoch [381/500], Train Loss: 0.1943\n",
      "Epoch [382/500], Train Loss: 0.1943\n",
      "Epoch [383/500], Train Loss: 0.1946\n",
      "Epoch [384/500], Train Loss: 0.1942\n",
      "Epoch [385/500], Train Loss: 0.1940\n",
      "Epoch [386/500], Train Loss: 0.1939\n",
      "Epoch [387/500], Train Loss: 0.1937\n",
      "Epoch [388/500], Train Loss: 0.1939\n",
      "Epoch [389/500], Train Loss: 0.1934\n",
      "Epoch [390/500], Train Loss: 0.1936\n",
      "Epoch [391/500], Train Loss: 0.1936\n",
      "Epoch [392/500], Train Loss: 0.1933\n",
      "Epoch [393/500], Train Loss: 0.1936\n",
      "Epoch [394/500], Train Loss: 0.1933\n",
      "Epoch [395/500], Train Loss: 0.1931\n",
      "Epoch [396/500], Train Loss: 0.1927\n",
      "Epoch [397/500], Train Loss: 0.1929\n",
      "Epoch [398/500], Train Loss: 0.1929\n",
      "Epoch [399/500], Train Loss: 0.1927\n",
      "Epoch [400/500], Train Loss: 0.1928\n",
      "Epoch [401/500], Train Loss: 0.1927\n",
      "Epoch [402/500], Train Loss: 0.1923\n",
      "Epoch [403/500], Train Loss: 0.1927\n",
      "Epoch [404/500], Train Loss: 0.1925\n",
      "Epoch [405/500], Train Loss: 0.1921\n",
      "Epoch [406/500], Train Loss: 0.1923\n",
      "Epoch [407/500], Train Loss: 0.1921\n",
      "Epoch [408/500], Train Loss: 0.1920\n",
      "Epoch [409/500], Train Loss: 0.1919\n",
      "Epoch [410/500], Train Loss: 0.1921\n",
      "Epoch [411/500], Train Loss: 0.1919\n",
      "Epoch [412/500], Train Loss: 0.1915\n",
      "Epoch [413/500], Train Loss: 0.1918\n",
      "Epoch [414/500], Train Loss: 0.1916\n",
      "Epoch [415/500], Train Loss: 0.1916\n",
      "Epoch [416/500], Train Loss: 0.1916\n",
      "Epoch [417/500], Train Loss: 0.1911\n",
      "Epoch [418/500], Train Loss: 0.1913\n",
      "Epoch [419/500], Train Loss: 0.1912\n",
      "Epoch [420/500], Train Loss: 0.1909\n",
      "Epoch [421/500], Train Loss: 0.1909\n",
      "Epoch [422/500], Train Loss: 0.1909\n",
      "Epoch [423/500], Train Loss: 0.1909\n",
      "Epoch [424/500], Train Loss: 0.1905\n",
      "Epoch [425/500], Train Loss: 0.1907\n",
      "Epoch [426/500], Train Loss: 0.1908\n",
      "Epoch [427/500], Train Loss: 0.1908\n",
      "Epoch [428/500], Train Loss: 0.1904\n",
      "Epoch [429/500], Train Loss: 0.1904\n",
      "Epoch [430/500], Train Loss: 0.1904\n",
      "Epoch [431/500], Train Loss: 0.1902\n",
      "Epoch [432/500], Train Loss: 0.1901\n",
      "Epoch [433/500], Train Loss: 0.1900\n",
      "Epoch [434/500], Train Loss: 0.1899\n",
      "Epoch [435/500], Train Loss: 0.1900\n",
      "Epoch [436/500], Train Loss: 0.1901\n",
      "Epoch [437/500], Train Loss: 0.1901\n",
      "Epoch [438/500], Train Loss: 0.1900\n",
      "Epoch [439/500], Train Loss: 0.1899\n",
      "Epoch [440/500], Train Loss: 0.1894\n",
      "Epoch [441/500], Train Loss: 0.1897\n",
      "Epoch [442/500], Train Loss: 0.1894\n",
      "Epoch [443/500], Train Loss: 0.1893\n",
      "Epoch [444/500], Train Loss: 0.1894\n",
      "Epoch [445/500], Train Loss: 0.1895\n",
      "Epoch [446/500], Train Loss: 0.1889\n",
      "Epoch [447/500], Train Loss: 0.1889\n",
      "Epoch [448/500], Train Loss: 0.1892\n",
      "Epoch [449/500], Train Loss: 0.1891\n",
      "Epoch [450/500], Train Loss: 0.1888\n",
      "Epoch [451/500], Train Loss: 0.1889\n",
      "Epoch [452/500], Train Loss: 0.1888\n",
      "Epoch [453/500], Train Loss: 0.1890\n",
      "Epoch [454/500], Train Loss: 0.1884\n",
      "Epoch [455/500], Train Loss: 0.1886\n",
      "Epoch [456/500], Train Loss: 0.1887\n",
      "Epoch [457/500], Train Loss: 0.1885\n",
      "Epoch [458/500], Train Loss: 0.1885\n",
      "Epoch [459/500], Train Loss: 0.1882\n",
      "Epoch [460/500], Train Loss: 0.1883\n",
      "Epoch [461/500], Train Loss: 0.1883\n",
      "Epoch [462/500], Train Loss: 0.1879\n",
      "Epoch [463/500], Train Loss: 0.1881\n",
      "Epoch [464/500], Train Loss: 0.1881\n",
      "Epoch [465/500], Train Loss: 0.1879\n",
      "Epoch [466/500], Train Loss: 0.1878\n",
      "Epoch [467/500], Train Loss: 0.1880\n",
      "Epoch [468/500], Train Loss: 0.1879\n",
      "Epoch [469/500], Train Loss: 0.1876\n",
      "Epoch [470/500], Train Loss: 0.1874\n",
      "Epoch [471/500], Train Loss: 0.1876\n",
      "Epoch [472/500], Train Loss: 0.1875\n",
      "Epoch [473/500], Train Loss: 0.1876\n",
      "Epoch [474/500], Train Loss: 0.1873\n",
      "Epoch [475/500], Train Loss: 0.1872\n",
      "Epoch [476/500], Train Loss: 0.1870\n",
      "Epoch [477/500], Train Loss: 0.1869\n",
      "Epoch [478/500], Train Loss: 0.1873\n",
      "Epoch [479/500], Train Loss: 0.1871\n",
      "Epoch [480/500], Train Loss: 0.1868\n",
      "Epoch [481/500], Train Loss: 0.1872\n",
      "Epoch [482/500], Train Loss: 0.1871\n",
      "Epoch [483/500], Train Loss: 0.1870\n",
      "Epoch [484/500], Train Loss: 0.1868\n",
      "Epoch [485/500], Train Loss: 0.1866\n",
      "Epoch [486/500], Train Loss: 0.1868\n",
      "Epoch [487/500], Train Loss: 0.1866\n",
      "Epoch [488/500], Train Loss: 0.1865\n",
      "Epoch [489/500], Train Loss: 0.1861\n",
      "Epoch [490/500], Train Loss: 0.1867\n",
      "Epoch [491/500], Train Loss: 0.1863\n",
      "Epoch [492/500], Train Loss: 0.1864\n",
      "Epoch [493/500], Train Loss: 0.1862\n",
      "Epoch [494/500], Train Loss: 0.1863\n",
      "Epoch [495/500], Train Loss: 0.1860\n",
      "Epoch [496/500], Train Loss: 0.1862\n",
      "Epoch [497/500], Train Loss: 0.1861\n",
      "Epoch [498/500], Train Loss: 0.1859\n",
      "Epoch [499/500], Train Loss: 0.1860\n",
      "Epoch [500/500], Train Loss: 0.1857\n",
      "Final model saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/models_unphased/chr7/final_model_chr7.pth\n",
      "Individual R^2 scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr7/individual_r2_scores_chr7.csv\n",
      "Individual IQS scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr7/individual_iqs_scores_chr7.csv\n",
      "Individual AUC ROC curves saved in: ../../Data/model_results_unphased_all_PRS/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/performance_metrics.csv\n",
      "Total SNPs:  475\n",
      "PRS313 SNPs:  21\n",
      "Total SNPs used for Training:  454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-30 10:15:05,927] Trial 51 finished with value: 0.2571044263931421 and parameters: {'learning_rate': 0.0010547016944786214, 'l1_coef': 2.3292680367119465e-05, 'patience': 10, 'batch_size': 32}. Best is trial 17 with value: 0.11601511778739784.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 8 - Best hyperparameters: {'learning_rate': 0.0012197976599892346, 'l1_coef': 1.0397855148759009e-05, 'patience': 6, 'batch_size': 32}\n",
      "Chr 8 - Best value: 0.1160\n",
      "Epoch [1/500], Train Loss: 0.5153\n",
      "Epoch [2/500], Train Loss: 0.4592\n",
      "Epoch [3/500], Train Loss: 0.4331\n",
      "Epoch [4/500], Train Loss: 0.4125\n",
      "Epoch [5/500], Train Loss: 0.3956\n",
      "Epoch [6/500], Train Loss: 0.3817\n",
      "Epoch [7/500], Train Loss: 0.3689\n",
      "Epoch [8/500], Train Loss: 0.3582\n",
      "Epoch [9/500], Train Loss: 0.3496\n",
      "Epoch [10/500], Train Loss: 0.3410\n",
      "Epoch [11/500], Train Loss: 0.3344\n",
      "Epoch [12/500], Train Loss: 0.3277\n",
      "Epoch [13/500], Train Loss: 0.3214\n",
      "Epoch [14/500], Train Loss: 0.3159\n",
      "Epoch [15/500], Train Loss: 0.3114\n",
      "Epoch [16/500], Train Loss: 0.3065\n",
      "Epoch [17/500], Train Loss: 0.3019\n",
      "Epoch [18/500], Train Loss: 0.2979\n",
      "Epoch [19/500], Train Loss: 0.2947\n",
      "Epoch [20/500], Train Loss: 0.2915\n",
      "Epoch [21/500], Train Loss: 0.2884\n",
      "Epoch [22/500], Train Loss: 0.2857\n",
      "Epoch [23/500], Train Loss: 0.2819\n",
      "Epoch [24/500], Train Loss: 0.2796\n",
      "Epoch [25/500], Train Loss: 0.2769\n",
      "Epoch [26/500], Train Loss: 0.2749\n",
      "Epoch [27/500], Train Loss: 0.2722\n",
      "Epoch [28/500], Train Loss: 0.2706\n",
      "Epoch [29/500], Train Loss: 0.2689\n",
      "Epoch [30/500], Train Loss: 0.2664\n",
      "Epoch [31/500], Train Loss: 0.2640\n",
      "Epoch [32/500], Train Loss: 0.2626\n",
      "Epoch [33/500], Train Loss: 0.2607\n",
      "Epoch [34/500], Train Loss: 0.2594\n",
      "Epoch [35/500], Train Loss: 0.2576\n",
      "Epoch [36/500], Train Loss: 0.2562\n",
      "Epoch [37/500], Train Loss: 0.2549\n",
      "Epoch [38/500], Train Loss: 0.2534\n",
      "Epoch [39/500], Train Loss: 0.2521\n",
      "Epoch [40/500], Train Loss: 0.2503\n",
      "Epoch [41/500], Train Loss: 0.2491\n",
      "Epoch [42/500], Train Loss: 0.2478\n",
      "Epoch [43/500], Train Loss: 0.2472\n",
      "Epoch [44/500], Train Loss: 0.2458\n",
      "Epoch [45/500], Train Loss: 0.2450\n",
      "Epoch [46/500], Train Loss: 0.2433\n",
      "Epoch [47/500], Train Loss: 0.2424\n",
      "Epoch [48/500], Train Loss: 0.2423\n",
      "Epoch [49/500], Train Loss: 0.2410\n",
      "Epoch [50/500], Train Loss: 0.2401\n",
      "Epoch [51/500], Train Loss: 0.2391\n",
      "Epoch [52/500], Train Loss: 0.2380\n",
      "Epoch [53/500], Train Loss: 0.2376\n",
      "Epoch [54/500], Train Loss: 0.2368\n",
      "Epoch [55/500], Train Loss: 0.2352\n",
      "Epoch [56/500], Train Loss: 0.2343\n",
      "Epoch [57/500], Train Loss: 0.2346\n",
      "Epoch [58/500], Train Loss: 0.2335\n",
      "Epoch [59/500], Train Loss: 0.2325\n",
      "Epoch [60/500], Train Loss: 0.2325\n",
      "Epoch [61/500], Train Loss: 0.2313\n",
      "Epoch [62/500], Train Loss: 0.2308\n",
      "Epoch [63/500], Train Loss: 0.2301\n",
      "Epoch [64/500], Train Loss: 0.2289\n",
      "Epoch [65/500], Train Loss: 0.2288\n",
      "Epoch [66/500], Train Loss: 0.2286\n",
      "Epoch [67/500], Train Loss: 0.2275\n",
      "Epoch [68/500], Train Loss: 0.2267\n",
      "Epoch [69/500], Train Loss: 0.2266\n",
      "Epoch [70/500], Train Loss: 0.2258\n",
      "Epoch [71/500], Train Loss: 0.2253\n",
      "Epoch [72/500], Train Loss: 0.2252\n",
      "Epoch [73/500], Train Loss: 0.2246\n",
      "Epoch [74/500], Train Loss: 0.2237\n",
      "Epoch [75/500], Train Loss: 0.2233\n",
      "Epoch [76/500], Train Loss: 0.2234\n",
      "Epoch [77/500], Train Loss: 0.2229\n",
      "Epoch [78/500], Train Loss: 0.2221\n",
      "Epoch [79/500], Train Loss: 0.2214\n",
      "Epoch [80/500], Train Loss: 0.2208\n",
      "Epoch [81/500], Train Loss: 0.2204\n",
      "Epoch [82/500], Train Loss: 0.2206\n",
      "Epoch [83/500], Train Loss: 0.2204\n",
      "Epoch [84/500], Train Loss: 0.2198\n",
      "Epoch [85/500], Train Loss: 0.2191\n",
      "Epoch [86/500], Train Loss: 0.2189\n",
      "Epoch [87/500], Train Loss: 0.2184\n",
      "Epoch [88/500], Train Loss: 0.2184\n",
      "Epoch [89/500], Train Loss: 0.2178\n",
      "Epoch [90/500], Train Loss: 0.2173\n",
      "Epoch [91/500], Train Loss: 0.2171\n",
      "Epoch [92/500], Train Loss: 0.2168\n",
      "Epoch [93/500], Train Loss: 0.2162\n",
      "Epoch [94/500], Train Loss: 0.2160\n",
      "Epoch [95/500], Train Loss: 0.2159\n",
      "Epoch [96/500], Train Loss: 0.2154\n",
      "Epoch [97/500], Train Loss: 0.2159\n",
      "Epoch [98/500], Train Loss: 0.2148\n",
      "Epoch [99/500], Train Loss: 0.2146\n",
      "Epoch [100/500], Train Loss: 0.2144\n",
      "Epoch [101/500], Train Loss: 0.2139\n",
      "Epoch [102/500], Train Loss: 0.2139\n",
      "Epoch [103/500], Train Loss: 0.2138\n",
      "Epoch [104/500], Train Loss: 0.2130\n",
      "Epoch [105/500], Train Loss: 0.2128\n",
      "Epoch [106/500], Train Loss: 0.2127\n",
      "Epoch [107/500], Train Loss: 0.2122\n",
      "Epoch [108/500], Train Loss: 0.2123\n",
      "Epoch [109/500], Train Loss: 0.2119\n",
      "Epoch [110/500], Train Loss: 0.2122\n",
      "Epoch [111/500], Train Loss: 0.2114\n",
      "Epoch [112/500], Train Loss: 0.2109\n",
      "Epoch [113/500], Train Loss: 0.2107\n",
      "Epoch [114/500], Train Loss: 0.2104\n",
      "Epoch [115/500], Train Loss: 0.2104\n",
      "Epoch [116/500], Train Loss: 0.2098\n",
      "Epoch [117/500], Train Loss: 0.2101\n",
      "Epoch [118/500], Train Loss: 0.2100\n",
      "Epoch [119/500], Train Loss: 0.2096\n",
      "Epoch [120/500], Train Loss: 0.2092\n",
      "Epoch [121/500], Train Loss: 0.2091\n",
      "Epoch [122/500], Train Loss: 0.2093\n",
      "Epoch [123/500], Train Loss: 0.2089\n",
      "Epoch [124/500], Train Loss: 0.2086\n",
      "Epoch [125/500], Train Loss: 0.2083\n",
      "Epoch [126/500], Train Loss: 0.2081\n",
      "Epoch [127/500], Train Loss: 0.2079\n",
      "Epoch [128/500], Train Loss: 0.2084\n",
      "Epoch [129/500], Train Loss: 0.2076\n",
      "Epoch [130/500], Train Loss: 0.2075\n",
      "Epoch [131/500], Train Loss: 0.2075\n",
      "Epoch [132/500], Train Loss: 0.2077\n",
      "Epoch [133/500], Train Loss: 0.2071\n",
      "Epoch [134/500], Train Loss: 0.2073\n",
      "Epoch [135/500], Train Loss: 0.2073\n",
      "Epoch [136/500], Train Loss: 0.2061\n",
      "Epoch [137/500], Train Loss: 0.2068\n",
      "Epoch [138/500], Train Loss: 0.2061\n",
      "Epoch [139/500], Train Loss: 0.2062\n",
      "Epoch [140/500], Train Loss: 0.2068\n",
      "Epoch [141/500], Train Loss: 0.2061\n",
      "Epoch [142/500], Train Loss: 0.2060\n",
      "Epoch [143/500], Train Loss: 0.2060\n",
      "Epoch [144/500], Train Loss: 0.2054\n",
      "Epoch [145/500], Train Loss: 0.2057\n",
      "Epoch [146/500], Train Loss: 0.2055\n",
      "Epoch [147/500], Train Loss: 0.2054\n",
      "Epoch [148/500], Train Loss: 0.2047\n",
      "Epoch [149/500], Train Loss: 0.2047\n",
      "Epoch [150/500], Train Loss: 0.2050\n",
      "Epoch [151/500], Train Loss: 0.2052\n",
      "Epoch [152/500], Train Loss: 0.2049\n",
      "Epoch [153/500], Train Loss: 0.2038\n",
      "Epoch [154/500], Train Loss: 0.2039\n",
      "Epoch [155/500], Train Loss: 0.2040\n",
      "Epoch [156/500], Train Loss: 0.2039\n",
      "Epoch [157/500], Train Loss: 0.2037\n",
      "Epoch [158/500], Train Loss: 0.2038\n",
      "Epoch [159/500], Train Loss: 0.2039\n",
      "Epoch [160/500], Train Loss: 0.2035\n",
      "Epoch [161/500], Train Loss: 0.2039\n",
      "Epoch [162/500], Train Loss: 0.2032\n",
      "Epoch [163/500], Train Loss: 0.2035\n",
      "Epoch [164/500], Train Loss: 0.2030\n",
      "Epoch [165/500], Train Loss: 0.2028\n",
      "Epoch [166/500], Train Loss: 0.2031\n",
      "Epoch [167/500], Train Loss: 0.2026\n",
      "Epoch [168/500], Train Loss: 0.2028\n",
      "Epoch [169/500], Train Loss: 0.2030\n",
      "Epoch [170/500], Train Loss: 0.2025\n",
      "Epoch [171/500], Train Loss: 0.2021\n",
      "Epoch [172/500], Train Loss: 0.2022\n",
      "Epoch [173/500], Train Loss: 0.2022\n",
      "Epoch [174/500], Train Loss: 0.2022\n",
      "Epoch [175/500], Train Loss: 0.2018\n",
      "Epoch [176/500], Train Loss: 0.2023\n",
      "Epoch [177/500], Train Loss: 0.2012\n",
      "Epoch [178/500], Train Loss: 0.2022\n",
      "Epoch [179/500], Train Loss: 0.2021\n",
      "Epoch [180/500], Train Loss: 0.2018\n",
      "Epoch [181/500], Train Loss: 0.2015\n",
      "Epoch [182/500], Train Loss: 0.2015\n",
      "Epoch [183/500], Train Loss: 0.2014\n",
      "Early stopping at epoch 183\n",
      "Final model saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/models_unphased/chr8/final_model_chr8.pth\n",
      "Individual R^2 scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr8/individual_r2_scores_chr8.csv\n",
      "Individual IQS scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr8/individual_iqs_scores_chr8.csv\n",
      "Individual AUC ROC curves saved in: ../../Data/model_results_unphased_all_PRS/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/performance_metrics.csv\n",
      "Total SNPs:  388\n",
      "PRS313 SNPs:  14\n",
      "Total SNPs used for Training:  374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-04-30 10:15:12,925] Trial 51 finished with value: 0.26292283878876616 and parameters: {'learning_rate': 0.022038030748484788, 'l1_coef': 1.6185418086062093e-05, 'patience': 18, 'batch_size': 32}. Best is trial 21 with value: 0.11073745116591453.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 9 - Best hyperparameters: {'learning_rate': 0.05822594013574142, 'l1_coef': 1.5307383394547932e-05, 'patience': 14, 'batch_size': 256}\n",
      "Chr 9 - Best value: 0.1107\n",
      "Epoch [1/500], Train Loss: 1.6723\n",
      "Epoch [2/500], Train Loss: 1.0683\n",
      "Epoch [3/500], Train Loss: 0.8159\n",
      "Epoch [4/500], Train Loss: 0.6909\n",
      "Epoch [5/500], Train Loss: 0.6291\n",
      "Epoch [6/500], Train Loss: 0.5992\n",
      "Epoch [7/500], Train Loss: 0.5878\n",
      "Epoch [8/500], Train Loss: 0.5733\n",
      "Epoch [9/500], Train Loss: 0.5676\n",
      "Epoch [10/500], Train Loss: 0.5642\n",
      "Epoch [11/500], Train Loss: 0.5599\n",
      "Epoch [12/500], Train Loss: 0.5592\n",
      "Epoch [13/500], Train Loss: 0.5513\n",
      "Epoch [14/500], Train Loss: 0.5481\n",
      "Epoch [15/500], Train Loss: 0.5450\n",
      "Epoch [16/500], Train Loss: 0.5464\n",
      "Epoch [17/500], Train Loss: 0.5419\n",
      "Epoch [18/500], Train Loss: 0.5405\n",
      "Epoch [19/500], Train Loss: 0.5413\n",
      "Epoch [20/500], Train Loss: 0.5364\n",
      "Epoch [21/500], Train Loss: 0.5393\n",
      "Epoch [22/500], Train Loss: 0.5392\n",
      "Epoch [23/500], Train Loss: 0.5354\n",
      "Epoch [24/500], Train Loss: 0.5359\n",
      "Epoch [25/500], Train Loss: 0.5316\n",
      "Epoch [26/500], Train Loss: 0.5291\n",
      "Epoch [27/500], Train Loss: 0.5309\n",
      "Epoch [28/500], Train Loss: 0.5301\n",
      "Epoch [29/500], Train Loss: 0.5317\n",
      "Epoch [30/500], Train Loss: 0.5328\n",
      "Epoch [31/500], Train Loss: 0.5327\n",
      "Epoch [32/500], Train Loss: 0.5314\n",
      "Epoch [33/500], Train Loss: 0.5142\n",
      "Epoch [34/500], Train Loss: 0.5144\n",
      "Epoch [35/500], Train Loss: 0.5142\n",
      "Epoch [36/500], Train Loss: 0.5096\n",
      "Epoch [37/500], Train Loss: 0.5134\n",
      "Epoch [38/500], Train Loss: 0.5089\n",
      "Epoch [39/500], Train Loss: 0.5150\n",
      "Epoch [40/500], Train Loss: 0.5113\n",
      "Epoch [41/500], Train Loss: 0.5134\n",
      "Epoch [42/500], Train Loss: 0.5089\n",
      "Epoch [43/500], Train Loss: 0.5138\n",
      "Epoch [44/500], Train Loss: 0.5130\n",
      "Epoch [45/500], Train Loss: 0.5103\n",
      "Epoch [46/500], Train Loss: 0.5129\n",
      "Epoch [47/500], Train Loss: 0.5086\n",
      "Epoch [48/500], Train Loss: 0.5100\n",
      "Epoch [49/500], Train Loss: 0.5151\n",
      "Epoch [50/500], Train Loss: 0.5056\n",
      "Epoch [51/500], Train Loss: 0.4964\n",
      "Epoch [52/500], Train Loss: 0.4797\n",
      "Epoch [53/500], Train Loss: 0.3750\n",
      "Epoch [54/500], Train Loss: 0.2633\n",
      "Epoch [55/500], Train Loss: 0.2499\n",
      "Epoch [56/500], Train Loss: 0.2481\n",
      "Epoch [57/500], Train Loss: 0.2466\n",
      "Epoch [58/500], Train Loss: 0.2447\n",
      "Epoch [59/500], Train Loss: 0.2451\n",
      "Epoch [60/500], Train Loss: 0.2451\n",
      "Epoch [61/500], Train Loss: 0.2447\n",
      "Epoch [62/500], Train Loss: 0.2442\n",
      "Epoch [63/500], Train Loss: 0.2437\n",
      "Epoch [64/500], Train Loss: 0.2432\n",
      "Epoch [65/500], Train Loss: 0.2434\n",
      "Epoch [66/500], Train Loss: 0.2429\n",
      "Epoch [67/500], Train Loss: 0.2426\n",
      "Epoch [68/500], Train Loss: 0.2426\n",
      "Epoch [69/500], Train Loss: 0.2425\n",
      "Epoch [70/500], Train Loss: 0.2423\n",
      "Epoch [71/500], Train Loss: 0.2424\n",
      "Epoch [72/500], Train Loss: 0.2420\n",
      "Epoch [73/500], Train Loss: 0.2417\n",
      "Epoch [74/500], Train Loss: 0.2417\n",
      "Epoch [75/500], Train Loss: 0.2412\n",
      "Epoch [76/500], Train Loss: 0.2418\n",
      "Epoch [77/500], Train Loss: 0.2413\n",
      "Epoch [78/500], Train Loss: 0.2419\n",
      "Epoch [79/500], Train Loss: 0.2413\n",
      "Epoch [80/500], Train Loss: 0.2418\n",
      "Epoch [81/500], Train Loss: 0.2420\n",
      "Epoch [82/500], Train Loss: 0.2405\n",
      "Epoch [83/500], Train Loss: 0.2399\n",
      "Epoch [84/500], Train Loss: 0.2400\n",
      "Epoch [85/500], Train Loss: 0.2400\n",
      "Epoch [86/500], Train Loss: 0.2399\n",
      "Epoch [87/500], Train Loss: 0.2397\n",
      "Epoch [88/500], Train Loss: 0.2399\n",
      "Epoch [89/500], Train Loss: 0.2399\n",
      "Epoch [90/500], Train Loss: 0.2400\n",
      "Epoch [91/500], Train Loss: 0.2397\n",
      "Epoch [92/500], Train Loss: 0.2398\n",
      "Epoch [93/500], Train Loss: 0.2397\n",
      "Epoch [94/500], Train Loss: 0.2397\n",
      "Epoch [95/500], Train Loss: 0.2397\n",
      "Epoch [96/500], Train Loss: 0.2397\n",
      "Epoch [97/500], Train Loss: 0.2397\n",
      "Epoch [98/500], Train Loss: 0.2399\n",
      "Epoch [99/500], Train Loss: 0.2395\n",
      "Epoch [100/500], Train Loss: 0.2395\n",
      "Epoch [101/500], Train Loss: 0.2395\n",
      "Epoch [102/500], Train Loss: 0.2395\n",
      "Epoch [103/500], Train Loss: 0.2396\n",
      "Epoch [104/500], Train Loss: 0.2397\n",
      "Epoch [105/500], Train Loss: 0.2397\n",
      "Epoch [106/500], Train Loss: 0.2397\n",
      "Epoch [107/500], Train Loss: 0.2398\n",
      "Epoch [108/500], Train Loss: 0.2395\n",
      "Epoch [109/500], Train Loss: 0.2394\n",
      "Epoch [110/500], Train Loss: 0.2395\n",
      "Epoch [111/500], Train Loss: 0.2396\n",
      "Epoch [112/500], Train Loss: 0.2396\n",
      "Epoch [113/500], Train Loss: 0.2396\n",
      "Epoch [114/500], Train Loss: 0.2398\n",
      "Epoch [115/500], Train Loss: 0.2395\n",
      "Epoch [116/500], Train Loss: 0.2395\n",
      "Epoch [117/500], Train Loss: 0.2397\n",
      "Epoch [118/500], Train Loss: 0.2396\n",
      "Epoch [119/500], Train Loss: 0.2396\n",
      "Epoch [120/500], Train Loss: 0.2396\n",
      "Epoch [121/500], Train Loss: 0.2395\n",
      "Epoch [122/500], Train Loss: 0.2397\n",
      "Epoch [123/500], Train Loss: 0.2394\n",
      "Early stopping at epoch 123\n",
      "Final model saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/models_unphased/chr9/final_model_chr9.pth\n",
      "Individual R^2 scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr9/individual_r2_scores_chr9.csv\n",
      "Individual IQS scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr9/individual_iqs_scores_chr9.csv\n",
      "Individual AUC ROC curves saved in: ../../Data/model_results_unphased_all_PRS/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/performance_metrics.csv\n",
      "Total SNPs:  546\n",
      "PRS313 SNPs:  18\n",
      "Total SNPs used for Training:  528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-04-30 10:15:22,097] Trial 51 finished with value: 0.20796320821557726 and parameters: {'learning_rate': 0.0001569302693108911, 'l1_coef': 1.6432141602439407e-05, 'patience': 12, 'batch_size': 64}. Best is trial 31 with value: 0.10043132305145264.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 10 - Best hyperparameters: {'learning_rate': 0.00022093387007935644, 'l1_coef': 1.0430280711765954e-05, 'patience': 5, 'batch_size': 32}\n",
      "Chr 10 - Best value: 0.1004\n",
      "Epoch [1/500], Train Loss: 0.5688\n",
      "Epoch [2/500], Train Loss: 0.4924\n",
      "Epoch [3/500], Train Loss: 0.4696\n",
      "Epoch [4/500], Train Loss: 0.4545\n",
      "Epoch [5/500], Train Loss: 0.4430\n",
      "Epoch [6/500], Train Loss: 0.4335\n",
      "Epoch [7/500], Train Loss: 0.4253\n",
      "Epoch [8/500], Train Loss: 0.4180\n",
      "Epoch [9/500], Train Loss: 0.4115\n",
      "Epoch [10/500], Train Loss: 0.4054\n",
      "Epoch [11/500], Train Loss: 0.3994\n",
      "Epoch [12/500], Train Loss: 0.3937\n",
      "Epoch [13/500], Train Loss: 0.3883\n",
      "Epoch [14/500], Train Loss: 0.3834\n",
      "Epoch [15/500], Train Loss: 0.3784\n",
      "Epoch [16/500], Train Loss: 0.3740\n",
      "Epoch [17/500], Train Loss: 0.3693\n",
      "Epoch [18/500], Train Loss: 0.3650\n",
      "Epoch [19/500], Train Loss: 0.3611\n",
      "Epoch [20/500], Train Loss: 0.3570\n",
      "Epoch [21/500], Train Loss: 0.3531\n",
      "Epoch [22/500], Train Loss: 0.3493\n",
      "Epoch [23/500], Train Loss: 0.3455\n",
      "Epoch [24/500], Train Loss: 0.3425\n",
      "Epoch [25/500], Train Loss: 0.3389\n",
      "Epoch [26/500], Train Loss: 0.3357\n",
      "Epoch [27/500], Train Loss: 0.3321\n",
      "Epoch [28/500], Train Loss: 0.3292\n",
      "Epoch [29/500], Train Loss: 0.3265\n",
      "Epoch [30/500], Train Loss: 0.3235\n",
      "Epoch [31/500], Train Loss: 0.3205\n",
      "Epoch [32/500], Train Loss: 0.3179\n",
      "Epoch [33/500], Train Loss: 0.3152\n",
      "Epoch [34/500], Train Loss: 0.3126\n",
      "Epoch [35/500], Train Loss: 0.3100\n",
      "Epoch [36/500], Train Loss: 0.3074\n",
      "Epoch [37/500], Train Loss: 0.3053\n",
      "Epoch [38/500], Train Loss: 0.3027\n",
      "Epoch [39/500], Train Loss: 0.3005\n",
      "Epoch [40/500], Train Loss: 0.2983\n",
      "Epoch [41/500], Train Loss: 0.2959\n",
      "Epoch [42/500], Train Loss: 0.2941\n",
      "Epoch [43/500], Train Loss: 0.2919\n",
      "Epoch [44/500], Train Loss: 0.2898\n",
      "Epoch [45/500], Train Loss: 0.2880\n",
      "Epoch [46/500], Train Loss: 0.2860\n",
      "Epoch [47/500], Train Loss: 0.2838\n",
      "Epoch [48/500], Train Loss: 0.2818\n",
      "Epoch [49/500], Train Loss: 0.2803\n",
      "Epoch [50/500], Train Loss: 0.2785\n",
      "Epoch [51/500], Train Loss: 0.2767\n",
      "Epoch [52/500], Train Loss: 0.2749\n",
      "Epoch [53/500], Train Loss: 0.2734\n",
      "Epoch [54/500], Train Loss: 0.2717\n",
      "Epoch [55/500], Train Loss: 0.2701\n",
      "Epoch [56/500], Train Loss: 0.2685\n",
      "Epoch [57/500], Train Loss: 0.2667\n",
      "Epoch [58/500], Train Loss: 0.2653\n",
      "Epoch [59/500], Train Loss: 0.2639\n",
      "Epoch [60/500], Train Loss: 0.2625\n",
      "Epoch [61/500], Train Loss: 0.2610\n",
      "Epoch [62/500], Train Loss: 0.2595\n",
      "Epoch [63/500], Train Loss: 0.2583\n",
      "Epoch [64/500], Train Loss: 0.2569\n",
      "Epoch [65/500], Train Loss: 0.2555\n",
      "Epoch [66/500], Train Loss: 0.2541\n",
      "Epoch [67/500], Train Loss: 0.2530\n",
      "Epoch [68/500], Train Loss: 0.2518\n",
      "Epoch [69/500], Train Loss: 0.2504\n",
      "Epoch [70/500], Train Loss: 0.2490\n",
      "Epoch [71/500], Train Loss: 0.2481\n",
      "Epoch [72/500], Train Loss: 0.2468\n",
      "Epoch [73/500], Train Loss: 0.2457\n",
      "Epoch [74/500], Train Loss: 0.2446\n",
      "Epoch [75/500], Train Loss: 0.2433\n",
      "Epoch [76/500], Train Loss: 0.2424\n",
      "Epoch [77/500], Train Loss: 0.2413\n",
      "Epoch [78/500], Train Loss: 0.2401\n",
      "Epoch [79/500], Train Loss: 0.2392\n",
      "Epoch [80/500], Train Loss: 0.2380\n",
      "Epoch [81/500], Train Loss: 0.2371\n",
      "Epoch [82/500], Train Loss: 0.2360\n",
      "Epoch [83/500], Train Loss: 0.2351\n",
      "Epoch [84/500], Train Loss: 0.2341\n",
      "Epoch [85/500], Train Loss: 0.2332\n",
      "Epoch [86/500], Train Loss: 0.2323\n",
      "Epoch [87/500], Train Loss: 0.2313\n",
      "Epoch [88/500], Train Loss: 0.2304\n",
      "Epoch [89/500], Train Loss: 0.2295\n",
      "Epoch [90/500], Train Loss: 0.2285\n",
      "Epoch [91/500], Train Loss: 0.2276\n",
      "Epoch [92/500], Train Loss: 0.2269\n",
      "Epoch [93/500], Train Loss: 0.2260\n",
      "Epoch [94/500], Train Loss: 0.2250\n",
      "Epoch [95/500], Train Loss: 0.2245\n",
      "Epoch [96/500], Train Loss: 0.2234\n",
      "Epoch [97/500], Train Loss: 0.2226\n",
      "Epoch [98/500], Train Loss: 0.2218\n",
      "Epoch [99/500], Train Loss: 0.2211\n",
      "Epoch [100/500], Train Loss: 0.2205\n",
      "Epoch [101/500], Train Loss: 0.2197\n",
      "Epoch [102/500], Train Loss: 0.2189\n",
      "Epoch [103/500], Train Loss: 0.2182\n",
      "Epoch [104/500], Train Loss: 0.2175\n",
      "Epoch [105/500], Train Loss: 0.2170\n",
      "Epoch [106/500], Train Loss: 0.2159\n",
      "Epoch [107/500], Train Loss: 0.2152\n",
      "Epoch [108/500], Train Loss: 0.2144\n",
      "Epoch [109/500], Train Loss: 0.2139\n",
      "Epoch [110/500], Train Loss: 0.2134\n",
      "Epoch [111/500], Train Loss: 0.2127\n",
      "Epoch [112/500], Train Loss: 0.2122\n",
      "Epoch [113/500], Train Loss: 0.2114\n",
      "Epoch [114/500], Train Loss: 0.2107\n",
      "Epoch [115/500], Train Loss: 0.2102\n",
      "Epoch [116/500], Train Loss: 0.2095\n",
      "Epoch [117/500], Train Loss: 0.2089\n",
      "Epoch [118/500], Train Loss: 0.2082\n",
      "Epoch [119/500], Train Loss: 0.2077\n",
      "Epoch [120/500], Train Loss: 0.2071\n",
      "Epoch [121/500], Train Loss: 0.2067\n",
      "Epoch [122/500], Train Loss: 0.2062\n",
      "Epoch [123/500], Train Loss: 0.2051\n",
      "Epoch [124/500], Train Loss: 0.2048\n",
      "Epoch [125/500], Train Loss: 0.2041\n",
      "Epoch [126/500], Train Loss: 0.2035\n",
      "Epoch [127/500], Train Loss: 0.2031\n",
      "Epoch [128/500], Train Loss: 0.2027\n",
      "Epoch [129/500], Train Loss: 0.2022\n",
      "Epoch [130/500], Train Loss: 0.2016\n",
      "Epoch [131/500], Train Loss: 0.2011\n",
      "Epoch [132/500], Train Loss: 0.2007\n",
      "Epoch [133/500], Train Loss: 0.2002\n",
      "Epoch [134/500], Train Loss: 0.1996\n",
      "Epoch [135/500], Train Loss: 0.1991\n",
      "Epoch [136/500], Train Loss: 0.1987\n",
      "Epoch [137/500], Train Loss: 0.1980\n",
      "Epoch [138/500], Train Loss: 0.1977\n",
      "Epoch [139/500], Train Loss: 0.1973\n",
      "Epoch [140/500], Train Loss: 0.1971\n",
      "Epoch [141/500], Train Loss: 0.1963\n",
      "Epoch [142/500], Train Loss: 0.1958\n",
      "Epoch [143/500], Train Loss: 0.1955\n",
      "Epoch [144/500], Train Loss: 0.1950\n",
      "Epoch [145/500], Train Loss: 0.1946\n",
      "Epoch [146/500], Train Loss: 0.1940\n",
      "Epoch [147/500], Train Loss: 0.1938\n",
      "Epoch [148/500], Train Loss: 0.1931\n",
      "Epoch [149/500], Train Loss: 0.1929\n",
      "Epoch [150/500], Train Loss: 0.1926\n",
      "Epoch [151/500], Train Loss: 0.1923\n",
      "Epoch [152/500], Train Loss: 0.1918\n",
      "Epoch [153/500], Train Loss: 0.1912\n",
      "Epoch [154/500], Train Loss: 0.1910\n",
      "Epoch [155/500], Train Loss: 0.1905\n",
      "Epoch [156/500], Train Loss: 0.1904\n",
      "Epoch [157/500], Train Loss: 0.1898\n",
      "Epoch [158/500], Train Loss: 0.1896\n",
      "Epoch [159/500], Train Loss: 0.1892\n",
      "Epoch [160/500], Train Loss: 0.1888\n",
      "Epoch [161/500], Train Loss: 0.1886\n",
      "Epoch [162/500], Train Loss: 0.1879\n",
      "Epoch [163/500], Train Loss: 0.1879\n",
      "Epoch [164/500], Train Loss: 0.1873\n",
      "Epoch [165/500], Train Loss: 0.1868\n",
      "Epoch [166/500], Train Loss: 0.1866\n",
      "Epoch [167/500], Train Loss: 0.1863\n",
      "Epoch [168/500], Train Loss: 0.1859\n",
      "Epoch [169/500], Train Loss: 0.1858\n",
      "Epoch [170/500], Train Loss: 0.1855\n",
      "Epoch [171/500], Train Loss: 0.1850\n",
      "Epoch [172/500], Train Loss: 0.1846\n",
      "Epoch [173/500], Train Loss: 0.1845\n",
      "Epoch [174/500], Train Loss: 0.1842\n",
      "Epoch [175/500], Train Loss: 0.1837\n",
      "Epoch [176/500], Train Loss: 0.1837\n",
      "Epoch [177/500], Train Loss: 0.1831\n",
      "Epoch [178/500], Train Loss: 0.1828\n",
      "Epoch [179/500], Train Loss: 0.1827\n",
      "Epoch [180/500], Train Loss: 0.1822\n",
      "Epoch [181/500], Train Loss: 0.1820\n",
      "Epoch [182/500], Train Loss: 0.1818\n",
      "Epoch [183/500], Train Loss: 0.1814\n",
      "Epoch [184/500], Train Loss: 0.1813\n",
      "Epoch [185/500], Train Loss: 0.1808\n",
      "Epoch [186/500], Train Loss: 0.1807\n",
      "Epoch [187/500], Train Loss: 0.1803\n",
      "Epoch [188/500], Train Loss: 0.1801\n",
      "Epoch [189/500], Train Loss: 0.1796\n",
      "Epoch [190/500], Train Loss: 0.1795\n",
      "Epoch [191/500], Train Loss: 0.1791\n",
      "Epoch [192/500], Train Loss: 0.1789\n",
      "Epoch [193/500], Train Loss: 0.1785\n",
      "Epoch [194/500], Train Loss: 0.1784\n",
      "Epoch [195/500], Train Loss: 0.1781\n",
      "Epoch [196/500], Train Loss: 0.1777\n",
      "Epoch [197/500], Train Loss: 0.1777\n",
      "Epoch [198/500], Train Loss: 0.1774\n",
      "Epoch [199/500], Train Loss: 0.1774\n",
      "Epoch [200/500], Train Loss: 0.1771\n",
      "Epoch [201/500], Train Loss: 0.1768\n",
      "Epoch [202/500], Train Loss: 0.1764\n",
      "Epoch [203/500], Train Loss: 0.1761\n",
      "Epoch [204/500], Train Loss: 0.1761\n",
      "Epoch [205/500], Train Loss: 0.1755\n",
      "Epoch [206/500], Train Loss: 0.1755\n",
      "Epoch [207/500], Train Loss: 0.1752\n",
      "Epoch [208/500], Train Loss: 0.1751\n",
      "Epoch [209/500], Train Loss: 0.1748\n",
      "Epoch [210/500], Train Loss: 0.1749\n",
      "Epoch [211/500], Train Loss: 0.1746\n",
      "Epoch [212/500], Train Loss: 0.1743\n",
      "Epoch [213/500], Train Loss: 0.1739\n",
      "Epoch [214/500], Train Loss: 0.1737\n",
      "Epoch [215/500], Train Loss: 0.1735\n",
      "Epoch [216/500], Train Loss: 0.1733\n",
      "Epoch [217/500], Train Loss: 0.1732\n",
      "Epoch [218/500], Train Loss: 0.1731\n",
      "Epoch [219/500], Train Loss: 0.1727\n",
      "Epoch [220/500], Train Loss: 0.1725\n",
      "Epoch [221/500], Train Loss: 0.1724\n",
      "Epoch [222/500], Train Loss: 0.1722\n",
      "Epoch [223/500], Train Loss: 0.1719\n",
      "Epoch [224/500], Train Loss: 0.1717\n",
      "Epoch [225/500], Train Loss: 0.1717\n",
      "Epoch [226/500], Train Loss: 0.1714\n",
      "Epoch [227/500], Train Loss: 0.1712\n",
      "Epoch [228/500], Train Loss: 0.1710\n",
      "Epoch [229/500], Train Loss: 0.1711\n",
      "Epoch [230/500], Train Loss: 0.1710\n",
      "Epoch [231/500], Train Loss: 0.1704\n",
      "Epoch [232/500], Train Loss: 0.1704\n",
      "Epoch [233/500], Train Loss: 0.1703\n",
      "Epoch [234/500], Train Loss: 0.1700\n",
      "Epoch [235/500], Train Loss: 0.1696\n",
      "Epoch [236/500], Train Loss: 0.1696\n",
      "Epoch [237/500], Train Loss: 0.1694\n",
      "Epoch [238/500], Train Loss: 0.1693\n",
      "Epoch [239/500], Train Loss: 0.1691\n",
      "Epoch [240/500], Train Loss: 0.1689\n",
      "Epoch [241/500], Train Loss: 0.1688\n",
      "Epoch [242/500], Train Loss: 0.1687\n",
      "Epoch [243/500], Train Loss: 0.1684\n",
      "Epoch [244/500], Train Loss: 0.1682\n",
      "Epoch [245/500], Train Loss: 0.1680\n",
      "Epoch [246/500], Train Loss: 0.1678\n",
      "Epoch [247/500], Train Loss: 0.1677\n",
      "Epoch [248/500], Train Loss: 0.1676\n",
      "Epoch [249/500], Train Loss: 0.1676\n",
      "Epoch [250/500], Train Loss: 0.1671\n",
      "Epoch [251/500], Train Loss: 0.1670\n",
      "Epoch [252/500], Train Loss: 0.1668\n",
      "Epoch [253/500], Train Loss: 0.1668\n",
      "Epoch [254/500], Train Loss: 0.1667\n",
      "Epoch [255/500], Train Loss: 0.1666\n",
      "Epoch [256/500], Train Loss: 0.1665\n",
      "Epoch [257/500], Train Loss: 0.1662\n",
      "Epoch [258/500], Train Loss: 0.1659\n",
      "Epoch [259/500], Train Loss: 0.1660\n",
      "Epoch [260/500], Train Loss: 0.1659\n",
      "Epoch [261/500], Train Loss: 0.1655\n",
      "Epoch [262/500], Train Loss: 0.1656\n",
      "Epoch [263/500], Train Loss: 0.1654\n",
      "Epoch [264/500], Train Loss: 0.1651\n",
      "Epoch [265/500], Train Loss: 0.1651\n",
      "Epoch [266/500], Train Loss: 0.1649\n",
      "Epoch [267/500], Train Loss: 0.1648\n",
      "Epoch [268/500], Train Loss: 0.1645\n",
      "Epoch [269/500], Train Loss: 0.1644\n",
      "Epoch [270/500], Train Loss: 0.1647\n",
      "Epoch [271/500], Train Loss: 0.1642\n",
      "Epoch [272/500], Train Loss: 0.1641\n",
      "Epoch [273/500], Train Loss: 0.1640\n",
      "Epoch [274/500], Train Loss: 0.1638\n",
      "Epoch [275/500], Train Loss: 0.1637\n",
      "Epoch [276/500], Train Loss: 0.1637\n",
      "Epoch [277/500], Train Loss: 0.1635\n",
      "Epoch [278/500], Train Loss: 0.1633\n",
      "Epoch [279/500], Train Loss: 0.1632\n",
      "Epoch [280/500], Train Loss: 0.1632\n",
      "Epoch [281/500], Train Loss: 0.1630\n",
      "Epoch [282/500], Train Loss: 0.1628\n",
      "Epoch [283/500], Train Loss: 0.1628\n",
      "Epoch [284/500], Train Loss: 0.1624\n",
      "Epoch [285/500], Train Loss: 0.1625\n",
      "Epoch [286/500], Train Loss: 0.1623\n",
      "Epoch [287/500], Train Loss: 0.1621\n",
      "Epoch [288/500], Train Loss: 0.1619\n",
      "Epoch [289/500], Train Loss: 0.1621\n",
      "Epoch [290/500], Train Loss: 0.1619\n",
      "Epoch [291/500], Train Loss: 0.1617\n",
      "Epoch [292/500], Train Loss: 0.1615\n",
      "Epoch [293/500], Train Loss: 0.1615\n",
      "Epoch [294/500], Train Loss: 0.1613\n",
      "Epoch [295/500], Train Loss: 0.1612\n",
      "Epoch [296/500], Train Loss: 0.1611\n",
      "Epoch [297/500], Train Loss: 0.1610\n",
      "Epoch [298/500], Train Loss: 0.1609\n",
      "Epoch [299/500], Train Loss: 0.1608\n",
      "Epoch [300/500], Train Loss: 0.1606\n",
      "Epoch [301/500], Train Loss: 0.1608\n",
      "Epoch [302/500], Train Loss: 0.1605\n",
      "Epoch [303/500], Train Loss: 0.1604\n",
      "Epoch [304/500], Train Loss: 0.1602\n",
      "Epoch [305/500], Train Loss: 0.1603\n",
      "Epoch [306/500], Train Loss: 0.1601\n",
      "Epoch [307/500], Train Loss: 0.1600\n",
      "Epoch [308/500], Train Loss: 0.1598\n",
      "Epoch [309/500], Train Loss: 0.1597\n",
      "Epoch [310/500], Train Loss: 0.1594\n",
      "Epoch [311/500], Train Loss: 0.1595\n",
      "Epoch [312/500], Train Loss: 0.1597\n",
      "Epoch [313/500], Train Loss: 0.1592\n",
      "Epoch [314/500], Train Loss: 0.1592\n",
      "Epoch [315/500], Train Loss: 0.1592\n",
      "Epoch [316/500], Train Loss: 0.1590\n",
      "Epoch [317/500], Train Loss: 0.1588\n",
      "Epoch [318/500], Train Loss: 0.1589\n",
      "Epoch [319/500], Train Loss: 0.1587\n",
      "Epoch [320/500], Train Loss: 0.1587\n",
      "Epoch [321/500], Train Loss: 0.1585\n",
      "Epoch [322/500], Train Loss: 0.1585\n",
      "Epoch [323/500], Train Loss: 0.1582\n",
      "Epoch [324/500], Train Loss: 0.1583\n",
      "Epoch [325/500], Train Loss: 0.1580\n",
      "Epoch [326/500], Train Loss: 0.1580\n",
      "Epoch [327/500], Train Loss: 0.1579\n",
      "Epoch [328/500], Train Loss: 0.1578\n",
      "Epoch [329/500], Train Loss: 0.1577\n",
      "Epoch [330/500], Train Loss: 0.1577\n",
      "Epoch [331/500], Train Loss: 0.1575\n",
      "Epoch [332/500], Train Loss: 0.1575\n",
      "Epoch [333/500], Train Loss: 0.1573\n",
      "Epoch [334/500], Train Loss: 0.1575\n",
      "Epoch [335/500], Train Loss: 0.1572\n",
      "Epoch [336/500], Train Loss: 0.1571\n",
      "Epoch [337/500], Train Loss: 0.1570\n",
      "Epoch [338/500], Train Loss: 0.1570\n",
      "Epoch [339/500], Train Loss: 0.1569\n",
      "Epoch [340/500], Train Loss: 0.1567\n",
      "Epoch [341/500], Train Loss: 0.1566\n",
      "Epoch [342/500], Train Loss: 0.1565\n",
      "Epoch [343/500], Train Loss: 0.1564\n",
      "Epoch [344/500], Train Loss: 0.1563\n",
      "Epoch [345/500], Train Loss: 0.1563\n",
      "Epoch [346/500], Train Loss: 0.1561\n",
      "Epoch [347/500], Train Loss: 0.1561\n",
      "Epoch [348/500], Train Loss: 0.1561\n",
      "Epoch [349/500], Train Loss: 0.1560\n",
      "Epoch [350/500], Train Loss: 0.1558\n",
      "Epoch [351/500], Train Loss: 0.1560\n",
      "Epoch [352/500], Train Loss: 0.1557\n",
      "Epoch [353/500], Train Loss: 0.1555\n",
      "Epoch [354/500], Train Loss: 0.1554\n",
      "Epoch [355/500], Train Loss: 0.1554\n",
      "Epoch [356/500], Train Loss: 0.1554\n",
      "Epoch [357/500], Train Loss: 0.1554\n",
      "Epoch [358/500], Train Loss: 0.1554\n",
      "Epoch [359/500], Train Loss: 0.1552\n",
      "Epoch [360/500], Train Loss: 0.1553\n",
      "Epoch [361/500], Train Loss: 0.1552\n",
      "Epoch [362/500], Train Loss: 0.1550\n",
      "Epoch [363/500], Train Loss: 0.1549\n",
      "Epoch [364/500], Train Loss: 0.1548\n",
      "Epoch [365/500], Train Loss: 0.1547\n",
      "Epoch [366/500], Train Loss: 0.1550\n",
      "Epoch [367/500], Train Loss: 0.1546\n",
      "Epoch [368/500], Train Loss: 0.1545\n",
      "Epoch [369/500], Train Loss: 0.1544\n",
      "Epoch [370/500], Train Loss: 0.1542\n",
      "Epoch [371/500], Train Loss: 0.1545\n",
      "Epoch [372/500], Train Loss: 0.1542\n",
      "Epoch [373/500], Train Loss: 0.1541\n",
      "Epoch [374/500], Train Loss: 0.1540\n",
      "Epoch [375/500], Train Loss: 0.1540\n",
      "Epoch [376/500], Train Loss: 0.1538\n",
      "Epoch [377/500], Train Loss: 0.1537\n",
      "Epoch [378/500], Train Loss: 0.1538\n",
      "Epoch [379/500], Train Loss: 0.1536\n",
      "Epoch [380/500], Train Loss: 0.1536\n",
      "Epoch [381/500], Train Loss: 0.1533\n",
      "Epoch [382/500], Train Loss: 0.1536\n",
      "Epoch [383/500], Train Loss: 0.1533\n",
      "Epoch [384/500], Train Loss: 0.1534\n",
      "Epoch [385/500], Train Loss: 0.1536\n",
      "Epoch [386/500], Train Loss: 0.1531\n",
      "Epoch [387/500], Train Loss: 0.1531\n",
      "Epoch [388/500], Train Loss: 0.1532\n",
      "Epoch [389/500], Train Loss: 0.1532\n",
      "Epoch [390/500], Train Loss: 0.1530\n",
      "Epoch [391/500], Train Loss: 0.1529\n",
      "Epoch [392/500], Train Loss: 0.1526\n",
      "Epoch [393/500], Train Loss: 0.1527\n",
      "Epoch [394/500], Train Loss: 0.1528\n",
      "Epoch [395/500], Train Loss: 0.1523\n",
      "Epoch [396/500], Train Loss: 0.1526\n",
      "Epoch [397/500], Train Loss: 0.1524\n",
      "Epoch [398/500], Train Loss: 0.1524\n",
      "Epoch [399/500], Train Loss: 0.1524\n",
      "Epoch [400/500], Train Loss: 0.1522\n",
      "Epoch [401/500], Train Loss: 0.1522\n",
      "Epoch [402/500], Train Loss: 0.1520\n",
      "Epoch [403/500], Train Loss: 0.1522\n",
      "Epoch [404/500], Train Loss: 0.1519\n",
      "Epoch [405/500], Train Loss: 0.1520\n",
      "Epoch [406/500], Train Loss: 0.1521\n",
      "Epoch [407/500], Train Loss: 0.1517\n",
      "Epoch [408/500], Train Loss: 0.1519\n",
      "Epoch [409/500], Train Loss: 0.1517\n",
      "Epoch [410/500], Train Loss: 0.1516\n",
      "Epoch [411/500], Train Loss: 0.1514\n",
      "Epoch [412/500], Train Loss: 0.1515\n",
      "Epoch [413/500], Train Loss: 0.1514\n",
      "Epoch [414/500], Train Loss: 0.1514\n",
      "Epoch [415/500], Train Loss: 0.1514\n",
      "Epoch [416/500], Train Loss: 0.1515\n",
      "Epoch [417/500], Train Loss: 0.1513\n",
      "Epoch [418/500], Train Loss: 0.1512\n",
      "Epoch [419/500], Train Loss: 0.1510\n",
      "Epoch [420/500], Train Loss: 0.1512\n",
      "Epoch [421/500], Train Loss: 0.1511\n",
      "Epoch [422/500], Train Loss: 0.1508\n",
      "Epoch [423/500], Train Loss: 0.1509\n",
      "Epoch [424/500], Train Loss: 0.1509\n",
      "Epoch [425/500], Train Loss: 0.1507\n",
      "Epoch [426/500], Train Loss: 0.1506\n",
      "Epoch [427/500], Train Loss: 0.1506\n",
      "Epoch [428/500], Train Loss: 0.1503\n",
      "Epoch [429/500], Train Loss: 0.1505\n",
      "Epoch [430/500], Train Loss: 0.1503\n",
      "Epoch [431/500], Train Loss: 0.1503\n",
      "Epoch [432/500], Train Loss: 0.1503\n",
      "Epoch [433/500], Train Loss: 0.1503\n",
      "Epoch [434/500], Train Loss: 0.1504\n",
      "Epoch [435/500], Train Loss: 0.1500\n",
      "Epoch [436/500], Train Loss: 0.1500\n",
      "Epoch [437/500], Train Loss: 0.1503\n",
      "Epoch [438/500], Train Loss: 0.1499\n",
      "Epoch [439/500], Train Loss: 0.1498\n",
      "Epoch [440/500], Train Loss: 0.1501\n",
      "Epoch [441/500], Train Loss: 0.1499\n",
      "Epoch [442/500], Train Loss: 0.1497\n",
      "Epoch [443/500], Train Loss: 0.1495\n",
      "Epoch [444/500], Train Loss: 0.1495\n",
      "Epoch [445/500], Train Loss: 0.1496\n",
      "Epoch [446/500], Train Loss: 0.1494\n",
      "Epoch [447/500], Train Loss: 0.1494\n",
      "Epoch [448/500], Train Loss: 0.1494\n",
      "Epoch [449/500], Train Loss: 0.1493\n",
      "Epoch [450/500], Train Loss: 0.1494\n",
      "Epoch [451/500], Train Loss: 0.1493\n",
      "Epoch [452/500], Train Loss: 0.1493\n",
      "Epoch [453/500], Train Loss: 0.1492\n",
      "Epoch [454/500], Train Loss: 0.1493\n",
      "Epoch [455/500], Train Loss: 0.1491\n",
      "Epoch [456/500], Train Loss: 0.1490\n",
      "Epoch [457/500], Train Loss: 0.1490\n",
      "Epoch [458/500], Train Loss: 0.1489\n",
      "Epoch [459/500], Train Loss: 0.1489\n",
      "Epoch [460/500], Train Loss: 0.1487\n",
      "Epoch [461/500], Train Loss: 0.1487\n",
      "Epoch [462/500], Train Loss: 0.1487\n",
      "Epoch [463/500], Train Loss: 0.1488\n",
      "Epoch [464/500], Train Loss: 0.1485\n",
      "Epoch [465/500], Train Loss: 0.1486\n",
      "Epoch [466/500], Train Loss: 0.1485\n",
      "Epoch [467/500], Train Loss: 0.1485\n",
      "Epoch [468/500], Train Loss: 0.1485\n",
      "Epoch [469/500], Train Loss: 0.1484\n",
      "Epoch [470/500], Train Loss: 0.1483\n",
      "Epoch [471/500], Train Loss: 0.1482\n",
      "Epoch [472/500], Train Loss: 0.1482\n",
      "Epoch [473/500], Train Loss: 0.1482\n",
      "Epoch [474/500], Train Loss: 0.1484\n",
      "Epoch [475/500], Train Loss: 0.1480\n",
      "Epoch [476/500], Train Loss: 0.1484\n",
      "Epoch [477/500], Train Loss: 0.1479\n",
      "Epoch [478/500], Train Loss: 0.1479\n",
      "Epoch [479/500], Train Loss: 0.1479\n",
      "Epoch [480/500], Train Loss: 0.1479\n",
      "Epoch [481/500], Train Loss: 0.1477\n",
      "Epoch [482/500], Train Loss: 0.1478\n",
      "Epoch [483/500], Train Loss: 0.1477\n",
      "Epoch [484/500], Train Loss: 0.1478\n",
      "Epoch [485/500], Train Loss: 0.1477\n",
      "Epoch [486/500], Train Loss: 0.1475\n",
      "Epoch [487/500], Train Loss: 0.1474\n",
      "Epoch [488/500], Train Loss: 0.1476\n",
      "Epoch [489/500], Train Loss: 0.1478\n",
      "Epoch [490/500], Train Loss: 0.1472\n",
      "Epoch [491/500], Train Loss: 0.1475\n",
      "Epoch [492/500], Train Loss: 0.1472\n",
      "Epoch [493/500], Train Loss: 0.1475\n",
      "Epoch [494/500], Train Loss: 0.1473\n",
      "Epoch [495/500], Train Loss: 0.1471\n",
      "Epoch [496/500], Train Loss: 0.1471\n",
      "Epoch [497/500], Train Loss: 0.1470\n",
      "Epoch [498/500], Train Loss: 0.1470\n",
      "Epoch [499/500], Train Loss: 0.1471\n",
      "Epoch [500/500], Train Loss: 0.1471\n",
      "Final model saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/models_unphased/chr10/final_model_chr10.pth\n",
      "Individual R^2 scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr10/individual_r2_scores_chr10.csv\n",
      "Individual IQS scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr10/individual_iqs_scores_chr10.csv\n",
      "Individual AUC ROC curves saved in: ../../Data/model_results_unphased_all_PRS/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/performance_metrics.csv\n",
      "Total SNPs:  871\n",
      "PRS313 SNPs:  19\n",
      "Total SNPs used for Training:  852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-30 10:15:36,250] Trial 51 finished with value: 0.08456481844186783 and parameters: {'learning_rate': 0.010028189689892582, 'l1_coef': 1.0142119414700108e-05, 'patience': 5, 'batch_size': 256}. Best is trial 23 with value: 0.05782480022081963.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 11 - Best hyperparameters: {'learning_rate': 0.0002329404200995808, 'l1_coef': 1.2601501185021566e-05, 'patience': 9, 'batch_size': 32}\n",
      "Chr 11 - Best value: 0.0578\n",
      "Epoch [1/500], Train Loss: 0.5187\n",
      "Epoch [2/500], Train Loss: 0.4539\n",
      "Epoch [3/500], Train Loss: 0.4337\n",
      "Epoch [4/500], Train Loss: 0.4192\n",
      "Epoch [5/500], Train Loss: 0.4074\n",
      "Epoch [6/500], Train Loss: 0.3965\n",
      "Epoch [7/500], Train Loss: 0.3867\n",
      "Epoch [8/500], Train Loss: 0.3774\n",
      "Epoch [9/500], Train Loss: 0.3688\n",
      "Epoch [10/500], Train Loss: 0.3608\n",
      "Epoch [11/500], Train Loss: 0.3533\n",
      "Epoch [12/500], Train Loss: 0.3460\n",
      "Epoch [13/500], Train Loss: 0.3390\n",
      "Epoch [14/500], Train Loss: 0.3327\n",
      "Epoch [15/500], Train Loss: 0.3264\n",
      "Epoch [16/500], Train Loss: 0.3208\n",
      "Epoch [17/500], Train Loss: 0.3150\n",
      "Epoch [18/500], Train Loss: 0.3096\n",
      "Epoch [19/500], Train Loss: 0.3045\n",
      "Epoch [20/500], Train Loss: 0.2996\n",
      "Epoch [21/500], Train Loss: 0.2947\n",
      "Epoch [22/500], Train Loss: 0.2900\n",
      "Epoch [23/500], Train Loss: 0.2858\n",
      "Epoch [24/500], Train Loss: 0.2812\n",
      "Epoch [25/500], Train Loss: 0.2773\n",
      "Epoch [26/500], Train Loss: 0.2733\n",
      "Epoch [27/500], Train Loss: 0.2696\n",
      "Epoch [28/500], Train Loss: 0.2656\n",
      "Epoch [29/500], Train Loss: 0.2622\n",
      "Epoch [30/500], Train Loss: 0.2588\n",
      "Epoch [31/500], Train Loss: 0.2551\n",
      "Epoch [32/500], Train Loss: 0.2519\n",
      "Epoch [33/500], Train Loss: 0.2489\n",
      "Epoch [34/500], Train Loss: 0.2457\n",
      "Epoch [35/500], Train Loss: 0.2428\n",
      "Epoch [36/500], Train Loss: 0.2397\n",
      "Epoch [37/500], Train Loss: 0.2371\n",
      "Epoch [38/500], Train Loss: 0.2340\n",
      "Epoch [39/500], Train Loss: 0.2315\n",
      "Epoch [40/500], Train Loss: 0.2291\n",
      "Epoch [41/500], Train Loss: 0.2265\n",
      "Epoch [42/500], Train Loss: 0.2239\n",
      "Epoch [43/500], Train Loss: 0.2216\n",
      "Epoch [44/500], Train Loss: 0.2190\n",
      "Epoch [45/500], Train Loss: 0.2168\n",
      "Epoch [46/500], Train Loss: 0.2147\n",
      "Epoch [47/500], Train Loss: 0.2124\n",
      "Epoch [48/500], Train Loss: 0.2103\n",
      "Epoch [49/500], Train Loss: 0.2082\n",
      "Epoch [50/500], Train Loss: 0.2059\n",
      "Epoch [51/500], Train Loss: 0.2040\n",
      "Epoch [52/500], Train Loss: 0.2022\n",
      "Epoch [53/500], Train Loss: 0.2001\n",
      "Epoch [54/500], Train Loss: 0.1984\n",
      "Epoch [55/500], Train Loss: 0.1964\n",
      "Epoch [56/500], Train Loss: 0.1947\n",
      "Epoch [57/500], Train Loss: 0.1928\n",
      "Epoch [58/500], Train Loss: 0.1914\n",
      "Epoch [59/500], Train Loss: 0.1893\n",
      "Epoch [60/500], Train Loss: 0.1876\n",
      "Epoch [61/500], Train Loss: 0.1862\n",
      "Epoch [62/500], Train Loss: 0.1845\n",
      "Epoch [63/500], Train Loss: 0.1830\n",
      "Epoch [64/500], Train Loss: 0.1814\n",
      "Epoch [65/500], Train Loss: 0.1797\n",
      "Epoch [66/500], Train Loss: 0.1786\n",
      "Epoch [67/500], Train Loss: 0.1769\n",
      "Epoch [68/500], Train Loss: 0.1755\n",
      "Epoch [69/500], Train Loss: 0.1742\n",
      "Epoch [70/500], Train Loss: 0.1724\n",
      "Epoch [71/500], Train Loss: 0.1713\n",
      "Epoch [72/500], Train Loss: 0.1700\n",
      "Epoch [73/500], Train Loss: 0.1684\n",
      "Epoch [74/500], Train Loss: 0.1673\n",
      "Epoch [75/500], Train Loss: 0.1662\n",
      "Epoch [76/500], Train Loss: 0.1649\n",
      "Epoch [77/500], Train Loss: 0.1636\n",
      "Epoch [78/500], Train Loss: 0.1624\n",
      "Epoch [79/500], Train Loss: 0.1614\n",
      "Epoch [80/500], Train Loss: 0.1601\n",
      "Epoch [81/500], Train Loss: 0.1589\n",
      "Epoch [82/500], Train Loss: 0.1576\n",
      "Epoch [83/500], Train Loss: 0.1567\n",
      "Epoch [84/500], Train Loss: 0.1556\n",
      "Epoch [85/500], Train Loss: 0.1545\n",
      "Epoch [86/500], Train Loss: 0.1535\n",
      "Epoch [87/500], Train Loss: 0.1524\n",
      "Epoch [88/500], Train Loss: 0.1513\n",
      "Epoch [89/500], Train Loss: 0.1503\n",
      "Epoch [90/500], Train Loss: 0.1494\n",
      "Epoch [91/500], Train Loss: 0.1485\n",
      "Epoch [92/500], Train Loss: 0.1472\n",
      "Epoch [93/500], Train Loss: 0.1465\n",
      "Epoch [94/500], Train Loss: 0.1454\n",
      "Epoch [95/500], Train Loss: 0.1444\n",
      "Epoch [96/500], Train Loss: 0.1436\n",
      "Epoch [97/500], Train Loss: 0.1427\n",
      "Epoch [98/500], Train Loss: 0.1418\n",
      "Epoch [99/500], Train Loss: 0.1407\n",
      "Epoch [100/500], Train Loss: 0.1399\n",
      "Epoch [101/500], Train Loss: 0.1392\n",
      "Epoch [102/500], Train Loss: 0.1386\n",
      "Epoch [103/500], Train Loss: 0.1376\n",
      "Epoch [104/500], Train Loss: 0.1367\n",
      "Epoch [105/500], Train Loss: 0.1358\n",
      "Epoch [106/500], Train Loss: 0.1352\n",
      "Epoch [107/500], Train Loss: 0.1342\n",
      "Epoch [108/500], Train Loss: 0.1339\n",
      "Epoch [109/500], Train Loss: 0.1328\n",
      "Epoch [110/500], Train Loss: 0.1322\n",
      "Epoch [111/500], Train Loss: 0.1312\n",
      "Epoch [112/500], Train Loss: 0.1305\n",
      "Epoch [113/500], Train Loss: 0.1300\n",
      "Epoch [114/500], Train Loss: 0.1291\n",
      "Epoch [115/500], Train Loss: 0.1285\n",
      "Epoch [116/500], Train Loss: 0.1278\n",
      "Epoch [117/500], Train Loss: 0.1271\n",
      "Epoch [118/500], Train Loss: 0.1263\n",
      "Epoch [119/500], Train Loss: 0.1256\n",
      "Epoch [120/500], Train Loss: 0.1251\n",
      "Epoch [121/500], Train Loss: 0.1245\n",
      "Epoch [122/500], Train Loss: 0.1240\n",
      "Epoch [123/500], Train Loss: 0.1231\n",
      "Epoch [124/500], Train Loss: 0.1223\n",
      "Epoch [125/500], Train Loss: 0.1218\n",
      "Epoch [126/500], Train Loss: 0.1213\n",
      "Epoch [127/500], Train Loss: 0.1207\n",
      "Epoch [128/500], Train Loss: 0.1199\n",
      "Epoch [129/500], Train Loss: 0.1195\n",
      "Epoch [130/500], Train Loss: 0.1188\n",
      "Epoch [131/500], Train Loss: 0.1184\n",
      "Epoch [132/500], Train Loss: 0.1180\n",
      "Epoch [133/500], Train Loss: 0.1173\n",
      "Epoch [134/500], Train Loss: 0.1165\n",
      "Epoch [135/500], Train Loss: 0.1161\n",
      "Epoch [136/500], Train Loss: 0.1157\n",
      "Epoch [137/500], Train Loss: 0.1151\n",
      "Epoch [138/500], Train Loss: 0.1145\n",
      "Epoch [139/500], Train Loss: 0.1139\n",
      "Epoch [140/500], Train Loss: 0.1134\n",
      "Epoch [141/500], Train Loss: 0.1129\n",
      "Epoch [142/500], Train Loss: 0.1124\n",
      "Epoch [143/500], Train Loss: 0.1119\n",
      "Epoch [144/500], Train Loss: 0.1115\n",
      "Epoch [145/500], Train Loss: 0.1108\n",
      "Epoch [146/500], Train Loss: 0.1104\n",
      "Epoch [147/500], Train Loss: 0.1101\n",
      "Epoch [148/500], Train Loss: 0.1095\n",
      "Epoch [149/500], Train Loss: 0.1090\n",
      "Epoch [150/500], Train Loss: 0.1085\n",
      "Epoch [151/500], Train Loss: 0.1081\n",
      "Epoch [152/500], Train Loss: 0.1075\n",
      "Epoch [153/500], Train Loss: 0.1072\n",
      "Epoch [154/500], Train Loss: 0.1068\n",
      "Epoch [155/500], Train Loss: 0.1063\n",
      "Epoch [156/500], Train Loss: 0.1057\n",
      "Epoch [157/500], Train Loss: 0.1054\n",
      "Epoch [158/500], Train Loss: 0.1051\n",
      "Epoch [159/500], Train Loss: 0.1046\n",
      "Epoch [160/500], Train Loss: 0.1042\n",
      "Epoch [161/500], Train Loss: 0.1036\n",
      "Epoch [162/500], Train Loss: 0.1033\n",
      "Epoch [163/500], Train Loss: 0.1029\n",
      "Epoch [164/500], Train Loss: 0.1024\n",
      "Epoch [165/500], Train Loss: 0.1021\n",
      "Epoch [166/500], Train Loss: 0.1018\n",
      "Epoch [167/500], Train Loss: 0.1013\n",
      "Epoch [168/500], Train Loss: 0.1010\n",
      "Epoch [169/500], Train Loss: 0.1006\n",
      "Epoch [170/500], Train Loss: 0.1001\n",
      "Epoch [171/500], Train Loss: 0.0998\n",
      "Epoch [172/500], Train Loss: 0.0996\n",
      "Epoch [173/500], Train Loss: 0.0990\n",
      "Epoch [174/500], Train Loss: 0.0989\n",
      "Epoch [175/500], Train Loss: 0.0984\n",
      "Epoch [176/500], Train Loss: 0.0981\n",
      "Epoch [177/500], Train Loss: 0.0979\n",
      "Epoch [178/500], Train Loss: 0.0973\n",
      "Epoch [179/500], Train Loss: 0.0972\n",
      "Epoch [180/500], Train Loss: 0.0967\n",
      "Epoch [181/500], Train Loss: 0.0963\n",
      "Epoch [182/500], Train Loss: 0.0960\n",
      "Epoch [183/500], Train Loss: 0.0955\n",
      "Epoch [184/500], Train Loss: 0.0952\n",
      "Epoch [185/500], Train Loss: 0.0952\n",
      "Epoch [186/500], Train Loss: 0.0948\n",
      "Epoch [187/500], Train Loss: 0.0945\n",
      "Epoch [188/500], Train Loss: 0.0941\n",
      "Epoch [189/500], Train Loss: 0.0938\n",
      "Epoch [190/500], Train Loss: 0.0935\n",
      "Epoch [191/500], Train Loss: 0.0933\n",
      "Epoch [192/500], Train Loss: 0.0929\n",
      "Epoch [193/500], Train Loss: 0.0926\n",
      "Epoch [194/500], Train Loss: 0.0921\n",
      "Epoch [195/500], Train Loss: 0.0919\n",
      "Epoch [196/500], Train Loss: 0.0916\n",
      "Epoch [197/500], Train Loss: 0.0913\n",
      "Epoch [198/500], Train Loss: 0.0909\n",
      "Epoch [199/500], Train Loss: 0.0907\n",
      "Epoch [200/500], Train Loss: 0.0906\n",
      "Epoch [201/500], Train Loss: 0.0903\n",
      "Epoch [202/500], Train Loss: 0.0900\n",
      "Epoch [203/500], Train Loss: 0.0898\n",
      "Epoch [204/500], Train Loss: 0.0895\n",
      "Epoch [205/500], Train Loss: 0.0892\n",
      "Epoch [206/500], Train Loss: 0.0888\n",
      "Epoch [207/500], Train Loss: 0.0887\n",
      "Epoch [208/500], Train Loss: 0.0883\n",
      "Epoch [209/500], Train Loss: 0.0881\n",
      "Epoch [210/500], Train Loss: 0.0878\n",
      "Epoch [211/500], Train Loss: 0.0876\n",
      "Epoch [212/500], Train Loss: 0.0873\n",
      "Epoch [213/500], Train Loss: 0.0870\n",
      "Epoch [214/500], Train Loss: 0.0868\n",
      "Epoch [215/500], Train Loss: 0.0866\n",
      "Epoch [216/500], Train Loss: 0.0863\n",
      "Epoch [217/500], Train Loss: 0.0861\n",
      "Epoch [218/500], Train Loss: 0.0859\n",
      "Epoch [219/500], Train Loss: 0.0855\n",
      "Epoch [220/500], Train Loss: 0.0854\n",
      "Epoch [221/500], Train Loss: 0.0850\n",
      "Epoch [222/500], Train Loss: 0.0850\n",
      "Epoch [223/500], Train Loss: 0.0847\n",
      "Epoch [224/500], Train Loss: 0.0845\n",
      "Epoch [225/500], Train Loss: 0.0844\n",
      "Epoch [226/500], Train Loss: 0.0841\n",
      "Epoch [227/500], Train Loss: 0.0836\n",
      "Epoch [228/500], Train Loss: 0.0836\n",
      "Epoch [229/500], Train Loss: 0.0834\n",
      "Epoch [230/500], Train Loss: 0.0831\n",
      "Epoch [231/500], Train Loss: 0.0830\n",
      "Epoch [232/500], Train Loss: 0.0827\n",
      "Epoch [233/500], Train Loss: 0.0824\n",
      "Epoch [234/500], Train Loss: 0.0824\n",
      "Epoch [235/500], Train Loss: 0.0820\n",
      "Epoch [236/500], Train Loss: 0.0818\n",
      "Epoch [237/500], Train Loss: 0.0817\n",
      "Epoch [238/500], Train Loss: 0.0814\n",
      "Epoch [239/500], Train Loss: 0.0812\n",
      "Epoch [240/500], Train Loss: 0.0812\n",
      "Epoch [241/500], Train Loss: 0.0808\n",
      "Epoch [242/500], Train Loss: 0.0806\n",
      "Epoch [243/500], Train Loss: 0.0804\n",
      "Epoch [244/500], Train Loss: 0.0802\n",
      "Epoch [245/500], Train Loss: 0.0800\n",
      "Epoch [246/500], Train Loss: 0.0798\n",
      "Epoch [247/500], Train Loss: 0.0796\n",
      "Epoch [248/500], Train Loss: 0.0793\n",
      "Epoch [249/500], Train Loss: 0.0793\n",
      "Epoch [250/500], Train Loss: 0.0793\n",
      "Epoch [251/500], Train Loss: 0.0790\n",
      "Epoch [252/500], Train Loss: 0.0787\n",
      "Epoch [253/500], Train Loss: 0.0786\n",
      "Epoch [254/500], Train Loss: 0.0784\n",
      "Epoch [255/500], Train Loss: 0.0782\n",
      "Epoch [256/500], Train Loss: 0.0779\n",
      "Epoch [257/500], Train Loss: 0.0778\n",
      "Epoch [258/500], Train Loss: 0.0776\n",
      "Epoch [259/500], Train Loss: 0.0775\n",
      "Epoch [260/500], Train Loss: 0.0774\n",
      "Epoch [261/500], Train Loss: 0.0772\n",
      "Epoch [262/500], Train Loss: 0.0770\n",
      "Epoch [263/500], Train Loss: 0.0767\n",
      "Epoch [264/500], Train Loss: 0.0766\n",
      "Epoch [265/500], Train Loss: 0.0767\n",
      "Epoch [266/500], Train Loss: 0.0764\n",
      "Epoch [267/500], Train Loss: 0.0762\n",
      "Epoch [268/500], Train Loss: 0.0760\n",
      "Epoch [269/500], Train Loss: 0.0759\n",
      "Epoch [270/500], Train Loss: 0.0756\n",
      "Epoch [271/500], Train Loss: 0.0756\n",
      "Epoch [272/500], Train Loss: 0.0753\n",
      "Epoch [273/500], Train Loss: 0.0752\n",
      "Epoch [274/500], Train Loss: 0.0749\n",
      "Epoch [275/500], Train Loss: 0.0748\n",
      "Epoch [276/500], Train Loss: 0.0747\n",
      "Epoch [277/500], Train Loss: 0.0745\n",
      "Epoch [278/500], Train Loss: 0.0744\n",
      "Epoch [279/500], Train Loss: 0.0743\n",
      "Epoch [280/500], Train Loss: 0.0742\n",
      "Epoch [281/500], Train Loss: 0.0740\n",
      "Epoch [282/500], Train Loss: 0.0740\n",
      "Epoch [283/500], Train Loss: 0.0738\n",
      "Epoch [284/500], Train Loss: 0.0736\n",
      "Epoch [285/500], Train Loss: 0.0734\n",
      "Epoch [286/500], Train Loss: 0.0732\n",
      "Epoch [287/500], Train Loss: 0.0731\n",
      "Epoch [288/500], Train Loss: 0.0730\n",
      "Epoch [289/500], Train Loss: 0.0727\n",
      "Epoch [290/500], Train Loss: 0.0730\n",
      "Epoch [291/500], Train Loss: 0.0725\n",
      "Epoch [292/500], Train Loss: 0.0725\n",
      "Epoch [293/500], Train Loss: 0.0722\n",
      "Epoch [294/500], Train Loss: 0.0722\n",
      "Epoch [295/500], Train Loss: 0.0719\n",
      "Epoch [296/500], Train Loss: 0.0719\n",
      "Epoch [297/500], Train Loss: 0.0717\n",
      "Epoch [298/500], Train Loss: 0.0717\n",
      "Epoch [299/500], Train Loss: 0.0715\n",
      "Epoch [300/500], Train Loss: 0.0713\n",
      "Epoch [301/500], Train Loss: 0.0714\n",
      "Epoch [302/500], Train Loss: 0.0710\n",
      "Epoch [303/500], Train Loss: 0.0710\n",
      "Epoch [304/500], Train Loss: 0.0709\n",
      "Epoch [305/500], Train Loss: 0.0707\n",
      "Epoch [306/500], Train Loss: 0.0706\n",
      "Epoch [307/500], Train Loss: 0.0705\n",
      "Epoch [308/500], Train Loss: 0.0704\n",
      "Epoch [309/500], Train Loss: 0.0703\n",
      "Epoch [310/500], Train Loss: 0.0702\n",
      "Epoch [311/500], Train Loss: 0.0701\n",
      "Epoch [312/500], Train Loss: 0.0698\n",
      "Epoch [313/500], Train Loss: 0.0697\n",
      "Epoch [314/500], Train Loss: 0.0696\n",
      "Epoch [315/500], Train Loss: 0.0697\n",
      "Epoch [316/500], Train Loss: 0.0694\n",
      "Epoch [317/500], Train Loss: 0.0694\n",
      "Epoch [318/500], Train Loss: 0.0692\n",
      "Epoch [319/500], Train Loss: 0.0691\n",
      "Epoch [320/500], Train Loss: 0.0689\n",
      "Epoch [321/500], Train Loss: 0.0689\n",
      "Epoch [322/500], Train Loss: 0.0688\n",
      "Epoch [323/500], Train Loss: 0.0686\n",
      "Epoch [324/500], Train Loss: 0.0686\n",
      "Epoch [325/500], Train Loss: 0.0684\n",
      "Epoch [326/500], Train Loss: 0.0682\n",
      "Epoch [327/500], Train Loss: 0.0683\n",
      "Epoch [328/500], Train Loss: 0.0679\n",
      "Epoch [329/500], Train Loss: 0.0679\n",
      "Epoch [330/500], Train Loss: 0.0680\n",
      "Epoch [331/500], Train Loss: 0.0678\n",
      "Epoch [332/500], Train Loss: 0.0676\n",
      "Epoch [333/500], Train Loss: 0.0676\n",
      "Epoch [334/500], Train Loss: 0.0675\n",
      "Epoch [335/500], Train Loss: 0.0673\n",
      "Epoch [336/500], Train Loss: 0.0672\n",
      "Epoch [337/500], Train Loss: 0.0671\n",
      "Epoch [338/500], Train Loss: 0.0670\n",
      "Epoch [339/500], Train Loss: 0.0668\n",
      "Epoch [340/500], Train Loss: 0.0669\n",
      "Epoch [341/500], Train Loss: 0.0666\n",
      "Epoch [342/500], Train Loss: 0.0668\n",
      "Epoch [343/500], Train Loss: 0.0665\n",
      "Epoch [344/500], Train Loss: 0.0664\n",
      "Epoch [345/500], Train Loss: 0.0662\n",
      "Epoch [346/500], Train Loss: 0.0662\n",
      "Epoch [347/500], Train Loss: 0.0661\n",
      "Epoch [348/500], Train Loss: 0.0660\n",
      "Epoch [349/500], Train Loss: 0.0660\n",
      "Epoch [350/500], Train Loss: 0.0659\n",
      "Epoch [351/500], Train Loss: 0.0658\n",
      "Epoch [352/500], Train Loss: 0.0656\n",
      "Epoch [353/500], Train Loss: 0.0656\n",
      "Epoch [354/500], Train Loss: 0.0655\n",
      "Epoch [355/500], Train Loss: 0.0655\n",
      "Epoch [356/500], Train Loss: 0.0653\n",
      "Epoch [357/500], Train Loss: 0.0652\n",
      "Epoch [358/500], Train Loss: 0.0651\n",
      "Epoch [359/500], Train Loss: 0.0651\n",
      "Epoch [360/500], Train Loss: 0.0649\n",
      "Epoch [361/500], Train Loss: 0.0649\n",
      "Epoch [362/500], Train Loss: 0.0647\n",
      "Epoch [363/500], Train Loss: 0.0647\n",
      "Epoch [364/500], Train Loss: 0.0646\n",
      "Epoch [365/500], Train Loss: 0.0644\n",
      "Epoch [366/500], Train Loss: 0.0644\n",
      "Epoch [367/500], Train Loss: 0.0643\n",
      "Epoch [368/500], Train Loss: 0.0642\n",
      "Epoch [369/500], Train Loss: 0.0641\n",
      "Epoch [370/500], Train Loss: 0.0639\n",
      "Epoch [371/500], Train Loss: 0.0640\n",
      "Epoch [372/500], Train Loss: 0.0638\n",
      "Epoch [373/500], Train Loss: 0.0638\n",
      "Epoch [374/500], Train Loss: 0.0637\n",
      "Epoch [375/500], Train Loss: 0.0637\n",
      "Epoch [376/500], Train Loss: 0.0637\n",
      "Epoch [377/500], Train Loss: 0.0635\n",
      "Epoch [378/500], Train Loss: 0.0634\n",
      "Epoch [379/500], Train Loss: 0.0632\n",
      "Epoch [380/500], Train Loss: 0.0631\n",
      "Epoch [381/500], Train Loss: 0.0632\n",
      "Epoch [382/500], Train Loss: 0.0631\n",
      "Epoch [383/500], Train Loss: 0.0630\n",
      "Epoch [384/500], Train Loss: 0.0630\n",
      "Epoch [385/500], Train Loss: 0.0629\n",
      "Epoch [386/500], Train Loss: 0.0627\n",
      "Epoch [387/500], Train Loss: 0.0627\n",
      "Epoch [388/500], Train Loss: 0.0626\n",
      "Epoch [389/500], Train Loss: 0.0624\n",
      "Epoch [390/500], Train Loss: 0.0624\n",
      "Epoch [391/500], Train Loss: 0.0625\n",
      "Epoch [392/500], Train Loss: 0.0623\n",
      "Epoch [393/500], Train Loss: 0.0622\n",
      "Epoch [394/500], Train Loss: 0.0622\n",
      "Epoch [395/500], Train Loss: 0.0621\n",
      "Epoch [396/500], Train Loss: 0.0620\n",
      "Epoch [397/500], Train Loss: 0.0619\n",
      "Epoch [398/500], Train Loss: 0.0619\n",
      "Epoch [399/500], Train Loss: 0.0617\n",
      "Epoch [400/500], Train Loss: 0.0617\n",
      "Epoch [401/500], Train Loss: 0.0617\n",
      "Epoch [402/500], Train Loss: 0.0617\n",
      "Epoch [403/500], Train Loss: 0.0615\n",
      "Epoch [404/500], Train Loss: 0.0614\n",
      "Epoch [405/500], Train Loss: 0.0614\n",
      "Epoch [406/500], Train Loss: 0.0612\n",
      "Epoch [407/500], Train Loss: 0.0613\n",
      "Epoch [408/500], Train Loss: 0.0611\n",
      "Epoch [409/500], Train Loss: 0.0610\n",
      "Epoch [410/500], Train Loss: 0.0610\n",
      "Epoch [411/500], Train Loss: 0.0610\n",
      "Epoch [412/500], Train Loss: 0.0609\n",
      "Epoch [413/500], Train Loss: 0.0609\n",
      "Epoch [414/500], Train Loss: 0.0608\n",
      "Epoch [415/500], Train Loss: 0.0606\n",
      "Epoch [416/500], Train Loss: 0.0607\n",
      "Epoch [417/500], Train Loss: 0.0606\n",
      "Epoch [418/500], Train Loss: 0.0604\n",
      "Epoch [419/500], Train Loss: 0.0606\n",
      "Epoch [420/500], Train Loss: 0.0604\n",
      "Epoch [421/500], Train Loss: 0.0602\n",
      "Epoch [422/500], Train Loss: 0.0603\n",
      "Epoch [423/500], Train Loss: 0.0602\n",
      "Epoch [424/500], Train Loss: 0.0601\n",
      "Epoch [425/500], Train Loss: 0.0601\n",
      "Epoch [426/500], Train Loss: 0.0599\n",
      "Epoch [427/500], Train Loss: 0.0599\n",
      "Epoch [428/500], Train Loss: 0.0598\n",
      "Epoch [429/500], Train Loss: 0.0600\n",
      "Epoch [430/500], Train Loss: 0.0597\n",
      "Epoch [431/500], Train Loss: 0.0596\n",
      "Epoch [432/500], Train Loss: 0.0595\n",
      "Epoch [433/500], Train Loss: 0.0595\n",
      "Epoch [434/500], Train Loss: 0.0594\n",
      "Epoch [435/500], Train Loss: 0.0595\n",
      "Epoch [436/500], Train Loss: 0.0593\n",
      "Epoch [437/500], Train Loss: 0.0593\n",
      "Epoch [438/500], Train Loss: 0.0594\n",
      "Epoch [439/500], Train Loss: 0.0591\n",
      "Epoch [440/500], Train Loss: 0.0591\n",
      "Epoch [441/500], Train Loss: 0.0590\n",
      "Epoch [442/500], Train Loss: 0.0590\n",
      "Epoch [443/500], Train Loss: 0.0589\n",
      "Epoch [444/500], Train Loss: 0.0589\n",
      "Epoch [445/500], Train Loss: 0.0588\n",
      "Epoch [446/500], Train Loss: 0.0588\n",
      "Epoch [447/500], Train Loss: 0.0587\n",
      "Epoch [448/500], Train Loss: 0.0587\n",
      "Epoch [449/500], Train Loss: 0.0586\n",
      "Epoch [450/500], Train Loss: 0.0586\n",
      "Epoch [451/500], Train Loss: 0.0586\n",
      "Epoch [452/500], Train Loss: 0.0584\n",
      "Epoch [453/500], Train Loss: 0.0584\n",
      "Epoch [454/500], Train Loss: 0.0582\n",
      "Epoch [455/500], Train Loss: 0.0582\n",
      "Epoch [456/500], Train Loss: 0.0582\n",
      "Epoch [457/500], Train Loss: 0.0581\n",
      "Epoch [458/500], Train Loss: 0.0581\n",
      "Epoch [459/500], Train Loss: 0.0580\n",
      "Epoch [460/500], Train Loss: 0.0581\n",
      "Epoch [461/500], Train Loss: 0.0579\n",
      "Epoch [462/500], Train Loss: 0.0578\n",
      "Epoch [463/500], Train Loss: 0.0579\n",
      "Epoch [464/500], Train Loss: 0.0577\n",
      "Epoch [465/500], Train Loss: 0.0578\n",
      "Epoch [466/500], Train Loss: 0.0578\n",
      "Epoch [467/500], Train Loss: 0.0576\n",
      "Epoch [468/500], Train Loss: 0.0576\n",
      "Epoch [469/500], Train Loss: 0.0575\n",
      "Epoch [470/500], Train Loss: 0.0575\n",
      "Epoch [471/500], Train Loss: 0.0574\n",
      "Epoch [472/500], Train Loss: 0.0573\n",
      "Epoch [473/500], Train Loss: 0.0573\n",
      "Epoch [474/500], Train Loss: 0.0573\n",
      "Epoch [475/500], Train Loss: 0.0572\n",
      "Epoch [476/500], Train Loss: 0.0571\n",
      "Epoch [477/500], Train Loss: 0.0571\n",
      "Epoch [478/500], Train Loss: 0.0570\n",
      "Epoch [479/500], Train Loss: 0.0570\n",
      "Epoch [480/500], Train Loss: 0.0570\n",
      "Epoch [481/500], Train Loss: 0.0569\n",
      "Epoch [482/500], Train Loss: 0.0570\n",
      "Epoch [483/500], Train Loss: 0.0569\n",
      "Epoch [484/500], Train Loss: 0.0568\n",
      "Epoch [485/500], Train Loss: 0.0568\n",
      "Epoch [486/500], Train Loss: 0.0568\n",
      "Epoch [487/500], Train Loss: 0.0566\n",
      "Epoch [488/500], Train Loss: 0.0566\n",
      "Epoch [489/500], Train Loss: 0.0565\n",
      "Epoch [490/500], Train Loss: 0.0564\n",
      "Epoch [491/500], Train Loss: 0.0564\n",
      "Epoch [492/500], Train Loss: 0.0564\n",
      "Epoch [493/500], Train Loss: 0.0563\n",
      "Epoch [494/500], Train Loss: 0.0563\n",
      "Epoch [495/500], Train Loss: 0.0562\n",
      "Epoch [496/500], Train Loss: 0.0562\n",
      "Epoch [497/500], Train Loss: 0.0561\n",
      "Epoch [498/500], Train Loss: 0.0561\n",
      "Epoch [499/500], Train Loss: 0.0560\n",
      "Epoch [500/500], Train Loss: 0.0560\n",
      "Final model saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/models_unphased/chr11/final_model_chr11.pth\n",
      "Individual R^2 scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr11/individual_r2_scores_chr11.csv\n",
      "Individual IQS scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr11/individual_iqs_scores_chr11.csv\n",
      "Individual AUC ROC curves saved in: ../../Data/model_results_unphased_all_PRS/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/performance_metrics.csv\n",
      "Total SNPs:  630\n",
      "PRS313 SNPs:  17\n",
      "Total SNPs used for Training:  613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-04-30 10:15:54,609] Trial 51 finished with value: 0.5704960245352525 and parameters: {'learning_rate': 0.01954404097687797, 'l1_coef': 0.07575838536636692, 'patience': 10, 'batch_size': 32}. Best is trial 29 with value: 0.05914152525365353.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 12 - Best hyperparameters: {'learning_rate': 0.013368341073967381, 'l1_coef': 1.0168876770412278e-05, 'patience': 9, 'batch_size': 128}\n",
      "Chr 12 - Best value: 0.0591\n",
      "Epoch [1/500], Train Loss: 0.5868\n",
      "Epoch [2/500], Train Loss: 0.4398\n",
      "Epoch [3/500], Train Loss: 0.3777\n",
      "Epoch [4/500], Train Loss: 0.3438\n",
      "Epoch [5/500], Train Loss: 0.3232\n",
      "Epoch [6/500], Train Loss: 0.3042\n",
      "Epoch [7/500], Train Loss: 0.2893\n",
      "Epoch [8/500], Train Loss: 0.2795\n",
      "Epoch [9/500], Train Loss: 0.2699\n",
      "Epoch [10/500], Train Loss: 0.2642\n",
      "Epoch [11/500], Train Loss: 0.2541\n",
      "Epoch [12/500], Train Loss: 0.2484\n",
      "Epoch [13/500], Train Loss: 0.2422\n",
      "Epoch [14/500], Train Loss: 0.2385\n",
      "Epoch [15/500], Train Loss: 0.2337\n",
      "Epoch [16/500], Train Loss: 0.2293\n",
      "Epoch [17/500], Train Loss: 0.2281\n",
      "Epoch [18/500], Train Loss: 0.2246\n",
      "Epoch [19/500], Train Loss: 0.2214\n",
      "Epoch [20/500], Train Loss: 0.2177\n",
      "Epoch [21/500], Train Loss: 0.2155\n",
      "Epoch [22/500], Train Loss: 0.2126\n",
      "Epoch [23/500], Train Loss: 0.2117\n",
      "Epoch [24/500], Train Loss: 0.2096\n",
      "Epoch [25/500], Train Loss: 0.2097\n",
      "Epoch [26/500], Train Loss: 0.2056\n",
      "Epoch [27/500], Train Loss: 0.2034\n",
      "Epoch [28/500], Train Loss: 0.2018\n",
      "Epoch [29/500], Train Loss: 0.2020\n",
      "Epoch [30/500], Train Loss: 0.2007\n",
      "Epoch [31/500], Train Loss: 0.2021\n",
      "Epoch [32/500], Train Loss: 0.1984\n",
      "Epoch [33/500], Train Loss: 0.1976\n",
      "Epoch [34/500], Train Loss: 0.1948\n",
      "Epoch [35/500], Train Loss: 0.1961\n",
      "Epoch [36/500], Train Loss: 0.1944\n",
      "Epoch [37/500], Train Loss: 0.1924\n",
      "Epoch [38/500], Train Loss: 0.1915\n",
      "Epoch [39/500], Train Loss: 0.1926\n",
      "Epoch [40/500], Train Loss: 0.1895\n",
      "Epoch [41/500], Train Loss: 0.1874\n",
      "Epoch [42/500], Train Loss: 0.1886\n",
      "Epoch [43/500], Train Loss: 0.1909\n",
      "Epoch [44/500], Train Loss: 0.1889\n",
      "Epoch [45/500], Train Loss: 0.1863\n",
      "Epoch [46/500], Train Loss: 0.1852\n",
      "Epoch [47/500], Train Loss: 0.1850\n",
      "Epoch [48/500], Train Loss: 0.1860\n",
      "Epoch [49/500], Train Loss: 0.1873\n",
      "Epoch [50/500], Train Loss: 0.1846\n",
      "Epoch [51/500], Train Loss: 0.1822\n",
      "Epoch [52/500], Train Loss: 0.1848\n",
      "Epoch [53/500], Train Loss: 0.1828\n",
      "Epoch [54/500], Train Loss: 0.1828\n",
      "Epoch [55/500], Train Loss: 0.1850\n",
      "Epoch [56/500], Train Loss: 0.1867\n",
      "Epoch [57/500], Train Loss: 0.1826\n",
      "Epoch [58/500], Train Loss: 0.1748\n",
      "Epoch [59/500], Train Loss: 0.1735\n",
      "Epoch [60/500], Train Loss: 0.1732\n",
      "Epoch [61/500], Train Loss: 0.1732\n",
      "Epoch [62/500], Train Loss: 0.1730\n",
      "Epoch [63/500], Train Loss: 0.1729\n",
      "Epoch [64/500], Train Loss: 0.1732\n",
      "Epoch [65/500], Train Loss: 0.1729\n",
      "Epoch [66/500], Train Loss: 0.1730\n",
      "Epoch [67/500], Train Loss: 0.1729\n",
      "Epoch [68/500], Train Loss: 0.1725\n",
      "Epoch [69/500], Train Loss: 0.1730\n",
      "Epoch [70/500], Train Loss: 0.1727\n",
      "Epoch [71/500], Train Loss: 0.1726\n",
      "Epoch [72/500], Train Loss: 0.1725\n",
      "Epoch [73/500], Train Loss: 0.1726\n",
      "Epoch [74/500], Train Loss: 0.1725\n",
      "Epoch [75/500], Train Loss: 0.1724\n",
      "Epoch [76/500], Train Loss: 0.1723\n",
      "Epoch [77/500], Train Loss: 0.1722\n",
      "Epoch [78/500], Train Loss: 0.1723\n",
      "Epoch [79/500], Train Loss: 0.1721\n",
      "Epoch [80/500], Train Loss: 0.1723\n",
      "Epoch [81/500], Train Loss: 0.1722\n",
      "Epoch [82/500], Train Loss: 0.1726\n",
      "Epoch [83/500], Train Loss: 0.1723\n",
      "Epoch [84/500], Train Loss: 0.1724\n",
      "Epoch [85/500], Train Loss: 0.1723\n",
      "Epoch [86/500], Train Loss: 0.1715\n",
      "Epoch [87/500], Train Loss: 0.1710\n",
      "Epoch [88/500], Train Loss: 0.1710\n",
      "Epoch [89/500], Train Loss: 0.1712\n",
      "Epoch [90/500], Train Loss: 0.1715\n",
      "Epoch [91/500], Train Loss: 0.1714\n",
      "Epoch [92/500], Train Loss: 0.1711\n",
      "Epoch [93/500], Train Loss: 0.1712\n",
      "Epoch [94/500], Train Loss: 0.1713\n",
      "Epoch [95/500], Train Loss: 0.1710\n",
      "Epoch [96/500], Train Loss: 0.1710\n",
      "Epoch [97/500], Train Loss: 0.1711\n",
      "Epoch [98/500], Train Loss: 0.1713\n",
      "Epoch [99/500], Train Loss: 0.1713\n",
      "Epoch [100/500], Train Loss: 0.1711\n",
      "Epoch [101/500], Train Loss: 0.1710\n",
      "Epoch [102/500], Train Loss: 0.1712\n",
      "Epoch [103/500], Train Loss: 0.1713\n",
      "Epoch [104/500], Train Loss: 0.1712\n",
      "Early stopping at epoch 104\n",
      "Final model saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/models_unphased/chr12/final_model_chr12.pth\n",
      "Individual R^2 scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr12/individual_r2_scores_chr12.csv\n",
      "Individual IQS scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr12/individual_iqs_scores_chr12.csv\n",
      "Individual AUC ROC curves saved in: ../../Data/model_results_unphased_all_PRS/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/performance_metrics.csv\n",
      "Total SNPs:  110\n",
      "PRS313 SNPs:  5\n",
      "Total SNPs used for Training:  105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-30 10:15:57,691] Trial 51 finished with value: 0.10817819599594389 and parameters: {'learning_rate': 0.037741773677323495, 'l1_coef': 1.7889572827328185e-05, 'patience': 12, 'batch_size': 64}. Best is trial 51 with value: 0.10817819599594389.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 13 - Best hyperparameters: {'learning_rate': 0.037741773677323495, 'l1_coef': 1.7889572827328185e-05, 'patience': 12, 'batch_size': 64}\n",
      "Chr 13 - Best value: 0.1082\n",
      "Epoch [1/500], Train Loss: 0.2751\n",
      "Epoch [2/500], Train Loss: 0.1735\n",
      "Epoch [3/500], Train Loss: 0.1582\n",
      "Epoch [4/500], Train Loss: 0.1513\n",
      "Epoch [5/500], Train Loss: 0.1365\n",
      "Epoch [6/500], Train Loss: 0.1307\n",
      "Epoch [7/500], Train Loss: 0.1266\n",
      "Epoch [8/500], Train Loss: 0.1233\n",
      "Epoch [9/500], Train Loss: 0.1209\n",
      "Epoch [10/500], Train Loss: 0.1228\n",
      "Epoch [11/500], Train Loss: 0.1212\n",
      "Epoch [12/500], Train Loss: 0.1159\n",
      "Epoch [13/500], Train Loss: 0.1232\n",
      "Epoch [14/500], Train Loss: 0.1184\n",
      "Epoch [15/500], Train Loss: 0.1159\n",
      "Epoch [16/500], Train Loss: 0.1152\n",
      "Epoch [17/500], Train Loss: 0.1141\n",
      "Epoch [18/500], Train Loss: 0.1142\n",
      "Epoch [19/500], Train Loss: 0.1187\n",
      "Epoch [20/500], Train Loss: 0.1180\n",
      "Epoch [21/500], Train Loss: 0.1137\n",
      "Epoch [22/500], Train Loss: 0.1121\n",
      "Epoch [23/500], Train Loss: 0.1126\n",
      "Epoch [24/500], Train Loss: 0.1136\n",
      "Epoch [25/500], Train Loss: 0.1158\n",
      "Epoch [26/500], Train Loss: 0.1125\n",
      "Epoch [27/500], Train Loss: 0.1127\n",
      "Epoch [28/500], Train Loss: 0.1124\n",
      "Epoch [29/500], Train Loss: 0.1070\n",
      "Epoch [30/500], Train Loss: 0.1060\n",
      "Epoch [31/500], Train Loss: 0.1063\n",
      "Epoch [32/500], Train Loss: 0.1060\n",
      "Epoch [33/500], Train Loss: 0.1073\n",
      "Epoch [34/500], Train Loss: 0.1063\n",
      "Epoch [35/500], Train Loss: 0.1067\n",
      "Epoch [36/500], Train Loss: 0.1050\n",
      "Epoch [37/500], Train Loss: 0.1057\n",
      "Epoch [38/500], Train Loss: 0.1061\n",
      "Epoch [39/500], Train Loss: 0.1076\n",
      "Epoch [40/500], Train Loss: 0.1063\n",
      "Epoch [41/500], Train Loss: 0.1049\n",
      "Epoch [42/500], Train Loss: 0.1049\n",
      "Epoch [43/500], Train Loss: 0.1050\n",
      "Epoch [44/500], Train Loss: 0.1051\n",
      "Epoch [45/500], Train Loss: 0.1047\n",
      "Epoch [46/500], Train Loss: 0.1063\n",
      "Epoch [47/500], Train Loss: 0.1045\n",
      "Epoch [48/500], Train Loss: 0.1059\n",
      "Epoch [49/500], Train Loss: 0.1059\n",
      "Epoch [50/500], Train Loss: 0.1059\n",
      "Epoch [51/500], Train Loss: 0.1058\n",
      "Epoch [52/500], Train Loss: 0.1042\n",
      "Epoch [53/500], Train Loss: 0.1055\n",
      "Epoch [54/500], Train Loss: 0.1055\n",
      "Epoch [55/500], Train Loss: 0.1062\n",
      "Epoch [56/500], Train Loss: 0.1050\n",
      "Epoch [57/500], Train Loss: 0.1050\n",
      "Epoch [58/500], Train Loss: 0.1054\n",
      "Epoch [59/500], Train Loss: 0.1055\n",
      "Epoch [60/500], Train Loss: 0.1051\n",
      "Epoch [61/500], Train Loss: 0.1045\n",
      "Epoch [62/500], Train Loss: 0.1035\n",
      "Epoch [63/500], Train Loss: 0.1051\n",
      "Epoch [64/500], Train Loss: 0.1045\n",
      "Epoch [65/500], Train Loss: 0.1045\n",
      "Epoch [66/500], Train Loss: 0.1036\n",
      "Epoch [67/500], Train Loss: 0.1042\n",
      "Epoch [68/500], Train Loss: 0.1057\n",
      "Epoch [69/500], Train Loss: 0.1038\n",
      "Epoch [70/500], Train Loss: 0.1050\n",
      "Epoch [71/500], Train Loss: 0.1034\n",
      "Epoch [72/500], Train Loss: 0.1037\n",
      "Epoch [73/500], Train Loss: 0.1039\n",
      "Epoch [74/500], Train Loss: 0.1032\n",
      "Epoch [75/500], Train Loss: 0.1059\n",
      "Epoch [76/500], Train Loss: 0.1034\n",
      "Epoch [77/500], Train Loss: 0.1033\n",
      "Epoch [78/500], Train Loss: 0.1037\n",
      "Epoch [79/500], Train Loss: 0.1042\n",
      "Epoch [80/500], Train Loss: 0.1032\n",
      "Epoch [81/500], Train Loss: 0.1048\n",
      "Epoch [82/500], Train Loss: 0.1036\n",
      "Epoch [83/500], Train Loss: 0.1043\n",
      "Epoch [84/500], Train Loss: 0.1041\n",
      "Epoch [85/500], Train Loss: 0.1041\n",
      "Epoch [86/500], Train Loss: 0.1038\n",
      "Early stopping at epoch 86\n",
      "Final model saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/models_unphased/chr13/final_model_chr13.pth\n",
      "Individual R^2 scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr13/individual_r2_scores_chr13.csv\n",
      "Individual IQS scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr13/individual_iqs_scores_chr13.csv\n",
      "Individual AUC ROC curves saved in: ../../Data/model_results_unphased_all_PRS/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/performance_metrics.csv\n",
      "Total SNPs:  164\n",
      "PRS313 SNPs:  8\n",
      "Total SNPs used for Training:  156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-30 10:15:59,973] Trial 51 finished with value: 0.2314489848911762 and parameters: {'learning_rate': 0.09224445915934029, 'l1_coef': 1.4485510348257023e-05, 'patience': 18, 'batch_size': 128}. Best is trial 36 with value: 0.07342382765242031.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 14 - Best hyperparameters: {'learning_rate': 0.04900486554871739, 'l1_coef': 1.160024455652371e-05, 'patience': 20, 'batch_size': 64}\n",
      "Chr 14 - Best value: 0.0734\n",
      "Epoch [1/500], Train Loss: 0.4748\n",
      "Epoch [2/500], Train Loss: 0.3071\n",
      "Epoch [3/500], Train Loss: 0.2859\n",
      "Epoch [4/500], Train Loss: 0.2684\n",
      "Epoch [5/500], Train Loss: 0.2610\n",
      "Epoch [6/500], Train Loss: 0.2629\n",
      "Epoch [7/500], Train Loss: 0.2560\n",
      "Epoch [8/500], Train Loss: 0.2585\n",
      "Epoch [9/500], Train Loss: 0.2524\n",
      "Epoch [10/500], Train Loss: 0.2487\n",
      "Epoch [11/500], Train Loss: 0.2505\n",
      "Epoch [12/500], Train Loss: 0.2492\n",
      "Epoch [13/500], Train Loss: 0.2445\n",
      "Epoch [14/500], Train Loss: 0.2521\n",
      "Epoch [15/500], Train Loss: 0.2490\n",
      "Epoch [16/500], Train Loss: 0.2439\n",
      "Epoch [17/500], Train Loss: 0.2529\n",
      "Epoch [18/500], Train Loss: 0.2416\n",
      "Epoch [19/500], Train Loss: 0.2475\n",
      "Epoch [20/500], Train Loss: 0.2469\n",
      "Epoch [21/500], Train Loss: 0.2424\n",
      "Epoch [22/500], Train Loss: 0.2506\n",
      "Epoch [23/500], Train Loss: 0.2476\n",
      "Epoch [24/500], Train Loss: 0.2396\n",
      "Epoch [25/500], Train Loss: 0.2476\n",
      "Epoch [26/500], Train Loss: 0.2470\n",
      "Epoch [27/500], Train Loss: 0.2411\n",
      "Epoch [28/500], Train Loss: 0.2534\n",
      "Epoch [29/500], Train Loss: 0.2595\n",
      "Epoch [30/500], Train Loss: 0.2462\n",
      "Epoch [31/500], Train Loss: 0.2259\n",
      "Epoch [32/500], Train Loss: 0.2222\n",
      "Epoch [33/500], Train Loss: 0.2223\n",
      "Epoch [34/500], Train Loss: 0.2200\n",
      "Epoch [35/500], Train Loss: 0.2202\n",
      "Epoch [36/500], Train Loss: 0.2199\n",
      "Epoch [37/500], Train Loss: 0.2206\n",
      "Epoch [38/500], Train Loss: 0.2217\n",
      "Epoch [39/500], Train Loss: 0.2221\n",
      "Epoch [40/500], Train Loss: 0.2217\n",
      "Epoch [41/500], Train Loss: 0.2187\n",
      "Epoch [42/500], Train Loss: 0.2206\n",
      "Epoch [43/500], Train Loss: 0.2203\n",
      "Epoch [44/500], Train Loss: 0.2209\n",
      "Epoch [45/500], Train Loss: 0.2184\n",
      "Epoch [46/500], Train Loss: 0.2198\n",
      "Epoch [47/500], Train Loss: 0.2229\n",
      "Epoch [48/500], Train Loss: 0.2224\n",
      "Epoch [49/500], Train Loss: 0.2194\n",
      "Epoch [50/500], Train Loss: 0.2192\n",
      "Epoch [51/500], Train Loss: 0.2186\n",
      "Epoch [52/500], Train Loss: 0.2210\n",
      "Epoch [53/500], Train Loss: 0.2190\n",
      "Epoch [54/500], Train Loss: 0.2173\n",
      "Epoch [55/500], Train Loss: 0.2191\n",
      "Epoch [56/500], Train Loss: 0.2171\n",
      "Epoch [57/500], Train Loss: 0.2203\n",
      "Epoch [58/500], Train Loss: 0.2165\n",
      "Epoch [59/500], Train Loss: 0.2188\n",
      "Epoch [60/500], Train Loss: 0.2173\n",
      "Epoch [61/500], Train Loss: 0.2171\n",
      "Epoch [62/500], Train Loss: 0.2168\n",
      "Epoch [63/500], Train Loss: 0.2183\n",
      "Epoch [64/500], Train Loss: 0.2168\n",
      "Epoch [65/500], Train Loss: 0.2166\n",
      "Epoch [66/500], Train Loss: 0.2165\n",
      "Epoch [67/500], Train Loss: 0.2170\n",
      "Epoch [68/500], Train Loss: 0.2176\n",
      "Epoch [69/500], Train Loss: 0.2169\n",
      "Epoch [70/500], Train Loss: 0.2171\n",
      "Epoch [71/500], Train Loss: 0.2166\n",
      "Epoch [72/500], Train Loss: 0.2175\n",
      "Epoch [73/500], Train Loss: 0.2178\n",
      "Epoch [74/500], Train Loss: 0.2178\n",
      "Epoch [75/500], Train Loss: 0.2181\n",
      "Epoch [76/500], Train Loss: 0.2178\n",
      "Epoch [77/500], Train Loss: 0.2171\n",
      "Epoch [78/500], Train Loss: 0.2171\n",
      "Early stopping at epoch 78\n",
      "Final model saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/models_unphased/chr14/final_model_chr14.pth\n",
      "Individual R^2 scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr14/individual_r2_scores_chr14.csv\n",
      "Individual IQS scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr14/individual_iqs_scores_chr14.csv\n",
      "Individual AUC ROC curves saved in: ../../Data/model_results_unphased_all_PRS/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/performance_metrics.csv\n",
      "Total SNPs:  197\n",
      "PRS313 SNPs:  7\n",
      "Total SNPs used for Training:  190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-30 10:16:02,921] Trial 51 finished with value: 0.17873024745629382 and parameters: {'learning_rate': 0.005160721839857431, 'l1_coef': 5.685922594991786e-05, 'patience': 9, 'batch_size': 32}. Best is trial 7 with value: 0.07289292319462849.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 15 - Best hyperparameters: {'learning_rate': 0.00031046985754389273, 'l1_coef': 1.7067190785696914e-05, 'patience': 5, 'batch_size': 32}\n",
      "Chr 15 - Best value: 0.0729\n",
      "Epoch [1/500], Train Loss: 0.5827\n",
      "Epoch [2/500], Train Loss: 0.4825\n",
      "Epoch [3/500], Train Loss: 0.4524\n",
      "Epoch [4/500], Train Loss: 0.4352\n",
      "Epoch [5/500], Train Loss: 0.4220\n",
      "Epoch [6/500], Train Loss: 0.4113\n",
      "Epoch [7/500], Train Loss: 0.4029\n",
      "Epoch [8/500], Train Loss: 0.3940\n",
      "Epoch [9/500], Train Loss: 0.3864\n",
      "Epoch [10/500], Train Loss: 0.3789\n",
      "Epoch [11/500], Train Loss: 0.3723\n",
      "Epoch [12/500], Train Loss: 0.3661\n",
      "Epoch [13/500], Train Loss: 0.3604\n",
      "Epoch [14/500], Train Loss: 0.3548\n",
      "Epoch [15/500], Train Loss: 0.3492\n",
      "Epoch [16/500], Train Loss: 0.3446\n",
      "Epoch [17/500], Train Loss: 0.3401\n",
      "Epoch [18/500], Train Loss: 0.3354\n",
      "Epoch [19/500], Train Loss: 0.3307\n",
      "Epoch [20/500], Train Loss: 0.3272\n",
      "Epoch [21/500], Train Loss: 0.3227\n",
      "Epoch [22/500], Train Loss: 0.3190\n",
      "Epoch [23/500], Train Loss: 0.3151\n",
      "Epoch [24/500], Train Loss: 0.3122\n",
      "Epoch [25/500], Train Loss: 0.3084\n",
      "Epoch [26/500], Train Loss: 0.3058\n",
      "Epoch [27/500], Train Loss: 0.3021\n",
      "Epoch [28/500], Train Loss: 0.2990\n",
      "Epoch [29/500], Train Loss: 0.2966\n",
      "Epoch [30/500], Train Loss: 0.2933\n",
      "Epoch [31/500], Train Loss: 0.2910\n",
      "Epoch [32/500], Train Loss: 0.2881\n",
      "Epoch [33/500], Train Loss: 0.2855\n",
      "Epoch [34/500], Train Loss: 0.2827\n",
      "Epoch [35/500], Train Loss: 0.2807\n",
      "Epoch [36/500], Train Loss: 0.2783\n",
      "Epoch [37/500], Train Loss: 0.2761\n",
      "Epoch [38/500], Train Loss: 0.2742\n",
      "Epoch [39/500], Train Loss: 0.2719\n",
      "Epoch [40/500], Train Loss: 0.2696\n",
      "Epoch [41/500], Train Loss: 0.2681\n",
      "Epoch [42/500], Train Loss: 0.2661\n",
      "Epoch [43/500], Train Loss: 0.2640\n",
      "Epoch [44/500], Train Loss: 0.2622\n",
      "Epoch [45/500], Train Loss: 0.2611\n",
      "Epoch [46/500], Train Loss: 0.2595\n",
      "Epoch [47/500], Train Loss: 0.2573\n",
      "Epoch [48/500], Train Loss: 0.2556\n",
      "Epoch [49/500], Train Loss: 0.2540\n",
      "Epoch [50/500], Train Loss: 0.2528\n",
      "Epoch [51/500], Train Loss: 0.2510\n",
      "Epoch [52/500], Train Loss: 0.2498\n",
      "Epoch [53/500], Train Loss: 0.2483\n",
      "Epoch [54/500], Train Loss: 0.2466\n",
      "Epoch [55/500], Train Loss: 0.2460\n",
      "Epoch [56/500], Train Loss: 0.2440\n",
      "Epoch [57/500], Train Loss: 0.2429\n",
      "Epoch [58/500], Train Loss: 0.2412\n",
      "Epoch [59/500], Train Loss: 0.2403\n",
      "Epoch [60/500], Train Loss: 0.2389\n",
      "Epoch [61/500], Train Loss: 0.2378\n",
      "Epoch [62/500], Train Loss: 0.2367\n",
      "Epoch [63/500], Train Loss: 0.2352\n",
      "Epoch [64/500], Train Loss: 0.2347\n",
      "Epoch [65/500], Train Loss: 0.2337\n",
      "Epoch [66/500], Train Loss: 0.2326\n",
      "Epoch [67/500], Train Loss: 0.2310\n",
      "Epoch [68/500], Train Loss: 0.2302\n",
      "Epoch [69/500], Train Loss: 0.2294\n",
      "Epoch [70/500], Train Loss: 0.2285\n",
      "Epoch [71/500], Train Loss: 0.2275\n",
      "Epoch [72/500], Train Loss: 0.2265\n",
      "Epoch [73/500], Train Loss: 0.2256\n",
      "Epoch [74/500], Train Loss: 0.2248\n",
      "Epoch [75/500], Train Loss: 0.2239\n",
      "Epoch [76/500], Train Loss: 0.2229\n",
      "Epoch [77/500], Train Loss: 0.2218\n",
      "Epoch [78/500], Train Loss: 0.2210\n",
      "Epoch [79/500], Train Loss: 0.2205\n",
      "Epoch [80/500], Train Loss: 0.2194\n",
      "Epoch [81/500], Train Loss: 0.2195\n",
      "Epoch [82/500], Train Loss: 0.2183\n",
      "Epoch [83/500], Train Loss: 0.2172\n",
      "Epoch [84/500], Train Loss: 0.2165\n",
      "Epoch [85/500], Train Loss: 0.2157\n",
      "Epoch [86/500], Train Loss: 0.2150\n",
      "Epoch [87/500], Train Loss: 0.2144\n",
      "Epoch [88/500], Train Loss: 0.2136\n",
      "Epoch [89/500], Train Loss: 0.2127\n",
      "Epoch [90/500], Train Loss: 0.2124\n",
      "Epoch [91/500], Train Loss: 0.2120\n",
      "Epoch [92/500], Train Loss: 0.2108\n",
      "Epoch [93/500], Train Loss: 0.2105\n",
      "Epoch [94/500], Train Loss: 0.2097\n",
      "Epoch [95/500], Train Loss: 0.2090\n",
      "Epoch [96/500], Train Loss: 0.2085\n",
      "Epoch [97/500], Train Loss: 0.2080\n",
      "Epoch [98/500], Train Loss: 0.2072\n",
      "Epoch [99/500], Train Loss: 0.2065\n",
      "Epoch [100/500], Train Loss: 0.2064\n",
      "Epoch [101/500], Train Loss: 0.2054\n",
      "Epoch [102/500], Train Loss: 0.2052\n",
      "Epoch [103/500], Train Loss: 0.2044\n",
      "Epoch [104/500], Train Loss: 0.2037\n",
      "Epoch [105/500], Train Loss: 0.2030\n",
      "Epoch [106/500], Train Loss: 0.2030\n",
      "Epoch [107/500], Train Loss: 0.2029\n",
      "Epoch [108/500], Train Loss: 0.2023\n",
      "Epoch [109/500], Train Loss: 0.2015\n",
      "Epoch [110/500], Train Loss: 0.2006\n",
      "Epoch [111/500], Train Loss: 0.2002\n",
      "Epoch [112/500], Train Loss: 0.2001\n",
      "Epoch [113/500], Train Loss: 0.1997\n",
      "Epoch [114/500], Train Loss: 0.1990\n",
      "Epoch [115/500], Train Loss: 0.1986\n",
      "Epoch [116/500], Train Loss: 0.1981\n",
      "Epoch [117/500], Train Loss: 0.1977\n",
      "Epoch [118/500], Train Loss: 0.1971\n",
      "Epoch [119/500], Train Loss: 0.1968\n",
      "Epoch [120/500], Train Loss: 0.1963\n",
      "Epoch [121/500], Train Loss: 0.1960\n",
      "Epoch [122/500], Train Loss: 0.1959\n",
      "Epoch [123/500], Train Loss: 0.1955\n",
      "Epoch [124/500], Train Loss: 0.1948\n",
      "Epoch [125/500], Train Loss: 0.1945\n",
      "Epoch [126/500], Train Loss: 0.1939\n",
      "Epoch [127/500], Train Loss: 0.1939\n",
      "Epoch [128/500], Train Loss: 0.1930\n",
      "Epoch [129/500], Train Loss: 0.1927\n",
      "Epoch [130/500], Train Loss: 0.1925\n",
      "Epoch [131/500], Train Loss: 0.1918\n",
      "Epoch [132/500], Train Loss: 0.1918\n",
      "Epoch [133/500], Train Loss: 0.1913\n",
      "Epoch [134/500], Train Loss: 0.1911\n",
      "Epoch [135/500], Train Loss: 0.1906\n",
      "Epoch [136/500], Train Loss: 0.1903\n",
      "Epoch [137/500], Train Loss: 0.1901\n",
      "Epoch [138/500], Train Loss: 0.1897\n",
      "Epoch [139/500], Train Loss: 0.1896\n",
      "Epoch [140/500], Train Loss: 0.1894\n",
      "Epoch [141/500], Train Loss: 0.1888\n",
      "Epoch [142/500], Train Loss: 0.1884\n",
      "Epoch [143/500], Train Loss: 0.1879\n",
      "Epoch [144/500], Train Loss: 0.1876\n",
      "Epoch [145/500], Train Loss: 0.1875\n",
      "Epoch [146/500], Train Loss: 0.1873\n",
      "Epoch [147/500], Train Loss: 0.1867\n",
      "Epoch [148/500], Train Loss: 0.1864\n",
      "Epoch [149/500], Train Loss: 0.1860\n",
      "Epoch [150/500], Train Loss: 0.1860\n",
      "Epoch [151/500], Train Loss: 0.1855\n",
      "Epoch [152/500], Train Loss: 0.1854\n",
      "Epoch [153/500], Train Loss: 0.1851\n",
      "Epoch [154/500], Train Loss: 0.1848\n",
      "Epoch [155/500], Train Loss: 0.1848\n",
      "Epoch [156/500], Train Loss: 0.1843\n",
      "Epoch [157/500], Train Loss: 0.1838\n",
      "Epoch [158/500], Train Loss: 0.1835\n",
      "Epoch [159/500], Train Loss: 0.1839\n",
      "Epoch [160/500], Train Loss: 0.1834\n",
      "Epoch [161/500], Train Loss: 0.1832\n",
      "Epoch [162/500], Train Loss: 0.1827\n",
      "Epoch [163/500], Train Loss: 0.1827\n",
      "Epoch [164/500], Train Loss: 0.1825\n",
      "Epoch [165/500], Train Loss: 0.1819\n",
      "Epoch [166/500], Train Loss: 0.1820\n",
      "Epoch [167/500], Train Loss: 0.1817\n",
      "Epoch [168/500], Train Loss: 0.1817\n",
      "Epoch [169/500], Train Loss: 0.1809\n",
      "Epoch [170/500], Train Loss: 0.1806\n",
      "Epoch [171/500], Train Loss: 0.1805\n",
      "Epoch [172/500], Train Loss: 0.1807\n",
      "Epoch [173/500], Train Loss: 0.1803\n",
      "Epoch [174/500], Train Loss: 0.1798\n",
      "Epoch [175/500], Train Loss: 0.1800\n",
      "Epoch [176/500], Train Loss: 0.1795\n",
      "Epoch [177/500], Train Loss: 0.1794\n",
      "Epoch [178/500], Train Loss: 0.1791\n",
      "Epoch [179/500], Train Loss: 0.1788\n",
      "Epoch [180/500], Train Loss: 0.1785\n",
      "Epoch [181/500], Train Loss: 0.1783\n",
      "Epoch [182/500], Train Loss: 0.1782\n",
      "Epoch [183/500], Train Loss: 0.1782\n",
      "Epoch [184/500], Train Loss: 0.1778\n",
      "Epoch [185/500], Train Loss: 0.1777\n",
      "Epoch [186/500], Train Loss: 0.1775\n",
      "Epoch [187/500], Train Loss: 0.1773\n",
      "Epoch [188/500], Train Loss: 0.1773\n",
      "Epoch [189/500], Train Loss: 0.1770\n",
      "Epoch [190/500], Train Loss: 0.1771\n",
      "Epoch [191/500], Train Loss: 0.1767\n",
      "Epoch [192/500], Train Loss: 0.1763\n",
      "Epoch [193/500], Train Loss: 0.1763\n",
      "Epoch [194/500], Train Loss: 0.1758\n",
      "Epoch [195/500], Train Loss: 0.1758\n",
      "Epoch [196/500], Train Loss: 0.1759\n",
      "Epoch [197/500], Train Loss: 0.1754\n",
      "Epoch [198/500], Train Loss: 0.1752\n",
      "Epoch [199/500], Train Loss: 0.1754\n",
      "Epoch [200/500], Train Loss: 0.1749\n",
      "Epoch [201/500], Train Loss: 0.1749\n",
      "Epoch [202/500], Train Loss: 0.1744\n",
      "Epoch [203/500], Train Loss: 0.1741\n",
      "Epoch [204/500], Train Loss: 0.1748\n",
      "Epoch [205/500], Train Loss: 0.1746\n",
      "Epoch [206/500], Train Loss: 0.1738\n",
      "Epoch [207/500], Train Loss: 0.1737\n",
      "Epoch [208/500], Train Loss: 0.1740\n",
      "Epoch [209/500], Train Loss: 0.1736\n",
      "Epoch [210/500], Train Loss: 0.1735\n",
      "Epoch [211/500], Train Loss: 0.1731\n",
      "Epoch [212/500], Train Loss: 0.1731\n",
      "Epoch [213/500], Train Loss: 0.1734\n",
      "Epoch [214/500], Train Loss: 0.1726\n",
      "Epoch [215/500], Train Loss: 0.1728\n",
      "Epoch [216/500], Train Loss: 0.1730\n",
      "Epoch [217/500], Train Loss: 0.1722\n",
      "Epoch [218/500], Train Loss: 0.1722\n",
      "Epoch [219/500], Train Loss: 0.1721\n",
      "Epoch [220/500], Train Loss: 0.1719\n",
      "Epoch [221/500], Train Loss: 0.1717\n",
      "Epoch [222/500], Train Loss: 0.1720\n",
      "Epoch [223/500], Train Loss: 0.1720\n",
      "Epoch [224/500], Train Loss: 0.1719\n",
      "Epoch [225/500], Train Loss: 0.1715\n",
      "Epoch [226/500], Train Loss: 0.1711\n",
      "Epoch [227/500], Train Loss: 0.1711\n",
      "Epoch [228/500], Train Loss: 0.1706\n",
      "Epoch [229/500], Train Loss: 0.1708\n",
      "Epoch [230/500], Train Loss: 0.1710\n",
      "Epoch [231/500], Train Loss: 0.1704\n",
      "Epoch [232/500], Train Loss: 0.1707\n",
      "Epoch [233/500], Train Loss: 0.1702\n",
      "Epoch [234/500], Train Loss: 0.1698\n",
      "Epoch [235/500], Train Loss: 0.1703\n",
      "Epoch [236/500], Train Loss: 0.1700\n",
      "Epoch [237/500], Train Loss: 0.1694\n",
      "Epoch [238/500], Train Loss: 0.1696\n",
      "Epoch [239/500], Train Loss: 0.1695\n",
      "Epoch [240/500], Train Loss: 0.1692\n",
      "Epoch [241/500], Train Loss: 0.1688\n",
      "Epoch [242/500], Train Loss: 0.1692\n",
      "Epoch [243/500], Train Loss: 0.1690\n",
      "Epoch [244/500], Train Loss: 0.1686\n",
      "Epoch [245/500], Train Loss: 0.1688\n",
      "Epoch [246/500], Train Loss: 0.1693\n",
      "Epoch [247/500], Train Loss: 0.1685\n",
      "Epoch [248/500], Train Loss: 0.1684\n",
      "Epoch [249/500], Train Loss: 0.1684\n",
      "Epoch [250/500], Train Loss: 0.1682\n",
      "Epoch [251/500], Train Loss: 0.1681\n",
      "Epoch [252/500], Train Loss: 0.1676\n",
      "Epoch [253/500], Train Loss: 0.1682\n",
      "Epoch [254/500], Train Loss: 0.1680\n",
      "Epoch [255/500], Train Loss: 0.1673\n",
      "Epoch [256/500], Train Loss: 0.1677\n",
      "Epoch [257/500], Train Loss: 0.1675\n",
      "Epoch [258/500], Train Loss: 0.1672\n",
      "Epoch [259/500], Train Loss: 0.1672\n",
      "Epoch [260/500], Train Loss: 0.1668\n",
      "Epoch [261/500], Train Loss: 0.1669\n",
      "Epoch [262/500], Train Loss: 0.1669\n",
      "Epoch [263/500], Train Loss: 0.1665\n",
      "Epoch [264/500], Train Loss: 0.1669\n",
      "Epoch [265/500], Train Loss: 0.1668\n",
      "Epoch [266/500], Train Loss: 0.1667\n",
      "Epoch [267/500], Train Loss: 0.1664\n",
      "Epoch [268/500], Train Loss: 0.1664\n",
      "Epoch [269/500], Train Loss: 0.1661\n",
      "Epoch [270/500], Train Loss: 0.1659\n",
      "Epoch [271/500], Train Loss: 0.1657\n",
      "Epoch [272/500], Train Loss: 0.1663\n",
      "Epoch [273/500], Train Loss: 0.1659\n",
      "Epoch [274/500], Train Loss: 0.1656\n",
      "Epoch [275/500], Train Loss: 0.1659\n",
      "Epoch [276/500], Train Loss: 0.1659\n",
      "Epoch [277/500], Train Loss: 0.1654\n",
      "Epoch [278/500], Train Loss: 0.1658\n",
      "Epoch [279/500], Train Loss: 0.1655\n",
      "Epoch [280/500], Train Loss: 0.1649\n",
      "Epoch [281/500], Train Loss: 0.1653\n",
      "Epoch [282/500], Train Loss: 0.1653\n",
      "Epoch [283/500], Train Loss: 0.1651\n",
      "Epoch [284/500], Train Loss: 0.1648\n",
      "Epoch [285/500], Train Loss: 0.1651\n",
      "Epoch [286/500], Train Loss: 0.1643\n",
      "Epoch [287/500], Train Loss: 0.1650\n",
      "Epoch [288/500], Train Loss: 0.1647\n",
      "Epoch [289/500], Train Loss: 0.1644\n",
      "Epoch [290/500], Train Loss: 0.1641\n",
      "Epoch [291/500], Train Loss: 0.1647\n",
      "Epoch [292/500], Train Loss: 0.1640\n",
      "Epoch [293/500], Train Loss: 0.1639\n",
      "Epoch [294/500], Train Loss: 0.1638\n",
      "Epoch [295/500], Train Loss: 0.1642\n",
      "Epoch [296/500], Train Loss: 0.1637\n",
      "Epoch [297/500], Train Loss: 0.1635\n",
      "Epoch [298/500], Train Loss: 0.1641\n",
      "Epoch [299/500], Train Loss: 0.1635\n",
      "Epoch [300/500], Train Loss: 0.1636\n",
      "Epoch [301/500], Train Loss: 0.1635\n",
      "Epoch [302/500], Train Loss: 0.1634\n",
      "Epoch [303/500], Train Loss: 0.1634\n",
      "Epoch [304/500], Train Loss: 0.1634\n",
      "Epoch [305/500], Train Loss: 0.1629\n",
      "Epoch [306/500], Train Loss: 0.1629\n",
      "Epoch [307/500], Train Loss: 0.1632\n",
      "Epoch [308/500], Train Loss: 0.1631\n",
      "Epoch [309/500], Train Loss: 0.1631\n",
      "Epoch [310/500], Train Loss: 0.1628\n",
      "Epoch [311/500], Train Loss: 0.1627\n",
      "Epoch [312/500], Train Loss: 0.1626\n",
      "Epoch [313/500], Train Loss: 0.1626\n",
      "Epoch [314/500], Train Loss: 0.1626\n",
      "Epoch [315/500], Train Loss: 0.1624\n",
      "Epoch [316/500], Train Loss: 0.1629\n",
      "Epoch [317/500], Train Loss: 0.1620\n",
      "Epoch [318/500], Train Loss: 0.1622\n",
      "Epoch [319/500], Train Loss: 0.1622\n",
      "Epoch [320/500], Train Loss: 0.1622\n",
      "Epoch [321/500], Train Loss: 0.1626\n",
      "Epoch [322/500], Train Loss: 0.1619\n",
      "Epoch [323/500], Train Loss: 0.1617\n",
      "Epoch [324/500], Train Loss: 0.1620\n",
      "Epoch [325/500], Train Loss: 0.1618\n",
      "Epoch [326/500], Train Loss: 0.1618\n",
      "Epoch [327/500], Train Loss: 0.1615\n",
      "Epoch [328/500], Train Loss: 0.1616\n",
      "Epoch [329/500], Train Loss: 0.1615\n",
      "Epoch [330/500], Train Loss: 0.1615\n",
      "Epoch [331/500], Train Loss: 0.1610\n",
      "Epoch [332/500], Train Loss: 0.1612\n",
      "Epoch [333/500], Train Loss: 0.1613\n",
      "Epoch [334/500], Train Loss: 0.1611\n",
      "Epoch [335/500], Train Loss: 0.1612\n",
      "Epoch [336/500], Train Loss: 0.1609\n",
      "Epoch [337/500], Train Loss: 0.1611\n",
      "Epoch [338/500], Train Loss: 0.1611\n",
      "Epoch [339/500], Train Loss: 0.1607\n",
      "Epoch [340/500], Train Loss: 0.1607\n",
      "Epoch [341/500], Train Loss: 0.1607\n",
      "Epoch [342/500], Train Loss: 0.1608\n",
      "Epoch [343/500], Train Loss: 0.1605\n",
      "Epoch [344/500], Train Loss: 0.1606\n",
      "Epoch [345/500], Train Loss: 0.1604\n",
      "Epoch [346/500], Train Loss: 0.1604\n",
      "Epoch [347/500], Train Loss: 0.1608\n",
      "Epoch [348/500], Train Loss: 0.1605\n",
      "Epoch [349/500], Train Loss: 0.1604\n",
      "Epoch [350/500], Train Loss: 0.1600\n",
      "Epoch [351/500], Train Loss: 0.1602\n",
      "Epoch [352/500], Train Loss: 0.1601\n",
      "Epoch [353/500], Train Loss: 0.1599\n",
      "Epoch [354/500], Train Loss: 0.1600\n",
      "Epoch [355/500], Train Loss: 0.1598\n",
      "Epoch [356/500], Train Loss: 0.1597\n",
      "Epoch [357/500], Train Loss: 0.1596\n",
      "Epoch [358/500], Train Loss: 0.1594\n",
      "Epoch [359/500], Train Loss: 0.1596\n",
      "Epoch [360/500], Train Loss: 0.1599\n",
      "Epoch [361/500], Train Loss: 0.1596\n",
      "Epoch [362/500], Train Loss: 0.1597\n",
      "Epoch [363/500], Train Loss: 0.1592\n",
      "Epoch [364/500], Train Loss: 0.1592\n",
      "Epoch [365/500], Train Loss: 0.1591\n",
      "Epoch [366/500], Train Loss: 0.1591\n",
      "Epoch [367/500], Train Loss: 0.1590\n",
      "Epoch [368/500], Train Loss: 0.1590\n",
      "Epoch [369/500], Train Loss: 0.1591\n",
      "Epoch [370/500], Train Loss: 0.1591\n",
      "Epoch [371/500], Train Loss: 0.1591\n",
      "Epoch [372/500], Train Loss: 0.1587\n",
      "Epoch [373/500], Train Loss: 0.1587\n",
      "Epoch [374/500], Train Loss: 0.1585\n",
      "Epoch [375/500], Train Loss: 0.1590\n",
      "Epoch [376/500], Train Loss: 0.1586\n",
      "Epoch [377/500], Train Loss: 0.1587\n",
      "Epoch [378/500], Train Loss: 0.1585\n",
      "Epoch [379/500], Train Loss: 0.1587\n",
      "Early stopping at epoch 379\n",
      "Final model saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/models_unphased/chr15/final_model_chr15.pth\n",
      "Individual R^2 scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr15/individual_r2_scores_chr15.csv\n",
      "Individual IQS scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr15/individual_iqs_scores_chr15.csv\n",
      "Individual AUC ROC curves saved in: ../../Data/model_results_unphased_all_PRS/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/performance_metrics.csv\n",
      "Total SNPs:  356\n",
      "PRS313 SNPs:  14\n",
      "Total SNPs used for Training:  342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-04-30 10:16:10,928] Trial 51 finished with value: 0.1415694100516183 and parameters: {'learning_rate': 0.029438412179662383, 'l1_coef': 1.6960842966286154e-05, 'patience': 9, 'batch_size': 64}. Best is trial 19 with value: 0.08846894302047217.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 16 - Best hyperparameters: {'learning_rate': 0.0003280120292913132, 'l1_coef': 1.0215158769106902e-05, 'patience': 14, 'batch_size': 32}\n",
      "Chr 16 - Best value: 0.0885\n",
      "Epoch [1/500], Train Loss: 0.5473\n",
      "Epoch [2/500], Train Loss: 0.4663\n",
      "Epoch [3/500], Train Loss: 0.4454\n",
      "Epoch [4/500], Train Loss: 0.4300\n",
      "Epoch [5/500], Train Loss: 0.4172\n",
      "Epoch [6/500], Train Loss: 0.4063\n",
      "Epoch [7/500], Train Loss: 0.3964\n",
      "Epoch [8/500], Train Loss: 0.3875\n",
      "Epoch [9/500], Train Loss: 0.3792\n",
      "Epoch [10/500], Train Loss: 0.3717\n",
      "Epoch [11/500], Train Loss: 0.3644\n",
      "Epoch [12/500], Train Loss: 0.3576\n",
      "Epoch [13/500], Train Loss: 0.3511\n",
      "Epoch [14/500], Train Loss: 0.3451\n",
      "Epoch [15/500], Train Loss: 0.3396\n",
      "Epoch [16/500], Train Loss: 0.3335\n",
      "Epoch [17/500], Train Loss: 0.3285\n",
      "Epoch [18/500], Train Loss: 0.3234\n",
      "Epoch [19/500], Train Loss: 0.3188\n",
      "Epoch [20/500], Train Loss: 0.3143\n",
      "Epoch [21/500], Train Loss: 0.3098\n",
      "Epoch [22/500], Train Loss: 0.3055\n",
      "Epoch [23/500], Train Loss: 0.3014\n",
      "Epoch [24/500], Train Loss: 0.2979\n",
      "Epoch [25/500], Train Loss: 0.2937\n",
      "Epoch [26/500], Train Loss: 0.2902\n",
      "Epoch [27/500], Train Loss: 0.2865\n",
      "Epoch [28/500], Train Loss: 0.2830\n",
      "Epoch [29/500], Train Loss: 0.2800\n",
      "Epoch [30/500], Train Loss: 0.2766\n",
      "Epoch [31/500], Train Loss: 0.2739\n",
      "Epoch [32/500], Train Loss: 0.2709\n",
      "Epoch [33/500], Train Loss: 0.2681\n",
      "Epoch [34/500], Train Loss: 0.2654\n",
      "Epoch [35/500], Train Loss: 0.2624\n",
      "Epoch [36/500], Train Loss: 0.2598\n",
      "Epoch [37/500], Train Loss: 0.2578\n",
      "Epoch [38/500], Train Loss: 0.2550\n",
      "Epoch [39/500], Train Loss: 0.2530\n",
      "Epoch [40/500], Train Loss: 0.2503\n",
      "Epoch [41/500], Train Loss: 0.2484\n",
      "Epoch [42/500], Train Loss: 0.2461\n",
      "Epoch [43/500], Train Loss: 0.2439\n",
      "Epoch [44/500], Train Loss: 0.2416\n",
      "Epoch [45/500], Train Loss: 0.2399\n",
      "Epoch [46/500], Train Loss: 0.2381\n",
      "Epoch [47/500], Train Loss: 0.2361\n",
      "Epoch [48/500], Train Loss: 0.2340\n",
      "Epoch [49/500], Train Loss: 0.2324\n",
      "Epoch [50/500], Train Loss: 0.2306\n",
      "Epoch [51/500], Train Loss: 0.2289\n",
      "Epoch [52/500], Train Loss: 0.2271\n",
      "Epoch [53/500], Train Loss: 0.2259\n",
      "Epoch [54/500], Train Loss: 0.2237\n",
      "Epoch [55/500], Train Loss: 0.2224\n",
      "Epoch [56/500], Train Loss: 0.2209\n",
      "Epoch [57/500], Train Loss: 0.2194\n",
      "Epoch [58/500], Train Loss: 0.2182\n",
      "Epoch [59/500], Train Loss: 0.2167\n",
      "Epoch [60/500], Train Loss: 0.2151\n",
      "Epoch [61/500], Train Loss: 0.2140\n",
      "Epoch [62/500], Train Loss: 0.2122\n",
      "Epoch [63/500], Train Loss: 0.2110\n",
      "Epoch [64/500], Train Loss: 0.2100\n",
      "Epoch [65/500], Train Loss: 0.2086\n",
      "Epoch [66/500], Train Loss: 0.2073\n",
      "Epoch [67/500], Train Loss: 0.2063\n",
      "Epoch [68/500], Train Loss: 0.2048\n",
      "Epoch [69/500], Train Loss: 0.2037\n",
      "Epoch [70/500], Train Loss: 0.2027\n",
      "Epoch [71/500], Train Loss: 0.2014\n",
      "Epoch [72/500], Train Loss: 0.2004\n",
      "Epoch [73/500], Train Loss: 0.1993\n",
      "Epoch [74/500], Train Loss: 0.1984\n",
      "Epoch [75/500], Train Loss: 0.1971\n",
      "Epoch [76/500], Train Loss: 0.1964\n",
      "Epoch [77/500], Train Loss: 0.1956\n",
      "Epoch [78/500], Train Loss: 0.1942\n",
      "Epoch [79/500], Train Loss: 0.1936\n",
      "Epoch [80/500], Train Loss: 0.1924\n",
      "Epoch [81/500], Train Loss: 0.1917\n",
      "Epoch [82/500], Train Loss: 0.1904\n",
      "Epoch [83/500], Train Loss: 0.1900\n",
      "Epoch [84/500], Train Loss: 0.1886\n",
      "Epoch [85/500], Train Loss: 0.1878\n",
      "Epoch [86/500], Train Loss: 0.1868\n",
      "Epoch [87/500], Train Loss: 0.1864\n",
      "Epoch [88/500], Train Loss: 0.1853\n",
      "Epoch [89/500], Train Loss: 0.1846\n",
      "Epoch [90/500], Train Loss: 0.1838\n",
      "Epoch [91/500], Train Loss: 0.1830\n",
      "Epoch [92/500], Train Loss: 0.1822\n",
      "Epoch [93/500], Train Loss: 0.1814\n",
      "Epoch [94/500], Train Loss: 0.1806\n",
      "Epoch [95/500], Train Loss: 0.1799\n",
      "Epoch [96/500], Train Loss: 0.1790\n",
      "Epoch [97/500], Train Loss: 0.1783\n",
      "Epoch [98/500], Train Loss: 0.1778\n",
      "Epoch [99/500], Train Loss: 0.1767\n",
      "Epoch [100/500], Train Loss: 0.1766\n",
      "Epoch [101/500], Train Loss: 0.1755\n",
      "Epoch [102/500], Train Loss: 0.1751\n",
      "Epoch [103/500], Train Loss: 0.1747\n",
      "Epoch [104/500], Train Loss: 0.1739\n",
      "Epoch [105/500], Train Loss: 0.1730\n",
      "Epoch [106/500], Train Loss: 0.1725\n",
      "Epoch [107/500], Train Loss: 0.1723\n",
      "Epoch [108/500], Train Loss: 0.1713\n",
      "Epoch [109/500], Train Loss: 0.1708\n",
      "Epoch [110/500], Train Loss: 0.1702\n",
      "Epoch [111/500], Train Loss: 0.1696\n",
      "Epoch [112/500], Train Loss: 0.1691\n",
      "Epoch [113/500], Train Loss: 0.1686\n",
      "Epoch [114/500], Train Loss: 0.1679\n",
      "Epoch [115/500], Train Loss: 0.1673\n",
      "Epoch [116/500], Train Loss: 0.1667\n",
      "Epoch [117/500], Train Loss: 0.1662\n",
      "Epoch [118/500], Train Loss: 0.1657\n",
      "Epoch [119/500], Train Loss: 0.1652\n",
      "Epoch [120/500], Train Loss: 0.1648\n",
      "Epoch [121/500], Train Loss: 0.1641\n",
      "Epoch [122/500], Train Loss: 0.1636\n",
      "Epoch [123/500], Train Loss: 0.1634\n",
      "Epoch [124/500], Train Loss: 0.1626\n",
      "Epoch [125/500], Train Loss: 0.1621\n",
      "Epoch [126/500], Train Loss: 0.1617\n",
      "Epoch [127/500], Train Loss: 0.1612\n",
      "Epoch [128/500], Train Loss: 0.1609\n",
      "Epoch [129/500], Train Loss: 0.1604\n",
      "Epoch [130/500], Train Loss: 0.1597\n",
      "Epoch [131/500], Train Loss: 0.1594\n",
      "Epoch [132/500], Train Loss: 0.1589\n",
      "Epoch [133/500], Train Loss: 0.1587\n",
      "Epoch [134/500], Train Loss: 0.1581\n",
      "Epoch [135/500], Train Loss: 0.1580\n",
      "Epoch [136/500], Train Loss: 0.1573\n",
      "Epoch [137/500], Train Loss: 0.1568\n",
      "Epoch [138/500], Train Loss: 0.1564\n",
      "Epoch [139/500], Train Loss: 0.1563\n",
      "Epoch [140/500], Train Loss: 0.1556\n",
      "Epoch [141/500], Train Loss: 0.1555\n",
      "Epoch [142/500], Train Loss: 0.1549\n",
      "Epoch [143/500], Train Loss: 0.1543\n",
      "Epoch [144/500], Train Loss: 0.1544\n",
      "Epoch [145/500], Train Loss: 0.1538\n",
      "Epoch [146/500], Train Loss: 0.1536\n",
      "Epoch [147/500], Train Loss: 0.1534\n",
      "Epoch [148/500], Train Loss: 0.1528\n",
      "Epoch [149/500], Train Loss: 0.1526\n",
      "Epoch [150/500], Train Loss: 0.1519\n",
      "Epoch [151/500], Train Loss: 0.1516\n",
      "Epoch [152/500], Train Loss: 0.1512\n",
      "Epoch [153/500], Train Loss: 0.1508\n",
      "Epoch [154/500], Train Loss: 0.1504\n",
      "Epoch [155/500], Train Loss: 0.1503\n",
      "Epoch [156/500], Train Loss: 0.1503\n",
      "Epoch [157/500], Train Loss: 0.1498\n",
      "Epoch [158/500], Train Loss: 0.1494\n",
      "Epoch [159/500], Train Loss: 0.1491\n",
      "Epoch [160/500], Train Loss: 0.1489\n",
      "Epoch [161/500], Train Loss: 0.1485\n",
      "Epoch [162/500], Train Loss: 0.1482\n",
      "Epoch [163/500], Train Loss: 0.1478\n",
      "Epoch [164/500], Train Loss: 0.1476\n",
      "Epoch [165/500], Train Loss: 0.1474\n",
      "Epoch [166/500], Train Loss: 0.1471\n",
      "Epoch [167/500], Train Loss: 0.1468\n",
      "Epoch [168/500], Train Loss: 0.1464\n",
      "Epoch [169/500], Train Loss: 0.1463\n",
      "Epoch [170/500], Train Loss: 0.1458\n",
      "Epoch [171/500], Train Loss: 0.1455\n",
      "Epoch [172/500], Train Loss: 0.1455\n",
      "Epoch [173/500], Train Loss: 0.1449\n",
      "Epoch [174/500], Train Loss: 0.1447\n",
      "Epoch [175/500], Train Loss: 0.1446\n",
      "Epoch [176/500], Train Loss: 0.1441\n",
      "Epoch [177/500], Train Loss: 0.1441\n",
      "Epoch [178/500], Train Loss: 0.1437\n",
      "Epoch [179/500], Train Loss: 0.1436\n",
      "Epoch [180/500], Train Loss: 0.1431\n",
      "Epoch [181/500], Train Loss: 0.1429\n",
      "Epoch [182/500], Train Loss: 0.1427\n",
      "Epoch [183/500], Train Loss: 0.1424\n",
      "Epoch [184/500], Train Loss: 0.1423\n",
      "Epoch [185/500], Train Loss: 0.1421\n",
      "Epoch [186/500], Train Loss: 0.1417\n",
      "Epoch [187/500], Train Loss: 0.1418\n",
      "Epoch [188/500], Train Loss: 0.1415\n",
      "Epoch [189/500], Train Loss: 0.1412\n",
      "Epoch [190/500], Train Loss: 0.1409\n",
      "Epoch [191/500], Train Loss: 0.1408\n",
      "Epoch [192/500], Train Loss: 0.1404\n",
      "Epoch [193/500], Train Loss: 0.1403\n",
      "Epoch [194/500], Train Loss: 0.1402\n",
      "Epoch [195/500], Train Loss: 0.1397\n",
      "Epoch [196/500], Train Loss: 0.1395\n",
      "Epoch [197/500], Train Loss: 0.1394\n",
      "Epoch [198/500], Train Loss: 0.1391\n",
      "Epoch [199/500], Train Loss: 0.1388\n",
      "Epoch [200/500], Train Loss: 0.1385\n",
      "Epoch [201/500], Train Loss: 0.1385\n",
      "Epoch [202/500], Train Loss: 0.1382\n",
      "Epoch [203/500], Train Loss: 0.1380\n",
      "Epoch [204/500], Train Loss: 0.1379\n",
      "Epoch [205/500], Train Loss: 0.1377\n",
      "Epoch [206/500], Train Loss: 0.1376\n",
      "Epoch [207/500], Train Loss: 0.1373\n",
      "Epoch [208/500], Train Loss: 0.1374\n",
      "Epoch [209/500], Train Loss: 0.1367\n",
      "Epoch [210/500], Train Loss: 0.1366\n",
      "Epoch [211/500], Train Loss: 0.1368\n",
      "Epoch [212/500], Train Loss: 0.1363\n",
      "Epoch [213/500], Train Loss: 0.1362\n",
      "Epoch [214/500], Train Loss: 0.1360\n",
      "Epoch [215/500], Train Loss: 0.1358\n",
      "Epoch [216/500], Train Loss: 0.1354\n",
      "Epoch [217/500], Train Loss: 0.1356\n",
      "Epoch [218/500], Train Loss: 0.1353\n",
      "Epoch [219/500], Train Loss: 0.1352\n",
      "Epoch [220/500], Train Loss: 0.1348\n",
      "Epoch [221/500], Train Loss: 0.1350\n",
      "Epoch [222/500], Train Loss: 0.1346\n",
      "Epoch [223/500], Train Loss: 0.1343\n",
      "Epoch [224/500], Train Loss: 0.1342\n",
      "Epoch [225/500], Train Loss: 0.1343\n",
      "Epoch [226/500], Train Loss: 0.1337\n",
      "Epoch [227/500], Train Loss: 0.1337\n",
      "Epoch [228/500], Train Loss: 0.1337\n",
      "Epoch [229/500], Train Loss: 0.1335\n",
      "Epoch [230/500], Train Loss: 0.1332\n",
      "Epoch [231/500], Train Loss: 0.1332\n",
      "Epoch [232/500], Train Loss: 0.1330\n",
      "Epoch [233/500], Train Loss: 0.1330\n",
      "Epoch [234/500], Train Loss: 0.1326\n",
      "Epoch [235/500], Train Loss: 0.1326\n",
      "Epoch [236/500], Train Loss: 0.1323\n",
      "Epoch [237/500], Train Loss: 0.1324\n",
      "Epoch [238/500], Train Loss: 0.1321\n",
      "Epoch [239/500], Train Loss: 0.1321\n",
      "Epoch [240/500], Train Loss: 0.1321\n",
      "Epoch [241/500], Train Loss: 0.1314\n",
      "Epoch [242/500], Train Loss: 0.1316\n",
      "Epoch [243/500], Train Loss: 0.1315\n",
      "Epoch [244/500], Train Loss: 0.1313\n",
      "Epoch [245/500], Train Loss: 0.1313\n",
      "Epoch [246/500], Train Loss: 0.1311\n",
      "Epoch [247/500], Train Loss: 0.1309\n",
      "Epoch [248/500], Train Loss: 0.1309\n",
      "Epoch [249/500], Train Loss: 0.1306\n",
      "Epoch [250/500], Train Loss: 0.1303\n",
      "Epoch [251/500], Train Loss: 0.1304\n",
      "Epoch [252/500], Train Loss: 0.1303\n",
      "Epoch [253/500], Train Loss: 0.1302\n",
      "Epoch [254/500], Train Loss: 0.1304\n",
      "Epoch [255/500], Train Loss: 0.1297\n",
      "Epoch [256/500], Train Loss: 0.1295\n",
      "Epoch [257/500], Train Loss: 0.1297\n",
      "Epoch [258/500], Train Loss: 0.1294\n",
      "Epoch [259/500], Train Loss: 0.1291\n",
      "Epoch [260/500], Train Loss: 0.1295\n",
      "Epoch [261/500], Train Loss: 0.1291\n",
      "Epoch [262/500], Train Loss: 0.1288\n",
      "Epoch [263/500], Train Loss: 0.1290\n",
      "Epoch [264/500], Train Loss: 0.1288\n",
      "Epoch [265/500], Train Loss: 0.1287\n",
      "Epoch [266/500], Train Loss: 0.1284\n",
      "Epoch [267/500], Train Loss: 0.1282\n",
      "Epoch [268/500], Train Loss: 0.1283\n",
      "Epoch [269/500], Train Loss: 0.1282\n",
      "Epoch [270/500], Train Loss: 0.1281\n",
      "Epoch [271/500], Train Loss: 0.1280\n",
      "Epoch [272/500], Train Loss: 0.1277\n",
      "Epoch [273/500], Train Loss: 0.1276\n",
      "Epoch [274/500], Train Loss: 0.1277\n",
      "Epoch [275/500], Train Loss: 0.1277\n",
      "Epoch [276/500], Train Loss: 0.1273\n",
      "Epoch [277/500], Train Loss: 0.1272\n",
      "Epoch [278/500], Train Loss: 0.1273\n",
      "Epoch [279/500], Train Loss: 0.1272\n",
      "Epoch [280/500], Train Loss: 0.1272\n",
      "Epoch [281/500], Train Loss: 0.1270\n",
      "Epoch [282/500], Train Loss: 0.1268\n",
      "Epoch [283/500], Train Loss: 0.1264\n",
      "Epoch [284/500], Train Loss: 0.1265\n",
      "Epoch [285/500], Train Loss: 0.1263\n",
      "Epoch [286/500], Train Loss: 0.1262\n",
      "Epoch [287/500], Train Loss: 0.1263\n",
      "Epoch [288/500], Train Loss: 0.1262\n",
      "Epoch [289/500], Train Loss: 0.1262\n",
      "Epoch [290/500], Train Loss: 0.1260\n",
      "Epoch [291/500], Train Loss: 0.1258\n",
      "Epoch [292/500], Train Loss: 0.1257\n",
      "Epoch [293/500], Train Loss: 0.1255\n",
      "Epoch [294/500], Train Loss: 0.1255\n",
      "Epoch [295/500], Train Loss: 0.1254\n",
      "Epoch [296/500], Train Loss: 0.1254\n",
      "Epoch [297/500], Train Loss: 0.1253\n",
      "Epoch [298/500], Train Loss: 0.1251\n",
      "Epoch [299/500], Train Loss: 0.1251\n",
      "Epoch [300/500], Train Loss: 0.1250\n",
      "Epoch [301/500], Train Loss: 0.1248\n",
      "Epoch [302/500], Train Loss: 0.1247\n",
      "Epoch [303/500], Train Loss: 0.1247\n",
      "Epoch [304/500], Train Loss: 0.1248\n",
      "Epoch [305/500], Train Loss: 0.1247\n",
      "Epoch [306/500], Train Loss: 0.1245\n",
      "Epoch [307/500], Train Loss: 0.1243\n",
      "Epoch [308/500], Train Loss: 0.1241\n",
      "Epoch [309/500], Train Loss: 0.1242\n",
      "Epoch [310/500], Train Loss: 0.1240\n",
      "Epoch [311/500], Train Loss: 0.1240\n",
      "Epoch [312/500], Train Loss: 0.1239\n",
      "Epoch [313/500], Train Loss: 0.1238\n",
      "Epoch [314/500], Train Loss: 0.1239\n",
      "Epoch [315/500], Train Loss: 0.1236\n",
      "Epoch [316/500], Train Loss: 0.1238\n",
      "Epoch [317/500], Train Loss: 0.1234\n",
      "Epoch [318/500], Train Loss: 0.1234\n",
      "Epoch [319/500], Train Loss: 0.1235\n",
      "Epoch [320/500], Train Loss: 0.1235\n",
      "Epoch [321/500], Train Loss: 0.1234\n",
      "Epoch [322/500], Train Loss: 0.1233\n",
      "Epoch [323/500], Train Loss: 0.1232\n",
      "Epoch [324/500], Train Loss: 0.1231\n",
      "Epoch [325/500], Train Loss: 0.1228\n",
      "Epoch [326/500], Train Loss: 0.1228\n",
      "Epoch [327/500], Train Loss: 0.1227\n",
      "Epoch [328/500], Train Loss: 0.1227\n",
      "Epoch [329/500], Train Loss: 0.1225\n",
      "Epoch [330/500], Train Loss: 0.1224\n",
      "Epoch [331/500], Train Loss: 0.1225\n",
      "Epoch [332/500], Train Loss: 0.1225\n",
      "Epoch [333/500], Train Loss: 0.1223\n",
      "Epoch [334/500], Train Loss: 0.1220\n",
      "Epoch [335/500], Train Loss: 0.1222\n",
      "Epoch [336/500], Train Loss: 0.1221\n",
      "Epoch [337/500], Train Loss: 0.1219\n",
      "Epoch [338/500], Train Loss: 0.1218\n",
      "Epoch [339/500], Train Loss: 0.1218\n",
      "Epoch [340/500], Train Loss: 0.1218\n",
      "Epoch [341/500], Train Loss: 0.1217\n",
      "Epoch [342/500], Train Loss: 0.1216\n",
      "Epoch [343/500], Train Loss: 0.1215\n",
      "Epoch [344/500], Train Loss: 0.1215\n",
      "Epoch [345/500], Train Loss: 0.1213\n",
      "Epoch [346/500], Train Loss: 0.1215\n",
      "Epoch [347/500], Train Loss: 0.1213\n",
      "Epoch [348/500], Train Loss: 0.1213\n",
      "Epoch [349/500], Train Loss: 0.1212\n",
      "Epoch [350/500], Train Loss: 0.1212\n",
      "Epoch [351/500], Train Loss: 0.1212\n",
      "Epoch [352/500], Train Loss: 0.1212\n",
      "Epoch [353/500], Train Loss: 0.1214\n",
      "Epoch [354/500], Train Loss: 0.1209\n",
      "Epoch [355/500], Train Loss: 0.1208\n",
      "Epoch [356/500], Train Loss: 0.1207\n",
      "Epoch [357/500], Train Loss: 0.1211\n",
      "Epoch [358/500], Train Loss: 0.1208\n",
      "Epoch [359/500], Train Loss: 0.1207\n",
      "Epoch [360/500], Train Loss: 0.1204\n",
      "Epoch [361/500], Train Loss: 0.1203\n",
      "Epoch [362/500], Train Loss: 0.1203\n",
      "Epoch [363/500], Train Loss: 0.1202\n",
      "Epoch [364/500], Train Loss: 0.1200\n",
      "Epoch [365/500], Train Loss: 0.1200\n",
      "Epoch [366/500], Train Loss: 0.1201\n",
      "Epoch [367/500], Train Loss: 0.1200\n",
      "Epoch [368/500], Train Loss: 0.1199\n",
      "Epoch [369/500], Train Loss: 0.1200\n",
      "Epoch [370/500], Train Loss: 0.1200\n",
      "Epoch [371/500], Train Loss: 0.1197\n",
      "Epoch [372/500], Train Loss: 0.1198\n",
      "Epoch [373/500], Train Loss: 0.1197\n",
      "Epoch [374/500], Train Loss: 0.1198\n",
      "Epoch [375/500], Train Loss: 0.1195\n",
      "Epoch [376/500], Train Loss: 0.1195\n",
      "Epoch [377/500], Train Loss: 0.1195\n",
      "Epoch [378/500], Train Loss: 0.1196\n",
      "Epoch [379/500], Train Loss: 0.1193\n",
      "Epoch [380/500], Train Loss: 0.1193\n",
      "Epoch [381/500], Train Loss: 0.1193\n",
      "Epoch [382/500], Train Loss: 0.1193\n",
      "Epoch [383/500], Train Loss: 0.1192\n",
      "Epoch [384/500], Train Loss: 0.1190\n",
      "Epoch [385/500], Train Loss: 0.1190\n",
      "Epoch [386/500], Train Loss: 0.1189\n",
      "Epoch [387/500], Train Loss: 0.1189\n",
      "Epoch [388/500], Train Loss: 0.1189\n",
      "Epoch [389/500], Train Loss: 0.1191\n",
      "Epoch [390/500], Train Loss: 0.1190\n",
      "Epoch [391/500], Train Loss: 0.1187\n",
      "Epoch [392/500], Train Loss: 0.1186\n",
      "Epoch [393/500], Train Loss: 0.1185\n",
      "Epoch [394/500], Train Loss: 0.1183\n",
      "Epoch [395/500], Train Loss: 0.1185\n",
      "Epoch [396/500], Train Loss: 0.1184\n",
      "Epoch [397/500], Train Loss: 0.1186\n",
      "Epoch [398/500], Train Loss: 0.1184\n",
      "Epoch [399/500], Train Loss: 0.1182\n",
      "Epoch [400/500], Train Loss: 0.1184\n",
      "Epoch [401/500], Train Loss: 0.1180\n",
      "Epoch [402/500], Train Loss: 0.1181\n",
      "Epoch [403/500], Train Loss: 0.1182\n",
      "Epoch [404/500], Train Loss: 0.1183\n",
      "Epoch [405/500], Train Loss: 0.1182\n",
      "Epoch [406/500], Train Loss: 0.1180\n",
      "Epoch [407/500], Train Loss: 0.1181\n",
      "Epoch [408/500], Train Loss: 0.1177\n",
      "Epoch [409/500], Train Loss: 0.1173\n",
      "Epoch [410/500], Train Loss: 0.1172\n",
      "Epoch [411/500], Train Loss: 0.1172\n",
      "Epoch [412/500], Train Loss: 0.1173\n",
      "Epoch [413/500], Train Loss: 0.1172\n",
      "Epoch [414/500], Train Loss: 0.1173\n",
      "Epoch [415/500], Train Loss: 0.1176\n",
      "Epoch [416/500], Train Loss: 0.1172\n",
      "Epoch [417/500], Train Loss: 0.1171\n",
      "Epoch [418/500], Train Loss: 0.1173\n",
      "Epoch [419/500], Train Loss: 0.1171\n",
      "Epoch [420/500], Train Loss: 0.1172\n",
      "Epoch [421/500], Train Loss: 0.1175\n",
      "Epoch [422/500], Train Loss: 0.1172\n",
      "Epoch [423/500], Train Loss: 0.1172\n",
      "Epoch [424/500], Train Loss: 0.1172\n",
      "Epoch [425/500], Train Loss: 0.1171\n",
      "Epoch [426/500], Train Loss: 0.1174\n",
      "Epoch [427/500], Train Loss: 0.1173\n",
      "Epoch [428/500], Train Loss: 0.1175\n",
      "Epoch [429/500], Train Loss: 0.1172\n",
      "Epoch [430/500], Train Loss: 0.1173\n",
      "Epoch [431/500], Train Loss: 0.1172\n",
      "Epoch [432/500], Train Loss: 0.1172\n",
      "Epoch [433/500], Train Loss: 0.1175\n",
      "Epoch [434/500], Train Loss: 0.1174\n",
      "Epoch [435/500], Train Loss: 0.1173\n",
      "Epoch [436/500], Train Loss: 0.1172\n",
      "Epoch [437/500], Train Loss: 0.1172\n",
      "Epoch [438/500], Train Loss: 0.1175\n",
      "Epoch [439/500], Train Loss: 0.1170\n",
      "Epoch [440/500], Train Loss: 0.1171\n",
      "Epoch [441/500], Train Loss: 0.1173\n",
      "Epoch [442/500], Train Loss: 0.1172\n",
      "Epoch [443/500], Train Loss: 0.1173\n",
      "Epoch [444/500], Train Loss: 0.1171\n",
      "Epoch [445/500], Train Loss: 0.1171\n",
      "Epoch [446/500], Train Loss: 0.1173\n",
      "Epoch [447/500], Train Loss: 0.1173\n",
      "Epoch [448/500], Train Loss: 0.1171\n",
      "Epoch [449/500], Train Loss: 0.1175\n",
      "Epoch [450/500], Train Loss: 0.1171\n",
      "Epoch [451/500], Train Loss: 0.1171\n",
      "Epoch [452/500], Train Loss: 0.1174\n",
      "Epoch [453/500], Train Loss: 0.1172\n",
      "Early stopping at epoch 453\n",
      "Final model saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/models_unphased/chr16/final_model_chr16.pth\n",
      "Individual R^2 scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr16/individual_r2_scores_chr16.csv\n",
      "Individual IQS scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr16/individual_iqs_scores_chr16.csv\n",
      "Individual AUC ROC curves saved in: ../../Data/model_results_unphased_all_PRS/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/performance_metrics.csv\n",
      "Total SNPs:  178\n",
      "PRS313 SNPs:  9\n",
      "Total SNPs used for Training:  169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-04-30 10:16:22,459] Trial 51 finished with value: 0.1804597333073616 and parameters: {'learning_rate': 0.011811863607846414, 'l1_coef': 1.6778229036348106e-05, 'patience': 12, 'batch_size': 32}. Best is trial 25 with value: 0.03733763907636915.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 17 - Best hyperparameters: {'learning_rate': 0.01875699494745296, 'l1_coef': 1.0107606908789018e-05, 'patience': 16, 'batch_size': 64}\n",
      "Chr 17 - Best value: 0.0373\n",
      "Epoch [1/500], Train Loss: 0.3377\n",
      "Epoch [2/500], Train Loss: 0.2443\n",
      "Epoch [3/500], Train Loss: 0.2226\n",
      "Epoch [4/500], Train Loss: 0.2117\n",
      "Epoch [5/500], Train Loss: 0.2055\n",
      "Epoch [6/500], Train Loss: 0.1992\n",
      "Epoch [7/500], Train Loss: 0.1962\n",
      "Epoch [8/500], Train Loss: 0.1924\n",
      "Epoch [9/500], Train Loss: 0.1888\n",
      "Epoch [10/500], Train Loss: 0.1890\n",
      "Epoch [11/500], Train Loss: 0.1890\n",
      "Epoch [12/500], Train Loss: 0.1866\n",
      "Epoch [13/500], Train Loss: 0.1858\n",
      "Epoch [14/500], Train Loss: 0.1839\n",
      "Epoch [15/500], Train Loss: 0.1860\n",
      "Epoch [16/500], Train Loss: 0.1824\n",
      "Epoch [17/500], Train Loss: 0.1837\n",
      "Epoch [18/500], Train Loss: 0.1816\n",
      "Epoch [19/500], Train Loss: 0.1815\n",
      "Epoch [20/500], Train Loss: 0.1821\n",
      "Epoch [21/500], Train Loss: 0.1816\n",
      "Epoch [22/500], Train Loss: 0.1780\n",
      "Epoch [23/500], Train Loss: 0.1783\n",
      "Epoch [24/500], Train Loss: 0.1773\n",
      "Epoch [25/500], Train Loss: 0.1770\n",
      "Epoch [26/500], Train Loss: 0.1780\n",
      "Epoch [27/500], Train Loss: 0.1786\n",
      "Epoch [28/500], Train Loss: 0.1767\n",
      "Epoch [29/500], Train Loss: 0.1750\n",
      "Epoch [30/500], Train Loss: 0.1770\n",
      "Epoch [31/500], Train Loss: 0.1796\n",
      "Epoch [32/500], Train Loss: 0.1746\n",
      "Epoch [33/500], Train Loss: 0.1733\n",
      "Epoch [34/500], Train Loss: 0.1746\n",
      "Epoch [35/500], Train Loss: 0.1764\n",
      "Epoch [36/500], Train Loss: 0.1738\n",
      "Epoch [37/500], Train Loss: 0.1733\n",
      "Epoch [38/500], Train Loss: 0.1743\n",
      "Epoch [39/500], Train Loss: 0.1749\n",
      "Epoch [40/500], Train Loss: 0.1675\n",
      "Epoch [41/500], Train Loss: 0.1668\n",
      "Epoch [42/500], Train Loss: 0.1662\n",
      "Epoch [43/500], Train Loss: 0.1658\n",
      "Epoch [44/500], Train Loss: 0.1672\n",
      "Epoch [45/500], Train Loss: 0.1664\n",
      "Epoch [46/500], Train Loss: 0.1660\n",
      "Epoch [47/500], Train Loss: 0.1668\n",
      "Epoch [48/500], Train Loss: 0.1670\n",
      "Epoch [49/500], Train Loss: 0.1667\n",
      "Epoch [50/500], Train Loss: 0.1656\n",
      "Epoch [51/500], Train Loss: 0.1656\n",
      "Epoch [52/500], Train Loss: 0.1655\n",
      "Epoch [53/500], Train Loss: 0.1651\n",
      "Epoch [54/500], Train Loss: 0.1655\n",
      "Epoch [55/500], Train Loss: 0.1660\n",
      "Epoch [56/500], Train Loss: 0.1654\n",
      "Epoch [57/500], Train Loss: 0.1650\n",
      "Epoch [58/500], Train Loss: 0.1662\n",
      "Epoch [59/500], Train Loss: 0.1661\n",
      "Epoch [60/500], Train Loss: 0.1663\n",
      "Epoch [61/500], Train Loss: 0.1650\n",
      "Epoch [62/500], Train Loss: 0.1664\n",
      "Epoch [63/500], Train Loss: 0.1656\n",
      "Epoch [64/500], Train Loss: 0.1655\n",
      "Epoch [65/500], Train Loss: 0.1654\n",
      "Epoch [66/500], Train Loss: 0.1649\n",
      "Epoch [67/500], Train Loss: 0.1657\n",
      "Epoch [68/500], Train Loss: 0.1654\n",
      "Epoch [69/500], Train Loss: 0.1669\n",
      "Epoch [70/500], Train Loss: 0.1652\n",
      "Epoch [71/500], Train Loss: 0.1653\n",
      "Epoch [72/500], Train Loss: 0.1653\n",
      "Epoch [73/500], Train Loss: 0.1658\n",
      "Epoch [74/500], Train Loss: 0.1650\n",
      "Epoch [75/500], Train Loss: 0.1661\n",
      "Epoch [76/500], Train Loss: 0.1659\n",
      "Epoch [77/500], Train Loss: 0.1648\n",
      "Epoch [78/500], Train Loss: 0.1667\n",
      "Epoch [79/500], Train Loss: 0.1657\n",
      "Epoch [80/500], Train Loss: 0.1651\n",
      "Epoch [81/500], Train Loss: 0.1659\n",
      "Epoch [82/500], Train Loss: 0.1657\n",
      "Epoch [83/500], Train Loss: 0.1654\n",
      "Epoch [84/500], Train Loss: 0.1657\n",
      "Epoch [85/500], Train Loss: 0.1651\n",
      "Epoch [86/500], Train Loss: 0.1653\n",
      "Epoch [87/500], Train Loss: 0.1656\n",
      "Epoch [88/500], Train Loss: 0.1653\n",
      "Epoch [89/500], Train Loss: 0.1651\n",
      "Epoch [90/500], Train Loss: 0.1650\n",
      "Epoch [91/500], Train Loss: 0.1654\n",
      "Epoch [92/500], Train Loss: 0.1664\n",
      "Epoch [93/500], Train Loss: 0.1653\n",
      "Early stopping at epoch 93\n",
      "Final model saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/models_unphased/chr17/final_model_chr17.pth\n",
      "Individual R^2 scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr17/individual_r2_scores_chr17.csv\n",
      "Individual IQS scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr17/individual_iqs_scores_chr17.csv\n",
      "Individual AUC ROC curves saved in: ../../Data/model_results_unphased_all_PRS/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/performance_metrics.csv\n",
      "Total SNPs:  274\n",
      "PRS313 SNPs:  9\n",
      "Total SNPs used for Training:  265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-04-30 10:16:25,521] Trial 51 finished with value: 0.17057546686667663 and parameters: {'learning_rate': 0.06684500720555978, 'l1_coef': 1.9180233950342432e-05, 'patience': 16, 'batch_size': 32}. Best is trial 29 with value: 0.08387428413216884.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 18 - Best hyperparameters: {'learning_rate': 0.06940442887866152, 'l1_coef': 2.1499468833674833e-05, 'patience': 17, 'batch_size': 32}\n",
      "Chr 18 - Best value: 0.0839\n",
      "Epoch [1/500], Train Loss: 0.8155\n",
      "Epoch [2/500], Train Loss: 0.2518\n",
      "Epoch [3/500], Train Loss: 0.2495\n",
      "Epoch [4/500], Train Loss: 0.2272\n",
      "Epoch [5/500], Train Loss: 0.2233\n",
      "Epoch [6/500], Train Loss: 0.2243\n",
      "Epoch [7/500], Train Loss: 0.2282\n",
      "Epoch [8/500], Train Loss: 0.2406\n",
      "Epoch [9/500], Train Loss: 0.2411\n",
      "Epoch [10/500], Train Loss: 0.2508\n",
      "Epoch [11/500], Train Loss: 0.2332\n",
      "Epoch [12/500], Train Loss: 0.1764\n",
      "Epoch [13/500], Train Loss: 0.1663\n",
      "Epoch [14/500], Train Loss: 0.1625\n",
      "Epoch [15/500], Train Loss: 0.1635\n",
      "Epoch [16/500], Train Loss: 0.1604\n",
      "Epoch [17/500], Train Loss: 0.1602\n",
      "Epoch [18/500], Train Loss: 0.1590\n",
      "Epoch [19/500], Train Loss: 0.1590\n",
      "Epoch [20/500], Train Loss: 0.1595\n",
      "Epoch [21/500], Train Loss: 0.1592\n",
      "Epoch [22/500], Train Loss: 0.1583\n",
      "Epoch [23/500], Train Loss: 0.1603\n",
      "Epoch [24/500], Train Loss: 0.1573\n",
      "Epoch [25/500], Train Loss: 0.1573\n",
      "Epoch [26/500], Train Loss: 0.1569\n",
      "Epoch [27/500], Train Loss: 0.1577\n",
      "Epoch [28/500], Train Loss: 0.1586\n",
      "Epoch [29/500], Train Loss: 0.1572\n",
      "Epoch [30/500], Train Loss: 0.1581\n",
      "Epoch [31/500], Train Loss: 0.1576\n",
      "Epoch [32/500], Train Loss: 0.1576\n",
      "Epoch [33/500], Train Loss: 0.1508\n",
      "Epoch [34/500], Train Loss: 0.1505\n",
      "Epoch [35/500], Train Loss: 0.1501\n",
      "Epoch [36/500], Train Loss: 0.1503\n",
      "Epoch [37/500], Train Loss: 0.1506\n",
      "Epoch [38/500], Train Loss: 0.1501\n",
      "Epoch [39/500], Train Loss: 0.1503\n",
      "Epoch [40/500], Train Loss: 0.1503\n",
      "Epoch [41/500], Train Loss: 0.1506\n",
      "Epoch [42/500], Train Loss: 0.1500\n",
      "Epoch [43/500], Train Loss: 0.1505\n",
      "Epoch [44/500], Train Loss: 0.1499\n",
      "Epoch [45/500], Train Loss: 0.1499\n",
      "Epoch [46/500], Train Loss: 0.1500\n",
      "Epoch [47/500], Train Loss: 0.1503\n",
      "Epoch [48/500], Train Loss: 0.1499\n",
      "Epoch [49/500], Train Loss: 0.1502\n",
      "Epoch [50/500], Train Loss: 0.1498\n",
      "Epoch [51/500], Train Loss: 0.1492\n",
      "Epoch [52/500], Train Loss: 0.1492\n",
      "Epoch [53/500], Train Loss: 0.1491\n",
      "Epoch [54/500], Train Loss: 0.1491\n",
      "Epoch [55/500], Train Loss: 0.1494\n",
      "Epoch [56/500], Train Loss: 0.1491\n",
      "Epoch [57/500], Train Loss: 0.1489\n",
      "Epoch [58/500], Train Loss: 0.1493\n",
      "Epoch [59/500], Train Loss: 0.1491\n",
      "Epoch [60/500], Train Loss: 0.1495\n",
      "Epoch [61/500], Train Loss: 0.1492\n",
      "Epoch [62/500], Train Loss: 0.1490\n",
      "Epoch [63/500], Train Loss: 0.1490\n",
      "Epoch [64/500], Train Loss: 0.1493\n",
      "Epoch [65/500], Train Loss: 0.1491\n",
      "Epoch [66/500], Train Loss: 0.1495\n",
      "Epoch [67/500], Train Loss: 0.1488\n",
      "Epoch [68/500], Train Loss: 0.1490\n",
      "Epoch [69/500], Train Loss: 0.1491\n",
      "Epoch [70/500], Train Loss: 0.1490\n",
      "Epoch [71/500], Train Loss: 0.1494\n",
      "Epoch [72/500], Train Loss: 0.1494\n",
      "Epoch [73/500], Train Loss: 0.1493\n",
      "Epoch [74/500], Train Loss: 0.1488\n",
      "Epoch [75/500], Train Loss: 0.1486\n",
      "Epoch [76/500], Train Loss: 0.1487\n",
      "Epoch [77/500], Train Loss: 0.1496\n",
      "Epoch [78/500], Train Loss: 0.1490\n",
      "Epoch [79/500], Train Loss: 0.1490\n",
      "Epoch [80/500], Train Loss: 0.1489\n",
      "Epoch [81/500], Train Loss: 0.1487\n",
      "Epoch [82/500], Train Loss: 0.1487\n",
      "Epoch [83/500], Train Loss: 0.1493\n",
      "Epoch [84/500], Train Loss: 0.1489\n",
      "Epoch [85/500], Train Loss: 0.1492\n",
      "Epoch [86/500], Train Loss: 0.1493\n",
      "Epoch [87/500], Train Loss: 0.1491\n",
      "Epoch [88/500], Train Loss: 0.1487\n",
      "Epoch [89/500], Train Loss: 0.1493\n",
      "Epoch [90/500], Train Loss: 0.1492\n",
      "Epoch [91/500], Train Loss: 0.1490\n",
      "Epoch [92/500], Train Loss: 0.1490\n",
      "Early stopping at epoch 92\n",
      "Final model saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/models_unphased/chr18/final_model_chr18.pth\n",
      "Individual R^2 scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr18/individual_r2_scores_chr18.csv\n",
      "Individual IQS scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr18/individual_iqs_scores_chr18.csv\n",
      "Individual AUC ROC curves saved in: ../../Data/model_results_unphased_all_PRS/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/performance_metrics.csv\n",
      "Total SNPs:  280\n",
      "PRS313 SNPs:  7\n",
      "Total SNPs used for Training:  273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-04-30 10:16:28,992] Trial 51 finished with value: 0.05365990317211702 and parameters: {'learning_rate': 0.0541193007360449, 'l1_coef': 1.6601253217205744e-05, 'patience': 12, 'batch_size': 32}. Best is trial 27 with value: 0.04425642937421799.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 19 - Best hyperparameters: {'learning_rate': 0.07098016523175216, 'l1_coef': 1.0298921090840162e-05, 'patience': 11, 'batch_size': 128}\n",
      "Chr 19 - Best value: 0.0443\n",
      "Epoch [1/500], Train Loss: 0.7952\n",
      "Epoch [2/500], Train Loss: 0.2552\n",
      "Epoch [3/500], Train Loss: 0.1461\n",
      "Epoch [4/500], Train Loss: 0.1086\n",
      "Epoch [5/500], Train Loss: 0.0903\n",
      "Epoch [6/500], Train Loss: 0.0807\n",
      "Epoch [7/500], Train Loss: 0.0741\n",
      "Epoch [8/500], Train Loss: 0.0708\n",
      "Epoch [9/500], Train Loss: 0.0670\n",
      "Epoch [10/500], Train Loss: 0.0639\n",
      "Epoch [11/500], Train Loss: 0.0611\n",
      "Epoch [12/500], Train Loss: 0.0579\n",
      "Epoch [13/500], Train Loss: 0.0558\n",
      "Epoch [14/500], Train Loss: 0.0560\n",
      "Epoch [15/500], Train Loss: 0.0539\n",
      "Epoch [16/500], Train Loss: 0.0540\n",
      "Epoch [17/500], Train Loss: 0.0525\n",
      "Epoch [18/500], Train Loss: 0.0517\n",
      "Epoch [19/500], Train Loss: 0.0524\n",
      "Epoch [20/500], Train Loss: 0.0489\n",
      "Epoch [21/500], Train Loss: 0.0474\n",
      "Epoch [22/500], Train Loss: 0.0459\n",
      "Epoch [23/500], Train Loss: 0.0494\n",
      "Epoch [24/500], Train Loss: 0.0459\n",
      "Epoch [25/500], Train Loss: 0.0444\n",
      "Epoch [26/500], Train Loss: 0.0457\n",
      "Epoch [27/500], Train Loss: 0.0463\n",
      "Epoch [28/500], Train Loss: 0.0436\n",
      "Epoch [29/500], Train Loss: 0.0428\n",
      "Epoch [30/500], Train Loss: 0.0417\n",
      "Epoch [31/500], Train Loss: 0.0415\n",
      "Epoch [32/500], Train Loss: 0.0420\n",
      "Epoch [33/500], Train Loss: 0.0423\n",
      "Epoch [34/500], Train Loss: 0.0438\n",
      "Epoch [35/500], Train Loss: 0.0455\n",
      "Epoch [36/500], Train Loss: 0.0498\n",
      "Epoch [37/500], Train Loss: 0.0428\n",
      "Epoch [38/500], Train Loss: 0.0390\n",
      "Epoch [39/500], Train Loss: 0.0371\n",
      "Epoch [40/500], Train Loss: 0.0367\n",
      "Epoch [41/500], Train Loss: 0.0369\n",
      "Epoch [42/500], Train Loss: 0.0364\n",
      "Epoch [43/500], Train Loss: 0.0363\n",
      "Epoch [44/500], Train Loss: 0.0368\n",
      "Epoch [45/500], Train Loss: 0.0365\n",
      "Epoch [46/500], Train Loss: 0.0363\n",
      "Epoch [47/500], Train Loss: 0.0361\n",
      "Epoch [48/500], Train Loss: 0.0365\n",
      "Epoch [49/500], Train Loss: 0.0362\n",
      "Epoch [50/500], Train Loss: 0.0359\n",
      "Epoch [51/500], Train Loss: 0.0361\n",
      "Epoch [52/500], Train Loss: 0.0363\n",
      "Epoch [53/500], Train Loss: 0.0362\n",
      "Epoch [54/500], Train Loss: 0.0361\n",
      "Epoch [55/500], Train Loss: 0.0358\n",
      "Epoch [56/500], Train Loss: 0.0355\n",
      "Epoch [57/500], Train Loss: 0.0359\n",
      "Epoch [58/500], Train Loss: 0.0357\n",
      "Epoch [59/500], Train Loss: 0.0355\n",
      "Epoch [60/500], Train Loss: 0.0357\n",
      "Epoch [61/500], Train Loss: 0.0353\n",
      "Epoch [62/500], Train Loss: 0.0353\n",
      "Epoch [63/500], Train Loss: 0.0353\n",
      "Epoch [64/500], Train Loss: 0.0352\n",
      "Epoch [65/500], Train Loss: 0.0356\n",
      "Epoch [66/500], Train Loss: 0.0358\n",
      "Epoch [67/500], Train Loss: 0.0353\n",
      "Epoch [68/500], Train Loss: 0.0351\n",
      "Epoch [69/500], Train Loss: 0.0350\n",
      "Epoch [70/500], Train Loss: 0.0353\n",
      "Epoch [71/500], Train Loss: 0.0357\n",
      "Epoch [72/500], Train Loss: 0.0349\n",
      "Epoch [73/500], Train Loss: 0.0351\n",
      "Epoch [74/500], Train Loss: 0.0351\n",
      "Epoch [75/500], Train Loss: 0.0351\n",
      "Epoch [76/500], Train Loss: 0.0351\n",
      "Epoch [77/500], Train Loss: 0.0351\n",
      "Epoch [78/500], Train Loss: 0.0354\n",
      "Epoch [79/500], Train Loss: 0.0346\n",
      "Epoch [80/500], Train Loss: 0.0344\n",
      "Epoch [81/500], Train Loss: 0.0345\n",
      "Epoch [82/500], Train Loss: 0.0344\n",
      "Epoch [83/500], Train Loss: 0.0345\n",
      "Epoch [84/500], Train Loss: 0.0344\n",
      "Epoch [85/500], Train Loss: 0.0344\n",
      "Epoch [86/500], Train Loss: 0.0349\n",
      "Epoch [87/500], Train Loss: 0.0344\n",
      "Epoch [88/500], Train Loss: 0.0346\n",
      "Epoch [89/500], Train Loss: 0.0343\n",
      "Epoch [90/500], Train Loss: 0.0344\n",
      "Epoch [91/500], Train Loss: 0.0345\n",
      "Epoch [92/500], Train Loss: 0.0345\n",
      "Epoch [93/500], Train Loss: 0.0344\n",
      "Epoch [94/500], Train Loss: 0.0345\n",
      "Epoch [95/500], Train Loss: 0.0342\n",
      "Epoch [96/500], Train Loss: 0.0344\n",
      "Epoch [97/500], Train Loss: 0.0343\n",
      "Epoch [98/500], Train Loss: 0.0344\n",
      "Epoch [99/500], Train Loss: 0.0343\n",
      "Epoch [100/500], Train Loss: 0.0343\n",
      "Epoch [101/500], Train Loss: 0.0343\n",
      "Epoch [102/500], Train Loss: 0.0341\n",
      "Epoch [103/500], Train Loss: 0.0343\n",
      "Epoch [104/500], Train Loss: 0.0345\n",
      "Epoch [105/500], Train Loss: 0.0342\n",
      "Epoch [106/500], Train Loss: 0.0345\n",
      "Epoch [107/500], Train Loss: 0.0342\n",
      "Epoch [108/500], Train Loss: 0.0344\n",
      "Epoch [109/500], Train Loss: 0.0343\n",
      "Epoch [110/500], Train Loss: 0.0343\n",
      "Epoch [111/500], Train Loss: 0.0342\n",
      "Epoch [112/500], Train Loss: 0.0343\n",
      "Epoch [113/500], Train Loss: 0.0345\n",
      "Early stopping at epoch 113\n",
      "Final model saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/models_unphased/chr19/final_model_chr19.pth\n",
      "Individual R^2 scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr19/individual_r2_scores_chr19.csv\n",
      "Individual IQS scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr19/individual_iqs_scores_chr19.csv\n",
      "Individual AUC ROC curves saved in: ../../Data/model_results_unphased_all_PRS/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/performance_metrics.csv\n",
      "Total SNPs:  61\n",
      "PRS313 SNPs:  4\n",
      "Total SNPs used for Training:  57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-30 10:16:31,392] Trial 51 finished with value: 0.16642819259847913 and parameters: {'learning_rate': 0.059122094780524326, 'l1_coef': 2.4162510707354773e-05, 'patience': 13, 'batch_size': 64}. Best is trial 23 with value: 0.11483677476644516.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 20 - Best hyperparameters: {'learning_rate': 0.011727770284572503, 'l1_coef': 1.0588262176994207e-05, 'patience': 9, 'batch_size': 128}\n",
      "Chr 20 - Best value: 0.1148\n",
      "Epoch [1/500], Train Loss: 0.3474\n",
      "Epoch [2/500], Train Loss: 0.2138\n",
      "Epoch [3/500], Train Loss: 0.2090\n",
      "Epoch [4/500], Train Loss: 0.2010\n",
      "Epoch [5/500], Train Loss: 0.1963\n",
      "Epoch [6/500], Train Loss: 0.1947\n",
      "Epoch [7/500], Train Loss: 0.1913\n",
      "Epoch [8/500], Train Loss: 0.1887\n",
      "Epoch [9/500], Train Loss: 0.1870\n",
      "Epoch [10/500], Train Loss: 0.1861\n",
      "Epoch [11/500], Train Loss: 0.1843\n",
      "Epoch [12/500], Train Loss: 0.1822\n",
      "Epoch [13/500], Train Loss: 0.1826\n",
      "Epoch [14/500], Train Loss: 0.1808\n",
      "Epoch [15/500], Train Loss: 0.1802\n",
      "Epoch [16/500], Train Loss: 0.1789\n",
      "Epoch [17/500], Train Loss: 0.1780\n",
      "Epoch [18/500], Train Loss: 0.1777\n",
      "Epoch [19/500], Train Loss: 0.1777\n",
      "Epoch [20/500], Train Loss: 0.1761\n",
      "Epoch [21/500], Train Loss: 0.1748\n",
      "Epoch [22/500], Train Loss: 0.1748\n",
      "Epoch [23/500], Train Loss: 0.1744\n",
      "Epoch [24/500], Train Loss: 0.1752\n",
      "Epoch [25/500], Train Loss: 0.1739\n",
      "Epoch [26/500], Train Loss: 0.1731\n",
      "Epoch [27/500], Train Loss: 0.1723\n",
      "Epoch [28/500], Train Loss: 0.1719\n",
      "Epoch [29/500], Train Loss: 0.1713\n",
      "Epoch [30/500], Train Loss: 0.1728\n",
      "Epoch [31/500], Train Loss: 0.1716\n",
      "Epoch [32/500], Train Loss: 0.1711\n",
      "Epoch [33/500], Train Loss: 0.1701\n",
      "Epoch [34/500], Train Loss: 0.1701\n",
      "Epoch [35/500], Train Loss: 0.1693\n",
      "Epoch [36/500], Train Loss: 0.1702\n",
      "Epoch [37/500], Train Loss: 0.1687\n",
      "Epoch [38/500], Train Loss: 0.1698\n",
      "Epoch [39/500], Train Loss: 0.1685\n",
      "Epoch [40/500], Train Loss: 0.1686\n",
      "Epoch [41/500], Train Loss: 0.1677\n",
      "Epoch [42/500], Train Loss: 0.1688\n",
      "Epoch [43/500], Train Loss: 0.1673\n",
      "Epoch [44/500], Train Loss: 0.1689\n",
      "Epoch [45/500], Train Loss: 0.1674\n",
      "Epoch [46/500], Train Loss: 0.1660\n",
      "Epoch [47/500], Train Loss: 0.1666\n",
      "Epoch [48/500], Train Loss: 0.1672\n",
      "Epoch [49/500], Train Loss: 0.1665\n",
      "Epoch [50/500], Train Loss: 0.1663\n",
      "Epoch [51/500], Train Loss: 0.1666\n",
      "Epoch [52/500], Train Loss: 0.1663\n",
      "Epoch [53/500], Train Loss: 0.1651\n",
      "Epoch [54/500], Train Loss: 0.1648\n",
      "Epoch [55/500], Train Loss: 0.1651\n",
      "Epoch [56/500], Train Loss: 0.1647\n",
      "Epoch [57/500], Train Loss: 0.1650\n",
      "Epoch [58/500], Train Loss: 0.1648\n",
      "Epoch [59/500], Train Loss: 0.1640\n",
      "Epoch [60/500], Train Loss: 0.1642\n",
      "Epoch [61/500], Train Loss: 0.1648\n",
      "Epoch [62/500], Train Loss: 0.1644\n",
      "Epoch [63/500], Train Loss: 0.1647\n",
      "Epoch [64/500], Train Loss: 0.1637\n",
      "Epoch [65/500], Train Loss: 0.1644\n",
      "Epoch [66/500], Train Loss: 0.1648\n",
      "Epoch [67/500], Train Loss: 0.1643\n",
      "Epoch [68/500], Train Loss: 0.1656\n",
      "Epoch [69/500], Train Loss: 0.1646\n",
      "Epoch [70/500], Train Loss: 0.1651\n",
      "Epoch [71/500], Train Loss: 0.1640\n",
      "Epoch [72/500], Train Loss: 0.1648\n",
      "Epoch [73/500], Train Loss: 0.1649\n",
      "Early stopping at epoch 73\n",
      "Final model saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/models_unphased/chr20/final_model_chr20.pth\n",
      "Individual R^2 scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr20/individual_r2_scores_chr20.csv\n",
      "Individual IQS scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr20/individual_iqs_scores_chr20.csv\n",
      "Individual AUC ROC curves saved in: ../../Data/model_results_unphased_all_PRS/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/performance_metrics.csv\n",
      "Total SNPs:  44\n",
      "PRS313 SNPs:  5\n",
      "Total SNPs used for Training:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-30 10:16:35,114] Trial 51 finished with value: 0.20866690851174868 and parameters: {'learning_rate': 0.00425629587460915, 'l1_coef': 2.5980568836531632e-05, 'patience': 19, 'batch_size': 32}. Best is trial 27 with value: 0.0400828163006476.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 21 - Best hyperparameters: {'learning_rate': 0.07634654140233994, 'l1_coef': 1.331717466479133e-05, 'patience': 19, 'batch_size': 64}\n",
      "Chr 21 - Best value: 0.0401\n",
      "Epoch [1/500], Train Loss: 0.4141\n",
      "Epoch [2/500], Train Loss: 0.2711\n",
      "Epoch [3/500], Train Loss: 0.2404\n",
      "Epoch [4/500], Train Loss: 0.2337\n",
      "Epoch [5/500], Train Loss: 0.2256\n",
      "Epoch [6/500], Train Loss: 0.2236\n",
      "Epoch [7/500], Train Loss: 0.2190\n",
      "Epoch [8/500], Train Loss: 0.2169\n",
      "Epoch [9/500], Train Loss: 0.2185\n",
      "Epoch [10/500], Train Loss: 0.2199\n",
      "Epoch [11/500], Train Loss: 0.2182\n",
      "Epoch [12/500], Train Loss: 0.2169\n",
      "Epoch [13/500], Train Loss: 0.2207\n",
      "Epoch [14/500], Train Loss: 0.2147\n",
      "Epoch [15/500], Train Loss: 0.2141\n",
      "Epoch [16/500], Train Loss: 0.2201\n",
      "Epoch [17/500], Train Loss: 0.2209\n",
      "Epoch [18/500], Train Loss: 0.2192\n",
      "Epoch [19/500], Train Loss: 0.2154\n",
      "Epoch [20/500], Train Loss: 0.2151\n",
      "Epoch [21/500], Train Loss: 0.2159\n",
      "Epoch [22/500], Train Loss: 0.2031\n",
      "Epoch [23/500], Train Loss: 0.2027\n",
      "Epoch [24/500], Train Loss: 0.2028\n",
      "Epoch [25/500], Train Loss: 0.2031\n",
      "Epoch [26/500], Train Loss: 0.2023\n",
      "Epoch [27/500], Train Loss: 0.2047\n",
      "Epoch [28/500], Train Loss: 0.2046\n",
      "Epoch [29/500], Train Loss: 0.2039\n",
      "Epoch [30/500], Train Loss: 0.2041\n",
      "Epoch [31/500], Train Loss: 0.2025\n",
      "Epoch [32/500], Train Loss: 0.2018\n",
      "Epoch [33/500], Train Loss: 0.2017\n",
      "Epoch [34/500], Train Loss: 0.2039\n",
      "Epoch [35/500], Train Loss: 0.2025\n",
      "Epoch [36/500], Train Loss: 0.2049\n",
      "Epoch [37/500], Train Loss: 0.2023\n",
      "Epoch [38/500], Train Loss: 0.2048\n",
      "Epoch [39/500], Train Loss: 0.2010\n",
      "Epoch [40/500], Train Loss: 0.2029\n",
      "Epoch [41/500], Train Loss: 0.2016\n",
      "Epoch [42/500], Train Loss: 0.2007\n",
      "Epoch [43/500], Train Loss: 0.2007\n",
      "Epoch [44/500], Train Loss: 0.2016\n",
      "Epoch [45/500], Train Loss: 0.2009\n",
      "Epoch [46/500], Train Loss: 0.2012\n",
      "Epoch [47/500], Train Loss: 0.2020\n",
      "Epoch [48/500], Train Loss: 0.2017\n",
      "Epoch [49/500], Train Loss: 0.2027\n",
      "Epoch [50/500], Train Loss: 0.2039\n",
      "Epoch [51/500], Train Loss: 0.2018\n",
      "Epoch [52/500], Train Loss: 0.2014\n",
      "Epoch [53/500], Train Loss: 0.2017\n",
      "Epoch [54/500], Train Loss: 0.2074\n",
      "Epoch [55/500], Train Loss: 0.2008\n",
      "Epoch [56/500], Train Loss: 0.2008\n",
      "Epoch [57/500], Train Loss: 0.2068\n",
      "Epoch [58/500], Train Loss: 0.2023\n",
      "Epoch [59/500], Train Loss: 0.2014\n",
      "Epoch [60/500], Train Loss: 0.2013\n",
      "Epoch [61/500], Train Loss: 0.2011\n",
      "Early stopping at epoch 61\n",
      "Final model saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/models_unphased/chr21/final_model_chr21.pth\n",
      "Individual R^2 scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr21/individual_r2_scores_chr21.csv\n",
      "Individual IQS scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr21/individual_iqs_scores_chr21.csv\n",
      "Individual AUC ROC curves saved in: ../../Data/model_results_unphased_all_PRS/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/performance_metrics.csv\n",
      "Total SNPs:  397\n",
      "PRS313 SNPs:  11\n",
      "Total SNPs used for Training:  386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-30 10:16:37,198] Trial 51 finished with value: 0.20031897425651551 and parameters: {'learning_rate': 0.027286468996185014, 'l1_coef': 1.610250127616247e-05, 'patience': 10, 'batch_size': 64}. Best is trial 40 with value: 0.06794291579952608.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 22 - Best hyperparameters: {'learning_rate': 0.02943043957881216, 'l1_coef': 1.0232605917875621e-05, 'patience': 12, 'batch_size': 32}\n",
      "Chr 22 - Best value: 0.0679\n",
      "Epoch [1/500], Train Loss: 0.3877\n",
      "Epoch [2/500], Train Loss: 0.2737\n",
      "Epoch [3/500], Train Loss: 0.2511\n",
      "Epoch [4/500], Train Loss: 0.2488\n",
      "Epoch [5/500], Train Loss: 0.2414\n",
      "Epoch [6/500], Train Loss: 0.2249\n",
      "Epoch [7/500], Train Loss: 0.2303\n",
      "Epoch [8/500], Train Loss: 0.2174\n",
      "Epoch [9/500], Train Loss: 0.2177\n",
      "Epoch [10/500], Train Loss: 0.2222\n",
      "Epoch [11/500], Train Loss: 0.2170\n",
      "Epoch [12/500], Train Loss: 0.2150\n",
      "Epoch [13/500], Train Loss: 0.2105\n",
      "Epoch [14/500], Train Loss: 0.2217\n",
      "Epoch [15/500], Train Loss: 0.2207\n",
      "Epoch [16/500], Train Loss: 0.2137\n",
      "Epoch [17/500], Train Loss: 0.2130\n",
      "Epoch [18/500], Train Loss: 0.2163\n",
      "Epoch [19/500], Train Loss: 0.2125\n",
      "Epoch [20/500], Train Loss: 0.1824\n",
      "Epoch [21/500], Train Loss: 0.1783\n",
      "Epoch [22/500], Train Loss: 0.1776\n",
      "Epoch [23/500], Train Loss: 0.1772\n",
      "Epoch [24/500], Train Loss: 0.1764\n",
      "Epoch [25/500], Train Loss: 0.1767\n",
      "Epoch [26/500], Train Loss: 0.1763\n",
      "Epoch [27/500], Train Loss: 0.1758\n",
      "Epoch [28/500], Train Loss: 0.1756\n",
      "Epoch [29/500], Train Loss: 0.1762\n",
      "Epoch [30/500], Train Loss: 0.1768\n",
      "Epoch [31/500], Train Loss: 0.1760\n",
      "Epoch [32/500], Train Loss: 0.1755\n",
      "Epoch [33/500], Train Loss: 0.1755\n",
      "Epoch [34/500], Train Loss: 0.1762\n",
      "Epoch [35/500], Train Loss: 0.1760\n",
      "Epoch [36/500], Train Loss: 0.1750\n",
      "Epoch [37/500], Train Loss: 0.1770\n",
      "Epoch [38/500], Train Loss: 0.1761\n",
      "Epoch [39/500], Train Loss: 0.1758\n",
      "Epoch [40/500], Train Loss: 0.1758\n",
      "Epoch [41/500], Train Loss: 0.1756\n",
      "Epoch [42/500], Train Loss: 0.1755\n",
      "Epoch [43/500], Train Loss: 0.1722\n",
      "Epoch [44/500], Train Loss: 0.1718\n",
      "Epoch [45/500], Train Loss: 0.1717\n",
      "Epoch [46/500], Train Loss: 0.1715\n",
      "Epoch [47/500], Train Loss: 0.1716\n",
      "Epoch [48/500], Train Loss: 0.1715\n",
      "Epoch [49/500], Train Loss: 0.1715\n",
      "Epoch [50/500], Train Loss: 0.1715\n",
      "Epoch [51/500], Train Loss: 0.1718\n",
      "Epoch [52/500], Train Loss: 0.1715\n",
      "Epoch [53/500], Train Loss: 0.1717\n",
      "Epoch [54/500], Train Loss: 0.1717\n",
      "Epoch [55/500], Train Loss: 0.1716\n",
      "Epoch [56/500], Train Loss: 0.1713\n",
      "Epoch [57/500], Train Loss: 0.1714\n",
      "Epoch [58/500], Train Loss: 0.1711\n",
      "Epoch [59/500], Train Loss: 0.1712\n",
      "Epoch [60/500], Train Loss: 0.1715\n",
      "Epoch [61/500], Train Loss: 0.1711\n",
      "Epoch [62/500], Train Loss: 0.1711\n",
      "Epoch [63/500], Train Loss: 0.1712\n",
      "Epoch [64/500], Train Loss: 0.1711\n",
      "Epoch [65/500], Train Loss: 0.1708\n",
      "Epoch [66/500], Train Loss: 0.1712\n",
      "Epoch [67/500], Train Loss: 0.1711\n",
      "Epoch [68/500], Train Loss: 0.1712\n",
      "Epoch [69/500], Train Loss: 0.1712\n",
      "Epoch [70/500], Train Loss: 0.1712\n",
      "Epoch [71/500], Train Loss: 0.1710\n",
      "Epoch [72/500], Train Loss: 0.1711\n",
      "Epoch [73/500], Train Loss: 0.1709\n",
      "Epoch [74/500], Train Loss: 0.1711\n",
      "Epoch [75/500], Train Loss: 0.1713\n",
      "Epoch [76/500], Train Loss: 0.1712\n",
      "Epoch [77/500], Train Loss: 0.1711\n",
      "Early stopping at epoch 77\n",
      "Final model saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/models_unphased/chr22/final_model_chr22.pth\n",
      "Individual R^2 scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr22/individual_r2_scores_chr22.csv\n",
      "Individual IQS scores saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/chr22/individual_iqs_scores_chr22.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yg/982c10113957_2gb06y92y35sx509h/T/ipykernel_81857/73971118.py:42: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  iqs = (po - pc) / (1 - pc)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual AUC ROC curves saved in: ../../Data/model_results_unphased_all_PRS/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/performance_metrics.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "from sklearn.metrics import r2_score as sklearn_r2_score\n",
    "import optuna\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# Load and preprocess the data\n",
    "data_directory = '../../Data/Filtered_unphased_training_data/'\n",
    "start = 1\n",
    "\n",
    "# Initialize lists to store the performance metrics for each chromosome\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "false_positive_rates = []\n",
    "auc_rocs = []\n",
    "r2_scores = []\n",
    "iqs_scores = []\n",
    "\n",
    "# Create folders for saving files\n",
    "output_folder = \"../../Data/model_results_unphased_all_PRS/logistic_regression/\"\n",
    "model_folder = output_folder + \"models_unphased/\"\n",
    "csv_folder = output_folder + \"csv_files/\"\n",
    "curve_folder = output_folder + \"roc_curves/\"\n",
    "\n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "os.makedirs(csv_folder, exist_ok=True)\n",
    "os.makedirs(curve_folder, exist_ok=True)\n",
    "\n",
    "for chromosome_number in range(start, 23):\n",
    "    # Create subfolders for the current chromosome\n",
    "    chr_model_folder = model_folder + f\"chr{chromosome_number}/\"\n",
    "    chr_csv_folder = csv_folder + f\"chr{chromosome_number}/\"\n",
    "    chr_curve_folder = curve_folder + f\"chr{chromosome_number}/\"\n",
    "\n",
    "    os.makedirs(chr_model_folder, exist_ok=True)\n",
    "    os.makedirs(chr_csv_folder, exist_ok=True)\n",
    "    os.makedirs(chr_curve_folder, exist_ok=True)\n",
    "\n",
    "    file_name = data_directory + f\"23AndMe_PRS313_merged_chr{chromosome_number}_matching_combined.parquet\"\n",
    "    data = pd.read_parquet(file_name)\n",
    "\n",
    "\n",
    "    # Split the data into features and target\n",
    "    X = torch.tensor(data.filter(regex='^(?!.*PRS313_)').values, dtype=torch.float32)\n",
    "    y = torch.tensor(data.filter(regex='PRS313_').values, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    print(\"Total SNPs: \", data.shape[1])\n",
    "    print(\"PRS313 SNPs: \", y.shape[1])\n",
    "    # print(\"Known PRS313 SNPs: \", data[[col for col in data.columns if \"PRS313_Known\" in col]].shape[1])\n",
    "    # print(\"23AndMe SNPs with LD to Unknown PRS313 SNPs: \", data[[col for col in data.columns if \"PRS313_\" not in col]].shape[1])\n",
    "    print(\"Total SNPs used for Training: \", X.shape[1])\n",
    "\n",
    "    # Split the data into train-validation and test sets\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Define the logistic regression model with lasso regularization\n",
    "    class LogisticRegression(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim, l1_coef=0.0):\n",
    "            super(LogisticRegression, self).__init__()\n",
    "            self.linear = nn.Linear(input_dim, output_dim)\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "            self.l1_coef = l1_coef\n",
    "\n",
    "        def forward(self, x):\n",
    "            out = self.linear(x)\n",
    "            out = self.sigmoid(out)\n",
    "            return out\n",
    "\n",
    "        def l1_loss(self):\n",
    "            return self.l1_coef * torch.norm(self.linear.weight, p=1)\n",
    "        \n",
    "    # Set the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Set the hyperparameters for tuning\n",
    "    input_dim = X_train_val.shape[1]\n",
    "    output_dim = y_train_val.shape[1]\n",
    "    num_epochs = 500\n",
    "    batch_size = 128\n",
    "\n",
    "    # Define the objective function for Optuna with cross-validation and early stopping\n",
    "    def objective(trial):\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-1, log=True)\n",
    "        l1_coef = trial.suggest_float('l1_coef', 1e-5, 1e-1, log=True)\n",
    "        patience = trial.suggest_int('patience', 5, 20)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "\n",
    "        model = LogisticRegression(input_dim, output_dim, l1_coef).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        criterion = nn.BCELoss()\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=False)\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        fold_losses = []\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_val, y_train_val.argmax(dim=1))):\n",
    "            X_train, X_val = X_train_val[train_idx], X_train_val[val_idx]\n",
    "            y_train, y_val = y_train_val[train_idx], y_train_val[val_idx]\n",
    "\n",
    "            train_dataset = TensorDataset(X_train, y_train)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "            best_val_loss = float('inf')\n",
    "            counter = 0\n",
    "\n",
    "            for epoch in range(num_epochs):\n",
    "                train_loss = 0.0\n",
    "                for batch_X, batch_y in train_loader:\n",
    "                    batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "                    outputs = model(batch_X)\n",
    "                    loss = criterion(outputs, batch_y) + model.l1_loss()\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    train_loss += loss.item()\n",
    "\n",
    "                train_loss /= len(train_loader)\n",
    "\n",
    "                val_dataset = TensorDataset(X_val, y_val)\n",
    "                val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    val_loss = 0.0\n",
    "                    for batch_X, batch_y in val_loader:\n",
    "                        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                        outputs = model(batch_X)\n",
    "                        loss = criterion(outputs, batch_y) + model.l1_loss()\n",
    "                        val_loss += loss.item()\n",
    "\n",
    "                    val_loss /= len(val_loader)\n",
    "                    scheduler.step(val_loss)\n",
    "\n",
    "                    if val_loss < best_val_loss:\n",
    "                        best_val_loss = val_loss\n",
    "                        counter = 0\n",
    "                    else:\n",
    "                        counter += 1\n",
    "\n",
    "                    if counter >= patience:\n",
    "                        # print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                        break\n",
    "\n",
    "            fold_losses.append(best_val_loss)\n",
    "\n",
    "        return np.mean(fold_losses)\n",
    "\n",
    "    # Create the \"optuna_studies\" folder if it doesn't exist\n",
    "    os.makedirs(\"optuna_studies\", exist_ok=True)\n",
    "\n",
    "    # Create an Optuna study and optimize the hyperparameters\n",
    "    study_name = f\"chr{chromosome_number}_study\"\n",
    "    storage_name = f\"sqlite:///optuna_studies/{study_name}.db\"\n",
    "\n",
    "    # Check if the study exists\n",
    "\n",
    "    current_dir = os.getcwd()\n",
    "    study_exists = os.path.exists(current_dir + f\"/optuna_studies/{study_name}.db\")\n",
    "    \n",
    "    if study_exists:\n",
    "        # Load the existing study\n",
    "        study = optuna.load_study(study_name=study_name, storage=storage_name)\n",
    "    else:\n",
    "        # Create a new study\n",
    "        study = optuna.create_study(direction='minimize', study_name=study_name, storage=storage_name)\n",
    "\n",
    "    study.optimize(objective, n_trials=1, n_jobs=-1)\n",
    "\n",
    "    # Print the best hyperparameters and best value\n",
    "    print(f\"Chr {chromosome_number} - Best hyperparameters: {study.best_params}\")\n",
    "    print(f\"Chr {chromosome_number} - Best value: {study.best_value:.4f}\")\n",
    "\n",
    "    # Train the final model with the best hyperparameters and early stopping\n",
    "    best_learning_rate = study.best_params['learning_rate']\n",
    "    best_l1_coef = study.best_params['l1_coef']\n",
    "    best_patience = study.best_params['patience']\n",
    "    best_batch_size = study.best_params['batch_size']\n",
    "\n",
    "    model = LogisticRegression(input_dim, output_dim, best_l1_coef).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_learning_rate)\n",
    "    criterion = nn.BCELoss()\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=False)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_val, y_train_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=best_batch_size, shuffle=True)\n",
    "\n",
    "    best_train_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y) + model.l1_loss()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        # print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "        if train_loss < best_train_loss:\n",
    "            best_train_loss = train_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        if counter >= best_patience:\n",
    "            # print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "        scheduler.step(train_loss)\n",
    "\n",
    "    # Save the final model\n",
    "    model_save_path = chr_model_folder + f'final_model_chr{chromosome_number}.pth'\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"Final model saved at: {model_save_path}\")\n",
    "\n",
    "    # Evaluate the final model on the test set\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test.to(device))\n",
    "        test_preds = (test_outputs > 0.5).float()\n",
    "        test_accuracy = float(((test_preds > 0.5) == y_test).float().mean())\n",
    "        test_precision = precision_score(y_test.cpu().numpy(), test_preds.cpu().numpy(), average='micro')\n",
    "        test_recall = recall_score(y_test.cpu().numpy(), test_preds.cpu().numpy(), average='micro')\n",
    "        test_f1 = f1_score(y_test.cpu().numpy(), test_preds.cpu().numpy(), average='micro')\n",
    "        test_roc_auc = roc_auc_score(y_test.cpu().numpy(), test_outputs.cpu().numpy(), average='micro')\n",
    "        test_r2 = sklearn_r2_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "        test_iqs = calculate_iqs_unphased(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "\n",
    "        # Calculate false positive rate\n",
    "        cm = confusion_matrix(y_test.cpu().numpy().ravel(), test_preds.cpu().numpy().ravel())\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        test_fpr = fp / (fp + tn)\n",
    "\n",
    "        # Append performance metrics to the lists\n",
    "        accuracies.append(test_accuracy)\n",
    "        precisions.append(test_precision)\n",
    "        recalls.append(test_recall)\n",
    "        false_positive_rates.append(test_fpr)\n",
    "        auc_rocs.append(test_roc_auc)\n",
    "        r2_scores.append(test_r2)\n",
    "        iqs_scores.append(test_iqs)\n",
    "\n",
    "        # Calculate individual R^2 scores for each SNP\n",
    "        individual_r2_scores = sklearn_r2_score(y_test.cpu().numpy(), test_outputs.cpu().numpy(), multioutput='raw_values')\n",
    "\n",
    "        # Calculate individual IQS scores for each SNP\n",
    "        individual_iqs_scores = np.array([calculate_iqs_unphased(y_test.cpu().numpy()[:, i].reshape(-1, 1), test_outputs.cpu().numpy()[:, i].reshape(-1, 1)) for i in range(y_test.shape[1])])\n",
    "\n",
    "        # Get the names of the SNPs from the original dataframe\n",
    "        snp_names = data.filter(regex='Unknown').columns\n",
    "\n",
    "        # Save individual R^2 scores to a CSV file\n",
    "        csv_file = chr_csv_folder + f'individual_r2_scores_chr{chromosome_number}.csv'\n",
    "\n",
    "        with open(csv_file, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['SNP', 'R2 Score'])\n",
    "            for snp, r2_score in zip(snp_names, individual_r2_scores):\n",
    "                writer.writerow([snp, r2_score])\n",
    "\n",
    "        print(f\"Individual R^2 scores saved at: {csv_file}\")\n",
    "\n",
    "        # Save individual IQS scores to a CSV file\n",
    "        iqs_csv_file = chr_csv_folder + f'individual_iqs_scores_chr{chromosome_number}.csv'\n",
    "\n",
    "        with open(iqs_csv_file, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['SNP', 'IQS Score'])\n",
    "            for snp, iqs_score in zip(snp_names, individual_iqs_scores):\n",
    "                writer.writerow([snp, iqs_score])\n",
    "\n",
    "        print(f\"Individual IQS scores saved at: {iqs_csv_file}\")\n",
    "\n",
    "        # Save individual AUC ROC curves for each SNP\n",
    "        for i, snp in enumerate(snp_names):\n",
    "            try: \n",
    "                fpr, tpr, _ = roc_curve(y_test.cpu().numpy()[:, i], test_outputs.cpu().numpy()[:, i])\n",
    "                plt.figure()\n",
    "                plt.plot(fpr, tpr, label=f'AUC ROC = {roc_auc_score(y_test.cpu().numpy()[:, i], test_outputs.cpu().numpy()[:, i]):.4f}')\n",
    "                plt.xlabel('False Positive Rate')\n",
    "                plt.ylabel('True Positive Rate')\n",
    "                plt.title(f'AUC ROC Curve - {snp}')\n",
    "                plt.legend()\n",
    "                \n",
    "                curve_file = chr_curve_folder + f'auc_roc_curve_{snp}_chr{chromosome_number}.png'\n",
    "                plt.savefig(curve_file)\n",
    "                plt.close()\n",
    "            except ValueError:\n",
    "                # Save a placeholder image if there is insufficient data\n",
    "                plt.figure()\n",
    "                plt.axis('off')\n",
    "                plt.text(0.5, 0.5, \"Insufficient data for ROC curve\", ha='center', va='center')\n",
    "                curve_file = chr_curve_folder + f'auc_roc_curve_{snp}_chr{chromosome_number}.png'\n",
    "                plt.savefig(curve_file)\n",
    "                plt.close()\n",
    "\n",
    "                print(f\"Skipping SNP {snp} due to insufficient data\")\n",
    "\n",
    "\n",
    "        print(f\"Individual AUC ROC curves saved in: {curve_folder}\")\n",
    "\n",
    "        # Create a DataFrame to store the performance metrics for each chromosome\n",
    "        performance_df = pd.DataFrame({\n",
    "            'Chromosome': list(range(start, chromosome_number + 1)),\n",
    "            'Accuracy': accuracies,\n",
    "            'Precision': precisions,\n",
    "            'Recall': recalls,\n",
    "            'False Positive Rate': false_positive_rates,\n",
    "            'AUC ROC': auc_rocs,\n",
    "            'R2 Score': r2_scores,\n",
    "            'IQS Score': iqs_scores\n",
    "        })\n",
    "\n",
    "        # Save the performance metrics to a CSV file\n",
    "        performance_csv_file = csv_folder + 'performance_metrics.csv'\n",
    "        performance_df.to_csv(performance_csv_file, index=False)\n",
    "        print(f\"Performance metrics saved at: {performance_csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRS313 SNPs saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/prs313_snps.csv\n",
      "Total number of PRS313 SNPs: 314\n"
     ]
    }
   ],
   "source": [
    "# Loop through all the training datasets and document the PRS313 SNPs in each dataset. Save this to a CSV file.\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data_directory = '../../Data/Filtered_unphased_training_data/'\n",
    "output_folder = \"../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/\"\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Initialize a list to store the PRS313 SNPs in each dataset\n",
    "prs313_snps = []\n",
    "\n",
    "for chromosome_number in range(1, 23):\n",
    "    file_name = data_directory + \\\n",
    "        f\"23AndMe_PRS313_merged_chr{chromosome_number}_matching_combined.parquet\"\n",
    "    data = pd.read_parquet(file_name)\n",
    "\n",
    "    prs313_snps.append(data.filter(regex='PRS313_').columns)\n",
    "\n",
    "# Create a DataFrame to store the PRS313 SNPs in each dataset\n",
    "prs313_df = pd.DataFrame({\n",
    "    'Chromosome': list(range(1, 23)),\n",
    "    'PRS313 SNPs': prs313_snps,\n",
    "    \"Number of PRS313 SNPs\": [len(snps) for snps in prs313_snps]\n",
    "})\n",
    "\n",
    "# Save the PRS313 SNPs to a CSV file\n",
    "prs313_csv_file = output_folder + 'prs313_snps.csv'\n",
    "prs313_df.to_csv(prs313_csv_file, index=False)\n",
    "print(f\"PRS313 SNPs saved at: {prs313_csv_file}\")\n",
    "\n",
    "# Print the total number of PRS313 SNPs in all datasets\n",
    "total_prs313_snps = sum(prs313_df[\"Number of PRS313 SNPs\"])\n",
    "print(f\"Total number of PRS313 SNPs: {total_prs313_snps}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     30\n",
       "1     21\n",
       "2     16\n",
       "3     11\n",
       "4     34\n",
       "5     20\n",
       "6     14\n",
       "7     21\n",
       "8     14\n",
       "9     18\n",
       "10    19\n",
       "11    17\n",
       "12     5\n",
       "13     8\n",
       "14     7\n",
       "15    14\n",
       "16     9\n",
       "17     9\n",
       "18     7\n",
       "19     4\n",
       "20     5\n",
       "21    11\n",
       "Name: Number of PRS313 SNPs, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prs313_df[\"Number of PRS313 SNPs\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
