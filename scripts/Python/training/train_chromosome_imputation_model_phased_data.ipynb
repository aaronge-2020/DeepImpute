{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IQS Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_iqs(true_genotypes, imputed_dosages):\n",
    "    # Convert imputed dosages to discrete values\n",
    "    imputed_discrete = np.round(imputed_dosages).astype(int)\n",
    "\n",
    "    # Clip the imputed discrete values to be within the range of 0 to 2\n",
    "    imputed_discrete = np.clip(imputed_discrete, 0, 2)\n",
    "\n",
    "    # Create a contingency table\n",
    "    contingency_table = np.zeros((3, 3), dtype=int)\n",
    "\n",
    "    # Fill the contingency table\n",
    "    for true_geno, imputed_geno in zip(true_genotypes, imputed_discrete):\n",
    "        for true_allele, imputed_allele in zip(true_geno, imputed_geno):\n",
    "            contingency_table[int(true_allele), int(imputed_allele)] += 1\n",
    "\n",
    "    # Calculate the total number of genotypes\n",
    "    total_genotypes = np.sum(contingency_table)\n",
    "\n",
    "    # Calculate observed proportion of agreement (Po)\n",
    "    observed_agreement = np.trace(contingency_table) / total_genotypes\n",
    "\n",
    "    # Calculate marginal sums\n",
    "    row_marginals = np.sum(contingency_table, axis=1)\n",
    "    col_marginals = np.sum(contingency_table, axis=0)\n",
    "\n",
    "    # Calculate chance agreement (Pc)\n",
    "    chance_agreement = np.sum((row_marginals * col_marginals) / (total_genotypes ** 2))\n",
    "\n",
    "    # Calculate IQS\n",
    "    if chance_agreement == 1:  # To prevent division by zero in case of perfect chance agreement\n",
    "        iqs_score = 0\n",
    "    else:\n",
    "        iqs_score = (observed_agreement - chance_agreement) / (1 - chance_agreement)\n",
    "\n",
    "    return iqs_score\n",
    "\n",
    "# Example usage:\n",
    "true_genotypes = np.array([[0, 1, 2], [1, 2, 0], [2, 0, 1]])\n",
    "imputed_dosages = np.array([[0.1, 1.2, 1.9], [1.0, 1.8, 0.3], [2.0, 0.5, 1.4]])\n",
    "\n",
    "iqs_score = calculate_iqs(true_genotypes, imputed_dosages)\n",
    "print(f\"IQS Score: {iqs_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total SNPs:  4056\n",
      "PRS313 SNPs:  60\n",
      "Total SNPs used for Training:  3996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-06-18 11:44:23,327] Trial 84 finished with value: 0.18371078372001648 and parameters: {'learning_rate': 0.00023651832542293644, 'l1_coef': 1.6183137333100933e-05, 'patience': 8, 'batch_size': 256}. Best is trial 55 with value: 0.0865513201802969.\n",
      "[I 2024-06-18 11:44:25,513] Trial 89 finished with value: 0.17923060953617095 and parameters: {'learning_rate': 0.00025997567252216097, 'l1_coef': 1.6320712506802015e-05, 'patience': 8, 'batch_size': 256}. Best is trial 55 with value: 0.0865513201802969.\n",
      "[I 2024-06-18 11:44:26,057] Trial 82 finished with value: 0.1767739474773407 and parameters: {'learning_rate': 0.00027094527463766333, 'l1_coef': 1.6155766632101717e-05, 'patience': 8, 'batch_size': 256}. Best is trial 55 with value: 0.0865513201802969.\n",
      "[I 2024-06-18 11:44:26,476] Trial 81 finished with value: 0.17002045959234238 and parameters: {'learning_rate': 0.00029573312329341097, 'l1_coef': 1.5081674735551774e-05, 'patience': 8, 'batch_size': 256}. Best is trial 55 with value: 0.0865513201802969.\n",
      "[I 2024-06-18 11:44:27,668] Trial 83 finished with value: 0.1745196357369423 and parameters: {'learning_rate': 0.0002670546108498733, 'l1_coef': 1.4867254741883222e-05, 'patience': 8, 'batch_size': 256}. Best is trial 55 with value: 0.0865513201802969.\n",
      "[I 2024-06-18 11:44:29,797] Trial 86 finished with value: 0.1835630401968956 and parameters: {'learning_rate': 0.00023937733112507325, 'l1_coef': 1.6403700210819972e-05, 'patience': 7, 'batch_size': 256}. Best is trial 55 with value: 0.0865513201802969.\n",
      "[I 2024-06-18 11:44:31,237] Trial 87 finished with value: 0.1899745374917984 and parameters: {'learning_rate': 0.0002298588708104377, 'l1_coef': 1.822412152313776e-05, 'patience': 8, 'batch_size': 256}. Best is trial 55 with value: 0.0865513201802969.\n",
      "[I 2024-06-18 11:44:32,186] Trial 90 finished with value: 0.17819986790418624 and parameters: {'learning_rate': 0.0002576201752838273, 'l1_coef': 1.5705703231493558e-05, 'patience': 8, 'batch_size': 256}. Best is trial 55 with value: 0.0865513201802969.\n",
      "[I 2024-06-18 11:44:32,407] Trial 85 finished with value: 0.17765355706214905 and parameters: {'learning_rate': 0.0002779511170159451, 'l1_coef': 1.7115290344206605e-05, 'patience': 8, 'batch_size': 256}. Best is trial 55 with value: 0.0865513201802969.\n",
      "[I 2024-06-18 11:44:32,546] Trial 88 finished with value: 0.16868214160203934 and parameters: {'learning_rate': 0.00030996654801831335, 'l1_coef': 1.5569537374654075e-05, 'patience': 8, 'batch_size': 256}. Best is trial 55 with value: 0.0865513201802969.\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 1 - Best hyperparameters: {'learning_rate': 0.012254038414284674, 'l1_coef': 1.0101565691518604e-05, 'patience': 14, 'batch_size': 128}\n",
      "Chr 1 - Best value: 0.0866\n",
      "Epoch [1/500], Train Loss: 1.4850\n",
      "Epoch [2/500], Train Loss: 0.8234\n",
      "Epoch [3/500], Train Loss: 0.6218\n",
      "Epoch [4/500], Train Loss: 0.5539\n",
      "Epoch [5/500], Train Loss: 0.5144\n",
      "Epoch [6/500], Train Loss: 0.4836\n",
      "Epoch [7/500], Train Loss: 0.4656\n",
      "Epoch [8/500], Train Loss: 0.4446\n",
      "Epoch [9/500], Train Loss: 0.3014\n",
      "Epoch [10/500], Train Loss: 0.1575\n",
      "Epoch [11/500], Train Loss: 0.1416\n",
      "Epoch [12/500], Train Loss: 0.1305\n",
      "Epoch [13/500], Train Loss: 0.1247\n",
      "Epoch [14/500], Train Loss: 0.1213\n",
      "Epoch [15/500], Train Loss: 0.1177\n",
      "Epoch [16/500], Train Loss: 0.1138\n",
      "Epoch [17/500], Train Loss: 0.1109\n",
      "Epoch [18/500], Train Loss: 0.1086\n",
      "Epoch [19/500], Train Loss: 0.1062\n",
      "Epoch [20/500], Train Loss: 0.1044\n",
      "Epoch [21/500], Train Loss: 0.1026\n",
      "Epoch [22/500], Train Loss: 0.1018\n",
      "Epoch [23/500], Train Loss: 0.1007\n",
      "Epoch [24/500], Train Loss: 0.0996\n",
      "Epoch [25/500], Train Loss: 0.0975\n",
      "Epoch [26/500], Train Loss: 0.0961\n",
      "Epoch [27/500], Train Loss: 0.0951\n",
      "Epoch [28/500], Train Loss: 0.0949\n",
      "Epoch [29/500], Train Loss: 0.0935\n",
      "Epoch [30/500], Train Loss: 0.0921\n",
      "Epoch [31/500], Train Loss: 0.0911\n",
      "Epoch [32/500], Train Loss: 0.0896\n",
      "Epoch [33/500], Train Loss: 0.0896\n",
      "Epoch [34/500], Train Loss: 0.0891\n",
      "Epoch [35/500], Train Loss: 0.0880\n",
      "Epoch [36/500], Train Loss: 0.0873\n",
      "Epoch [37/500], Train Loss: 0.0865\n",
      "Epoch [38/500], Train Loss: 0.0862\n",
      "Epoch [39/500], Train Loss: 0.0863\n",
      "Epoch [40/500], Train Loss: 0.0844\n",
      "Epoch [41/500], Train Loss: 0.0839\n",
      "Epoch [42/500], Train Loss: 0.0837\n",
      "Epoch [43/500], Train Loss: 0.0833\n",
      "Epoch [44/500], Train Loss: 0.0827\n",
      "Epoch [45/500], Train Loss: 0.0825\n",
      "Epoch [46/500], Train Loss: 0.0819\n",
      "Epoch [47/500], Train Loss: 0.0818\n",
      "Epoch [48/500], Train Loss: 0.0812\n",
      "Epoch [49/500], Train Loss: 0.0812\n",
      "Epoch [50/500], Train Loss: 0.0804\n",
      "Epoch [51/500], Train Loss: 0.0804\n",
      "Epoch [52/500], Train Loss: 0.0800\n",
      "Epoch [53/500], Train Loss: 0.0805\n",
      "Epoch [54/500], Train Loss: 0.0794\n",
      "Epoch [55/500], Train Loss: 0.0783\n",
      "Epoch [56/500], Train Loss: 0.0786\n",
      "Epoch [57/500], Train Loss: 0.0790\n",
      "Epoch [58/500], Train Loss: 0.0785\n",
      "Epoch [59/500], Train Loss: 0.0789\n",
      "Epoch [60/500], Train Loss: 0.0782\n",
      "Epoch [61/500], Train Loss: 0.0781\n",
      "Epoch [62/500], Train Loss: 0.0786\n",
      "Epoch [63/500], Train Loss: 0.0781\n",
      "Epoch [64/500], Train Loss: 0.0774\n",
      "Epoch [65/500], Train Loss: 0.0771\n",
      "Epoch [66/500], Train Loss: 0.0778\n",
      "Epoch [67/500], Train Loss: 0.0767\n",
      "Epoch [68/500], Train Loss: 0.0767\n",
      "Epoch [69/500], Train Loss: 0.0770\n",
      "Epoch [70/500], Train Loss: 0.0757\n",
      "Epoch [71/500], Train Loss: 0.0753\n",
      "Epoch [72/500], Train Loss: 0.0762\n",
      "Epoch [73/500], Train Loss: 0.0764\n",
      "Epoch [74/500], Train Loss: 0.0771\n",
      "Epoch [75/500], Train Loss: 0.0770\n",
      "Epoch [76/500], Train Loss: 0.0761\n",
      "Epoch [77/500], Train Loss: 0.0759\n",
      "Epoch [78/500], Train Loss: 0.0705\n",
      "Epoch [79/500], Train Loss: 0.0678\n",
      "Epoch [80/500], Train Loss: 0.0666\n",
      "Epoch [81/500], Train Loss: 0.0661\n",
      "Epoch [82/500], Train Loss: 0.0658\n",
      "Epoch [83/500], Train Loss: 0.0656\n",
      "Epoch [84/500], Train Loss: 0.0655\n",
      "Epoch [85/500], Train Loss: 0.0654\n",
      "Epoch [86/500], Train Loss: 0.0653\n",
      "Epoch [87/500], Train Loss: 0.0652\n",
      "Epoch [88/500], Train Loss: 0.0652\n",
      "Epoch [89/500], Train Loss: 0.0651\n",
      "Epoch [90/500], Train Loss: 0.0651\n",
      "Epoch [91/500], Train Loss: 0.0651\n",
      "Epoch [92/500], Train Loss: 0.0650\n",
      "Epoch [93/500], Train Loss: 0.0651\n",
      "Epoch [94/500], Train Loss: 0.0651\n",
      "Epoch [95/500], Train Loss: 0.0650\n",
      "Epoch [96/500], Train Loss: 0.0650\n",
      "Epoch [97/500], Train Loss: 0.0650\n",
      "Epoch [98/500], Train Loss: 0.0650\n",
      "Epoch [99/500], Train Loss: 0.0649\n",
      "Epoch [100/500], Train Loss: 0.0649\n",
      "Epoch [101/500], Train Loss: 0.0648\n",
      "Epoch [102/500], Train Loss: 0.0648\n",
      "Epoch [103/500], Train Loss: 0.0648\n",
      "Epoch [104/500], Train Loss: 0.0648\n",
      "Epoch [105/500], Train Loss: 0.0648\n",
      "Epoch [106/500], Train Loss: 0.0648\n",
      "Epoch [107/500], Train Loss: 0.0647\n",
      "Epoch [108/500], Train Loss: 0.0648\n",
      "Epoch [109/500], Train Loss: 0.0647\n",
      "Epoch [110/500], Train Loss: 0.0647\n",
      "Epoch [111/500], Train Loss: 0.0647\n",
      "Epoch [112/500], Train Loss: 0.0646\n",
      "Epoch [113/500], Train Loss: 0.0647\n",
      "Epoch [114/500], Train Loss: 0.0647\n",
      "Epoch [115/500], Train Loss: 0.0646\n",
      "Epoch [116/500], Train Loss: 0.0646\n",
      "Epoch [117/500], Train Loss: 0.0646\n",
      "Epoch [118/500], Train Loss: 0.0646\n",
      "Epoch [119/500], Train Loss: 0.0646\n",
      "Epoch [120/500], Train Loss: 0.0646\n",
      "Epoch [121/500], Train Loss: 0.0646\n",
      "Epoch [122/500], Train Loss: 0.0646\n",
      "Epoch [123/500], Train Loss: 0.0646\n",
      "Epoch [124/500], Train Loss: 0.0645\n",
      "Epoch [125/500], Train Loss: 0.0645\n",
      "Epoch [126/500], Train Loss: 0.0645\n",
      "Epoch [127/500], Train Loss: 0.0645\n",
      "Epoch [128/500], Train Loss: 0.0645\n",
      "Epoch [129/500], Train Loss: 0.0645\n",
      "Epoch [130/500], Train Loss: 0.0644\n",
      "Epoch [131/500], Train Loss: 0.0644\n",
      "Epoch [132/500], Train Loss: 0.0644\n",
      "Epoch [133/500], Train Loss: 0.0644\n",
      "Epoch [134/500], Train Loss: 0.0644\n",
      "Epoch [135/500], Train Loss: 0.0644\n",
      "Epoch [136/500], Train Loss: 0.0644\n",
      "Epoch [137/500], Train Loss: 0.0643\n",
      "Epoch [138/500], Train Loss: 0.0643\n",
      "Epoch [139/500], Train Loss: 0.0643\n",
      "Epoch [140/500], Train Loss: 0.0643\n",
      "Epoch [141/500], Train Loss: 0.0643\n",
      "Epoch [142/500], Train Loss: 0.0643\n",
      "Epoch [143/500], Train Loss: 0.0642\n",
      "Epoch [144/500], Train Loss: 0.0643\n",
      "Epoch [145/500], Train Loss: 0.0642\n",
      "Epoch [146/500], Train Loss: 0.0641\n",
      "Epoch [147/500], Train Loss: 0.0642\n",
      "Epoch [148/500], Train Loss: 0.0641\n",
      "Epoch [149/500], Train Loss: 0.0641\n",
      "Epoch [150/500], Train Loss: 0.0642\n",
      "Epoch [151/500], Train Loss: 0.0642\n",
      "Epoch [152/500], Train Loss: 0.0642\n",
      "Epoch [153/500], Train Loss: 0.0642\n",
      "Epoch [154/500], Train Loss: 0.0641\n",
      "Epoch [155/500], Train Loss: 0.0635\n",
      "Epoch [156/500], Train Loss: 0.0631\n",
      "Epoch [157/500], Train Loss: 0.0630\n",
      "Epoch [158/500], Train Loss: 0.0629\n",
      "Epoch [159/500], Train Loss: 0.0629\n",
      "Epoch [160/500], Train Loss: 0.0629\n",
      "Epoch [161/500], Train Loss: 0.0630\n",
      "Epoch [162/500], Train Loss: 0.0629\n",
      "Epoch [163/500], Train Loss: 0.0629\n",
      "Epoch [164/500], Train Loss: 0.0629\n",
      "Epoch [165/500], Train Loss: 0.0629\n",
      "Epoch [166/500], Train Loss: 0.0629\n",
      "Epoch [167/500], Train Loss: 0.0629\n",
      "Epoch [168/500], Train Loss: 0.0629\n",
      "Epoch [169/500], Train Loss: 0.0629\n",
      "Epoch [170/500], Train Loss: 0.0628\n",
      "Epoch [171/500], Train Loss: 0.0629\n",
      "Epoch [172/500], Train Loss: 0.0629\n",
      "Epoch [173/500], Train Loss: 0.0628\n",
      "Epoch [174/500], Train Loss: 0.0629\n",
      "Epoch [175/500], Train Loss: 0.0629\n",
      "Epoch [176/500], Train Loss: 0.0628\n",
      "Epoch [177/500], Train Loss: 0.0628\n",
      "Epoch [178/500], Train Loss: 0.0629\n",
      "Epoch [179/500], Train Loss: 0.0629\n",
      "Epoch [180/500], Train Loss: 0.0628\n",
      "Epoch [181/500], Train Loss: 0.0629\n",
      "Epoch [182/500], Train Loss: 0.0629\n",
      "Epoch [183/500], Train Loss: 0.0627\n",
      "Epoch [184/500], Train Loss: 0.0627\n",
      "Epoch [185/500], Train Loss: 0.0627\n",
      "Epoch [186/500], Train Loss: 0.0627\n",
      "Epoch [187/500], Train Loss: 0.0627\n",
      "Epoch [188/500], Train Loss: 0.0628\n",
      "Epoch [189/500], Train Loss: 0.0627\n",
      "Epoch [190/500], Train Loss: 0.0628\n",
      "Epoch [191/500], Train Loss: 0.0627\n",
      "Epoch [192/500], Train Loss: 0.0627\n",
      "Epoch [193/500], Train Loss: 0.0627\n",
      "Epoch [194/500], Train Loss: 0.0627\n",
      "Epoch [195/500], Train Loss: 0.0627\n",
      "Epoch [196/500], Train Loss: 0.0627\n",
      "Epoch [197/500], Train Loss: 0.0627\n",
      "Epoch [198/500], Train Loss: 0.0627\n",
      "Epoch [199/500], Train Loss: 0.0627\n",
      "Epoch [200/500], Train Loss: 0.0627\n",
      "Epoch [201/500], Train Loss: 0.0627\n",
      "Epoch [202/500], Train Loss: 0.0627\n",
      "Epoch [203/500], Train Loss: 0.0627\n",
      "Epoch [204/500], Train Loss: 0.0627\n",
      "Epoch [205/500], Train Loss: 0.0627\n",
      "Epoch [206/500], Train Loss: 0.0627\n",
      "Epoch [207/500], Train Loss: 0.0627\n",
      "Epoch [208/500], Train Loss: 0.0627\n",
      "Epoch [209/500], Train Loss: 0.0627\n",
      "Epoch [210/500], Train Loss: 0.0627\n",
      "Epoch [211/500], Train Loss: 0.0627\n",
      "Epoch [212/500], Train Loss: 0.0627\n",
      "Epoch [213/500], Train Loss: 0.0627\n",
      "Epoch [214/500], Train Loss: 0.0627\n",
      "Epoch [215/500], Train Loss: 0.0627\n",
      "Epoch [216/500], Train Loss: 0.0627\n",
      "Epoch [217/500], Train Loss: 0.0627\n",
      "Epoch [218/500], Train Loss: 0.0627\n",
      "Epoch [219/500], Train Loss: 0.0627\n",
      "Epoch [220/500], Train Loss: 0.0627\n",
      "Epoch [221/500], Train Loss: 0.0627\n",
      "Epoch [222/500], Train Loss: 0.0627\n",
      "Epoch [223/500], Train Loss: 0.0627\n",
      "Epoch [224/500], Train Loss: 0.0627\n",
      "Early stopping at epoch 224\n",
      "Final model saved at: ../../../Data/model_results/logistic_regression/models/chr1/final_model_chr1.pth\n",
      "Individual R^2 scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr1/individual_r2_scores_chr1.csv\n",
      "Individual IQS scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr1/individual_iqs_scores_chr1.csv\n",
      "Individual AUC ROC curves saved in: ../../../Data/model_results/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../../Data/model_results/logistic_regression/csv_files/performance_metrics.csv\n",
      "Total SNPs:  2088\n",
      "PRS313 SNPs:  42\n",
      "Total SNPs used for Training:  2046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-06-18 11:45:50,525] Trial 57 finished with value: 0.11485631838440895 and parameters: {'learning_rate': 0.016521549557693263, 'l1_coef': 1.8485221379272658e-05, 'patience': 12, 'batch_size': 128}. Best is trial 25 with value: 0.09493996109813452.\n",
      "[I 2024-06-18 11:45:59,381] Trial 54 finished with value: 0.11340021379292012 and parameters: {'learning_rate': 0.01702029055657234, 'l1_coef': 1.7350493209451e-05, 'patience': 14, 'batch_size': 128}. Best is trial 25 with value: 0.09493996109813452.\n",
      "[I 2024-06-18 11:46:02,917] Trial 52 finished with value: 0.1146919772028923 and parameters: {'learning_rate': 0.0161801200121631, 'l1_coef': 1.9402323060655733e-05, 'patience': 14, 'batch_size': 128}. Best is trial 25 with value: 0.09493996109813452.\n",
      "[I 2024-06-18 11:46:03,743] Trial 59 finished with value: 0.10882821343839169 and parameters: {'learning_rate': 0.01610085996523582, 'l1_coef': 1.782756342700332e-05, 'patience': 14, 'batch_size': 128}. Best is trial 25 with value: 0.09493996109813452.\n",
      "[I 2024-06-18 11:46:05,145] Trial 51 finished with value: 0.10721468217670918 and parameters: {'learning_rate': 0.018700042550330663, 'l1_coef': 1.7440142558586e-05, 'patience': 14, 'batch_size': 128}. Best is trial 25 with value: 0.09493996109813452.\n",
      "[I 2024-06-18 11:46:06,833] Trial 58 finished with value: 0.10662334114313125 and parameters: {'learning_rate': 0.014968067264613052, 'l1_coef': 1.8638819263753677e-05, 'patience': 14, 'batch_size': 128}. Best is trial 25 with value: 0.09493996109813452.\n",
      "[I 2024-06-18 11:46:07,055] Trial 55 finished with value: 0.10616731829941273 and parameters: {'learning_rate': 0.016809706263013712, 'l1_coef': 1.7391185992600126e-05, 'patience': 14, 'batch_size': 128}. Best is trial 25 with value: 0.09493996109813452.\n",
      "[I 2024-06-18 11:46:07,140] Trial 53 finished with value: 0.10880797430872917 and parameters: {'learning_rate': 0.01675512263938256, 'l1_coef': 1.813290804364395e-05, 'patience': 14, 'batch_size': 128}. Best is trial 25 with value: 0.09493996109813452.\n",
      "[I 2024-06-18 11:46:07,275] Trial 56 finished with value: 0.10905521884560584 and parameters: {'learning_rate': 0.012576282905669194, 'l1_coef': 1.7200444767773946e-05, 'patience': 14, 'batch_size': 128}. Best is trial 25 with value: 0.09493996109813452.\n",
      "[I 2024-06-18 11:46:08,387] Trial 60 finished with value: 0.11237899996340275 and parameters: {'learning_rate': 0.016087438459658435, 'l1_coef': 1.7359671114882448e-05, 'patience': 14, 'batch_size': 128}. Best is trial 25 with value: 0.09493996109813452.\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 2 - Best hyperparameters: {'learning_rate': 0.02154387590823124, 'l1_coef': 1.1376943462129345e-05, 'patience': 17, 'batch_size': 128}\n",
      "Chr 2 - Best value: 0.0949\n",
      "Epoch [1/500], Train Loss: 1.9818\n",
      "Epoch [2/500], Train Loss: 1.2570\n",
      "Epoch [3/500], Train Loss: 1.0592\n",
      "Epoch [4/500], Train Loss: 0.9875\n",
      "Epoch [5/500], Train Loss: 0.9400\n",
      "Epoch [6/500], Train Loss: 0.9310\n",
      "Epoch [7/500], Train Loss: 0.9015\n",
      "Epoch [8/500], Train Loss: 0.8749\n",
      "Epoch [9/500], Train Loss: 0.8502\n",
      "Epoch [10/500], Train Loss: 0.7036\n",
      "Epoch [11/500], Train Loss: 0.3951\n",
      "Epoch [12/500], Train Loss: 0.1729\n",
      "Epoch [13/500], Train Loss: 0.1588\n",
      "Epoch [14/500], Train Loss: 0.1511\n",
      "Epoch [15/500], Train Loss: 0.1399\n",
      "Epoch [16/500], Train Loss: 0.1144\n",
      "Epoch [17/500], Train Loss: 0.1058\n",
      "Epoch [18/500], Train Loss: 0.1011\n",
      "Epoch [19/500], Train Loss: 0.0984\n",
      "Epoch [20/500], Train Loss: 0.0963\n",
      "Epoch [21/500], Train Loss: 0.0952\n",
      "Epoch [22/500], Train Loss: 0.0950\n",
      "Epoch [23/500], Train Loss: 0.0939\n",
      "Epoch [24/500], Train Loss: 0.0915\n",
      "Epoch [25/500], Train Loss: 0.0892\n",
      "Epoch [26/500], Train Loss: 0.0875\n",
      "Epoch [27/500], Train Loss: 0.0866\n",
      "Epoch [28/500], Train Loss: 0.0863\n",
      "Epoch [29/500], Train Loss: 0.0865\n",
      "Epoch [30/500], Train Loss: 0.0849\n",
      "Epoch [31/500], Train Loss: 0.0840\n",
      "Epoch [32/500], Train Loss: 0.0835\n",
      "Epoch [33/500], Train Loss: 0.0822\n",
      "Epoch [34/500], Train Loss: 0.0810\n",
      "Epoch [35/500], Train Loss: 0.0816\n",
      "Epoch [36/500], Train Loss: 0.0811\n",
      "Epoch [37/500], Train Loss: 0.0820\n",
      "Epoch [38/500], Train Loss: 0.0813\n",
      "Epoch [39/500], Train Loss: 0.0794\n",
      "Epoch [40/500], Train Loss: 0.0789\n",
      "Epoch [41/500], Train Loss: 0.0796\n",
      "Epoch [42/500], Train Loss: 0.0781\n",
      "Epoch [43/500], Train Loss: 0.0777\n",
      "Epoch [44/500], Train Loss: 0.0782\n",
      "Epoch [45/500], Train Loss: 0.0781\n",
      "Epoch [46/500], Train Loss: 0.0751\n",
      "Epoch [47/500], Train Loss: 0.0754\n",
      "Epoch [48/500], Train Loss: 0.0763\n",
      "Epoch [49/500], Train Loss: 0.0762\n",
      "Epoch [50/500], Train Loss: 0.0752\n",
      "Epoch [51/500], Train Loss: 0.0742\n",
      "Epoch [52/500], Train Loss: 0.0744\n",
      "Epoch [53/500], Train Loss: 0.0743\n",
      "Epoch [54/500], Train Loss: 0.0739\n",
      "Epoch [55/500], Train Loss: 0.0740\n",
      "Epoch [56/500], Train Loss: 0.0732\n",
      "Epoch [57/500], Train Loss: 0.0726\n",
      "Epoch [58/500], Train Loss: 0.0730\n",
      "Epoch [59/500], Train Loss: 0.0737\n",
      "Epoch [60/500], Train Loss: 0.0743\n",
      "Epoch [61/500], Train Loss: 0.0741\n",
      "Epoch [62/500], Train Loss: 0.0733\n",
      "Epoch [63/500], Train Loss: 0.0729\n",
      "Epoch [64/500], Train Loss: 0.0670\n",
      "Epoch [65/500], Train Loss: 0.0653\n",
      "Epoch [66/500], Train Loss: 0.0646\n",
      "Epoch [67/500], Train Loss: 0.0644\n",
      "Epoch [68/500], Train Loss: 0.0641\n",
      "Epoch [69/500], Train Loss: 0.0640\n",
      "Epoch [70/500], Train Loss: 0.0639\n",
      "Epoch [71/500], Train Loss: 0.0638\n",
      "Epoch [72/500], Train Loss: 0.0637\n",
      "Epoch [73/500], Train Loss: 0.0636\n",
      "Epoch [74/500], Train Loss: 0.0635\n",
      "Epoch [75/500], Train Loss: 0.0635\n",
      "Epoch [76/500], Train Loss: 0.0635\n",
      "Epoch [77/500], Train Loss: 0.0634\n",
      "Epoch [78/500], Train Loss: 0.0634\n",
      "Epoch [79/500], Train Loss: 0.0633\n",
      "Epoch [80/500], Train Loss: 0.0633\n",
      "Epoch [81/500], Train Loss: 0.0633\n",
      "Epoch [82/500], Train Loss: 0.0632\n",
      "Epoch [83/500], Train Loss: 0.0633\n",
      "Epoch [84/500], Train Loss: 0.0632\n",
      "Epoch [85/500], Train Loss: 0.0633\n",
      "Epoch [86/500], Train Loss: 0.0632\n",
      "Epoch [87/500], Train Loss: 0.0632\n",
      "Epoch [88/500], Train Loss: 0.0632\n",
      "Epoch [89/500], Train Loss: 0.0631\n",
      "Epoch [90/500], Train Loss: 0.0631\n",
      "Epoch [91/500], Train Loss: 0.0632\n",
      "Epoch [92/500], Train Loss: 0.0630\n",
      "Epoch [93/500], Train Loss: 0.0630\n",
      "Epoch [94/500], Train Loss: 0.0630\n",
      "Epoch [95/500], Train Loss: 0.0630\n",
      "Epoch [96/500], Train Loss: 0.0629\n",
      "Epoch [97/500], Train Loss: 0.0630\n",
      "Epoch [98/500], Train Loss: 0.0630\n",
      "Epoch [99/500], Train Loss: 0.0629\n",
      "Epoch [100/500], Train Loss: 0.0629\n",
      "Epoch [101/500], Train Loss: 0.0627\n",
      "Epoch [102/500], Train Loss: 0.0628\n",
      "Epoch [103/500], Train Loss: 0.0628\n",
      "Epoch [104/500], Train Loss: 0.0627\n",
      "Epoch [105/500], Train Loss: 0.0627\n",
      "Epoch [106/500], Train Loss: 0.0627\n",
      "Epoch [107/500], Train Loss: 0.0627\n",
      "Epoch [108/500], Train Loss: 0.0627\n",
      "Epoch [109/500], Train Loss: 0.0626\n",
      "Epoch [110/500], Train Loss: 0.0626\n",
      "Epoch [111/500], Train Loss: 0.0627\n",
      "Epoch [112/500], Train Loss: 0.0627\n",
      "Epoch [113/500], Train Loss: 0.0627\n",
      "Epoch [114/500], Train Loss: 0.0625\n",
      "Epoch [115/500], Train Loss: 0.0626\n",
      "Epoch [116/500], Train Loss: 0.0625\n",
      "Epoch [117/500], Train Loss: 0.0625\n",
      "Epoch [118/500], Train Loss: 0.0624\n",
      "Epoch [119/500], Train Loss: 0.0625\n",
      "Epoch [120/500], Train Loss: 0.0625\n",
      "Epoch [121/500], Train Loss: 0.0624\n",
      "Epoch [122/500], Train Loss: 0.0624\n",
      "Epoch [123/500], Train Loss: 0.0624\n",
      "Epoch [124/500], Train Loss: 0.0624\n",
      "Epoch [125/500], Train Loss: 0.0623\n",
      "Epoch [126/500], Train Loss: 0.0624\n",
      "Epoch [127/500], Train Loss: 0.0623\n",
      "Epoch [128/500], Train Loss: 0.0624\n",
      "Epoch [129/500], Train Loss: 0.0625\n",
      "Epoch [130/500], Train Loss: 0.0623\n",
      "Epoch [131/500], Train Loss: 0.0622\n",
      "Epoch [132/500], Train Loss: 0.0622\n",
      "Epoch [133/500], Train Loss: 0.0622\n",
      "Epoch [134/500], Train Loss: 0.0621\n",
      "Epoch [135/500], Train Loss: 0.0622\n",
      "Epoch [136/500], Train Loss: 0.0621\n",
      "Epoch [137/500], Train Loss: 0.0621\n",
      "Epoch [138/500], Train Loss: 0.0622\n",
      "Epoch [139/500], Train Loss: 0.0622\n",
      "Epoch [140/500], Train Loss: 0.0620\n",
      "Epoch [141/500], Train Loss: 0.0621\n",
      "Epoch [142/500], Train Loss: 0.0620\n",
      "Epoch [143/500], Train Loss: 0.0620\n",
      "Epoch [144/500], Train Loss: 0.0620\n",
      "Epoch [145/500], Train Loss: 0.0620\n",
      "Epoch [146/500], Train Loss: 0.0619\n",
      "Epoch [147/500], Train Loss: 0.0618\n",
      "Epoch [148/500], Train Loss: 0.0619\n",
      "Epoch [149/500], Train Loss: 0.0618\n",
      "Epoch [150/500], Train Loss: 0.0619\n",
      "Epoch [151/500], Train Loss: 0.0617\n",
      "Epoch [152/500], Train Loss: 0.0619\n",
      "Epoch [153/500], Train Loss: 0.0619\n",
      "Epoch [154/500], Train Loss: 0.0619\n",
      "Epoch [155/500], Train Loss: 0.0619\n",
      "Epoch [156/500], Train Loss: 0.0621\n",
      "Epoch [157/500], Train Loss: 0.0618\n",
      "Epoch [158/500], Train Loss: 0.0610\n",
      "Epoch [159/500], Train Loss: 0.0608\n",
      "Epoch [160/500], Train Loss: 0.0607\n",
      "Epoch [161/500], Train Loss: 0.0607\n",
      "Epoch [162/500], Train Loss: 0.0606\n",
      "Epoch [163/500], Train Loss: 0.0606\n",
      "Epoch [164/500], Train Loss: 0.0607\n",
      "Epoch [165/500], Train Loss: 0.0605\n",
      "Epoch [166/500], Train Loss: 0.0606\n",
      "Epoch [167/500], Train Loss: 0.0606\n",
      "Epoch [168/500], Train Loss: 0.0606\n",
      "Epoch [169/500], Train Loss: 0.0605\n",
      "Epoch [170/500], Train Loss: 0.0605\n",
      "Epoch [171/500], Train Loss: 0.0606\n",
      "Epoch [172/500], Train Loss: 0.0605\n",
      "Epoch [173/500], Train Loss: 0.0606\n",
      "Epoch [174/500], Train Loss: 0.0606\n",
      "Epoch [175/500], Train Loss: 0.0606\n",
      "Epoch [176/500], Train Loss: 0.0605\n",
      "Epoch [177/500], Train Loss: 0.0604\n",
      "Epoch [178/500], Train Loss: 0.0604\n",
      "Epoch [179/500], Train Loss: 0.0604\n",
      "Epoch [180/500], Train Loss: 0.0604\n",
      "Epoch [181/500], Train Loss: 0.0605\n",
      "Epoch [182/500], Train Loss: 0.0605\n",
      "Epoch [183/500], Train Loss: 0.0604\n",
      "Epoch [184/500], Train Loss: 0.0604\n",
      "Epoch [185/500], Train Loss: 0.0604\n",
      "Epoch [186/500], Train Loss: 0.0604\n",
      "Epoch [187/500], Train Loss: 0.0604\n",
      "Epoch [188/500], Train Loss: 0.0604\n",
      "Epoch [189/500], Train Loss: 0.0604\n",
      "Epoch [190/500], Train Loss: 0.0604\n",
      "Epoch [191/500], Train Loss: 0.0604\n",
      "Epoch [192/500], Train Loss: 0.0604\n",
      "Epoch [193/500], Train Loss: 0.0604\n",
      "Epoch [194/500], Train Loss: 0.0604\n",
      "Epoch [195/500], Train Loss: 0.0604\n",
      "Epoch [196/500], Train Loss: 0.0604\n",
      "Epoch [197/500], Train Loss: 0.0604\n",
      "Epoch [198/500], Train Loss: 0.0604\n",
      "Epoch [199/500], Train Loss: 0.0604\n",
      "Epoch [200/500], Train Loss: 0.0604\n",
      "Epoch [201/500], Train Loss: 0.0604\n",
      "Epoch [202/500], Train Loss: 0.0604\n",
      "Epoch [203/500], Train Loss: 0.0604\n",
      "Epoch [204/500], Train Loss: 0.0604\n",
      "Epoch [205/500], Train Loss: 0.0604\n",
      "Epoch [206/500], Train Loss: 0.0604\n",
      "Epoch [207/500], Train Loss: 0.0604\n",
      "Epoch [208/500], Train Loss: 0.0604\n",
      "Epoch [209/500], Train Loss: 0.0604\n",
      "Epoch [210/500], Train Loss: 0.0604\n",
      "Epoch [211/500], Train Loss: 0.0604\n",
      "Epoch [212/500], Train Loss: 0.0604\n",
      "Epoch [213/500], Train Loss: 0.0604\n",
      "Epoch [214/500], Train Loss: 0.0604\n",
      "Epoch [215/500], Train Loss: 0.0604\n",
      "Epoch [216/500], Train Loss: 0.0604\n",
      "Epoch [217/500], Train Loss: 0.0604\n",
      "Epoch [218/500], Train Loss: 0.0604\n",
      "Epoch [219/500], Train Loss: 0.0604\n",
      "Epoch [220/500], Train Loss: 0.0604\n",
      "Epoch [221/500], Train Loss: 0.0604\n",
      "Epoch [222/500], Train Loss: 0.0604\n",
      "Epoch [223/500], Train Loss: 0.0604\n",
      "Epoch [224/500], Train Loss: 0.0604\n",
      "Epoch [225/500], Train Loss: 0.0604\n",
      "Epoch [226/500], Train Loss: 0.0604\n",
      "Epoch [227/500], Train Loss: 0.0604\n",
      "Early stopping at epoch 227\n",
      "Final model saved at: ../../../Data/model_results/logistic_regression/models/chr2/final_model_chr2.pth\n",
      "Individual R^2 scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr2/individual_r2_scores_chr2.csv\n",
      "Individual IQS scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr2/individual_iqs_scores_chr2.csv\n",
      "Individual AUC ROC curves saved in: ../../../Data/model_results/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../../Data/model_results/logistic_regression/csv_files/performance_metrics.csv\n",
      "Total SNPs:  2510\n",
      "PRS313 SNPs:  32\n",
      "Total SNPs used for Training:  2478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-06-18 11:48:48,386] Trial 54 finished with value: 0.046257991982357846 and parameters: {'learning_rate': 0.003725170108721471, 'l1_coef': 1.0410588177326541e-05, 'patience': 17, 'batch_size': 64}. Best is trial 4 with value: 0.04438067571474956.\n",
      "[I 2024-06-18 11:49:00,952] Trial 59 finished with value: 0.046641488852245463 and parameters: {'learning_rate': 0.0037394428051745446, 'l1_coef': 1.0618844933444048e-05, 'patience': 17, 'batch_size': 64}. Best is trial 4 with value: 0.04438067571474956.\n",
      "[I 2024-06-18 11:49:01,028] Trial 55 finished with value: 0.04912701744054045 and parameters: {'learning_rate': 0.003739549867485792, 'l1_coef': 1.0516936204202721e-05, 'patience': 17, 'batch_size': 64}. Best is trial 4 with value: 0.04438067571474956.\n",
      "[I 2024-06-18 11:49:01,522] Trial 60 finished with value: 0.04651105201670101 and parameters: {'learning_rate': 0.003833431954175403, 'l1_coef': 1.0609556893970465e-05, 'patience': 17, 'batch_size': 64}. Best is trial 4 with value: 0.04438067571474956.\n",
      "[I 2024-06-18 11:49:01,612] Trial 52 finished with value: 0.05079677530697413 and parameters: {'learning_rate': 0.0033669480576353434, 'l1_coef': 1.0669237850394144e-05, 'patience': 17, 'batch_size': 64}. Best is trial 4 with value: 0.04438067571474956.\n",
      "[I 2024-06-18 11:49:04,113] Trial 56 finished with value: 0.04978293074028833 and parameters: {'learning_rate': 0.004095807131234439, 'l1_coef': 1.0789464124940311e-05, 'patience': 17, 'batch_size': 64}. Best is trial 4 with value: 0.04438067571474956.\n",
      "[I 2024-06-18 11:49:04,594] Trial 58 finished with value: 0.04639458246529102 and parameters: {'learning_rate': 0.00372933279575369, 'l1_coef': 1.0545105021835243e-05, 'patience': 17, 'batch_size': 64}. Best is trial 4 with value: 0.04438067571474956.\n",
      "[I 2024-06-18 11:49:05,737] Trial 51 finished with value: 0.04671225122043064 and parameters: {'learning_rate': 0.004413054363642356, 'l1_coef': 1.2130451770923823e-05, 'patience': 17, 'batch_size': 64}. Best is trial 4 with value: 0.04438067571474956.\n",
      "[I 2024-06-18 11:49:10,047] Trial 53 finished with value: 0.051760660750525336 and parameters: {'learning_rate': 0.004287853885996124, 'l1_coef': 1.014670294013092e-05, 'patience': 17, 'batch_size': 64}. Best is trial 4 with value: 0.04438067571474956.\n",
      "[I 2024-06-18 11:49:15,112] Trial 57 finished with value: 0.04623120081211839 and parameters: {'learning_rate': 0.004377626506350332, 'l1_coef': 1.076292310073308e-05, 'patience': 17, 'batch_size': 64}. Best is trial 4 with value: 0.04438067571474956.\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 3 - Best hyperparameters: {'learning_rate': 0.0014528755690903109, 'l1_coef': 1.074745971712647e-05, 'patience': 18, 'batch_size': 32}\n",
      "Chr 3 - Best value: 0.0444\n",
      "Epoch [1/500], Train Loss: 0.4344\n",
      "Epoch [2/500], Train Loss: 0.3248\n",
      "Epoch [3/500], Train Loss: 0.2665\n",
      "Epoch [4/500], Train Loss: 0.2297\n",
      "Epoch [5/500], Train Loss: 0.2025\n",
      "Epoch [6/500], Train Loss: 0.1816\n",
      "Epoch [7/500], Train Loss: 0.1657\n",
      "Epoch [8/500], Train Loss: 0.1522\n",
      "Epoch [9/500], Train Loss: 0.1418\n",
      "Epoch [10/500], Train Loss: 0.1331\n",
      "Epoch [11/500], Train Loss: 0.1249\n",
      "Epoch [12/500], Train Loss: 0.1184\n",
      "Epoch [13/500], Train Loss: 0.1132\n",
      "Epoch [14/500], Train Loss: 0.1081\n",
      "Epoch [15/500], Train Loss: 0.1036\n",
      "Epoch [16/500], Train Loss: 0.0994\n",
      "Epoch [17/500], Train Loss: 0.0958\n",
      "Epoch [18/500], Train Loss: 0.0924\n",
      "Epoch [19/500], Train Loss: 0.0895\n",
      "Epoch [20/500], Train Loss: 0.0869\n",
      "Epoch [21/500], Train Loss: 0.0841\n",
      "Epoch [22/500], Train Loss: 0.0819\n",
      "Epoch [23/500], Train Loss: 0.0798\n",
      "Epoch [24/500], Train Loss: 0.0779\n",
      "Epoch [25/500], Train Loss: 0.0760\n",
      "Epoch [26/500], Train Loss: 0.0745\n",
      "Epoch [27/500], Train Loss: 0.0726\n",
      "Epoch [28/500], Train Loss: 0.0713\n",
      "Epoch [29/500], Train Loss: 0.0699\n",
      "Epoch [30/500], Train Loss: 0.0686\n",
      "Epoch [31/500], Train Loss: 0.0675\n",
      "Epoch [32/500], Train Loss: 0.0664\n",
      "Epoch [33/500], Train Loss: 0.0651\n",
      "Epoch [34/500], Train Loss: 0.0643\n",
      "Epoch [35/500], Train Loss: 0.0632\n",
      "Epoch [36/500], Train Loss: 0.0623\n",
      "Epoch [37/500], Train Loss: 0.0613\n",
      "Epoch [38/500], Train Loss: 0.0605\n",
      "Epoch [39/500], Train Loss: 0.0597\n",
      "Epoch [40/500], Train Loss: 0.0589\n",
      "Epoch [41/500], Train Loss: 0.0581\n",
      "Epoch [42/500], Train Loss: 0.0577\n",
      "Epoch [43/500], Train Loss: 0.0569\n",
      "Epoch [44/500], Train Loss: 0.0562\n",
      "Epoch [45/500], Train Loss: 0.0555\n",
      "Epoch [46/500], Train Loss: 0.0547\n",
      "Epoch [47/500], Train Loss: 0.0543\n",
      "Epoch [48/500], Train Loss: 0.0540\n",
      "Epoch [49/500], Train Loss: 0.0532\n",
      "Epoch [50/500], Train Loss: 0.0528\n",
      "Epoch [51/500], Train Loss: 0.0524\n",
      "Epoch [52/500], Train Loss: 0.0518\n",
      "Epoch [53/500], Train Loss: 0.0513\n",
      "Epoch [54/500], Train Loss: 0.0509\n",
      "Epoch [55/500], Train Loss: 0.0504\n",
      "Epoch [56/500], Train Loss: 0.0499\n",
      "Epoch [57/500], Train Loss: 0.0494\n",
      "Epoch [58/500], Train Loss: 0.0492\n",
      "Epoch [59/500], Train Loss: 0.0486\n",
      "Epoch [60/500], Train Loss: 0.0482\n",
      "Epoch [61/500], Train Loss: 0.0479\n",
      "Epoch [62/500], Train Loss: 0.0475\n",
      "Epoch [63/500], Train Loss: 0.0473\n",
      "Epoch [64/500], Train Loss: 0.0468\n",
      "Epoch [65/500], Train Loss: 0.0465\n",
      "Epoch [66/500], Train Loss: 0.0463\n",
      "Epoch [67/500], Train Loss: 0.0459\n",
      "Epoch [68/500], Train Loss: 0.0455\n",
      "Epoch [69/500], Train Loss: 0.0454\n",
      "Epoch [70/500], Train Loss: 0.0450\n",
      "Epoch [71/500], Train Loss: 0.0448\n",
      "Epoch [72/500], Train Loss: 0.0445\n",
      "Epoch [73/500], Train Loss: 0.0444\n",
      "Epoch [74/500], Train Loss: 0.0440\n",
      "Epoch [75/500], Train Loss: 0.0437\n",
      "Epoch [76/500], Train Loss: 0.0435\n",
      "Epoch [77/500], Train Loss: 0.0432\n",
      "Epoch [78/500], Train Loss: 0.0430\n",
      "Epoch [79/500], Train Loss: 0.0426\n",
      "Epoch [80/500], Train Loss: 0.0424\n",
      "Epoch [81/500], Train Loss: 0.0421\n",
      "Epoch [82/500], Train Loss: 0.0419\n",
      "Epoch [83/500], Train Loss: 0.0417\n",
      "Epoch [84/500], Train Loss: 0.0416\n",
      "Epoch [85/500], Train Loss: 0.0413\n",
      "Epoch [86/500], Train Loss: 0.0412\n",
      "Epoch [87/500], Train Loss: 0.0408\n",
      "Epoch [88/500], Train Loss: 0.0408\n",
      "Epoch [89/500], Train Loss: 0.0407\n",
      "Epoch [90/500], Train Loss: 0.0404\n",
      "Epoch [91/500], Train Loss: 0.0402\n",
      "Epoch [92/500], Train Loss: 0.0401\n",
      "Epoch [93/500], Train Loss: 0.0398\n",
      "Epoch [94/500], Train Loss: 0.0395\n",
      "Epoch [95/500], Train Loss: 0.0393\n",
      "Epoch [96/500], Train Loss: 0.0393\n",
      "Epoch [97/500], Train Loss: 0.0390\n",
      "Epoch [98/500], Train Loss: 0.0389\n",
      "Epoch [99/500], Train Loss: 0.0387\n",
      "Epoch [100/500], Train Loss: 0.0387\n",
      "Epoch [101/500], Train Loss: 0.0386\n",
      "Epoch [102/500], Train Loss: 0.0384\n",
      "Epoch [103/500], Train Loss: 0.0383\n",
      "Epoch [104/500], Train Loss: 0.0380\n",
      "Epoch [105/500], Train Loss: 0.0380\n",
      "Epoch [106/500], Train Loss: 0.0379\n",
      "Epoch [107/500], Train Loss: 0.0377\n",
      "Epoch [108/500], Train Loss: 0.0376\n",
      "Epoch [109/500], Train Loss: 0.0374\n",
      "Epoch [110/500], Train Loss: 0.0373\n",
      "Epoch [111/500], Train Loss: 0.0371\n",
      "Epoch [112/500], Train Loss: 0.0370\n",
      "Epoch [113/500], Train Loss: 0.0368\n",
      "Epoch [114/500], Train Loss: 0.0367\n",
      "Epoch [115/500], Train Loss: 0.0366\n",
      "Epoch [116/500], Train Loss: 0.0366\n",
      "Epoch [117/500], Train Loss: 0.0364\n",
      "Epoch [118/500], Train Loss: 0.0364\n",
      "Epoch [119/500], Train Loss: 0.0361\n",
      "Epoch [120/500], Train Loss: 0.0361\n",
      "Epoch [121/500], Train Loss: 0.0360\n",
      "Epoch [122/500], Train Loss: 0.0361\n",
      "Epoch [123/500], Train Loss: 0.0358\n",
      "Epoch [124/500], Train Loss: 0.0357\n",
      "Epoch [125/500], Train Loss: 0.0356\n",
      "Epoch [126/500], Train Loss: 0.0354\n",
      "Epoch [127/500], Train Loss: 0.0353\n",
      "Epoch [128/500], Train Loss: 0.0354\n",
      "Epoch [129/500], Train Loss: 0.0352\n",
      "Epoch [130/500], Train Loss: 0.0352\n",
      "Epoch [131/500], Train Loss: 0.0351\n",
      "Epoch [132/500], Train Loss: 0.0349\n",
      "Epoch [133/500], Train Loss: 0.0347\n",
      "Epoch [134/500], Train Loss: 0.0347\n",
      "Epoch [135/500], Train Loss: 0.0347\n",
      "Epoch [136/500], Train Loss: 0.0343\n",
      "Epoch [137/500], Train Loss: 0.0344\n",
      "Epoch [138/500], Train Loss: 0.0344\n",
      "Epoch [139/500], Train Loss: 0.0345\n",
      "Epoch [140/500], Train Loss: 0.0344\n",
      "Epoch [141/500], Train Loss: 0.0345\n",
      "Epoch [142/500], Train Loss: 0.0341\n",
      "Epoch [143/500], Train Loss: 0.0340\n",
      "Epoch [144/500], Train Loss: 0.0342\n",
      "Epoch [145/500], Train Loss: 0.0341\n",
      "Epoch [146/500], Train Loss: 0.0339\n",
      "Epoch [147/500], Train Loss: 0.0337\n",
      "Epoch [148/500], Train Loss: 0.0338\n",
      "Epoch [149/500], Train Loss: 0.0336\n",
      "Epoch [150/500], Train Loss: 0.0337\n",
      "Epoch [151/500], Train Loss: 0.0335\n",
      "Epoch [152/500], Train Loss: 0.0337\n",
      "Epoch [153/500], Train Loss: 0.0335\n",
      "Epoch [154/500], Train Loss: 0.0333\n",
      "Epoch [155/500], Train Loss: 0.0333\n",
      "Epoch [156/500], Train Loss: 0.0333\n",
      "Epoch [157/500], Train Loss: 0.0333\n",
      "Epoch [158/500], Train Loss: 0.0332\n",
      "Epoch [159/500], Train Loss: 0.0330\n",
      "Epoch [160/500], Train Loss: 0.0331\n",
      "Epoch [161/500], Train Loss: 0.0330\n",
      "Epoch [162/500], Train Loss: 0.0328\n",
      "Epoch [163/500], Train Loss: 0.0330\n",
      "Epoch [164/500], Train Loss: 0.0331\n",
      "Epoch [165/500], Train Loss: 0.0329\n",
      "Epoch [166/500], Train Loss: 0.0329\n",
      "Epoch [167/500], Train Loss: 0.0328\n",
      "Epoch [168/500], Train Loss: 0.0328\n",
      "Epoch [169/500], Train Loss: 0.0327\n",
      "Epoch [170/500], Train Loss: 0.0328\n",
      "Epoch [171/500], Train Loss: 0.0325\n",
      "Epoch [172/500], Train Loss: 0.0325\n",
      "Epoch [173/500], Train Loss: 0.0325\n",
      "Epoch [174/500], Train Loss: 0.0324\n",
      "Epoch [175/500], Train Loss: 0.0325\n",
      "Epoch [176/500], Train Loss: 0.0323\n",
      "Epoch [177/500], Train Loss: 0.0323\n",
      "Epoch [178/500], Train Loss: 0.0323\n",
      "Epoch [179/500], Train Loss: 0.0321\n",
      "Epoch [180/500], Train Loss: 0.0322\n",
      "Epoch [181/500], Train Loss: 0.0323\n",
      "Epoch [182/500], Train Loss: 0.0321\n",
      "Epoch [183/500], Train Loss: 0.0320\n",
      "Epoch [184/500], Train Loss: 0.0320\n",
      "Epoch [185/500], Train Loss: 0.0320\n",
      "Epoch [186/500], Train Loss: 0.0320\n",
      "Epoch [187/500], Train Loss: 0.0318\n",
      "Epoch [188/500], Train Loss: 0.0319\n",
      "Epoch [189/500], Train Loss: 0.0321\n",
      "Epoch [190/500], Train Loss: 0.0319\n",
      "Epoch [191/500], Train Loss: 0.0319\n",
      "Epoch [192/500], Train Loss: 0.0318\n",
      "Epoch [193/500], Train Loss: 0.0318\n",
      "Epoch [194/500], Train Loss: 0.0317\n",
      "Epoch [195/500], Train Loss: 0.0316\n",
      "Epoch [196/500], Train Loss: 0.0317\n",
      "Epoch [197/500], Train Loss: 0.0319\n",
      "Epoch [198/500], Train Loss: 0.0319\n",
      "Epoch [199/500], Train Loss: 0.0315\n",
      "Epoch [200/500], Train Loss: 0.0316\n",
      "Epoch [201/500], Train Loss: 0.0315\n",
      "Epoch [202/500], Train Loss: 0.0313\n",
      "Epoch [203/500], Train Loss: 0.0314\n",
      "Epoch [204/500], Train Loss: 0.0316\n",
      "Epoch [205/500], Train Loss: 0.0315\n",
      "Epoch [206/500], Train Loss: 0.0314\n",
      "Epoch [207/500], Train Loss: 0.0312\n",
      "Epoch [208/500], Train Loss: 0.0313\n",
      "Epoch [209/500], Train Loss: 0.0314\n",
      "Epoch [210/500], Train Loss: 0.0313\n",
      "Epoch [211/500], Train Loss: 0.0312\n",
      "Epoch [212/500], Train Loss: 0.0312\n",
      "Epoch [213/500], Train Loss: 0.0312\n",
      "Epoch [214/500], Train Loss: 0.0313\n",
      "Epoch [215/500], Train Loss: 0.0312\n",
      "Epoch [216/500], Train Loss: 0.0311\n",
      "Epoch [217/500], Train Loss: 0.0312\n",
      "Epoch [218/500], Train Loss: 0.0311\n",
      "Epoch [219/500], Train Loss: 0.0311\n",
      "Epoch [220/500], Train Loss: 0.0311\n",
      "Epoch [221/500], Train Loss: 0.0309\n",
      "Epoch [222/500], Train Loss: 0.0310\n",
      "Epoch [223/500], Train Loss: 0.0311\n",
      "Epoch [224/500], Train Loss: 0.0310\n",
      "Epoch [225/500], Train Loss: 0.0310\n",
      "Epoch [226/500], Train Loss: 0.0308\n",
      "Epoch [227/500], Train Loss: 0.0311\n",
      "Epoch [228/500], Train Loss: 0.0310\n",
      "Epoch [229/500], Train Loss: 0.0310\n",
      "Epoch [230/500], Train Loss: 0.0309\n",
      "Epoch [231/500], Train Loss: 0.0308\n",
      "Epoch [232/500], Train Loss: 0.0309\n",
      "Epoch [233/500], Train Loss: 0.0307\n",
      "Epoch [234/500], Train Loss: 0.0307\n",
      "Epoch [235/500], Train Loss: 0.0306\n",
      "Epoch [236/500], Train Loss: 0.0308\n",
      "Epoch [237/500], Train Loss: 0.0308\n",
      "Epoch [238/500], Train Loss: 0.0307\n",
      "Epoch [239/500], Train Loss: 0.0307\n",
      "Epoch [240/500], Train Loss: 0.0306\n",
      "Epoch [241/500], Train Loss: 0.0307\n",
      "Epoch [242/500], Train Loss: 0.0293\n",
      "Epoch [243/500], Train Loss: 0.0289\n",
      "Epoch [244/500], Train Loss: 0.0287\n",
      "Epoch [245/500], Train Loss: 0.0286\n",
      "Epoch [246/500], Train Loss: 0.0286\n",
      "Epoch [247/500], Train Loss: 0.0286\n",
      "Epoch [248/500], Train Loss: 0.0286\n",
      "Epoch [249/500], Train Loss: 0.0286\n",
      "Epoch [250/500], Train Loss: 0.0285\n",
      "Epoch [251/500], Train Loss: 0.0285\n",
      "Epoch [252/500], Train Loss: 0.0285\n",
      "Epoch [253/500], Train Loss: 0.0285\n",
      "Epoch [254/500], Train Loss: 0.0285\n",
      "Epoch [255/500], Train Loss: 0.0285\n",
      "Epoch [256/500], Train Loss: 0.0285\n",
      "Epoch [257/500], Train Loss: 0.0285\n",
      "Epoch [258/500], Train Loss: 0.0285\n",
      "Epoch [259/500], Train Loss: 0.0285\n",
      "Epoch [260/500], Train Loss: 0.0285\n",
      "Epoch [261/500], Train Loss: 0.0285\n",
      "Epoch [262/500], Train Loss: 0.0285\n",
      "Epoch [263/500], Train Loss: 0.0285\n",
      "Epoch [264/500], Train Loss: 0.0285\n",
      "Epoch [265/500], Train Loss: 0.0285\n",
      "Epoch [266/500], Train Loss: 0.0285\n",
      "Epoch [267/500], Train Loss: 0.0285\n",
      "Epoch [268/500], Train Loss: 0.0285\n",
      "Epoch [269/500], Train Loss: 0.0285\n",
      "Epoch [270/500], Train Loss: 0.0285\n",
      "Epoch [271/500], Train Loss: 0.0285\n",
      "Epoch [272/500], Train Loss: 0.0285\n",
      "Epoch [273/500], Train Loss: 0.0285\n",
      "Epoch [274/500], Train Loss: 0.0285\n",
      "Epoch [275/500], Train Loss: 0.0285\n",
      "Epoch [276/500], Train Loss: 0.0285\n",
      "Epoch [277/500], Train Loss: 0.0285\n",
      "Epoch [278/500], Train Loss: 0.0285\n",
      "Epoch [279/500], Train Loss: 0.0285\n",
      "Epoch [280/500], Train Loss: 0.0283\n",
      "Epoch [281/500], Train Loss: 0.0283\n",
      "Epoch [282/500], Train Loss: 0.0283\n",
      "Epoch [283/500], Train Loss: 0.0283\n",
      "Epoch [284/500], Train Loss: 0.0283\n",
      "Epoch [285/500], Train Loss: 0.0282\n",
      "Epoch [286/500], Train Loss: 0.0283\n",
      "Epoch [287/500], Train Loss: 0.0282\n",
      "Epoch [288/500], Train Loss: 0.0283\n",
      "Epoch [289/500], Train Loss: 0.0282\n",
      "Epoch [290/500], Train Loss: 0.0282\n",
      "Epoch [291/500], Train Loss: 0.0282\n",
      "Epoch [292/500], Train Loss: 0.0282\n",
      "Epoch [293/500], Train Loss: 0.0282\n",
      "Epoch [294/500], Train Loss: 0.0282\n",
      "Epoch [295/500], Train Loss: 0.0282\n",
      "Epoch [296/500], Train Loss: 0.0282\n",
      "Epoch [297/500], Train Loss: 0.0282\n",
      "Epoch [298/500], Train Loss: 0.0282\n",
      "Epoch [299/500], Train Loss: 0.0282\n",
      "Epoch [300/500], Train Loss: 0.0282\n",
      "Epoch [301/500], Train Loss: 0.0282\n",
      "Epoch [302/500], Train Loss: 0.0282\n",
      "Epoch [303/500], Train Loss: 0.0282\n",
      "Epoch [304/500], Train Loss: 0.0282\n",
      "Epoch [305/500], Train Loss: 0.0282\n",
      "Epoch [306/500], Train Loss: 0.0282\n",
      "Epoch [307/500], Train Loss: 0.0282\n",
      "Epoch [308/500], Train Loss: 0.0282\n",
      "Epoch [309/500], Train Loss: 0.0282\n",
      "Epoch [310/500], Train Loss: 0.0282\n",
      "Epoch [311/500], Train Loss: 0.0282\n",
      "Epoch [312/500], Train Loss: 0.0282\n",
      "Epoch [313/500], Train Loss: 0.0282\n",
      "Epoch [314/500], Train Loss: 0.0282\n",
      "Epoch [315/500], Train Loss: 0.0282\n",
      "Epoch [316/500], Train Loss: 0.0282\n",
      "Epoch [317/500], Train Loss: 0.0282\n",
      "Epoch [318/500], Train Loss: 0.0282\n",
      "Epoch [319/500], Train Loss: 0.0282\n",
      "Epoch [320/500], Train Loss: 0.0282\n",
      "Epoch [321/500], Train Loss: 0.0282\n",
      "Epoch [322/500], Train Loss: 0.0282\n",
      "Epoch [323/500], Train Loss: 0.0282\n",
      "Epoch [324/500], Train Loss: 0.0282\n",
      "Epoch [325/500], Train Loss: 0.0282\n",
      "Epoch [326/500], Train Loss: 0.0282\n",
      "Epoch [327/500], Train Loss: 0.0282\n",
      "Epoch [328/500], Train Loss: 0.0282\n",
      "Epoch [329/500], Train Loss: 0.0282\n",
      "Epoch [330/500], Train Loss: 0.0282\n",
      "Epoch [331/500], Train Loss: 0.0282\n",
      "Early stopping at epoch 331\n",
      "Final model saved at: ../../../Data/model_results/logistic_regression/models/chr3/final_model_chr3.pth\n",
      "Individual R^2 scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr3/individual_r2_scores_chr3.csv\n",
      "Individual IQS scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr3/individual_iqs_scores_chr3.csv\n",
      "Individual AUC ROC curves saved in: ../../../Data/model_results/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../../Data/model_results/logistic_regression/csv_files/performance_metrics.csv\n",
      "Total SNPs:  2976\n",
      "PRS313 SNPs:  22\n",
      "Total SNPs used for Training:  2954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-06-18 11:52:12,771] Trial 56 finished with value: 0.0798052256660802 and parameters: {'learning_rate': 0.006384330127793367, 'l1_coef': 2.3220731475718292e-05, 'patience': 15, 'batch_size': 64}. Best is trial 10 with value: 0.06264721751213073.\n",
      "[I 2024-06-18 11:52:15,053] Trial 54 finished with value: 0.11812034866639545 and parameters: {'learning_rate': 0.0014794803256235752, 'l1_coef': 6.815614299528405e-05, 'patience': 15, 'batch_size': 64}. Best is trial 10 with value: 0.06264721751213073.\n",
      "[I 2024-06-18 11:52:23,090] Trial 48 finished with value: 0.11650087514093943 and parameters: {'learning_rate': 0.0016948318506605812, 'l1_coef': 6.698567179962252e-05, 'patience': 15, 'batch_size': 64}. Best is trial 10 with value: 0.06264721751213073.\n",
      "[I 2024-06-18 11:52:47,769] Trial 52 finished with value: 0.11725795758622033 and parameters: {'learning_rate': 0.00165296324488275, 'l1_coef': 7.296065732083937e-05, 'patience': 14, 'batch_size': 64}. Best is trial 10 with value: 0.06264721751213073.\n",
      "[I 2024-06-18 11:52:55,172] Trial 49 finished with value: 0.11906613281794956 and parameters: {'learning_rate': 0.0014875574352837672, 'l1_coef': 6.86387879125458e-05, 'patience': 15, 'batch_size': 64}. Best is trial 10 with value: 0.06264721751213073.\n",
      "[I 2024-06-18 11:53:07,621] Trial 51 finished with value: 0.12688886033637184 and parameters: {'learning_rate': 0.001555078896461374, 'l1_coef': 7.01817629617598e-05, 'patience': 15, 'batch_size': 64}. Best is trial 10 with value: 0.06264721751213073.\n",
      "[I 2024-06-18 11:53:09,736] Trial 55 finished with value: 0.12534686625003816 and parameters: {'learning_rate': 0.0014719212818454892, 'l1_coef': 7.378781978049749e-05, 'patience': 15, 'batch_size': 64}. Best is trial 10 with value: 0.06264721751213073.\n",
      "[I 2024-06-18 11:53:12,218] Trial 53 finished with value: 0.12095066308975219 and parameters: {'learning_rate': 0.0016331332342926074, 'l1_coef': 7.094621340810252e-05, 'patience': 15, 'batch_size': 64}. Best is trial 10 with value: 0.06264721751213073.\n",
      "[I 2024-06-18 11:53:15,748] Trial 50 finished with value: 0.11890882466520583 and parameters: {'learning_rate': 0.0019037077833407464, 'l1_coef': 7.109134093658121e-05, 'patience': 14, 'batch_size': 64}. Best is trial 10 with value: 0.06264721751213073.\n",
      "[I 2024-06-18 11:53:18,209] Trial 57 finished with value: 0.127675247831004 and parameters: {'learning_rate': 0.0016446182866415632, 'l1_coef': 7.654852647087719e-05, 'patience': 15, 'batch_size': 64}. Best is trial 10 with value: 0.06264721751213073.\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 4 - Best hyperparameters: {'learning_rate': 0.028405218431256294, 'l1_coef': 1.82676872855965e-05, 'patience': 20, 'batch_size': 128}\n",
      "Chr 4 - Best value: 0.0626\n",
      "Epoch [1/500], Train Loss: 3.1886\n",
      "Epoch [2/500], Train Loss: 1.9401\n",
      "Epoch [3/500], Train Loss: 1.6592\n",
      "Epoch [4/500], Train Loss: 1.5975\n",
      "Epoch [5/500], Train Loss: 1.5638\n",
      "Epoch [6/500], Train Loss: 1.3461\n",
      "Epoch [7/500], Train Loss: 0.3583\n",
      "Epoch [8/500], Train Loss: 0.1473\n",
      "Epoch [9/500], Train Loss: 0.1189\n",
      "Epoch [10/500], Train Loss: 0.1102\n",
      "Epoch [11/500], Train Loss: 0.1031\n",
      "Epoch [12/500], Train Loss: 0.0969\n",
      "Epoch [13/500], Train Loss: 0.0925\n",
      "Epoch [14/500], Train Loss: 0.0900\n",
      "Epoch [15/500], Train Loss: 0.0874\n",
      "Epoch [16/500], Train Loss: 0.0836\n",
      "Epoch [17/500], Train Loss: 0.0812\n",
      "Epoch [18/500], Train Loss: 0.0787\n",
      "Epoch [19/500], Train Loss: 0.0779\n",
      "Epoch [20/500], Train Loss: 0.0759\n",
      "Epoch [21/500], Train Loss: 0.0748\n",
      "Epoch [22/500], Train Loss: 0.0733\n",
      "Epoch [23/500], Train Loss: 0.0725\n",
      "Epoch [24/500], Train Loss: 0.0723\n",
      "Epoch [25/500], Train Loss: 0.0715\n",
      "Epoch [26/500], Train Loss: 0.0710\n",
      "Epoch [27/500], Train Loss: 0.0718\n",
      "Epoch [28/500], Train Loss: 0.0700\n",
      "Epoch [29/500], Train Loss: 0.0681\n",
      "Epoch [30/500], Train Loss: 0.0660\n",
      "Epoch [31/500], Train Loss: 0.0653\n",
      "Epoch [32/500], Train Loss: 0.0638\n",
      "Epoch [33/500], Train Loss: 0.0634\n",
      "Epoch [34/500], Train Loss: 0.0636\n",
      "Epoch [35/500], Train Loss: 0.0637\n",
      "Epoch [36/500], Train Loss: 0.0623\n",
      "Epoch [37/500], Train Loss: 0.0615\n",
      "Epoch [38/500], Train Loss: 0.0612\n",
      "Epoch [39/500], Train Loss: 0.0620\n",
      "Epoch [40/500], Train Loss: 0.0614\n",
      "Epoch [41/500], Train Loss: 0.0611\n",
      "Epoch [42/500], Train Loss: 0.0604\n",
      "Epoch [43/500], Train Loss: 0.0628\n",
      "Epoch [44/500], Train Loss: 0.0618\n",
      "Epoch [45/500], Train Loss: 0.0600\n",
      "Epoch [46/500], Train Loss: 0.0591\n",
      "Epoch [47/500], Train Loss: 0.0586\n",
      "Epoch [48/500], Train Loss: 0.0576\n",
      "Epoch [49/500], Train Loss: 0.0576\n",
      "Epoch [50/500], Train Loss: 0.0572\n",
      "Epoch [51/500], Train Loss: 0.0574\n",
      "Epoch [52/500], Train Loss: 0.0571\n",
      "Epoch [53/500], Train Loss: 0.0572\n",
      "Epoch [54/500], Train Loss: 0.0578\n",
      "Epoch [55/500], Train Loss: 0.0593\n",
      "Epoch [56/500], Train Loss: 0.0578\n",
      "Epoch [57/500], Train Loss: 0.0560\n",
      "Epoch [58/500], Train Loss: 0.0561\n",
      "Epoch [59/500], Train Loss: 0.0556\n",
      "Epoch [60/500], Train Loss: 0.0548\n",
      "Epoch [61/500], Train Loss: 0.0552\n",
      "Epoch [62/500], Train Loss: 0.0562\n",
      "Epoch [63/500], Train Loss: 0.0572\n",
      "Epoch [64/500], Train Loss: 0.0572\n",
      "Epoch [65/500], Train Loss: 0.0564\n",
      "Epoch [66/500], Train Loss: 0.0558\n",
      "Epoch [67/500], Train Loss: 0.0519\n",
      "Epoch [68/500], Train Loss: 0.0492\n",
      "Epoch [69/500], Train Loss: 0.0483\n",
      "Epoch [70/500], Train Loss: 0.0478\n",
      "Epoch [71/500], Train Loss: 0.0476\n",
      "Epoch [72/500], Train Loss: 0.0473\n",
      "Epoch [73/500], Train Loss: 0.0473\n",
      "Epoch [74/500], Train Loss: 0.0471\n",
      "Epoch [75/500], Train Loss: 0.0471\n",
      "Epoch [76/500], Train Loss: 0.0470\n",
      "Epoch [77/500], Train Loss: 0.0469\n",
      "Epoch [78/500], Train Loss: 0.0470\n",
      "Epoch [79/500], Train Loss: 0.0470\n",
      "Epoch [80/500], Train Loss: 0.0469\n",
      "Epoch [81/500], Train Loss: 0.0469\n",
      "Epoch [82/500], Train Loss: 0.0468\n",
      "Epoch [83/500], Train Loss: 0.0468\n",
      "Epoch [84/500], Train Loss: 0.0467\n",
      "Epoch [85/500], Train Loss: 0.0467\n",
      "Epoch [86/500], Train Loss: 0.0467\n",
      "Epoch [87/500], Train Loss: 0.0467\n",
      "Epoch [88/500], Train Loss: 0.0467\n",
      "Epoch [89/500], Train Loss: 0.0467\n",
      "Epoch [90/500], Train Loss: 0.0467\n",
      "Epoch [91/500], Train Loss: 0.0466\n",
      "Epoch [92/500], Train Loss: 0.0465\n",
      "Epoch [93/500], Train Loss: 0.0466\n",
      "Epoch [94/500], Train Loss: 0.0466\n",
      "Epoch [95/500], Train Loss: 0.0466\n",
      "Epoch [96/500], Train Loss: 0.0465\n",
      "Epoch [97/500], Train Loss: 0.0465\n",
      "Epoch [98/500], Train Loss: 0.0465\n",
      "Epoch [99/500], Train Loss: 0.0465\n",
      "Epoch [100/500], Train Loss: 0.0466\n",
      "Epoch [101/500], Train Loss: 0.0465\n",
      "Epoch [102/500], Train Loss: 0.0465\n",
      "Epoch [103/500], Train Loss: 0.0464\n",
      "Epoch [104/500], Train Loss: 0.0466\n",
      "Epoch [105/500], Train Loss: 0.0465\n",
      "Epoch [106/500], Train Loss: 0.0464\n",
      "Epoch [107/500], Train Loss: 0.0464\n",
      "Epoch [108/500], Train Loss: 0.0464\n",
      "Epoch [109/500], Train Loss: 0.0464\n",
      "Epoch [110/500], Train Loss: 0.0464\n",
      "Epoch [111/500], Train Loss: 0.0463\n",
      "Epoch [112/500], Train Loss: 0.0464\n",
      "Epoch [113/500], Train Loss: 0.0463\n",
      "Epoch [114/500], Train Loss: 0.0463\n",
      "Epoch [115/500], Train Loss: 0.0463\n",
      "Epoch [116/500], Train Loss: 0.0463\n",
      "Epoch [117/500], Train Loss: 0.0463\n",
      "Epoch [118/500], Train Loss: 0.0462\n",
      "Epoch [119/500], Train Loss: 0.0461\n",
      "Epoch [120/500], Train Loss: 0.0462\n",
      "Epoch [121/500], Train Loss: 0.0462\n",
      "Epoch [122/500], Train Loss: 0.0462\n",
      "Epoch [123/500], Train Loss: 0.0461\n",
      "Epoch [124/500], Train Loss: 0.0461\n",
      "Epoch [125/500], Train Loss: 0.0462\n",
      "Epoch [126/500], Train Loss: 0.0461\n",
      "Epoch [127/500], Train Loss: 0.0461\n",
      "Epoch [128/500], Train Loss: 0.0461\n",
      "Epoch [129/500], Train Loss: 0.0461\n",
      "Epoch [130/500], Train Loss: 0.0461\n",
      "Epoch [131/500], Train Loss: 0.0462\n",
      "Epoch [132/500], Train Loss: 0.0460\n",
      "Epoch [133/500], Train Loss: 0.0460\n",
      "Epoch [134/500], Train Loss: 0.0460\n",
      "Epoch [135/500], Train Loss: 0.0460\n",
      "Epoch [136/500], Train Loss: 0.0459\n",
      "Epoch [137/500], Train Loss: 0.0459\n",
      "Epoch [138/500], Train Loss: 0.0459\n",
      "Epoch [139/500], Train Loss: 0.0459\n",
      "Epoch [140/500], Train Loss: 0.0460\n",
      "Epoch [141/500], Train Loss: 0.0461\n",
      "Epoch [142/500], Train Loss: 0.0459\n",
      "Epoch [143/500], Train Loss: 0.0458\n",
      "Epoch [144/500], Train Loss: 0.0459\n",
      "Epoch [145/500], Train Loss: 0.0455\n",
      "Epoch [146/500], Train Loss: 0.0451\n",
      "Epoch [147/500], Train Loss: 0.0450\n",
      "Epoch [148/500], Train Loss: 0.0450\n",
      "Epoch [149/500], Train Loss: 0.0449\n",
      "Epoch [150/500], Train Loss: 0.0449\n",
      "Epoch [151/500], Train Loss: 0.0449\n",
      "Epoch [152/500], Train Loss: 0.0449\n",
      "Epoch [153/500], Train Loss: 0.0450\n",
      "Epoch [154/500], Train Loss: 0.0449\n",
      "Epoch [155/500], Train Loss: 0.0449\n",
      "Epoch [156/500], Train Loss: 0.0449\n",
      "Epoch [157/500], Train Loss: 0.0449\n",
      "Epoch [158/500], Train Loss: 0.0449\n",
      "Epoch [159/500], Train Loss: 0.0449\n",
      "Epoch [160/500], Train Loss: 0.0449\n",
      "Epoch [161/500], Train Loss: 0.0448\n",
      "Epoch [162/500], Train Loss: 0.0449\n",
      "Epoch [163/500], Train Loss: 0.0449\n",
      "Epoch [164/500], Train Loss: 0.0449\n",
      "Epoch [165/500], Train Loss: 0.0449\n",
      "Epoch [166/500], Train Loss: 0.0449\n",
      "Epoch [167/500], Train Loss: 0.0449\n",
      "Epoch [168/500], Train Loss: 0.0448\n",
      "Epoch [169/500], Train Loss: 0.0448\n",
      "Epoch [170/500], Train Loss: 0.0447\n",
      "Epoch [171/500], Train Loss: 0.0448\n",
      "Epoch [172/500], Train Loss: 0.0448\n",
      "Epoch [173/500], Train Loss: 0.0448\n",
      "Epoch [174/500], Train Loss: 0.0447\n",
      "Epoch [175/500], Train Loss: 0.0448\n",
      "Epoch [176/500], Train Loss: 0.0447\n",
      "Epoch [177/500], Train Loss: 0.0447\n",
      "Epoch [178/500], Train Loss: 0.0448\n",
      "Epoch [179/500], Train Loss: 0.0447\n",
      "Epoch [180/500], Train Loss: 0.0448\n",
      "Epoch [181/500], Train Loss: 0.0447\n",
      "Epoch [182/500], Train Loss: 0.0448\n",
      "Epoch [183/500], Train Loss: 0.0447\n",
      "Epoch [184/500], Train Loss: 0.0447\n",
      "Epoch [185/500], Train Loss: 0.0447\n",
      "Epoch [186/500], Train Loss: 0.0448\n",
      "Epoch [187/500], Train Loss: 0.0448\n",
      "Epoch [188/500], Train Loss: 0.0448\n",
      "Epoch [189/500], Train Loss: 0.0447\n",
      "Epoch [190/500], Train Loss: 0.0448\n",
      "Epoch [191/500], Train Loss: 0.0448\n",
      "Epoch [192/500], Train Loss: 0.0447\n",
      "Epoch [193/500], Train Loss: 0.0447\n",
      "Epoch [194/500], Train Loss: 0.0448\n",
      "Epoch [195/500], Train Loss: 0.0447\n",
      "Epoch [196/500], Train Loss: 0.0448\n",
      "Epoch [197/500], Train Loss: 0.0447\n",
      "Epoch [198/500], Train Loss: 0.0448\n",
      "Epoch [199/500], Train Loss: 0.0447\n",
      "Epoch [200/500], Train Loss: 0.0447\n",
      "Epoch [201/500], Train Loss: 0.0448\n",
      "Epoch [202/500], Train Loss: 0.0447\n",
      "Epoch [203/500], Train Loss: 0.0448\n",
      "Epoch [204/500], Train Loss: 0.0447\n",
      "Epoch [205/500], Train Loss: 0.0448\n",
      "Epoch [206/500], Train Loss: 0.0447\n",
      "Epoch [207/500], Train Loss: 0.0447\n",
      "Epoch [208/500], Train Loss: 0.0447\n",
      "Epoch [209/500], Train Loss: 0.0447\n",
      "Epoch [210/500], Train Loss: 0.0448\n",
      "Epoch [211/500], Train Loss: 0.0447\n",
      "Epoch [212/500], Train Loss: 0.0447\n",
      "Epoch [213/500], Train Loss: 0.0447\n",
      "Epoch [214/500], Train Loss: 0.0447\n",
      "Epoch [215/500], Train Loss: 0.0447\n",
      "Epoch [216/500], Train Loss: 0.0448\n",
      "Epoch [217/500], Train Loss: 0.0448\n",
      "Epoch [218/500], Train Loss: 0.0447\n",
      "Epoch [219/500], Train Loss: 0.0447\n",
      "Epoch [220/500], Train Loss: 0.0448\n",
      "Epoch [221/500], Train Loss: 0.0448\n",
      "Epoch [222/500], Train Loss: 0.0447\n",
      "Epoch [223/500], Train Loss: 0.0448\n",
      "Epoch [224/500], Train Loss: 0.0447\n",
      "Epoch [225/500], Train Loss: 0.0448\n",
      "Epoch [226/500], Train Loss: 0.0447\n",
      "Epoch [227/500], Train Loss: 0.0447\n",
      "Epoch [228/500], Train Loss: 0.0448\n",
      "Early stopping at epoch 228\n",
      "Final model saved at: ../../../Data/model_results/logistic_regression/models/chr4/final_model_chr4.pth\n",
      "Individual R^2 scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr4/individual_r2_scores_chr4.csv\n",
      "Individual IQS scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr4/individual_iqs_scores_chr4.csv\n",
      "Individual AUC ROC curves saved in: ../../../Data/model_results/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../../Data/model_results/logistic_regression/csv_files/performance_metrics.csv\n",
      "Total SNPs:  4076\n",
      "PRS313 SNPs:  68\n",
      "Total SNPs used for Training:  4008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-06-18 11:56:19,004] Trial 16 finished with value: 0.10366071164608001 and parameters: {'learning_rate': 0.0023232412763265855, 'l1_coef': 1.1907966237269316e-05, 'patience': 5, 'batch_size': 256}. Best is trial 16 with value: 0.10366071164608001.\n",
      "[I 2024-06-18 11:56:20,619] Trial 11 finished with value: 0.10538749992847443 and parameters: {'learning_rate': 0.0019171492641988116, 'l1_coef': 1.1324352108789524e-05, 'patience': 5, 'batch_size': 256}. Best is trial 16 with value: 0.10366071164608001.\n",
      "[I 2024-06-18 11:56:23,841] Trial 12 finished with value: 0.11968241408467292 and parameters: {'learning_rate': 0.0019050776788936401, 'l1_coef': 1.8012014176749892e-05, 'patience': 5, 'batch_size': 256}. Best is trial 16 with value: 0.10366071164608001.\n",
      "[I 2024-06-18 11:56:33,280] Trial 20 finished with value: 0.09828574880957604 and parameters: {'learning_rate': 0.002371235745312867, 'l1_coef': 1.1652150483740447e-05, 'patience': 5, 'batch_size': 256}. Best is trial 20 with value: 0.09828574880957604.\n",
      "[I 2024-06-18 11:56:37,310] Trial 15 finished with value: 0.10203299224376679 and parameters: {'learning_rate': 0.002011289850160436, 'l1_coef': 1.2119222306746967e-05, 'patience': 5, 'batch_size': 256}. Best is trial 20 with value: 0.09828574880957604.\n",
      "[I 2024-06-18 11:56:40,534] Trial 19 finished with value: 0.2320791095495224 and parameters: {'learning_rate': 0.00010876528480121392, 'l1_coef': 1.0724533201766168e-05, 'patience': 15, 'batch_size': 256}. Best is trial 20 with value: 0.09828574880957604.\n",
      "[I 2024-06-18 11:56:40,631] Trial 14 finished with value: 0.09374495148658753 and parameters: {'learning_rate': 0.0020083341636623056, 'l1_coef': 1.0869851864798313e-05, 'patience': 15, 'batch_size': 256}. Best is trial 14 with value: 0.09374495148658753.\n",
      "[I 2024-06-18 11:56:42,216] Trial 13 finished with value: 0.10212002322077751 and parameters: {'learning_rate': 0.0019608711013450898, 'l1_coef': 1.3217432491736842e-05, 'patience': 5, 'batch_size': 256}. Best is trial 14 with value: 0.09374495148658753.\n",
      "[I 2024-06-18 11:56:42,850] Trial 17 finished with value: 0.10600727647542954 and parameters: {'learning_rate': 0.0016486318019283404, 'l1_coef': 1.4786465821269623e-05, 'patience': 5, 'batch_size': 256}. Best is trial 14 with value: 0.09374495148658753.\n",
      "[I 2024-06-18 11:56:43,050] Trial 18 finished with value: 0.23643142431974412 and parameters: {'learning_rate': 0.00010666890492631113, 'l1_coef': 1.1515019425730405e-05, 'patience': 5, 'batch_size': 256}. Best is trial 14 with value: 0.09374495148658753.\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 5 - Best hyperparameters: {'learning_rate': 0.0020083341636623056, 'l1_coef': 1.0869851864798313e-05, 'patience': 15, 'batch_size': 256}\n",
      "Chr 5 - Best value: 0.0937\n",
      "Epoch [1/500], Train Loss: 0.6081\n",
      "Epoch [2/500], Train Loss: 0.5121\n",
      "Epoch [3/500], Train Loss: 0.4675\n",
      "Epoch [4/500], Train Loss: 0.4366\n",
      "Epoch [5/500], Train Loss: 0.4107\n",
      "Epoch [6/500], Train Loss: 0.3894\n",
      "Epoch [7/500], Train Loss: 0.3711\n",
      "Epoch [8/500], Train Loss: 0.3542\n",
      "Epoch [9/500], Train Loss: 0.3399\n",
      "Epoch [10/500], Train Loss: 0.3277\n",
      "Epoch [11/500], Train Loss: 0.3165\n",
      "Epoch [12/500], Train Loss: 0.3058\n",
      "Epoch [13/500], Train Loss: 0.2963\n",
      "Epoch [14/500], Train Loss: 0.2871\n",
      "Epoch [15/500], Train Loss: 0.2795\n",
      "Epoch [16/500], Train Loss: 0.2722\n",
      "Epoch [17/500], Train Loss: 0.2656\n",
      "Epoch [18/500], Train Loss: 0.2595\n",
      "Epoch [19/500], Train Loss: 0.2535\n",
      "Epoch [20/500], Train Loss: 0.2482\n",
      "Epoch [21/500], Train Loss: 0.2429\n",
      "Epoch [22/500], Train Loss: 0.2381\n",
      "Epoch [23/500], Train Loss: 0.2340\n",
      "Epoch [24/500], Train Loss: 0.2297\n",
      "Epoch [25/500], Train Loss: 0.2257\n",
      "Epoch [26/500], Train Loss: 0.2222\n",
      "Epoch [27/500], Train Loss: 0.2185\n",
      "Epoch [28/500], Train Loss: 0.2148\n",
      "Epoch [29/500], Train Loss: 0.2117\n",
      "Epoch [30/500], Train Loss: 0.2086\n",
      "Epoch [31/500], Train Loss: 0.2057\n",
      "Epoch [32/500], Train Loss: 0.2029\n",
      "Epoch [33/500], Train Loss: 0.2004\n",
      "Epoch [34/500], Train Loss: 0.1980\n",
      "Epoch [35/500], Train Loss: 0.1953\n",
      "Epoch [36/500], Train Loss: 0.1931\n",
      "Epoch [37/500], Train Loss: 0.1909\n",
      "Epoch [38/500], Train Loss: 0.1888\n",
      "Epoch [39/500], Train Loss: 0.1866\n",
      "Epoch [40/500], Train Loss: 0.1846\n",
      "Epoch [41/500], Train Loss: 0.1829\n",
      "Epoch [42/500], Train Loss: 0.1811\n",
      "Epoch [43/500], Train Loss: 0.1791\n",
      "Epoch [44/500], Train Loss: 0.1774\n",
      "Epoch [45/500], Train Loss: 0.1758\n",
      "Epoch [46/500], Train Loss: 0.1744\n",
      "Epoch [47/500], Train Loss: 0.1726\n",
      "Epoch [48/500], Train Loss: 0.1710\n",
      "Epoch [49/500], Train Loss: 0.1696\n",
      "Epoch [50/500], Train Loss: 0.1683\n",
      "Epoch [51/500], Train Loss: 0.1668\n",
      "Epoch [52/500], Train Loss: 0.1655\n",
      "Epoch [53/500], Train Loss: 0.1643\n",
      "Epoch [54/500], Train Loss: 0.1630\n",
      "Epoch [55/500], Train Loss: 0.1618\n",
      "Epoch [56/500], Train Loss: 0.1607\n",
      "Epoch [57/500], Train Loss: 0.1594\n",
      "Epoch [58/500], Train Loss: 0.1581\n",
      "Epoch [59/500], Train Loss: 0.1570\n",
      "Epoch [60/500], Train Loss: 0.1561\n",
      "Epoch [61/500], Train Loss: 0.1551\n",
      "Epoch [62/500], Train Loss: 0.1539\n",
      "Epoch [63/500], Train Loss: 0.1529\n",
      "Epoch [64/500], Train Loss: 0.1521\n",
      "Epoch [65/500], Train Loss: 0.1511\n",
      "Epoch [66/500], Train Loss: 0.1500\n",
      "Epoch [67/500], Train Loss: 0.1491\n",
      "Epoch [68/500], Train Loss: 0.1481\n",
      "Epoch [69/500], Train Loss: 0.1473\n",
      "Epoch [70/500], Train Loss: 0.1464\n",
      "Epoch [71/500], Train Loss: 0.1458\n",
      "Epoch [72/500], Train Loss: 0.1450\n",
      "Epoch [73/500], Train Loss: 0.1441\n",
      "Epoch [74/500], Train Loss: 0.1435\n",
      "Epoch [75/500], Train Loss: 0.1427\n",
      "Epoch [76/500], Train Loss: 0.1421\n",
      "Epoch [77/500], Train Loss: 0.1412\n",
      "Epoch [78/500], Train Loss: 0.1406\n",
      "Epoch [79/500], Train Loss: 0.1397\n",
      "Epoch [80/500], Train Loss: 0.1389\n",
      "Epoch [81/500], Train Loss: 0.1382\n",
      "Epoch [82/500], Train Loss: 0.1375\n",
      "Epoch [83/500], Train Loss: 0.1370\n",
      "Epoch [84/500], Train Loss: 0.1363\n",
      "Epoch [85/500], Train Loss: 0.1358\n",
      "Epoch [86/500], Train Loss: 0.1352\n",
      "Epoch [87/500], Train Loss: 0.1345\n",
      "Epoch [88/500], Train Loss: 0.1339\n",
      "Epoch [89/500], Train Loss: 0.1332\n",
      "Epoch [90/500], Train Loss: 0.1328\n",
      "Epoch [91/500], Train Loss: 0.1320\n",
      "Epoch [92/500], Train Loss: 0.1315\n",
      "Epoch [93/500], Train Loss: 0.1310\n",
      "Epoch [94/500], Train Loss: 0.1306\n",
      "Epoch [95/500], Train Loss: 0.1300\n",
      "Epoch [96/500], Train Loss: 0.1293\n",
      "Epoch [97/500], Train Loss: 0.1288\n",
      "Epoch [98/500], Train Loss: 0.1283\n",
      "Epoch [99/500], Train Loss: 0.1275\n",
      "Epoch [100/500], Train Loss: 0.1272\n",
      "Epoch [101/500], Train Loss: 0.1268\n",
      "Epoch [102/500], Train Loss: 0.1264\n",
      "Epoch [103/500], Train Loss: 0.1258\n",
      "Epoch [104/500], Train Loss: 0.1252\n",
      "Epoch [105/500], Train Loss: 0.1248\n",
      "Epoch [106/500], Train Loss: 0.1244\n",
      "Epoch [107/500], Train Loss: 0.1239\n",
      "Epoch [108/500], Train Loss: 0.1234\n",
      "Epoch [109/500], Train Loss: 0.1229\n",
      "Epoch [110/500], Train Loss: 0.1225\n",
      "Epoch [111/500], Train Loss: 0.1220\n",
      "Epoch [112/500], Train Loss: 0.1216\n",
      "Epoch [113/500], Train Loss: 0.1212\n",
      "Epoch [114/500], Train Loss: 0.1208\n",
      "Epoch [115/500], Train Loss: 0.1205\n",
      "Epoch [116/500], Train Loss: 0.1201\n",
      "Epoch [117/500], Train Loss: 0.1197\n",
      "Epoch [118/500], Train Loss: 0.1193\n",
      "Epoch [119/500], Train Loss: 0.1189\n",
      "Epoch [120/500], Train Loss: 0.1184\n",
      "Epoch [121/500], Train Loss: 0.1180\n",
      "Epoch [122/500], Train Loss: 0.1178\n",
      "Epoch [123/500], Train Loss: 0.1173\n",
      "Epoch [124/500], Train Loss: 0.1168\n",
      "Epoch [125/500], Train Loss: 0.1165\n",
      "Epoch [126/500], Train Loss: 0.1161\n",
      "Epoch [127/500], Train Loss: 0.1157\n",
      "Epoch [128/500], Train Loss: 0.1153\n",
      "Epoch [129/500], Train Loss: 0.1150\n",
      "Epoch [130/500], Train Loss: 0.1147\n",
      "Epoch [131/500], Train Loss: 0.1146\n",
      "Epoch [132/500], Train Loss: 0.1142\n",
      "Epoch [133/500], Train Loss: 0.1139\n",
      "Epoch [134/500], Train Loss: 0.1136\n",
      "Epoch [135/500], Train Loss: 0.1132\n",
      "Epoch [136/500], Train Loss: 0.1127\n",
      "Epoch [137/500], Train Loss: 0.1124\n",
      "Epoch [138/500], Train Loss: 0.1121\n",
      "Epoch [139/500], Train Loss: 0.1118\n",
      "Epoch [140/500], Train Loss: 0.1115\n",
      "Epoch [141/500], Train Loss: 0.1113\n",
      "Epoch [142/500], Train Loss: 0.1110\n",
      "Epoch [143/500], Train Loss: 0.1106\n",
      "Epoch [144/500], Train Loss: 0.1102\n",
      "Epoch [145/500], Train Loss: 0.1100\n",
      "Epoch [146/500], Train Loss: 0.1096\n",
      "Epoch [147/500], Train Loss: 0.1094\n",
      "Epoch [148/500], Train Loss: 0.1092\n",
      "Epoch [149/500], Train Loss: 0.1087\n",
      "Epoch [150/500], Train Loss: 0.1085\n",
      "Epoch [151/500], Train Loss: 0.1082\n",
      "Epoch [152/500], Train Loss: 0.1079\n",
      "Epoch [153/500], Train Loss: 0.1077\n",
      "Epoch [154/500], Train Loss: 0.1074\n",
      "Epoch [155/500], Train Loss: 0.1072\n",
      "Epoch [156/500], Train Loss: 0.1071\n",
      "Epoch [157/500], Train Loss: 0.1067\n",
      "Epoch [158/500], Train Loss: 0.1064\n",
      "Epoch [159/500], Train Loss: 0.1060\n",
      "Epoch [160/500], Train Loss: 0.1058\n",
      "Epoch [161/500], Train Loss: 0.1057\n",
      "Epoch [162/500], Train Loss: 0.1053\n",
      "Epoch [163/500], Train Loss: 0.1051\n",
      "Epoch [164/500], Train Loss: 0.1049\n",
      "Epoch [165/500], Train Loss: 0.1046\n",
      "Epoch [166/500], Train Loss: 0.1043\n",
      "Epoch [167/500], Train Loss: 0.1040\n",
      "Epoch [168/500], Train Loss: 0.1038\n",
      "Epoch [169/500], Train Loss: 0.1034\n",
      "Epoch [170/500], Train Loss: 0.1033\n",
      "Epoch [171/500], Train Loss: 0.1030\n",
      "Epoch [172/500], Train Loss: 0.1030\n",
      "Epoch [173/500], Train Loss: 0.1027\n",
      "Epoch [174/500], Train Loss: 0.1024\n",
      "Epoch [175/500], Train Loss: 0.1021\n",
      "Epoch [176/500], Train Loss: 0.1020\n",
      "Epoch [177/500], Train Loss: 0.1018\n",
      "Epoch [178/500], Train Loss: 0.1016\n",
      "Epoch [179/500], Train Loss: 0.1014\n",
      "Epoch [180/500], Train Loss: 0.1009\n",
      "Epoch [181/500], Train Loss: 0.1007\n",
      "Epoch [182/500], Train Loss: 0.1005\n",
      "Epoch [183/500], Train Loss: 0.1004\n",
      "Epoch [184/500], Train Loss: 0.1002\n",
      "Epoch [185/500], Train Loss: 0.1000\n",
      "Epoch [186/500], Train Loss: 0.0997\n",
      "Epoch [187/500], Train Loss: 0.0996\n",
      "Epoch [188/500], Train Loss: 0.0994\n",
      "Epoch [189/500], Train Loss: 0.0992\n",
      "Epoch [190/500], Train Loss: 0.0990\n",
      "Epoch [191/500], Train Loss: 0.0987\n",
      "Epoch [192/500], Train Loss: 0.0984\n",
      "Epoch [193/500], Train Loss: 0.0981\n",
      "Epoch [194/500], Train Loss: 0.0980\n",
      "Epoch [195/500], Train Loss: 0.0979\n",
      "Epoch [196/500], Train Loss: 0.0978\n",
      "Epoch [197/500], Train Loss: 0.0976\n",
      "Epoch [198/500], Train Loss: 0.0973\n",
      "Epoch [199/500], Train Loss: 0.0972\n",
      "Epoch [200/500], Train Loss: 0.0971\n",
      "Epoch [201/500], Train Loss: 0.0969\n",
      "Epoch [202/500], Train Loss: 0.0967\n",
      "Epoch [203/500], Train Loss: 0.0964\n",
      "Epoch [204/500], Train Loss: 0.0962\n",
      "Epoch [205/500], Train Loss: 0.0960\n",
      "Epoch [206/500], Train Loss: 0.0958\n",
      "Epoch [207/500], Train Loss: 0.0956\n",
      "Epoch [208/500], Train Loss: 0.0954\n",
      "Epoch [209/500], Train Loss: 0.0953\n",
      "Epoch [210/500], Train Loss: 0.0952\n",
      "Epoch [211/500], Train Loss: 0.0950\n",
      "Epoch [212/500], Train Loss: 0.0947\n",
      "Epoch [213/500], Train Loss: 0.0947\n",
      "Epoch [214/500], Train Loss: 0.0945\n",
      "Epoch [215/500], Train Loss: 0.0943\n",
      "Epoch [216/500], Train Loss: 0.0941\n",
      "Epoch [217/500], Train Loss: 0.0940\n",
      "Epoch [218/500], Train Loss: 0.0939\n",
      "Epoch [219/500], Train Loss: 0.0937\n",
      "Epoch [220/500], Train Loss: 0.0936\n",
      "Epoch [221/500], Train Loss: 0.0934\n",
      "Epoch [222/500], Train Loss: 0.0932\n",
      "Epoch [223/500], Train Loss: 0.0930\n",
      "Epoch [224/500], Train Loss: 0.0928\n",
      "Epoch [225/500], Train Loss: 0.0926\n",
      "Epoch [226/500], Train Loss: 0.0924\n",
      "Epoch [227/500], Train Loss: 0.0923\n",
      "Epoch [228/500], Train Loss: 0.0922\n",
      "Epoch [229/500], Train Loss: 0.0921\n",
      "Epoch [230/500], Train Loss: 0.0920\n",
      "Epoch [231/500], Train Loss: 0.0916\n",
      "Epoch [232/500], Train Loss: 0.0916\n",
      "Epoch [233/500], Train Loss: 0.0915\n",
      "Epoch [234/500], Train Loss: 0.0913\n",
      "Epoch [235/500], Train Loss: 0.0911\n",
      "Epoch [236/500], Train Loss: 0.0910\n",
      "Epoch [237/500], Train Loss: 0.0907\n",
      "Epoch [238/500], Train Loss: 0.0907\n",
      "Epoch [239/500], Train Loss: 0.0906\n",
      "Epoch [240/500], Train Loss: 0.0903\n",
      "Epoch [241/500], Train Loss: 0.0901\n",
      "Epoch [242/500], Train Loss: 0.0901\n",
      "Epoch [243/500], Train Loss: 0.0899\n",
      "Epoch [244/500], Train Loss: 0.0899\n",
      "Epoch [245/500], Train Loss: 0.0898\n",
      "Epoch [246/500], Train Loss: 0.0898\n",
      "Epoch [247/500], Train Loss: 0.0895\n",
      "Epoch [248/500], Train Loss: 0.0895\n",
      "Epoch [249/500], Train Loss: 0.0893\n",
      "Epoch [250/500], Train Loss: 0.0892\n",
      "Epoch [251/500], Train Loss: 0.0890\n",
      "Epoch [252/500], Train Loss: 0.0888\n",
      "Epoch [253/500], Train Loss: 0.0888\n",
      "Epoch [254/500], Train Loss: 0.0886\n",
      "Epoch [255/500], Train Loss: 0.0884\n",
      "Epoch [256/500], Train Loss: 0.0882\n",
      "Epoch [257/500], Train Loss: 0.0882\n",
      "Epoch [258/500], Train Loss: 0.0881\n",
      "Epoch [259/500], Train Loss: 0.0880\n",
      "Epoch [260/500], Train Loss: 0.0878\n",
      "Epoch [261/500], Train Loss: 0.0877\n",
      "Epoch [262/500], Train Loss: 0.0876\n",
      "Epoch [263/500], Train Loss: 0.0875\n",
      "Epoch [264/500], Train Loss: 0.0874\n",
      "Epoch [265/500], Train Loss: 0.0872\n",
      "Epoch [266/500], Train Loss: 0.0870\n",
      "Epoch [267/500], Train Loss: 0.0868\n",
      "Epoch [268/500], Train Loss: 0.0868\n",
      "Epoch [269/500], Train Loss: 0.0867\n",
      "Epoch [270/500], Train Loss: 0.0865\n",
      "Epoch [271/500], Train Loss: 0.0865\n",
      "Epoch [272/500], Train Loss: 0.0863\n",
      "Epoch [273/500], Train Loss: 0.0862\n",
      "Epoch [274/500], Train Loss: 0.0861\n",
      "Epoch [275/500], Train Loss: 0.0859\n",
      "Epoch [276/500], Train Loss: 0.0857\n",
      "Epoch [277/500], Train Loss: 0.0857\n",
      "Epoch [278/500], Train Loss: 0.0856\n",
      "Epoch [279/500], Train Loss: 0.0856\n",
      "Epoch [280/500], Train Loss: 0.0854\n",
      "Epoch [281/500], Train Loss: 0.0853\n",
      "Epoch [282/500], Train Loss: 0.0851\n",
      "Epoch [283/500], Train Loss: 0.0851\n",
      "Epoch [284/500], Train Loss: 0.0849\n",
      "Epoch [285/500], Train Loss: 0.0848\n",
      "Epoch [286/500], Train Loss: 0.0847\n",
      "Epoch [287/500], Train Loss: 0.0846\n",
      "Epoch [288/500], Train Loss: 0.0844\n",
      "Epoch [289/500], Train Loss: 0.0843\n",
      "Epoch [290/500], Train Loss: 0.0842\n",
      "Epoch [291/500], Train Loss: 0.0842\n",
      "Epoch [292/500], Train Loss: 0.0841\n",
      "Epoch [293/500], Train Loss: 0.0841\n",
      "Epoch [294/500], Train Loss: 0.0840\n",
      "Epoch [295/500], Train Loss: 0.0838\n",
      "Epoch [296/500], Train Loss: 0.0836\n",
      "Epoch [297/500], Train Loss: 0.0834\n",
      "Epoch [298/500], Train Loss: 0.0835\n",
      "Epoch [299/500], Train Loss: 0.0832\n",
      "Epoch [300/500], Train Loss: 0.0832\n",
      "Epoch [301/500], Train Loss: 0.0832\n",
      "Epoch [302/500], Train Loss: 0.0830\n",
      "Epoch [303/500], Train Loss: 0.0830\n",
      "Epoch [304/500], Train Loss: 0.0829\n",
      "Epoch [305/500], Train Loss: 0.0828\n",
      "Epoch [306/500], Train Loss: 0.0827\n",
      "Epoch [307/500], Train Loss: 0.0826\n",
      "Epoch [308/500], Train Loss: 0.0825\n",
      "Epoch [309/500], Train Loss: 0.0824\n",
      "Epoch [310/500], Train Loss: 0.0823\n",
      "Epoch [311/500], Train Loss: 0.0821\n",
      "Epoch [312/500], Train Loss: 0.0821\n",
      "Epoch [313/500], Train Loss: 0.0819\n",
      "Epoch [314/500], Train Loss: 0.0817\n",
      "Epoch [315/500], Train Loss: 0.0819\n",
      "Epoch [316/500], Train Loss: 0.0817\n",
      "Epoch [317/500], Train Loss: 0.0816\n",
      "Epoch [318/500], Train Loss: 0.0816\n",
      "Epoch [319/500], Train Loss: 0.0815\n",
      "Epoch [320/500], Train Loss: 0.0814\n",
      "Epoch [321/500], Train Loss: 0.0814\n",
      "Epoch [322/500], Train Loss: 0.0812\n",
      "Epoch [323/500], Train Loss: 0.0810\n",
      "Epoch [324/500], Train Loss: 0.0810\n",
      "Epoch [325/500], Train Loss: 0.0809\n",
      "Epoch [326/500], Train Loss: 0.0809\n",
      "Epoch [327/500], Train Loss: 0.0807\n",
      "Epoch [328/500], Train Loss: 0.0808\n",
      "Epoch [329/500], Train Loss: 0.0806\n",
      "Epoch [330/500], Train Loss: 0.0805\n",
      "Epoch [331/500], Train Loss: 0.0805\n",
      "Epoch [332/500], Train Loss: 0.0804\n",
      "Epoch [333/500], Train Loss: 0.0803\n",
      "Epoch [334/500], Train Loss: 0.0803\n",
      "Epoch [335/500], Train Loss: 0.0801\n",
      "Epoch [336/500], Train Loss: 0.0800\n",
      "Epoch [337/500], Train Loss: 0.0799\n",
      "Epoch [338/500], Train Loss: 0.0797\n",
      "Epoch [339/500], Train Loss: 0.0797\n",
      "Epoch [340/500], Train Loss: 0.0796\n",
      "Epoch [341/500], Train Loss: 0.0795\n",
      "Epoch [342/500], Train Loss: 0.0793\n",
      "Epoch [343/500], Train Loss: 0.0795\n",
      "Epoch [344/500], Train Loss: 0.0793\n",
      "Epoch [345/500], Train Loss: 0.0792\n",
      "Epoch [346/500], Train Loss: 0.0791\n",
      "Epoch [347/500], Train Loss: 0.0791\n",
      "Epoch [348/500], Train Loss: 0.0791\n",
      "Epoch [349/500], Train Loss: 0.0791\n",
      "Epoch [350/500], Train Loss: 0.0791\n",
      "Epoch [351/500], Train Loss: 0.0790\n",
      "Epoch [352/500], Train Loss: 0.0788\n",
      "Epoch [353/500], Train Loss: 0.0787\n",
      "Epoch [354/500], Train Loss: 0.0786\n",
      "Epoch [355/500], Train Loss: 0.0786\n",
      "Epoch [356/500], Train Loss: 0.0785\n",
      "Epoch [357/500], Train Loss: 0.0784\n",
      "Epoch [358/500], Train Loss: 0.0783\n",
      "Epoch [359/500], Train Loss: 0.0782\n",
      "Epoch [360/500], Train Loss: 0.0782\n",
      "Epoch [361/500], Train Loss: 0.0781\n",
      "Epoch [362/500], Train Loss: 0.0780\n",
      "Epoch [363/500], Train Loss: 0.0780\n",
      "Epoch [364/500], Train Loss: 0.0779\n",
      "Epoch [365/500], Train Loss: 0.0778\n",
      "Epoch [366/500], Train Loss: 0.0777\n",
      "Epoch [367/500], Train Loss: 0.0777\n",
      "Epoch [368/500], Train Loss: 0.0776\n",
      "Epoch [369/500], Train Loss: 0.0774\n",
      "Epoch [370/500], Train Loss: 0.0774\n",
      "Epoch [371/500], Train Loss: 0.0774\n",
      "Epoch [372/500], Train Loss: 0.0773\n",
      "Epoch [373/500], Train Loss: 0.0773\n",
      "Epoch [374/500], Train Loss: 0.0773\n",
      "Epoch [375/500], Train Loss: 0.0772\n",
      "Epoch [376/500], Train Loss: 0.0770\n",
      "Epoch [377/500], Train Loss: 0.0770\n",
      "Epoch [378/500], Train Loss: 0.0771\n",
      "Epoch [379/500], Train Loss: 0.0768\n",
      "Epoch [380/500], Train Loss: 0.0767\n",
      "Epoch [381/500], Train Loss: 0.0766\n",
      "Epoch [382/500], Train Loss: 0.0766\n",
      "Epoch [383/500], Train Loss: 0.0766\n",
      "Epoch [384/500], Train Loss: 0.0765\n",
      "Epoch [385/500], Train Loss: 0.0765\n",
      "Epoch [386/500], Train Loss: 0.0764\n",
      "Epoch [387/500], Train Loss: 0.0764\n",
      "Epoch [388/500], Train Loss: 0.0763\n",
      "Epoch [389/500], Train Loss: 0.0763\n",
      "Epoch [390/500], Train Loss: 0.0762\n",
      "Epoch [391/500], Train Loss: 0.0761\n",
      "Epoch [392/500], Train Loss: 0.0761\n",
      "Epoch [393/500], Train Loss: 0.0759\n",
      "Epoch [394/500], Train Loss: 0.0760\n",
      "Epoch [395/500], Train Loss: 0.0759\n",
      "Epoch [396/500], Train Loss: 0.0760\n",
      "Epoch [397/500], Train Loss: 0.0760\n",
      "Epoch [398/500], Train Loss: 0.0757\n",
      "Epoch [399/500], Train Loss: 0.0757\n",
      "Epoch [400/500], Train Loss: 0.0755\n",
      "Epoch [401/500], Train Loss: 0.0754\n",
      "Epoch [402/500], Train Loss: 0.0755\n",
      "Epoch [403/500], Train Loss: 0.0756\n",
      "Epoch [404/500], Train Loss: 0.0755\n",
      "Epoch [405/500], Train Loss: 0.0754\n",
      "Epoch [406/500], Train Loss: 0.0750\n",
      "Epoch [407/500], Train Loss: 0.0751\n",
      "Epoch [408/500], Train Loss: 0.0752\n",
      "Epoch [409/500], Train Loss: 0.0750\n",
      "Epoch [410/500], Train Loss: 0.0750\n",
      "Epoch [411/500], Train Loss: 0.0750\n",
      "Epoch [412/500], Train Loss: 0.0748\n",
      "Epoch [413/500], Train Loss: 0.0748\n",
      "Epoch [414/500], Train Loss: 0.0747\n",
      "Epoch [415/500], Train Loss: 0.0748\n",
      "Epoch [416/500], Train Loss: 0.0747\n",
      "Epoch [417/500], Train Loss: 0.0745\n",
      "Epoch [418/500], Train Loss: 0.0747\n",
      "Epoch [419/500], Train Loss: 0.0746\n",
      "Epoch [420/500], Train Loss: 0.0745\n",
      "Epoch [421/500], Train Loss: 0.0744\n",
      "Epoch [422/500], Train Loss: 0.0742\n",
      "Epoch [423/500], Train Loss: 0.0743\n",
      "Epoch [424/500], Train Loss: 0.0742\n",
      "Epoch [425/500], Train Loss: 0.0742\n",
      "Epoch [426/500], Train Loss: 0.0743\n",
      "Epoch [427/500], Train Loss: 0.0741\n",
      "Epoch [428/500], Train Loss: 0.0741\n",
      "Epoch [429/500], Train Loss: 0.0740\n",
      "Epoch [430/500], Train Loss: 0.0740\n",
      "Epoch [431/500], Train Loss: 0.0740\n",
      "Epoch [432/500], Train Loss: 0.0740\n",
      "Epoch [433/500], Train Loss: 0.0737\n",
      "Epoch [434/500], Train Loss: 0.0736\n",
      "Epoch [435/500], Train Loss: 0.0736\n",
      "Epoch [436/500], Train Loss: 0.0736\n",
      "Epoch [437/500], Train Loss: 0.0735\n",
      "Epoch [438/500], Train Loss: 0.0735\n",
      "Epoch [439/500], Train Loss: 0.0735\n",
      "Epoch [440/500], Train Loss: 0.0735\n",
      "Epoch [441/500], Train Loss: 0.0735\n",
      "Epoch [442/500], Train Loss: 0.0734\n",
      "Epoch [443/500], Train Loss: 0.0734\n",
      "Epoch [444/500], Train Loss: 0.0732\n",
      "Epoch [445/500], Train Loss: 0.0731\n",
      "Epoch [446/500], Train Loss: 0.0731\n",
      "Epoch [447/500], Train Loss: 0.0733\n",
      "Epoch [448/500], Train Loss: 0.0732\n",
      "Epoch [449/500], Train Loss: 0.0730\n",
      "Epoch [450/500], Train Loss: 0.0729\n",
      "Epoch [451/500], Train Loss: 0.0727\n",
      "Epoch [452/500], Train Loss: 0.0727\n",
      "Epoch [453/500], Train Loss: 0.0727\n",
      "Epoch [454/500], Train Loss: 0.0728\n",
      "Epoch [455/500], Train Loss: 0.0727\n",
      "Epoch [456/500], Train Loss: 0.0726\n",
      "Epoch [457/500], Train Loss: 0.0727\n",
      "Epoch [458/500], Train Loss: 0.0725\n",
      "Epoch [459/500], Train Loss: 0.0726\n",
      "Epoch [460/500], Train Loss: 0.0725\n",
      "Epoch [461/500], Train Loss: 0.0725\n",
      "Epoch [462/500], Train Loss: 0.0725\n",
      "Epoch [463/500], Train Loss: 0.0724\n",
      "Epoch [464/500], Train Loss: 0.0723\n",
      "Epoch [465/500], Train Loss: 0.0722\n",
      "Epoch [466/500], Train Loss: 0.0723\n",
      "Epoch [467/500], Train Loss: 0.0722\n",
      "Epoch [468/500], Train Loss: 0.0721\n",
      "Epoch [469/500], Train Loss: 0.0721\n",
      "Epoch [470/500], Train Loss: 0.0721\n",
      "Epoch [471/500], Train Loss: 0.0720\n",
      "Epoch [472/500], Train Loss: 0.0720\n",
      "Epoch [473/500], Train Loss: 0.0719\n",
      "Epoch [474/500], Train Loss: 0.0719\n",
      "Epoch [475/500], Train Loss: 0.0718\n",
      "Epoch [476/500], Train Loss: 0.0718\n",
      "Epoch [477/500], Train Loss: 0.0717\n",
      "Epoch [478/500], Train Loss: 0.0717\n",
      "Epoch [479/500], Train Loss: 0.0716\n",
      "Epoch [480/500], Train Loss: 0.0715\n",
      "Epoch [481/500], Train Loss: 0.0716\n",
      "Epoch [482/500], Train Loss: 0.0715\n",
      "Epoch [483/500], Train Loss: 0.0717\n",
      "Epoch [484/500], Train Loss: 0.0717\n",
      "Epoch [485/500], Train Loss: 0.0715\n",
      "Epoch [486/500], Train Loss: 0.0714\n",
      "Epoch [487/500], Train Loss: 0.0713\n",
      "Epoch [488/500], Train Loss: 0.0714\n",
      "Epoch [489/500], Train Loss: 0.0713\n",
      "Epoch [490/500], Train Loss: 0.0714\n",
      "Epoch [491/500], Train Loss: 0.0712\n",
      "Epoch [492/500], Train Loss: 0.0711\n",
      "Epoch [493/500], Train Loss: 0.0709\n",
      "Epoch [494/500], Train Loss: 0.0710\n",
      "Epoch [495/500], Train Loss: 0.0711\n",
      "Epoch [496/500], Train Loss: 0.0710\n",
      "Epoch [497/500], Train Loss: 0.0711\n",
      "Epoch [498/500], Train Loss: 0.0709\n",
      "Epoch [499/500], Train Loss: 0.0709\n",
      "Epoch [500/500], Train Loss: 0.0709\n",
      "Final model saved at: ../../../Data/model_results/logistic_regression/models/chr5/final_model_chr5.pth\n",
      "Individual R^2 scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr5/individual_r2_scores_chr5.csv\n",
      "Individual IQS scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr5/individual_iqs_scores_chr5.csv\n",
      "Individual AUC ROC curves saved in: ../../../Data/model_results/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../../Data/model_results/logistic_regression/csv_files/performance_metrics.csv\n",
      "Total SNPs:  1706\n",
      "PRS313 SNPs:  40\n",
      "Total SNPs used for Training:  1666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-06-18 12:01:56,405] Trial 28 finished with value: 0.1448026800384888 and parameters: {'learning_rate': 0.00012044600714687634, 'l1_coef': 1.3129473481457664e-05, 'patience': 15, 'batch_size': 32}. Best is trial 28 with value: 0.1448026800384888.\n",
      "[I 2024-06-18 12:01:57,576] Trial 27 finished with value: 0.1374111989369759 and parameters: {'learning_rate': 0.00013023881858474157, 'l1_coef': 1.1452876176725031e-05, 'patience': 15, 'batch_size': 32}. Best is trial 27 with value: 0.1374111989369759.\n",
      "[I 2024-06-18 12:02:04,329] Trial 24 finished with value: 0.14138092341331335 and parameters: {'learning_rate': 0.00016222939553995326, 'l1_coef': 1.4277410469897544e-05, 'patience': 15, 'batch_size': 32}. Best is trial 27 with value: 0.1374111989369759.\n",
      "[I 2024-06-18 12:02:14,645] Trial 21 finished with value: 0.1376157751450172 and parameters: {'learning_rate': 0.00011154694309329122, 'l1_coef': 1.0653188846671136e-05, 'patience': 15, 'batch_size': 32}. Best is trial 27 with value: 0.1374111989369759.\n",
      "[I 2024-06-18 12:02:15,224] Trial 30 finished with value: 0.15015043444358386 and parameters: {'learning_rate': 0.00013296552189084675, 'l1_coef': 1.4849042610567093e-05, 'patience': 15, 'batch_size': 32}. Best is trial 27 with value: 0.1374111989369759.\n",
      "[I 2024-06-18 12:02:25,338] Trial 29 finished with value: 0.1434797065762373 and parameters: {'learning_rate': 0.00011562860613158602, 'l1_coef': 1.2464782833454667e-05, 'patience': 15, 'batch_size': 32}. Best is trial 27 with value: 0.1374111989369759.\n",
      "[I 2024-06-18 12:02:32,481] Trial 26 finished with value: 0.1328807114408566 and parameters: {'learning_rate': 0.00011208164062831504, 'l1_coef': 1.0284689264438478e-05, 'patience': 15, 'batch_size': 32}. Best is trial 26 with value: 0.1328807114408566.\n",
      "[I 2024-06-18 12:02:33,055] Trial 23 finished with value: 0.151188381589376 and parameters: {'learning_rate': 0.00013093604453661808, 'l1_coef': 1.9596460830420658e-05, 'patience': 15, 'batch_size': 32}. Best is trial 26 with value: 0.1328807114408566.\n",
      "[I 2024-06-18 12:02:35,102] Trial 22 finished with value: 0.14304100343814263 and parameters: {'learning_rate': 0.00010676512061504058, 'l1_coef': 1.3298244672503195e-05, 'patience': 15, 'batch_size': 32}. Best is trial 26 with value: 0.1328807114408566.\n",
      "[I 2024-06-18 12:02:35,897] Trial 25 finished with value: 0.1471364657466228 and parameters: {'learning_rate': 0.00010581767636346725, 'l1_coef': 1.6644560933334953e-05, 'patience': 15, 'batch_size': 32}. Best is trial 26 with value: 0.1328807114408566.\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 6 - Best hyperparameters: {'learning_rate': 0.00011208164062831504, 'l1_coef': 1.0284689264438478e-05, 'patience': 15, 'batch_size': 32}\n",
      "Chr 6 - Best value: 0.1329\n",
      "Epoch [1/500], Train Loss: 0.5885\n",
      "Epoch [2/500], Train Loss: 0.5362\n",
      "Epoch [3/500], Train Loss: 0.5205\n",
      "Epoch [4/500], Train Loss: 0.5089\n",
      "Epoch [5/500], Train Loss: 0.4991\n",
      "Epoch [6/500], Train Loss: 0.4899\n",
      "Epoch [7/500], Train Loss: 0.4814\n",
      "Epoch [8/500], Train Loss: 0.4736\n",
      "Epoch [9/500], Train Loss: 0.4659\n",
      "Epoch [10/500], Train Loss: 0.4585\n",
      "Epoch [11/500], Train Loss: 0.4517\n",
      "Epoch [12/500], Train Loss: 0.4449\n",
      "Epoch [13/500], Train Loss: 0.4385\n",
      "Epoch [14/500], Train Loss: 0.4321\n",
      "Epoch [15/500], Train Loss: 0.4260\n",
      "Epoch [16/500], Train Loss: 0.4205\n",
      "Epoch [17/500], Train Loss: 0.4145\n",
      "Epoch [18/500], Train Loss: 0.4093\n",
      "Epoch [19/500], Train Loss: 0.4039\n",
      "Epoch [20/500], Train Loss: 0.3990\n",
      "Epoch [21/500], Train Loss: 0.3940\n",
      "Epoch [22/500], Train Loss: 0.3894\n",
      "Epoch [23/500], Train Loss: 0.3847\n",
      "Epoch [24/500], Train Loss: 0.3801\n",
      "Epoch [25/500], Train Loss: 0.3757\n",
      "Epoch [26/500], Train Loss: 0.3713\n",
      "Epoch [27/500], Train Loss: 0.3674\n",
      "Epoch [28/500], Train Loss: 0.3632\n",
      "Epoch [29/500], Train Loss: 0.3594\n",
      "Epoch [30/500], Train Loss: 0.3557\n",
      "Epoch [31/500], Train Loss: 0.3519\n",
      "Epoch [32/500], Train Loss: 0.3484\n",
      "Epoch [33/500], Train Loss: 0.3447\n",
      "Epoch [34/500], Train Loss: 0.3414\n",
      "Epoch [35/500], Train Loss: 0.3380\n",
      "Epoch [36/500], Train Loss: 0.3348\n",
      "Epoch [37/500], Train Loss: 0.3317\n",
      "Epoch [38/500], Train Loss: 0.3283\n",
      "Epoch [39/500], Train Loss: 0.3253\n",
      "Epoch [40/500], Train Loss: 0.3225\n",
      "Epoch [41/500], Train Loss: 0.3196\n",
      "Epoch [42/500], Train Loss: 0.3167\n",
      "Epoch [43/500], Train Loss: 0.3138\n",
      "Epoch [44/500], Train Loss: 0.3112\n",
      "Epoch [45/500], Train Loss: 0.3085\n",
      "Epoch [46/500], Train Loss: 0.3057\n",
      "Epoch [47/500], Train Loss: 0.3034\n",
      "Epoch [48/500], Train Loss: 0.3009\n",
      "Epoch [49/500], Train Loss: 0.2984\n",
      "Epoch [50/500], Train Loss: 0.2959\n",
      "Epoch [51/500], Train Loss: 0.2937\n",
      "Epoch [52/500], Train Loss: 0.2913\n",
      "Epoch [53/500], Train Loss: 0.2891\n",
      "Epoch [54/500], Train Loss: 0.2869\n",
      "Epoch [55/500], Train Loss: 0.2847\n",
      "Epoch [56/500], Train Loss: 0.2825\n",
      "Epoch [57/500], Train Loss: 0.2804\n",
      "Epoch [58/500], Train Loss: 0.2784\n",
      "Epoch [59/500], Train Loss: 0.2763\n",
      "Epoch [60/500], Train Loss: 0.2743\n",
      "Epoch [61/500], Train Loss: 0.2723\n",
      "Epoch [62/500], Train Loss: 0.2704\n",
      "Epoch [63/500], Train Loss: 0.2686\n",
      "Epoch [64/500], Train Loss: 0.2667\n",
      "Epoch [65/500], Train Loss: 0.2647\n",
      "Epoch [66/500], Train Loss: 0.2629\n",
      "Epoch [67/500], Train Loss: 0.2612\n",
      "Epoch [68/500], Train Loss: 0.2595\n",
      "Epoch [69/500], Train Loss: 0.2576\n",
      "Epoch [70/500], Train Loss: 0.2561\n",
      "Epoch [71/500], Train Loss: 0.2544\n",
      "Epoch [72/500], Train Loss: 0.2528\n",
      "Epoch [73/500], Train Loss: 0.2511\n",
      "Epoch [74/500], Train Loss: 0.2494\n",
      "Epoch [75/500], Train Loss: 0.2479\n",
      "Epoch [76/500], Train Loss: 0.2464\n",
      "Epoch [77/500], Train Loss: 0.2448\n",
      "Epoch [78/500], Train Loss: 0.2434\n",
      "Epoch [79/500], Train Loss: 0.2420\n",
      "Epoch [80/500], Train Loss: 0.2406\n",
      "Epoch [81/500], Train Loss: 0.2391\n",
      "Epoch [82/500], Train Loss: 0.2376\n",
      "Epoch [83/500], Train Loss: 0.2362\n",
      "Epoch [84/500], Train Loss: 0.2349\n",
      "Epoch [85/500], Train Loss: 0.2333\n",
      "Epoch [86/500], Train Loss: 0.2322\n",
      "Epoch [87/500], Train Loss: 0.2310\n",
      "Epoch [88/500], Train Loss: 0.2296\n",
      "Epoch [89/500], Train Loss: 0.2282\n",
      "Epoch [90/500], Train Loss: 0.2270\n",
      "Epoch [91/500], Train Loss: 0.2257\n",
      "Epoch [92/500], Train Loss: 0.2246\n",
      "Epoch [93/500], Train Loss: 0.2233\n",
      "Epoch [94/500], Train Loss: 0.2221\n",
      "Epoch [95/500], Train Loss: 0.2210\n",
      "Epoch [96/500], Train Loss: 0.2198\n",
      "Epoch [97/500], Train Loss: 0.2187\n",
      "Epoch [98/500], Train Loss: 0.2176\n",
      "Epoch [99/500], Train Loss: 0.2166\n",
      "Epoch [100/500], Train Loss: 0.2153\n",
      "Epoch [101/500], Train Loss: 0.2141\n",
      "Epoch [102/500], Train Loss: 0.2132\n",
      "Epoch [103/500], Train Loss: 0.2120\n",
      "Epoch [104/500], Train Loss: 0.2110\n",
      "Epoch [105/500], Train Loss: 0.2100\n",
      "Epoch [106/500], Train Loss: 0.2089\n",
      "Epoch [107/500], Train Loss: 0.2079\n",
      "Epoch [108/500], Train Loss: 0.2069\n",
      "Epoch [109/500], Train Loss: 0.2060\n",
      "Epoch [110/500], Train Loss: 0.2049\n",
      "Epoch [111/500], Train Loss: 0.2039\n",
      "Epoch [112/500], Train Loss: 0.2030\n",
      "Epoch [113/500], Train Loss: 0.2022\n",
      "Epoch [114/500], Train Loss: 0.2013\n",
      "Epoch [115/500], Train Loss: 0.2004\n",
      "Epoch [116/500], Train Loss: 0.1994\n",
      "Epoch [117/500], Train Loss: 0.1986\n",
      "Epoch [118/500], Train Loss: 0.1976\n",
      "Epoch [119/500], Train Loss: 0.1969\n",
      "Epoch [120/500], Train Loss: 0.1958\n",
      "Epoch [121/500], Train Loss: 0.1950\n",
      "Epoch [122/500], Train Loss: 0.1943\n",
      "Epoch [123/500], Train Loss: 0.1933\n",
      "Epoch [124/500], Train Loss: 0.1925\n",
      "Epoch [125/500], Train Loss: 0.1918\n",
      "Epoch [126/500], Train Loss: 0.1908\n",
      "Epoch [127/500], Train Loss: 0.1900\n",
      "Epoch [128/500], Train Loss: 0.1893\n",
      "Epoch [129/500], Train Loss: 0.1885\n",
      "Epoch [130/500], Train Loss: 0.1878\n",
      "Epoch [131/500], Train Loss: 0.1870\n",
      "Epoch [132/500], Train Loss: 0.1862\n",
      "Epoch [133/500], Train Loss: 0.1856\n",
      "Epoch [134/500], Train Loss: 0.1848\n",
      "Epoch [135/500], Train Loss: 0.1838\n",
      "Epoch [136/500], Train Loss: 0.1834\n",
      "Epoch [137/500], Train Loss: 0.1827\n",
      "Epoch [138/500], Train Loss: 0.1819\n",
      "Epoch [139/500], Train Loss: 0.1812\n",
      "Epoch [140/500], Train Loss: 0.1804\n",
      "Epoch [141/500], Train Loss: 0.1796\n",
      "Epoch [142/500], Train Loss: 0.1792\n",
      "Epoch [143/500], Train Loss: 0.1784\n",
      "Epoch [144/500], Train Loss: 0.1778\n",
      "Epoch [145/500], Train Loss: 0.1771\n",
      "Epoch [146/500], Train Loss: 0.1766\n",
      "Epoch [147/500], Train Loss: 0.1758\n",
      "Epoch [148/500], Train Loss: 0.1751\n",
      "Epoch [149/500], Train Loss: 0.1745\n",
      "Epoch [150/500], Train Loss: 0.1739\n",
      "Epoch [151/500], Train Loss: 0.1733\n",
      "Epoch [152/500], Train Loss: 0.1727\n",
      "Epoch [153/500], Train Loss: 0.1721\n",
      "Epoch [154/500], Train Loss: 0.1714\n",
      "Epoch [155/500], Train Loss: 0.1711\n",
      "Epoch [156/500], Train Loss: 0.1702\n",
      "Epoch [157/500], Train Loss: 0.1698\n",
      "Epoch [158/500], Train Loss: 0.1693\n",
      "Epoch [159/500], Train Loss: 0.1686\n",
      "Epoch [160/500], Train Loss: 0.1680\n",
      "Epoch [161/500], Train Loss: 0.1675\n",
      "Epoch [162/500], Train Loss: 0.1670\n",
      "Epoch [163/500], Train Loss: 0.1662\n",
      "Epoch [164/500], Train Loss: 0.1658\n",
      "Epoch [165/500], Train Loss: 0.1653\n",
      "Epoch [166/500], Train Loss: 0.1647\n",
      "Epoch [167/500], Train Loss: 0.1642\n",
      "Epoch [168/500], Train Loss: 0.1637\n",
      "Epoch [169/500], Train Loss: 0.1632\n",
      "Epoch [170/500], Train Loss: 0.1626\n",
      "Epoch [171/500], Train Loss: 0.1622\n",
      "Epoch [172/500], Train Loss: 0.1616\n",
      "Epoch [173/500], Train Loss: 0.1612\n",
      "Epoch [174/500], Train Loss: 0.1607\n",
      "Epoch [175/500], Train Loss: 0.1602\n",
      "Epoch [176/500], Train Loss: 0.1597\n",
      "Epoch [177/500], Train Loss: 0.1593\n",
      "Epoch [178/500], Train Loss: 0.1587\n",
      "Epoch [179/500], Train Loss: 0.1583\n",
      "Epoch [180/500], Train Loss: 0.1578\n",
      "Epoch [181/500], Train Loss: 0.1572\n",
      "Epoch [182/500], Train Loss: 0.1568\n",
      "Epoch [183/500], Train Loss: 0.1564\n",
      "Epoch [184/500], Train Loss: 0.1559\n",
      "Epoch [185/500], Train Loss: 0.1555\n",
      "Epoch [186/500], Train Loss: 0.1551\n",
      "Epoch [187/500], Train Loss: 0.1546\n",
      "Epoch [188/500], Train Loss: 0.1543\n",
      "Epoch [189/500], Train Loss: 0.1538\n",
      "Epoch [190/500], Train Loss: 0.1533\n",
      "Epoch [191/500], Train Loss: 0.1530\n",
      "Epoch [192/500], Train Loss: 0.1525\n",
      "Epoch [193/500], Train Loss: 0.1521\n",
      "Epoch [194/500], Train Loss: 0.1518\n",
      "Epoch [195/500], Train Loss: 0.1512\n",
      "Epoch [196/500], Train Loss: 0.1509\n",
      "Epoch [197/500], Train Loss: 0.1504\n",
      "Epoch [198/500], Train Loss: 0.1500\n",
      "Epoch [199/500], Train Loss: 0.1497\n",
      "Epoch [200/500], Train Loss: 0.1493\n",
      "Epoch [201/500], Train Loss: 0.1489\n",
      "Epoch [202/500], Train Loss: 0.1485\n",
      "Epoch [203/500], Train Loss: 0.1481\n",
      "Epoch [204/500], Train Loss: 0.1478\n",
      "Epoch [205/500], Train Loss: 0.1474\n",
      "Epoch [206/500], Train Loss: 0.1470\n",
      "Epoch [207/500], Train Loss: 0.1466\n",
      "Epoch [208/500], Train Loss: 0.1463\n",
      "Epoch [209/500], Train Loss: 0.1459\n",
      "Epoch [210/500], Train Loss: 0.1455\n",
      "Epoch [211/500], Train Loss: 0.1452\n",
      "Epoch [212/500], Train Loss: 0.1448\n",
      "Epoch [213/500], Train Loss: 0.1445\n",
      "Epoch [214/500], Train Loss: 0.1442\n",
      "Epoch [215/500], Train Loss: 0.1437\n",
      "Epoch [216/500], Train Loss: 0.1434\n",
      "Epoch [217/500], Train Loss: 0.1430\n",
      "Epoch [218/500], Train Loss: 0.1426\n",
      "Epoch [219/500], Train Loss: 0.1425\n",
      "Epoch [220/500], Train Loss: 0.1420\n",
      "Epoch [221/500], Train Loss: 0.1417\n",
      "Epoch [222/500], Train Loss: 0.1414\n",
      "Epoch [223/500], Train Loss: 0.1411\n",
      "Epoch [224/500], Train Loss: 0.1407\n",
      "Epoch [225/500], Train Loss: 0.1405\n",
      "Epoch [226/500], Train Loss: 0.1401\n",
      "Epoch [227/500], Train Loss: 0.1397\n",
      "Epoch [228/500], Train Loss: 0.1394\n",
      "Epoch [229/500], Train Loss: 0.1392\n",
      "Epoch [230/500], Train Loss: 0.1388\n",
      "Epoch [231/500], Train Loss: 0.1386\n",
      "Epoch [232/500], Train Loss: 0.1383\n",
      "Epoch [233/500], Train Loss: 0.1380\n",
      "Epoch [234/500], Train Loss: 0.1376\n",
      "Epoch [235/500], Train Loss: 0.1373\n",
      "Epoch [236/500], Train Loss: 0.1371\n",
      "Epoch [237/500], Train Loss: 0.1368\n",
      "Epoch [238/500], Train Loss: 0.1365\n",
      "Epoch [239/500], Train Loss: 0.1361\n",
      "Epoch [240/500], Train Loss: 0.1358\n",
      "Epoch [241/500], Train Loss: 0.1356\n",
      "Epoch [242/500], Train Loss: 0.1354\n",
      "Epoch [243/500], Train Loss: 0.1352\n",
      "Epoch [244/500], Train Loss: 0.1349\n",
      "Epoch [245/500], Train Loss: 0.1344\n",
      "Epoch [246/500], Train Loss: 0.1343\n",
      "Epoch [247/500], Train Loss: 0.1339\n",
      "Epoch [248/500], Train Loss: 0.1337\n",
      "Epoch [249/500], Train Loss: 0.1335\n",
      "Epoch [250/500], Train Loss: 0.1332\n",
      "Epoch [251/500], Train Loss: 0.1329\n",
      "Epoch [252/500], Train Loss: 0.1326\n",
      "Epoch [253/500], Train Loss: 0.1324\n",
      "Epoch [254/500], Train Loss: 0.1321\n",
      "Epoch [255/500], Train Loss: 0.1319\n",
      "Epoch [256/500], Train Loss: 0.1316\n",
      "Epoch [257/500], Train Loss: 0.1314\n",
      "Epoch [258/500], Train Loss: 0.1311\n",
      "Epoch [259/500], Train Loss: 0.1308\n",
      "Epoch [260/500], Train Loss: 0.1307\n",
      "Epoch [261/500], Train Loss: 0.1303\n",
      "Epoch [262/500], Train Loss: 0.1302\n",
      "Epoch [263/500], Train Loss: 0.1299\n",
      "Epoch [264/500], Train Loss: 0.1297\n",
      "Epoch [265/500], Train Loss: 0.1294\n",
      "Epoch [266/500], Train Loss: 0.1292\n",
      "Epoch [267/500], Train Loss: 0.1289\n",
      "Epoch [268/500], Train Loss: 0.1288\n",
      "Epoch [269/500], Train Loss: 0.1286\n",
      "Epoch [270/500], Train Loss: 0.1283\n",
      "Epoch [271/500], Train Loss: 0.1280\n",
      "Epoch [272/500], Train Loss: 0.1278\n",
      "Epoch [273/500], Train Loss: 0.1276\n",
      "Epoch [274/500], Train Loss: 0.1273\n",
      "Epoch [275/500], Train Loss: 0.1271\n",
      "Epoch [276/500], Train Loss: 0.1269\n",
      "Epoch [277/500], Train Loss: 0.1266\n",
      "Epoch [278/500], Train Loss: 0.1265\n",
      "Epoch [279/500], Train Loss: 0.1262\n",
      "Epoch [280/500], Train Loss: 0.1261\n",
      "Epoch [281/500], Train Loss: 0.1258\n",
      "Epoch [282/500], Train Loss: 0.1256\n",
      "Epoch [283/500], Train Loss: 0.1253\n",
      "Epoch [284/500], Train Loss: 0.1252\n",
      "Epoch [285/500], Train Loss: 0.1251\n",
      "Epoch [286/500], Train Loss: 0.1248\n",
      "Epoch [287/500], Train Loss: 0.1245\n",
      "Epoch [288/500], Train Loss: 0.1244\n",
      "Epoch [289/500], Train Loss: 0.1241\n",
      "Epoch [290/500], Train Loss: 0.1240\n",
      "Epoch [291/500], Train Loss: 0.1237\n",
      "Epoch [292/500], Train Loss: 0.1236\n",
      "Epoch [293/500], Train Loss: 0.1234\n",
      "Epoch [294/500], Train Loss: 0.1232\n",
      "Epoch [295/500], Train Loss: 0.1230\n",
      "Epoch [296/500], Train Loss: 0.1227\n",
      "Epoch [297/500], Train Loss: 0.1225\n",
      "Epoch [298/500], Train Loss: 0.1223\n",
      "Epoch [299/500], Train Loss: 0.1221\n",
      "Epoch [300/500], Train Loss: 0.1220\n",
      "Epoch [301/500], Train Loss: 0.1218\n",
      "Epoch [302/500], Train Loss: 0.1216\n",
      "Epoch [303/500], Train Loss: 0.1214\n",
      "Epoch [304/500], Train Loss: 0.1212\n",
      "Epoch [305/500], Train Loss: 0.1211\n",
      "Epoch [306/500], Train Loss: 0.1208\n",
      "Epoch [307/500], Train Loss: 0.1207\n",
      "Epoch [308/500], Train Loss: 0.1205\n",
      "Epoch [309/500], Train Loss: 0.1204\n",
      "Epoch [310/500], Train Loss: 0.1202\n",
      "Epoch [311/500], Train Loss: 0.1199\n",
      "Epoch [312/500], Train Loss: 0.1198\n",
      "Epoch [313/500], Train Loss: 0.1196\n",
      "Epoch [314/500], Train Loss: 0.1193\n",
      "Epoch [315/500], Train Loss: 0.1194\n",
      "Epoch [316/500], Train Loss: 0.1191\n",
      "Epoch [317/500], Train Loss: 0.1190\n",
      "Epoch [318/500], Train Loss: 0.1187\n",
      "Epoch [319/500], Train Loss: 0.1187\n",
      "Epoch [320/500], Train Loss: 0.1184\n",
      "Epoch [321/500], Train Loss: 0.1182\n",
      "Epoch [322/500], Train Loss: 0.1181\n",
      "Epoch [323/500], Train Loss: 0.1179\n",
      "Epoch [324/500], Train Loss: 0.1178\n",
      "Epoch [325/500], Train Loss: 0.1175\n",
      "Epoch [326/500], Train Loss: 0.1174\n",
      "Epoch [327/500], Train Loss: 0.1172\n",
      "Epoch [328/500], Train Loss: 0.1172\n",
      "Epoch [329/500], Train Loss: 0.1170\n",
      "Epoch [330/500], Train Loss: 0.1168\n",
      "Epoch [331/500], Train Loss: 0.1166\n",
      "Epoch [332/500], Train Loss: 0.1165\n",
      "Epoch [333/500], Train Loss: 0.1163\n",
      "Epoch [334/500], Train Loss: 0.1162\n",
      "Epoch [335/500], Train Loss: 0.1160\n",
      "Epoch [336/500], Train Loss: 0.1158\n",
      "Epoch [337/500], Train Loss: 0.1158\n",
      "Epoch [338/500], Train Loss: 0.1155\n",
      "Epoch [339/500], Train Loss: 0.1154\n",
      "Epoch [340/500], Train Loss: 0.1153\n",
      "Epoch [341/500], Train Loss: 0.1151\n",
      "Epoch [342/500], Train Loss: 0.1150\n",
      "Epoch [343/500], Train Loss: 0.1148\n",
      "Epoch [344/500], Train Loss: 0.1146\n",
      "Epoch [345/500], Train Loss: 0.1145\n",
      "Epoch [346/500], Train Loss: 0.1145\n",
      "Epoch [347/500], Train Loss: 0.1142\n",
      "Epoch [348/500], Train Loss: 0.1142\n",
      "Epoch [349/500], Train Loss: 0.1139\n",
      "Epoch [350/500], Train Loss: 0.1138\n",
      "Epoch [351/500], Train Loss: 0.1136\n",
      "Epoch [352/500], Train Loss: 0.1135\n",
      "Epoch [353/500], Train Loss: 0.1133\n",
      "Epoch [354/500], Train Loss: 0.1132\n",
      "Epoch [355/500], Train Loss: 0.1130\n",
      "Epoch [356/500], Train Loss: 0.1129\n",
      "Epoch [357/500], Train Loss: 0.1129\n",
      "Epoch [358/500], Train Loss: 0.1127\n",
      "Epoch [359/500], Train Loss: 0.1126\n",
      "Epoch [360/500], Train Loss: 0.1125\n",
      "Epoch [361/500], Train Loss: 0.1123\n",
      "Epoch [362/500], Train Loss: 0.1122\n",
      "Epoch [363/500], Train Loss: 0.1120\n",
      "Epoch [364/500], Train Loss: 0.1118\n",
      "Epoch [365/500], Train Loss: 0.1118\n",
      "Epoch [366/500], Train Loss: 0.1117\n",
      "Epoch [367/500], Train Loss: 0.1114\n",
      "Epoch [368/500], Train Loss: 0.1113\n",
      "Epoch [369/500], Train Loss: 0.1112\n",
      "Epoch [370/500], Train Loss: 0.1110\n",
      "Epoch [371/500], Train Loss: 0.1111\n",
      "Epoch [372/500], Train Loss: 0.1108\n",
      "Epoch [373/500], Train Loss: 0.1107\n",
      "Epoch [374/500], Train Loss: 0.1106\n",
      "Epoch [375/500], Train Loss: 0.1105\n",
      "Epoch [376/500], Train Loss: 0.1103\n",
      "Epoch [377/500], Train Loss: 0.1103\n",
      "Epoch [378/500], Train Loss: 0.1101\n",
      "Epoch [379/500], Train Loss: 0.1100\n",
      "Epoch [380/500], Train Loss: 0.1099\n",
      "Epoch [381/500], Train Loss: 0.1098\n",
      "Epoch [382/500], Train Loss: 0.1096\n",
      "Epoch [383/500], Train Loss: 0.1095\n",
      "Epoch [384/500], Train Loss: 0.1094\n",
      "Epoch [385/500], Train Loss: 0.1093\n",
      "Epoch [386/500], Train Loss: 0.1092\n",
      "Epoch [387/500], Train Loss: 0.1091\n",
      "Epoch [388/500], Train Loss: 0.1090\n",
      "Epoch [389/500], Train Loss: 0.1088\n",
      "Epoch [390/500], Train Loss: 0.1086\n",
      "Epoch [391/500], Train Loss: 0.1085\n",
      "Epoch [392/500], Train Loss: 0.1085\n",
      "Epoch [393/500], Train Loss: 0.1084\n",
      "Epoch [394/500], Train Loss: 0.1081\n",
      "Epoch [395/500], Train Loss: 0.1081\n",
      "Epoch [396/500], Train Loss: 0.1081\n",
      "Epoch [397/500], Train Loss: 0.1079\n",
      "Epoch [398/500], Train Loss: 0.1079\n",
      "Epoch [399/500], Train Loss: 0.1076\n",
      "Epoch [400/500], Train Loss: 0.1076\n",
      "Epoch [401/500], Train Loss: 0.1075\n",
      "Epoch [402/500], Train Loss: 0.1074\n",
      "Epoch [403/500], Train Loss: 0.1072\n",
      "Epoch [404/500], Train Loss: 0.1071\n",
      "Epoch [405/500], Train Loss: 0.1070\n",
      "Epoch [406/500], Train Loss: 0.1069\n",
      "Epoch [407/500], Train Loss: 0.1068\n",
      "Epoch [408/500], Train Loss: 0.1067\n",
      "Epoch [409/500], Train Loss: 0.1066\n",
      "Epoch [410/500], Train Loss: 0.1066\n",
      "Epoch [411/500], Train Loss: 0.1064\n",
      "Epoch [412/500], Train Loss: 0.1064\n",
      "Epoch [413/500], Train Loss: 0.1062\n",
      "Epoch [414/500], Train Loss: 0.1061\n",
      "Epoch [415/500], Train Loss: 0.1061\n",
      "Epoch [416/500], Train Loss: 0.1059\n",
      "Epoch [417/500], Train Loss: 0.1058\n",
      "Epoch [418/500], Train Loss: 0.1056\n",
      "Epoch [419/500], Train Loss: 0.1056\n",
      "Epoch [420/500], Train Loss: 0.1055\n",
      "Epoch [421/500], Train Loss: 0.1054\n",
      "Epoch [422/500], Train Loss: 0.1053\n",
      "Epoch [423/500], Train Loss: 0.1052\n",
      "Epoch [424/500], Train Loss: 0.1051\n",
      "Epoch [425/500], Train Loss: 0.1050\n",
      "Epoch [426/500], Train Loss: 0.1049\n",
      "Epoch [427/500], Train Loss: 0.1048\n",
      "Epoch [428/500], Train Loss: 0.1047\n",
      "Epoch [429/500], Train Loss: 0.1047\n",
      "Epoch [430/500], Train Loss: 0.1045\n",
      "Epoch [431/500], Train Loss: 0.1044\n",
      "Epoch [432/500], Train Loss: 0.1043\n",
      "Epoch [433/500], Train Loss: 0.1042\n",
      "Epoch [434/500], Train Loss: 0.1041\n",
      "Epoch [435/500], Train Loss: 0.1040\n",
      "Epoch [436/500], Train Loss: 0.1040\n",
      "Epoch [437/500], Train Loss: 0.1038\n",
      "Epoch [438/500], Train Loss: 0.1038\n",
      "Epoch [439/500], Train Loss: 0.1037\n",
      "Epoch [440/500], Train Loss: 0.1036\n",
      "Epoch [441/500], Train Loss: 0.1035\n",
      "Epoch [442/500], Train Loss: 0.1034\n",
      "Epoch [443/500], Train Loss: 0.1034\n",
      "Epoch [444/500], Train Loss: 0.1032\n",
      "Epoch [445/500], Train Loss: 0.1031\n",
      "Epoch [446/500], Train Loss: 0.1031\n",
      "Epoch [447/500], Train Loss: 0.1029\n",
      "Epoch [448/500], Train Loss: 0.1029\n",
      "Epoch [449/500], Train Loss: 0.1028\n",
      "Epoch [450/500], Train Loss: 0.1027\n",
      "Epoch [451/500], Train Loss: 0.1026\n",
      "Epoch [452/500], Train Loss: 0.1026\n",
      "Epoch [453/500], Train Loss: 0.1024\n",
      "Epoch [454/500], Train Loss: 0.1023\n",
      "Epoch [455/500], Train Loss: 0.1023\n",
      "Epoch [456/500], Train Loss: 0.1022\n",
      "Epoch [457/500], Train Loss: 0.1021\n",
      "Epoch [458/500], Train Loss: 0.1021\n",
      "Epoch [459/500], Train Loss: 0.1019\n",
      "Epoch [460/500], Train Loss: 0.1018\n",
      "Epoch [461/500], Train Loss: 0.1018\n",
      "Epoch [462/500], Train Loss: 0.1016\n",
      "Epoch [463/500], Train Loss: 0.1016\n",
      "Epoch [464/500], Train Loss: 0.1015\n",
      "Epoch [465/500], Train Loss: 0.1014\n",
      "Epoch [466/500], Train Loss: 0.1014\n",
      "Epoch [467/500], Train Loss: 0.1013\n",
      "Epoch [468/500], Train Loss: 0.1012\n",
      "Epoch [469/500], Train Loss: 0.1012\n",
      "Epoch [470/500], Train Loss: 0.1011\n",
      "Epoch [471/500], Train Loss: 0.1010\n",
      "Epoch [472/500], Train Loss: 0.1009\n",
      "Epoch [473/500], Train Loss: 0.1008\n",
      "Epoch [474/500], Train Loss: 0.1008\n",
      "Epoch [475/500], Train Loss: 0.1006\n",
      "Epoch [476/500], Train Loss: 0.1005\n",
      "Epoch [477/500], Train Loss: 0.1006\n",
      "Epoch [478/500], Train Loss: 0.1005\n",
      "Epoch [479/500], Train Loss: 0.1004\n",
      "Epoch [480/500], Train Loss: 0.1002\n",
      "Epoch [481/500], Train Loss: 0.1002\n",
      "Epoch [482/500], Train Loss: 0.1001\n",
      "Epoch [483/500], Train Loss: 0.1000\n",
      "Epoch [484/500], Train Loss: 0.0999\n",
      "Epoch [485/500], Train Loss: 0.1000\n",
      "Epoch [486/500], Train Loss: 0.0998\n",
      "Epoch [487/500], Train Loss: 0.0997\n",
      "Epoch [488/500], Train Loss: 0.0997\n",
      "Epoch [489/500], Train Loss: 0.0996\n",
      "Epoch [490/500], Train Loss: 0.0995\n",
      "Epoch [491/500], Train Loss: 0.0995\n",
      "Epoch [492/500], Train Loss: 0.0994\n",
      "Epoch [493/500], Train Loss: 0.0993\n",
      "Epoch [494/500], Train Loss: 0.0993\n",
      "Epoch [495/500], Train Loss: 0.0991\n",
      "Epoch [496/500], Train Loss: 0.0991\n",
      "Epoch [497/500], Train Loss: 0.0989\n",
      "Epoch [498/500], Train Loss: 0.0990\n",
      "Epoch [499/500], Train Loss: 0.0988\n",
      "Epoch [500/500], Train Loss: 0.0988\n",
      "Final model saved at: ../../../Data/model_results/logistic_regression/models/chr6/final_model_chr6.pth\n",
      "Individual R^2 scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr6/individual_r2_scores_chr6.csv\n",
      "Individual IQS scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr6/individual_iqs_scores_chr6.csv\n",
      "Individual AUC ROC curves saved in: ../../../Data/model_results/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../../Data/model_results/logistic_regression/csv_files/performance_metrics.csv\n",
      "Total SNPs:  1470\n",
      "PRS313 SNPs:  28\n",
      "Total SNPs used for Training:  1442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-06-18 12:04:37,214] Trial 14 finished with value: 0.2029388654690522 and parameters: {'learning_rate': 0.012217341685773455, 'l1_coef': 0.00024800926780902354, 'patience': 11, 'batch_size': 32}. Best is trial 9 with value: 0.12938632162717673.\n",
      "[I 2024-06-18 12:04:41,128] Trial 19 finished with value: 0.08129436098612272 and parameters: {'learning_rate': 0.0122528085987631, 'l1_coef': 1.0198011241500657e-05, 'patience': 11, 'batch_size': 32}. Best is trial 19 with value: 0.08129436098612272.\n",
      "[I 2024-06-18 12:04:50,779] Trial 15 finished with value: 0.21050595068014583 and parameters: {'learning_rate': 0.01012496453703742, 'l1_coef': 0.0002774996172998803, 'patience': 11, 'batch_size': 32}. Best is trial 19 with value: 0.08129436098612272.\n",
      "[I 2024-06-18 12:04:55,243] Trial 16 finished with value: 0.08064935952425002 and parameters: {'learning_rate': 0.011097410078928614, 'l1_coef': 1.072551035716764e-05, 'patience': 11, 'batch_size': 32}. Best is trial 16 with value: 0.08064935952425002.\n",
      "[I 2024-06-18 12:06:33,760] Trial 17 finished with value: 0.21818903501217185 and parameters: {'learning_rate': 0.00014902273399966122, 'l1_coef': 0.00026461459113428777, 'patience': 11, 'batch_size': 32}. Best is trial 16 with value: 0.08064935952425002.\n",
      "[I 2024-06-18 12:06:47,864] Trial 11 finished with value: 0.22817620107760797 and parameters: {'learning_rate': 0.0001409945759971828, 'l1_coef': 0.000300518720812775, 'patience': 11, 'batch_size': 32}. Best is trial 16 with value: 0.08064935952425002.\n",
      "[I 2024-06-18 12:06:51,763] Trial 18 finished with value: 0.22697238578246187 and parameters: {'learning_rate': 0.00013997841379745658, 'l1_coef': 0.0002955144955312484, 'patience': 11, 'batch_size': 32}. Best is trial 16 with value: 0.08064935952425002.\n",
      "[I 2024-06-18 12:06:53,510] Trial 10 finished with value: 0.21175373930197497 and parameters: {'learning_rate': 0.00011940553852741194, 'l1_coef': 0.00021784231627119824, 'patience': 11, 'batch_size': 32}. Best is trial 16 with value: 0.08064935952425002.\n",
      "[I 2024-06-18 12:06:54,309] Trial 13 finished with value: 0.22028614764030166 and parameters: {'learning_rate': 0.00011487024990576484, 'l1_coef': 0.0002451431944669829, 'patience': 11, 'batch_size': 32}. Best is trial 16 with value: 0.08064935952425002.\n",
      "[I 2024-06-18 12:06:57,419] Trial 12 finished with value: 0.22223266936265507 and parameters: {'learning_rate': 0.00010370013457189493, 'l1_coef': 0.00024025335883539017, 'patience': 11, 'batch_size': 32}. Best is trial 16 with value: 0.08064935952425002.\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 7 - Best hyperparameters: {'learning_rate': 0.011097410078928614, 'l1_coef': 1.072551035716764e-05, 'patience': 11, 'batch_size': 32}\n",
      "Chr 7 - Best value: 0.0806\n",
      "Epoch [1/500], Train Loss: 0.4487\n",
      "Epoch [2/500], Train Loss: 0.2323\n",
      "Epoch [3/500], Train Loss: 0.1787\n",
      "Epoch [4/500], Train Loss: 0.1501\n",
      "Epoch [5/500], Train Loss: 0.1340\n",
      "Epoch [6/500], Train Loss: 0.1221\n",
      "Epoch [7/500], Train Loss: 0.1134\n",
      "Epoch [8/500], Train Loss: 0.1065\n",
      "Epoch [9/500], Train Loss: 0.1010\n",
      "Epoch [10/500], Train Loss: 0.0955\n",
      "Epoch [11/500], Train Loss: 0.0927\n",
      "Epoch [12/500], Train Loss: 0.0887\n",
      "Epoch [13/500], Train Loss: 0.0867\n",
      "Epoch [14/500], Train Loss: 0.0832\n",
      "Epoch [15/500], Train Loss: 0.0819\n",
      "Epoch [16/500], Train Loss: 0.0798\n",
      "Epoch [17/500], Train Loss: 0.0784\n",
      "Epoch [18/500], Train Loss: 0.0786\n",
      "Epoch [19/500], Train Loss: 0.0760\n",
      "Epoch [20/500], Train Loss: 0.0756\n",
      "Epoch [21/500], Train Loss: 0.0737\n",
      "Epoch [22/500], Train Loss: 0.0729\n",
      "Epoch [23/500], Train Loss: 0.0728\n",
      "Epoch [24/500], Train Loss: 0.0723\n",
      "Epoch [25/500], Train Loss: 0.0724\n",
      "Epoch [26/500], Train Loss: 0.0713\n",
      "Epoch [27/500], Train Loss: 0.0695\n",
      "Epoch [28/500], Train Loss: 0.0675\n",
      "Epoch [29/500], Train Loss: 0.0683\n",
      "Epoch [30/500], Train Loss: 0.0690\n",
      "Epoch [31/500], Train Loss: 0.0678\n",
      "Epoch [32/500], Train Loss: 0.0685\n",
      "Epoch [33/500], Train Loss: 0.0704\n",
      "Epoch [34/500], Train Loss: 0.0694\n",
      "Epoch [35/500], Train Loss: 0.0593\n",
      "Epoch [36/500], Train Loss: 0.0575\n",
      "Epoch [37/500], Train Loss: 0.0566\n",
      "Epoch [38/500], Train Loss: 0.0561\n",
      "Epoch [39/500], Train Loss: 0.0557\n",
      "Epoch [40/500], Train Loss: 0.0554\n",
      "Epoch [41/500], Train Loss: 0.0551\n",
      "Epoch [42/500], Train Loss: 0.0550\n",
      "Epoch [43/500], Train Loss: 0.0548\n",
      "Epoch [44/500], Train Loss: 0.0547\n",
      "Epoch [45/500], Train Loss: 0.0546\n",
      "Epoch [46/500], Train Loss: 0.0545\n",
      "Epoch [47/500], Train Loss: 0.0544\n",
      "Epoch [48/500], Train Loss: 0.0544\n",
      "Epoch [49/500], Train Loss: 0.0543\n",
      "Epoch [50/500], Train Loss: 0.0543\n",
      "Epoch [51/500], Train Loss: 0.0542\n",
      "Epoch [52/500], Train Loss: 0.0542\n",
      "Epoch [53/500], Train Loss: 0.0540\n",
      "Epoch [54/500], Train Loss: 0.0541\n",
      "Epoch [55/500], Train Loss: 0.0541\n",
      "Epoch [56/500], Train Loss: 0.0540\n",
      "Epoch [57/500], Train Loss: 0.0538\n",
      "Epoch [58/500], Train Loss: 0.0539\n",
      "Epoch [59/500], Train Loss: 0.0538\n",
      "Epoch [60/500], Train Loss: 0.0538\n",
      "Epoch [61/500], Train Loss: 0.0539\n",
      "Epoch [62/500], Train Loss: 0.0537\n",
      "Epoch [63/500], Train Loss: 0.0538\n",
      "Epoch [64/500], Train Loss: 0.0536\n",
      "Epoch [65/500], Train Loss: 0.0537\n",
      "Epoch [66/500], Train Loss: 0.0536\n",
      "Epoch [67/500], Train Loss: 0.0535\n",
      "Epoch [68/500], Train Loss: 0.0536\n",
      "Epoch [69/500], Train Loss: 0.0535\n",
      "Epoch [70/500], Train Loss: 0.0535\n",
      "Epoch [71/500], Train Loss: 0.0535\n",
      "Epoch [72/500], Train Loss: 0.0535\n",
      "Epoch [73/500], Train Loss: 0.0533\n",
      "Epoch [74/500], Train Loss: 0.0534\n",
      "Epoch [75/500], Train Loss: 0.0533\n",
      "Epoch [76/500], Train Loss: 0.0533\n",
      "Epoch [77/500], Train Loss: 0.0532\n",
      "Epoch [78/500], Train Loss: 0.0532\n",
      "Epoch [79/500], Train Loss: 0.0530\n",
      "Epoch [80/500], Train Loss: 0.0531\n",
      "Epoch [81/500], Train Loss: 0.0531\n",
      "Epoch [82/500], Train Loss: 0.0532\n",
      "Epoch [83/500], Train Loss: 0.0530\n",
      "Epoch [84/500], Train Loss: 0.0530\n",
      "Epoch [85/500], Train Loss: 0.0530\n",
      "Epoch [86/500], Train Loss: 0.0531\n",
      "Epoch [87/500], Train Loss: 0.0528\n",
      "Epoch [88/500], Train Loss: 0.0528\n",
      "Epoch [89/500], Train Loss: 0.0527\n",
      "Epoch [90/500], Train Loss: 0.0530\n",
      "Epoch [91/500], Train Loss: 0.0528\n",
      "Epoch [92/500], Train Loss: 0.0526\n",
      "Epoch [93/500], Train Loss: 0.0528\n",
      "Epoch [94/500], Train Loss: 0.0526\n",
      "Epoch [95/500], Train Loss: 0.0526\n",
      "Epoch [96/500], Train Loss: 0.0525\n",
      "Epoch [97/500], Train Loss: 0.0525\n",
      "Epoch [98/500], Train Loss: 0.0526\n",
      "Epoch [99/500], Train Loss: 0.0524\n",
      "Epoch [100/500], Train Loss: 0.0525\n",
      "Epoch [101/500], Train Loss: 0.0525\n",
      "Epoch [102/500], Train Loss: 0.0525\n",
      "Epoch [103/500], Train Loss: 0.0523\n",
      "Epoch [104/500], Train Loss: 0.0523\n",
      "Epoch [105/500], Train Loss: 0.0522\n",
      "Epoch [106/500], Train Loss: 0.0522\n",
      "Epoch [107/500], Train Loss: 0.0522\n",
      "Epoch [108/500], Train Loss: 0.0521\n",
      "Epoch [109/500], Train Loss: 0.0520\n",
      "Epoch [110/500], Train Loss: 0.0520\n",
      "Epoch [111/500], Train Loss: 0.0520\n",
      "Epoch [112/500], Train Loss: 0.0520\n",
      "Epoch [113/500], Train Loss: 0.0519\n",
      "Epoch [114/500], Train Loss: 0.0518\n",
      "Epoch [115/500], Train Loss: 0.0518\n",
      "Epoch [116/500], Train Loss: 0.0519\n",
      "Epoch [117/500], Train Loss: 0.0518\n",
      "Epoch [118/500], Train Loss: 0.0518\n",
      "Epoch [119/500], Train Loss: 0.0518\n",
      "Epoch [120/500], Train Loss: 0.0516\n",
      "Epoch [121/500], Train Loss: 0.0516\n",
      "Epoch [122/500], Train Loss: 0.0516\n",
      "Epoch [123/500], Train Loss: 0.0516\n",
      "Epoch [124/500], Train Loss: 0.0517\n",
      "Epoch [125/500], Train Loss: 0.0515\n",
      "Epoch [126/500], Train Loss: 0.0514\n",
      "Epoch [127/500], Train Loss: 0.0515\n",
      "Epoch [128/500], Train Loss: 0.0516\n",
      "Epoch [129/500], Train Loss: 0.0515\n",
      "Epoch [130/500], Train Loss: 0.0514\n",
      "Epoch [131/500], Train Loss: 0.0514\n",
      "Epoch [132/500], Train Loss: 0.0514\n",
      "Epoch [133/500], Train Loss: 0.0515\n",
      "Epoch [134/500], Train Loss: 0.0513\n",
      "Epoch [135/500], Train Loss: 0.0511\n",
      "Epoch [136/500], Train Loss: 0.0513\n",
      "Epoch [137/500], Train Loss: 0.0512\n",
      "Epoch [138/500], Train Loss: 0.0512\n",
      "Epoch [139/500], Train Loss: 0.0512\n",
      "Epoch [140/500], Train Loss: 0.0513\n",
      "Epoch [141/500], Train Loss: 0.0512\n",
      "Epoch [142/500], Train Loss: 0.0500\n",
      "Epoch [143/500], Train Loss: 0.0497\n",
      "Epoch [144/500], Train Loss: 0.0497\n",
      "Epoch [145/500], Train Loss: 0.0496\n",
      "Epoch [146/500], Train Loss: 0.0496\n",
      "Epoch [147/500], Train Loss: 0.0496\n",
      "Epoch [148/500], Train Loss: 0.0496\n",
      "Epoch [149/500], Train Loss: 0.0496\n",
      "Epoch [150/500], Train Loss: 0.0496\n",
      "Epoch [151/500], Train Loss: 0.0496\n",
      "Epoch [152/500], Train Loss: 0.0496\n",
      "Epoch [153/500], Train Loss: 0.0496\n",
      "Epoch [154/500], Train Loss: 0.0495\n",
      "Epoch [155/500], Train Loss: 0.0496\n",
      "Epoch [156/500], Train Loss: 0.0496\n",
      "Epoch [157/500], Train Loss: 0.0496\n",
      "Epoch [158/500], Train Loss: 0.0496\n",
      "Epoch [159/500], Train Loss: 0.0495\n",
      "Epoch [160/500], Train Loss: 0.0495\n",
      "Epoch [161/500], Train Loss: 0.0494\n",
      "Epoch [162/500], Train Loss: 0.0494\n",
      "Epoch [163/500], Train Loss: 0.0494\n",
      "Epoch [164/500], Train Loss: 0.0494\n",
      "Epoch [165/500], Train Loss: 0.0494\n",
      "Epoch [166/500], Train Loss: 0.0494\n",
      "Epoch [167/500], Train Loss: 0.0494\n",
      "Epoch [168/500], Train Loss: 0.0494\n",
      "Epoch [169/500], Train Loss: 0.0494\n",
      "Epoch [170/500], Train Loss: 0.0494\n",
      "Epoch [171/500], Train Loss: 0.0494\n",
      "Epoch [172/500], Train Loss: 0.0494\n",
      "Epoch [173/500], Train Loss: 0.0494\n",
      "Epoch [174/500], Train Loss: 0.0494\n",
      "Epoch [175/500], Train Loss: 0.0494\n",
      "Epoch [176/500], Train Loss: 0.0493\n",
      "Epoch [177/500], Train Loss: 0.0493\n",
      "Epoch [178/500], Train Loss: 0.0493\n",
      "Epoch [179/500], Train Loss: 0.0494\n",
      "Epoch [180/500], Train Loss: 0.0493\n",
      "Epoch [181/500], Train Loss: 0.0494\n",
      "Epoch [182/500], Train Loss: 0.0494\n",
      "Epoch [183/500], Train Loss: 0.0494\n",
      "Epoch [184/500], Train Loss: 0.0494\n",
      "Epoch [185/500], Train Loss: 0.0494\n",
      "Epoch [186/500], Train Loss: 0.0494\n",
      "Epoch [187/500], Train Loss: 0.0494\n",
      "Early stopping at epoch 187\n",
      "Final model saved at: ../../../Data/model_results/logistic_regression/models/chr7/final_model_chr7.pth\n",
      "Individual R^2 scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr7/individual_r2_scores_chr7.csv\n",
      "Individual IQS scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr7/individual_iqs_scores_chr7.csv\n",
      "Individual AUC ROC curves saved in: ../../../Data/model_results/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../../Data/model_results/logistic_regression/csv_files/performance_metrics.csv\n",
      "Total SNPs:  1598\n",
      "PRS313 SNPs:  42\n",
      "Total SNPs used for Training:  1556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-06-18 12:08:50,044] Trial 16 finished with value: 0.16171413702624185 and parameters: {'learning_rate': 0.010216466895955227, 'l1_coef': 5.7799715158518986e-05, 'patience': 20, 'batch_size': 64}. Best is trial 16 with value: 0.16171413702624185.\n",
      "[I 2024-06-18 12:08:52,418] Trial 10 finished with value: 0.16542784060750687 and parameters: {'learning_rate': 0.008003044946376763, 'l1_coef': 6.33547015495335e-05, 'patience': 20, 'batch_size': 64}. Best is trial 16 with value: 0.16171413702624185.\n",
      "[I 2024-06-18 12:08:59,011] Trial 18 finished with value: 0.16395897269248966 and parameters: {'learning_rate': 0.009838449467549658, 'l1_coef': 5.964117848342693e-05, 'patience': 19, 'batch_size': 64}. Best is trial 16 with value: 0.16171413702624185.\n",
      "[I 2024-06-18 12:09:00,421] Trial 14 finished with value: 0.17000248900481635 and parameters: {'learning_rate': 0.00841112224204875, 'l1_coef': 7.04439209409591e-05, 'patience': 19, 'batch_size': 64}. Best is trial 16 with value: 0.16171413702624185.\n",
      "[I 2024-06-18 12:09:50,861] Trial 17 finished with value: 0.16129488689558844 and parameters: {'learning_rate': 0.007119303290977644, 'l1_coef': 5.5087327131197635e-05, 'patience': 19, 'batch_size': 64}. Best is trial 17 with value: 0.16129488689558844.\n",
      "[I 2024-06-18 12:09:54,460] Trial 13 finished with value: 0.15182113349437715 and parameters: {'learning_rate': 0.006998305486984654, 'l1_coef': 5.051659353995837e-05, 'patience': 19, 'batch_size': 64}. Best is trial 13 with value: 0.15182113349437715.\n",
      "[I 2024-06-18 12:09:59,392] Trial 15 finished with value: 0.17143815713269372 and parameters: {'learning_rate': 0.011378612779584582, 'l1_coef': 6.461465029162277e-05, 'patience': 19, 'batch_size': 64}. Best is trial 13 with value: 0.15182113349437715.\n",
      "[I 2024-06-18 12:09:59,453] Trial 12 finished with value: 0.16584112218448094 and parameters: {'learning_rate': 0.009375588384717, 'l1_coef': 6.110577316996832e-05, 'patience': 20, 'batch_size': 64}. Best is trial 13 with value: 0.15182113349437715.\n",
      "[I 2024-06-18 12:10:00,101] Trial 11 finished with value: 0.16462042672293525 and parameters: {'learning_rate': 0.006992218798125946, 'l1_coef': 6.218590168347955e-05, 'patience': 19, 'batch_size': 64}. Best is trial 13 with value: 0.15182113349437715.\n",
      "[I 2024-06-18 12:10:00,134] Trial 19 finished with value: 0.1578686969620841 and parameters: {'learning_rate': 0.008532850413877005, 'l1_coef': 5.808367601379737e-05, 'patience': 19, 'batch_size': 64}. Best is trial 13 with value: 0.15182113349437715.\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 8 - Best hyperparameters: {'learning_rate': 0.006998305486984654, 'l1_coef': 5.051659353995837e-05, 'patience': 19, 'batch_size': 64}\n",
      "Chr 8 - Best value: 0.1518\n",
      "Epoch [1/500], Train Loss: 0.5835\n",
      "Epoch [2/500], Train Loss: 0.4033\n",
      "Epoch [3/500], Train Loss: 0.3390\n",
      "Epoch [4/500], Train Loss: 0.3086\n",
      "Epoch [5/500], Train Loss: 0.2844\n",
      "Epoch [6/500], Train Loss: 0.2668\n",
      "Epoch [7/500], Train Loss: 0.2511\n",
      "Epoch [8/500], Train Loss: 0.2409\n",
      "Epoch [9/500], Train Loss: 0.2335\n",
      "Epoch [10/500], Train Loss: 0.2244\n",
      "Epoch [11/500], Train Loss: 0.2180\n",
      "Epoch [12/500], Train Loss: 0.2130\n",
      "Epoch [13/500], Train Loss: 0.2098\n",
      "Epoch [14/500], Train Loss: 0.2044\n",
      "Epoch [15/500], Train Loss: 0.1984\n",
      "Epoch [16/500], Train Loss: 0.1956\n",
      "Epoch [17/500], Train Loss: 0.1930\n",
      "Epoch [18/500], Train Loss: 0.1903\n",
      "Epoch [19/500], Train Loss: 0.1894\n",
      "Epoch [20/500], Train Loss: 0.1855\n",
      "Epoch [21/500], Train Loss: 0.1870\n",
      "Epoch [22/500], Train Loss: 0.1847\n",
      "Epoch [23/500], Train Loss: 0.1833\n",
      "Epoch [24/500], Train Loss: 0.1776\n",
      "Epoch [25/500], Train Loss: 0.1776\n",
      "Epoch [26/500], Train Loss: 0.1753\n",
      "Epoch [27/500], Train Loss: 0.1756\n",
      "Epoch [28/500], Train Loss: 0.1728\n",
      "Epoch [29/500], Train Loss: 0.1752\n",
      "Epoch [30/500], Train Loss: 0.1731\n",
      "Epoch [31/500], Train Loss: 0.1706\n",
      "Epoch [32/500], Train Loss: 0.1704\n",
      "Epoch [33/500], Train Loss: 0.1701\n",
      "Epoch [34/500], Train Loss: 0.1691\n",
      "Epoch [35/500], Train Loss: 0.1689\n",
      "Epoch [36/500], Train Loss: 0.1682\n",
      "Epoch [37/500], Train Loss: 0.1672\n",
      "Epoch [38/500], Train Loss: 0.1671\n",
      "Epoch [39/500], Train Loss: 0.1663\n",
      "Epoch [40/500], Train Loss: 0.1648\n",
      "Epoch [41/500], Train Loss: 0.1639\n",
      "Epoch [42/500], Train Loss: 0.1656\n",
      "Epoch [43/500], Train Loss: 0.1653\n",
      "Epoch [44/500], Train Loss: 0.1642\n",
      "Epoch [45/500], Train Loss: 0.1636\n",
      "Epoch [46/500], Train Loss: 0.1619\n",
      "Epoch [47/500], Train Loss: 0.1615\n",
      "Epoch [48/500], Train Loss: 0.1623\n",
      "Epoch [49/500], Train Loss: 0.1611\n",
      "Epoch [50/500], Train Loss: 0.1623\n",
      "Epoch [51/500], Train Loss: 0.1622\n",
      "Epoch [52/500], Train Loss: 0.1600\n",
      "Epoch [53/500], Train Loss: 0.1613\n",
      "Epoch [54/500], Train Loss: 0.1596\n",
      "Epoch [55/500], Train Loss: 0.1621\n",
      "Epoch [56/500], Train Loss: 0.1607\n",
      "Epoch [57/500], Train Loss: 0.1587\n",
      "Epoch [58/500], Train Loss: 0.1603\n",
      "Epoch [59/500], Train Loss: 0.1604\n",
      "Epoch [60/500], Train Loss: 0.1596\n",
      "Epoch [61/500], Train Loss: 0.1587\n",
      "Epoch [62/500], Train Loss: 0.1603\n",
      "Epoch [63/500], Train Loss: 0.1611\n",
      "Epoch [64/500], Train Loss: 0.1485\n",
      "Epoch [65/500], Train Loss: 0.1424\n",
      "Epoch [66/500], Train Loss: 0.1413\n",
      "Epoch [67/500], Train Loss: 0.1407\n",
      "Epoch [68/500], Train Loss: 0.1406\n",
      "Epoch [69/500], Train Loss: 0.1402\n",
      "Epoch [70/500], Train Loss: 0.1404\n",
      "Epoch [71/500], Train Loss: 0.1399\n",
      "Epoch [72/500], Train Loss: 0.1400\n",
      "Epoch [73/500], Train Loss: 0.1401\n",
      "Epoch [74/500], Train Loss: 0.1399\n",
      "Epoch [75/500], Train Loss: 0.1397\n",
      "Epoch [76/500], Train Loss: 0.1402\n",
      "Epoch [77/500], Train Loss: 0.1401\n",
      "Epoch [78/500], Train Loss: 0.1399\n",
      "Epoch [79/500], Train Loss: 0.1396\n",
      "Epoch [80/500], Train Loss: 0.1397\n",
      "Epoch [81/500], Train Loss: 0.1393\n",
      "Epoch [82/500], Train Loss: 0.1398\n",
      "Epoch [83/500], Train Loss: 0.1401\n",
      "Epoch [84/500], Train Loss: 0.1393\n",
      "Epoch [85/500], Train Loss: 0.1393\n",
      "Epoch [86/500], Train Loss: 0.1399\n",
      "Epoch [87/500], Train Loss: 0.1400\n",
      "Epoch [88/500], Train Loss: 0.1383\n",
      "Epoch [89/500], Train Loss: 0.1381\n",
      "Epoch [90/500], Train Loss: 0.1378\n",
      "Epoch [91/500], Train Loss: 0.1377\n",
      "Epoch [92/500], Train Loss: 0.1375\n",
      "Epoch [93/500], Train Loss: 0.1374\n",
      "Epoch [94/500], Train Loss: 0.1376\n",
      "Epoch [95/500], Train Loss: 0.1378\n",
      "Epoch [96/500], Train Loss: 0.1376\n",
      "Epoch [97/500], Train Loss: 0.1375\n",
      "Epoch [98/500], Train Loss: 0.1374\n",
      "Epoch [99/500], Train Loss: 0.1370\n",
      "Epoch [100/500], Train Loss: 0.1373\n",
      "Epoch [101/500], Train Loss: 0.1374\n",
      "Epoch [102/500], Train Loss: 0.1374\n",
      "Epoch [103/500], Train Loss: 0.1372\n",
      "Epoch [104/500], Train Loss: 0.1370\n",
      "Epoch [105/500], Train Loss: 0.1372\n",
      "Epoch [106/500], Train Loss: 0.1371\n",
      "Epoch [107/500], Train Loss: 0.1376\n",
      "Epoch [108/500], Train Loss: 0.1373\n",
      "Epoch [109/500], Train Loss: 0.1371\n",
      "Epoch [110/500], Train Loss: 0.1372\n",
      "Epoch [111/500], Train Loss: 0.1372\n",
      "Epoch [112/500], Train Loss: 0.1372\n",
      "Epoch [113/500], Train Loss: 0.1366\n",
      "Epoch [114/500], Train Loss: 0.1374\n",
      "Epoch [115/500], Train Loss: 0.1372\n",
      "Epoch [116/500], Train Loss: 0.1370\n",
      "Epoch [117/500], Train Loss: 0.1369\n",
      "Epoch [118/500], Train Loss: 0.1373\n",
      "Epoch [119/500], Train Loss: 0.1372\n",
      "Epoch [120/500], Train Loss: 0.1371\n",
      "Epoch [121/500], Train Loss: 0.1375\n",
      "Epoch [122/500], Train Loss: 0.1371\n",
      "Epoch [123/500], Train Loss: 0.1373\n",
      "Epoch [124/500], Train Loss: 0.1372\n",
      "Epoch [125/500], Train Loss: 0.1369\n",
      "Epoch [126/500], Train Loss: 0.1369\n",
      "Epoch [127/500], Train Loss: 0.1373\n",
      "Epoch [128/500], Train Loss: 0.1369\n",
      "Epoch [129/500], Train Loss: 0.1373\n",
      "Epoch [130/500], Train Loss: 0.1372\n",
      "Epoch [131/500], Train Loss: 0.1370\n",
      "Epoch [132/500], Train Loss: 0.1372\n",
      "Early stopping at epoch 132\n",
      "Final model saved at: ../../../Data/model_results/logistic_regression/models/chr8/final_model_chr8.pth\n",
      "Individual R^2 scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr8/individual_r2_scores_chr8.csv\n",
      "Individual IQS scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr8/individual_iqs_scores_chr8.csv\n",
      "Individual AUC ROC curves saved in: ../../../Data/model_results/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../../Data/model_results/logistic_regression/csv_files/performance_metrics.csv\n",
      "Total SNPs:  1306\n",
      "PRS313 SNPs:  28\n",
      "Total SNPs used for Training:  1278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-06-18 12:10:32,272] Trial 8 finished with value: 11.523130798339844 and parameters: {'learning_rate': 0.09926867945539508, 'l1_coef': 8.780327247739423e-05, 'patience': 19, 'batch_size': 256}. Best is trial 8 with value: 11.523130798339844.\n",
      "[I 2024-06-18 12:10:39,547] Trial 7 finished with value: 0.1619011029601097 and parameters: {'learning_rate': 0.0024898795229624563, 'l1_coef': 1.35876606392741e-05, 'patience': 14, 'batch_size': 256}. Best is trial 7 with value: 0.1619011029601097.\n",
      "[I 2024-06-18 12:11:07,244] Trial 9 finished with value: 0.5189666211605072 and parameters: {'learning_rate': 0.010492904081249617, 'l1_coef': 0.0634166459985364, 'patience': 16, 'batch_size': 128}. Best is trial 7 with value: 0.1619011029601097.\n",
      "[I 2024-06-18 12:11:07,667] Trial 4 finished with value: 0.5186945080757142 and parameters: {'learning_rate': 0.03503731117279075, 'l1_coef': 0.014374944067070083, 'patience': 13, 'batch_size': 64}. Best is trial 7 with value: 0.1619011029601097.\n",
      "[I 2024-06-18 12:11:09,080] Trial 2 finished with value: 0.1562183365225792 and parameters: {'learning_rate': 0.0008037288463262255, 'l1_coef': 1.765204204350156e-05, 'patience': 10, 'batch_size': 128}. Best is trial 2 with value: 0.1562183365225792.\n",
      "[I 2024-06-18 12:11:10,388] Trial 0 finished with value: 0.507239219546318 and parameters: {'learning_rate': 0.0024132971368764876, 'l1_coef': 0.00412512978352297, 'patience': 19, 'batch_size': 128}. Best is trial 2 with value: 0.1562183365225792.\n",
      "[I 2024-06-18 12:11:28,227] Trial 1 finished with value: 0.11596847646511517 and parameters: {'learning_rate': 0.0070302885401241945, 'l1_coef': 1.9646986854779946e-05, 'patience': 12, 'batch_size': 32}. Best is trial 1 with value: 0.11596847646511517.\n",
      "[I 2024-06-18 12:11:36,817] Trial 5 finished with value: 0.5159644663333893 and parameters: {'learning_rate': 0.033265417971307616, 'l1_coef': 0.005607290068470828, 'patience': 19, 'batch_size': 32}. Best is trial 1 with value: 0.11596847646511517.\n",
      "[I 2024-06-18 12:11:38,674] Trial 6 finished with value: 0.10667803768928234 and parameters: {'learning_rate': 0.005943003216514158, 'l1_coef': 1.4184324432812446e-05, 'patience': 13, 'batch_size': 32}. Best is trial 6 with value: 0.10667803768928234.\n",
      "[I 2024-06-18 12:11:42,929] Trial 3 finished with value: 0.5190516744341169 and parameters: {'learning_rate': 0.0001790224650518822, 'l1_coef': 0.005650248897420378, 'patience': 20, 'batch_size': 64}. Best is trial 6 with value: 0.10667803768928234.\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 9 - Best hyperparameters: {'learning_rate': 0.005943003216514158, 'l1_coef': 1.4184324432812446e-05, 'patience': 13, 'batch_size': 32}\n",
      "Chr 9 - Best value: 0.1067\n",
      "Epoch [1/500], Train Loss: 0.4544\n",
      "Epoch [2/500], Train Loss: 0.2970\n",
      "Epoch [3/500], Train Loss: 0.2429\n",
      "Epoch [4/500], Train Loss: 0.2127\n",
      "Epoch [5/500], Train Loss: 0.1920\n",
      "Epoch [6/500], Train Loss: 0.1766\n",
      "Epoch [7/500], Train Loss: 0.1651\n",
      "Epoch [8/500], Train Loss: 0.1549\n",
      "Epoch [9/500], Train Loss: 0.1471\n",
      "Epoch [10/500], Train Loss: 0.1415\n",
      "Epoch [11/500], Train Loss: 0.1358\n",
      "Epoch [12/500], Train Loss: 0.1315\n",
      "Epoch [13/500], Train Loss: 0.1276\n",
      "Epoch [14/500], Train Loss: 0.1237\n",
      "Epoch [15/500], Train Loss: 0.1209\n",
      "Epoch [16/500], Train Loss: 0.1184\n",
      "Epoch [17/500], Train Loss: 0.1157\n",
      "Epoch [18/500], Train Loss: 0.1140\n",
      "Epoch [19/500], Train Loss: 0.1115\n",
      "Epoch [20/500], Train Loss: 0.1094\n",
      "Epoch [21/500], Train Loss: 0.1070\n",
      "Epoch [22/500], Train Loss: 0.1058\n",
      "Epoch [23/500], Train Loss: 0.1059\n",
      "Epoch [24/500], Train Loss: 0.1039\n",
      "Epoch [25/500], Train Loss: 0.1024\n",
      "Epoch [26/500], Train Loss: 0.1020\n",
      "Epoch [27/500], Train Loss: 0.1002\n",
      "Epoch [28/500], Train Loss: 0.0990\n",
      "Epoch [29/500], Train Loss: 0.1000\n",
      "Epoch [30/500], Train Loss: 0.0970\n",
      "Epoch [31/500], Train Loss: 0.0968\n",
      "Epoch [32/500], Train Loss: 0.0957\n",
      "Epoch [33/500], Train Loss: 0.0957\n",
      "Epoch [34/500], Train Loss: 0.0937\n",
      "Epoch [35/500], Train Loss: 0.0945\n",
      "Epoch [36/500], Train Loss: 0.0928\n",
      "Epoch [37/500], Train Loss: 0.0921\n",
      "Epoch [38/500], Train Loss: 0.0922\n",
      "Epoch [39/500], Train Loss: 0.0912\n",
      "Epoch [40/500], Train Loss: 0.0907\n",
      "Epoch [41/500], Train Loss: 0.0899\n",
      "Epoch [42/500], Train Loss: 0.0900\n",
      "Epoch [43/500], Train Loss: 0.0892\n",
      "Epoch [44/500], Train Loss: 0.0876\n",
      "Epoch [45/500], Train Loss: 0.0886\n",
      "Epoch [46/500], Train Loss: 0.0882\n",
      "Epoch [47/500], Train Loss: 0.0882\n",
      "Epoch [48/500], Train Loss: 0.0873\n",
      "Epoch [49/500], Train Loss: 0.0874\n",
      "Epoch [50/500], Train Loss: 0.0865\n",
      "Epoch [51/500], Train Loss: 0.0865\n",
      "Epoch [52/500], Train Loss: 0.0871\n",
      "Epoch [53/500], Train Loss: 0.0865\n",
      "Epoch [54/500], Train Loss: 0.0864\n",
      "Epoch [55/500], Train Loss: 0.0867\n",
      "Epoch [56/500], Train Loss: 0.0858\n",
      "Epoch [57/500], Train Loss: 0.0857\n",
      "Epoch [58/500], Train Loss: 0.0862\n",
      "Epoch [59/500], Train Loss: 0.0848\n",
      "Epoch [60/500], Train Loss: 0.0848\n",
      "Epoch [61/500], Train Loss: 0.0854\n",
      "Epoch [62/500], Train Loss: 0.0846\n",
      "Epoch [63/500], Train Loss: 0.0845\n",
      "Epoch [64/500], Train Loss: 0.0836\n",
      "Epoch [65/500], Train Loss: 0.0847\n",
      "Epoch [66/500], Train Loss: 0.0842\n",
      "Epoch [67/500], Train Loss: 0.0852\n",
      "Epoch [68/500], Train Loss: 0.0837\n",
      "Epoch [69/500], Train Loss: 0.0841\n",
      "Epoch [70/500], Train Loss: 0.0846\n",
      "Epoch [71/500], Train Loss: 0.0755\n",
      "Epoch [72/500], Train Loss: 0.0740\n",
      "Epoch [73/500], Train Loss: 0.0736\n",
      "Epoch [74/500], Train Loss: 0.0733\n",
      "Epoch [75/500], Train Loss: 0.0730\n",
      "Epoch [76/500], Train Loss: 0.0729\n",
      "Epoch [77/500], Train Loss: 0.0728\n",
      "Epoch [78/500], Train Loss: 0.0727\n",
      "Epoch [79/500], Train Loss: 0.0726\n",
      "Epoch [80/500], Train Loss: 0.0725\n",
      "Epoch [81/500], Train Loss: 0.0725\n",
      "Epoch [82/500], Train Loss: 0.0724\n",
      "Epoch [83/500], Train Loss: 0.0724\n",
      "Epoch [84/500], Train Loss: 0.0724\n",
      "Epoch [85/500], Train Loss: 0.0724\n",
      "Epoch [86/500], Train Loss: 0.0724\n",
      "Epoch [87/500], Train Loss: 0.0723\n",
      "Epoch [88/500], Train Loss: 0.0723\n",
      "Epoch [89/500], Train Loss: 0.0723\n",
      "Epoch [90/500], Train Loss: 0.0723\n",
      "Epoch [91/500], Train Loss: 0.0723\n",
      "Epoch [92/500], Train Loss: 0.0722\n",
      "Epoch [93/500], Train Loss: 0.0722\n",
      "Epoch [94/500], Train Loss: 0.0723\n",
      "Epoch [95/500], Train Loss: 0.0722\n",
      "Epoch [96/500], Train Loss: 0.0722\n",
      "Epoch [97/500], Train Loss: 0.0721\n",
      "Epoch [98/500], Train Loss: 0.0721\n",
      "Epoch [99/500], Train Loss: 0.0721\n",
      "Epoch [100/500], Train Loss: 0.0722\n",
      "Epoch [101/500], Train Loss: 0.0721\n",
      "Epoch [102/500], Train Loss: 0.0722\n",
      "Epoch [103/500], Train Loss: 0.0720\n",
      "Epoch [104/500], Train Loss: 0.0720\n",
      "Epoch [105/500], Train Loss: 0.0720\n",
      "Epoch [106/500], Train Loss: 0.0721\n",
      "Epoch [107/500], Train Loss: 0.0720\n",
      "Epoch [108/500], Train Loss: 0.0720\n",
      "Epoch [109/500], Train Loss: 0.0719\n",
      "Epoch [110/500], Train Loss: 0.0720\n",
      "Epoch [111/500], Train Loss: 0.0720\n",
      "Epoch [112/500], Train Loss: 0.0719\n",
      "Epoch [113/500], Train Loss: 0.0720\n",
      "Epoch [114/500], Train Loss: 0.0719\n",
      "Epoch [115/500], Train Loss: 0.0718\n",
      "Epoch [116/500], Train Loss: 0.0718\n",
      "Epoch [117/500], Train Loss: 0.0719\n",
      "Epoch [118/500], Train Loss: 0.0718\n",
      "Epoch [119/500], Train Loss: 0.0719\n",
      "Epoch [120/500], Train Loss: 0.0718\n",
      "Epoch [121/500], Train Loss: 0.0718\n",
      "Epoch [122/500], Train Loss: 0.0719\n",
      "Epoch [123/500], Train Loss: 0.0717\n",
      "Epoch [124/500], Train Loss: 0.0717\n",
      "Epoch [125/500], Train Loss: 0.0717\n",
      "Epoch [126/500], Train Loss: 0.0717\n",
      "Epoch [127/500], Train Loss: 0.0716\n",
      "Epoch [128/500], Train Loss: 0.0717\n",
      "Epoch [129/500], Train Loss: 0.0718\n",
      "Epoch [130/500], Train Loss: 0.0716\n",
      "Epoch [131/500], Train Loss: 0.0718\n",
      "Epoch [132/500], Train Loss: 0.0716\n",
      "Epoch [133/500], Train Loss: 0.0714\n",
      "Epoch [134/500], Train Loss: 0.0716\n",
      "Epoch [135/500], Train Loss: 0.0716\n",
      "Epoch [136/500], Train Loss: 0.0715\n",
      "Epoch [137/500], Train Loss: 0.0715\n",
      "Epoch [138/500], Train Loss: 0.0715\n",
      "Epoch [139/500], Train Loss: 0.0714\n",
      "Epoch [140/500], Train Loss: 0.0715\n",
      "Epoch [141/500], Train Loss: 0.0715\n",
      "Epoch [142/500], Train Loss: 0.0714\n",
      "Epoch [143/500], Train Loss: 0.0715\n",
      "Epoch [144/500], Train Loss: 0.0715\n",
      "Epoch [145/500], Train Loss: 0.0715\n",
      "Epoch [146/500], Train Loss: 0.0704\n",
      "Epoch [147/500], Train Loss: 0.0702\n",
      "Epoch [148/500], Train Loss: 0.0702\n",
      "Epoch [149/500], Train Loss: 0.0702\n",
      "Epoch [150/500], Train Loss: 0.0702\n",
      "Epoch [151/500], Train Loss: 0.0702\n",
      "Epoch [152/500], Train Loss: 0.0702\n",
      "Epoch [153/500], Train Loss: 0.0702\n",
      "Epoch [154/500], Train Loss: 0.0702\n",
      "Epoch [155/500], Train Loss: 0.0702\n",
      "Epoch [156/500], Train Loss: 0.0701\n",
      "Epoch [157/500], Train Loss: 0.0702\n",
      "Epoch [158/500], Train Loss: 0.0702\n",
      "Epoch [159/500], Train Loss: 0.0701\n",
      "Epoch [160/500], Train Loss: 0.0702\n",
      "Epoch [161/500], Train Loss: 0.0701\n",
      "Epoch [162/500], Train Loss: 0.0701\n",
      "Epoch [163/500], Train Loss: 0.0701\n",
      "Epoch [164/500], Train Loss: 0.0701\n",
      "Epoch [165/500], Train Loss: 0.0701\n",
      "Epoch [166/500], Train Loss: 0.0701\n",
      "Epoch [167/500], Train Loss: 0.0702\n",
      "Epoch [168/500], Train Loss: 0.0701\n",
      "Epoch [169/500], Train Loss: 0.0701\n",
      "Epoch [170/500], Train Loss: 0.0701\n",
      "Epoch [171/500], Train Loss: 0.0700\n",
      "Epoch [172/500], Train Loss: 0.0700\n",
      "Epoch [173/500], Train Loss: 0.0700\n",
      "Epoch [174/500], Train Loss: 0.0700\n",
      "Epoch [175/500], Train Loss: 0.0700\n",
      "Epoch [176/500], Train Loss: 0.0700\n",
      "Epoch [177/500], Train Loss: 0.0700\n",
      "Epoch [178/500], Train Loss: 0.0700\n",
      "Epoch [179/500], Train Loss: 0.0700\n",
      "Epoch [180/500], Train Loss: 0.0700\n",
      "Epoch [181/500], Train Loss: 0.0700\n",
      "Epoch [182/500], Train Loss: 0.0700\n",
      "Epoch [183/500], Train Loss: 0.0700\n",
      "Epoch [184/500], Train Loss: 0.0700\n",
      "Epoch [185/500], Train Loss: 0.0700\n",
      "Epoch [186/500], Train Loss: 0.0700\n",
      "Epoch [187/500], Train Loss: 0.0700\n",
      "Epoch [188/500], Train Loss: 0.0700\n",
      "Epoch [189/500], Train Loss: 0.0700\n",
      "Epoch [190/500], Train Loss: 0.0700\n",
      "Epoch [191/500], Train Loss: 0.0700\n",
      "Epoch [192/500], Train Loss: 0.0700\n",
      "Epoch [193/500], Train Loss: 0.0700\n",
      "Epoch [194/500], Train Loss: 0.0700\n",
      "Epoch [195/500], Train Loss: 0.0700\n",
      "Epoch [196/500], Train Loss: 0.0700\n",
      "Epoch [197/500], Train Loss: 0.0700\n",
      "Epoch [198/500], Train Loss: 0.0700\n",
      "Epoch [199/500], Train Loss: 0.0700\n",
      "Epoch [200/500], Train Loss: 0.0699\n",
      "Epoch [201/500], Train Loss: 0.0700\n",
      "Epoch [202/500], Train Loss: 0.0700\n",
      "Epoch [203/500], Train Loss: 0.0700\n",
      "Epoch [204/500], Train Loss: 0.0700\n",
      "Epoch [205/500], Train Loss: 0.0700\n",
      "Epoch [206/500], Train Loss: 0.0700\n",
      "Epoch [207/500], Train Loss: 0.0699\n",
      "Epoch [208/500], Train Loss: 0.0700\n",
      "Epoch [209/500], Train Loss: 0.0700\n",
      "Epoch [210/500], Train Loss: 0.0700\n",
      "Epoch [211/500], Train Loss: 0.0700\n",
      "Epoch [212/500], Train Loss: 0.0700\n",
      "Epoch [213/500], Train Loss: 0.0700\n",
      "Epoch [214/500], Train Loss: 0.0700\n",
      "Epoch [215/500], Train Loss: 0.0700\n",
      "Epoch [216/500], Train Loss: 0.0700\n",
      "Epoch [217/500], Train Loss: 0.0700\n",
      "Epoch [218/500], Train Loss: 0.0700\n",
      "Epoch [219/500], Train Loss: 0.0700\n",
      "Epoch [220/500], Train Loss: 0.0700\n",
      "Early stopping at epoch 220\n",
      "Final model saved at: ../../../Data/model_results/logistic_regression/models/chr9/final_model_chr9.pth\n",
      "Individual R^2 scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr9/individual_r2_scores_chr9.csv\n",
      "Individual IQS scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr9/individual_iqs_scores_chr9.csv\n",
      "Individual AUC ROC curves saved in: ../../../Data/model_results/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../../Data/model_results/logistic_regression/csv_files/performance_metrics.csv\n",
      "Total SNPs:  1884\n",
      "PRS313 SNPs:  36\n",
      "Total SNPs used for Training:  1848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-18 12:11:56,218] A new study created in RDB with name: chr10_study\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-06-18 12:12:23,249] Trial 3 finished with value: 0.8315802752971649 and parameters: {'learning_rate': 0.04316488646162698, 'l1_coef': 0.004020571974426502, 'patience': 6, 'batch_size': 256}. Best is trial 3 with value: 0.8315802752971649.\n",
      "[I 2024-06-18 12:12:29,671] Trial 4 finished with value: 1.3295094311237334 and parameters: {'learning_rate': 0.02660875630854928, 'l1_coef': 0.0170194537146354, 'patience': 6, 'batch_size': 256}. Best is trial 3 with value: 0.8315802752971649.\n",
      "[I 2024-06-18 12:13:03,086] Trial 0 finished with value: 0.28118076324462893 and parameters: {'learning_rate': 0.055332888810095765, 'l1_coef': 0.0004476137750530598, 'patience': 13, 'batch_size': 128}. Best is trial 0 with value: 0.28118076324462893.\n",
      "[I 2024-06-18 12:13:33,451] Trial 6 finished with value: 0.5402462762135726 and parameters: {'learning_rate': 0.004251237361785462, 'l1_coef': 0.02924264918127103, 'patience': 11, 'batch_size': 32}. Best is trial 0 with value: 0.28118076324462893.\n",
      "[I 2024-06-18 12:13:35,177] Trial 8 finished with value: 0.5009448119572231 and parameters: {'learning_rate': 0.0009198908865780434, 'l1_coef': 0.003192088288039803, 'patience': 19, 'batch_size': 64}. Best is trial 0 with value: 0.28118076324462893.\n",
      "[I 2024-06-18 12:13:35,778] Trial 2 finished with value: 0.3166774332523346 and parameters: {'learning_rate': 0.0002728708328826959, 'l1_coef': 0.00045521684460865205, 'patience': 13, 'batch_size': 128}. Best is trial 0 with value: 0.28118076324462893.\n",
      "[I 2024-06-18 12:13:39,723] Trial 7 finished with value: 0.5407540935736436 and parameters: {'learning_rate': 0.003151781683938601, 'l1_coef': 0.0077800397543389445, 'patience': 17, 'batch_size': 32}. Best is trial 0 with value: 0.28118076324462893.\n",
      "[I 2024-06-18 12:13:46,943] Trial 1 finished with value: 0.5401692431706648 and parameters: {'learning_rate': 0.08568541083617641, 'l1_coef': 0.014079789212576185, 'patience': 17, 'batch_size': 32}. Best is trial 0 with value: 0.28118076324462893.\n",
      "[I 2024-06-18 12:13:56,945] Trial 9 finished with value: 0.13339914003243813 and parameters: {'learning_rate': 0.0024068053397211277, 'l1_coef': 3.542020357212867e-05, 'patience': 17, 'batch_size': 32}. Best is trial 9 with value: 0.13339914003243813.\n",
      "[I 2024-06-18 12:13:57,872] Trial 5 finished with value: 1.8953813552856444 and parameters: {'learning_rate': 0.07919606825438791, 'l1_coef': 4.4915546761742306e-05, 'patience': 17, 'batch_size': 64}. Best is trial 9 with value: 0.13339914003243813.\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 10 - Best hyperparameters: {'learning_rate': 0.0024068053397211277, 'l1_coef': 3.542020357212867e-05, 'patience': 17, 'batch_size': 32}\n",
      "Chr 10 - Best value: 0.1334\n",
      "Epoch [1/500], Train Loss: 0.4991\n",
      "Epoch [2/500], Train Loss: 0.3793\n",
      "Epoch [3/500], Train Loss: 0.3239\n",
      "Epoch [4/500], Train Loss: 0.2909\n",
      "Epoch [5/500], Train Loss: 0.2663\n",
      "Epoch [6/500], Train Loss: 0.2489\n",
      "Epoch [7/500], Train Loss: 0.2345\n",
      "Epoch [8/500], Train Loss: 0.2238\n",
      "Epoch [9/500], Train Loss: 0.2140\n",
      "Epoch [10/500], Train Loss: 0.2066\n",
      "Epoch [11/500], Train Loss: 0.1995\n",
      "Epoch [12/500], Train Loss: 0.1934\n",
      "Epoch [13/500], Train Loss: 0.1893\n",
      "Epoch [14/500], Train Loss: 0.1844\n",
      "Epoch [15/500], Train Loss: 0.1800\n",
      "Epoch [16/500], Train Loss: 0.1763\n",
      "Epoch [17/500], Train Loss: 0.1733\n",
      "Epoch [18/500], Train Loss: 0.1701\n",
      "Epoch [19/500], Train Loss: 0.1681\n",
      "Epoch [20/500], Train Loss: 0.1650\n",
      "Epoch [21/500], Train Loss: 0.1636\n",
      "Epoch [22/500], Train Loss: 0.1615\n",
      "Epoch [23/500], Train Loss: 0.1595\n",
      "Epoch [24/500], Train Loss: 0.1575\n",
      "Epoch [25/500], Train Loss: 0.1565\n",
      "Epoch [26/500], Train Loss: 0.1541\n",
      "Epoch [27/500], Train Loss: 0.1536\n",
      "Epoch [28/500], Train Loss: 0.1513\n",
      "Epoch [29/500], Train Loss: 0.1500\n",
      "Epoch [30/500], Train Loss: 0.1489\n",
      "Epoch [31/500], Train Loss: 0.1488\n",
      "Epoch [32/500], Train Loss: 0.1476\n",
      "Epoch [33/500], Train Loss: 0.1454\n",
      "Epoch [34/500], Train Loss: 0.1449\n",
      "Epoch [35/500], Train Loss: 0.1437\n",
      "Epoch [36/500], Train Loss: 0.1431\n",
      "Epoch [37/500], Train Loss: 0.1428\n",
      "Epoch [38/500], Train Loss: 0.1415\n",
      "Epoch [39/500], Train Loss: 0.1413\n",
      "Epoch [40/500], Train Loss: 0.1405\n",
      "Epoch [41/500], Train Loss: 0.1404\n",
      "Epoch [42/500], Train Loss: 0.1392\n",
      "Epoch [43/500], Train Loss: 0.1386\n",
      "Epoch [44/500], Train Loss: 0.1383\n",
      "Epoch [45/500], Train Loss: 0.1376\n",
      "Epoch [46/500], Train Loss: 0.1371\n",
      "Epoch [47/500], Train Loss: 0.1364\n",
      "Epoch [48/500], Train Loss: 0.1367\n",
      "Epoch [49/500], Train Loss: 0.1359\n",
      "Epoch [50/500], Train Loss: 0.1352\n",
      "Epoch [51/500], Train Loss: 0.1350\n",
      "Epoch [52/500], Train Loss: 0.1349\n",
      "Epoch [53/500], Train Loss: 0.1340\n",
      "Epoch [54/500], Train Loss: 0.1342\n",
      "Epoch [55/500], Train Loss: 0.1333\n",
      "Epoch [56/500], Train Loss: 0.1338\n",
      "Epoch [57/500], Train Loss: 0.1340\n",
      "Epoch [58/500], Train Loss: 0.1333\n",
      "Epoch [59/500], Train Loss: 0.1331\n",
      "Epoch [60/500], Train Loss: 0.1319\n",
      "Epoch [61/500], Train Loss: 0.1319\n",
      "Epoch [62/500], Train Loss: 0.1315\n",
      "Epoch [63/500], Train Loss: 0.1318\n",
      "Epoch [64/500], Train Loss: 0.1315\n",
      "Epoch [65/500], Train Loss: 0.1317\n",
      "Epoch [66/500], Train Loss: 0.1309\n",
      "Epoch [67/500], Train Loss: 0.1306\n",
      "Epoch [68/500], Train Loss: 0.1303\n",
      "Epoch [69/500], Train Loss: 0.1294\n",
      "Epoch [70/500], Train Loss: 0.1305\n",
      "Epoch [71/500], Train Loss: 0.1299\n",
      "Epoch [72/500], Train Loss: 0.1295\n",
      "Epoch [73/500], Train Loss: 0.1300\n",
      "Epoch [74/500], Train Loss: 0.1298\n",
      "Epoch [75/500], Train Loss: 0.1288\n",
      "Epoch [76/500], Train Loss: 0.1292\n",
      "Epoch [77/500], Train Loss: 0.1283\n",
      "Epoch [78/500], Train Loss: 0.1291\n",
      "Epoch [79/500], Train Loss: 0.1282\n",
      "Epoch [80/500], Train Loss: 0.1285\n",
      "Epoch [81/500], Train Loss: 0.1282\n",
      "Epoch [82/500], Train Loss: 0.1288\n",
      "Epoch [83/500], Train Loss: 0.1278\n",
      "Epoch [84/500], Train Loss: 0.1277\n",
      "Epoch [85/500], Train Loss: 0.1276\n",
      "Epoch [86/500], Train Loss: 0.1280\n",
      "Epoch [87/500], Train Loss: 0.1272\n",
      "Epoch [88/500], Train Loss: 0.1279\n",
      "Epoch [89/500], Train Loss: 0.1274\n",
      "Epoch [90/500], Train Loss: 0.1276\n",
      "Epoch [91/500], Train Loss: 0.1280\n",
      "Epoch [92/500], Train Loss: 0.1277\n",
      "Epoch [93/500], Train Loss: 0.1274\n",
      "Epoch [94/500], Train Loss: 0.1195\n",
      "Epoch [95/500], Train Loss: 0.1177\n",
      "Epoch [96/500], Train Loss: 0.1170\n",
      "Epoch [97/500], Train Loss: 0.1168\n",
      "Epoch [98/500], Train Loss: 0.1164\n",
      "Epoch [99/500], Train Loss: 0.1164\n",
      "Epoch [100/500], Train Loss: 0.1164\n",
      "Epoch [101/500], Train Loss: 0.1162\n",
      "Epoch [102/500], Train Loss: 0.1163\n",
      "Epoch [103/500], Train Loss: 0.1163\n",
      "Epoch [104/500], Train Loss: 0.1162\n",
      "Epoch [105/500], Train Loss: 0.1162\n",
      "Epoch [106/500], Train Loss: 0.1163\n",
      "Epoch [107/500], Train Loss: 0.1162\n",
      "Epoch [108/500], Train Loss: 0.1161\n",
      "Epoch [109/500], Train Loss: 0.1160\n",
      "Epoch [110/500], Train Loss: 0.1162\n",
      "Epoch [111/500], Train Loss: 0.1161\n",
      "Epoch [112/500], Train Loss: 0.1162\n",
      "Epoch [113/500], Train Loss: 0.1161\n",
      "Epoch [114/500], Train Loss: 0.1161\n",
      "Epoch [115/500], Train Loss: 0.1161\n",
      "Epoch [116/500], Train Loss: 0.1153\n",
      "Epoch [117/500], Train Loss: 0.1151\n",
      "Epoch [118/500], Train Loss: 0.1150\n",
      "Epoch [119/500], Train Loss: 0.1151\n",
      "Epoch [120/500], Train Loss: 0.1150\n",
      "Epoch [121/500], Train Loss: 0.1150\n",
      "Epoch [122/500], Train Loss: 0.1149\n",
      "Epoch [123/500], Train Loss: 0.1149\n",
      "Epoch [124/500], Train Loss: 0.1149\n",
      "Epoch [125/500], Train Loss: 0.1149\n",
      "Epoch [126/500], Train Loss: 0.1149\n",
      "Epoch [127/500], Train Loss: 0.1149\n",
      "Epoch [128/500], Train Loss: 0.1148\n",
      "Epoch [129/500], Train Loss: 0.1150\n",
      "Epoch [130/500], Train Loss: 0.1149\n",
      "Epoch [131/500], Train Loss: 0.1149\n",
      "Epoch [132/500], Train Loss: 0.1150\n",
      "Epoch [133/500], Train Loss: 0.1151\n",
      "Epoch [134/500], Train Loss: 0.1149\n",
      "Epoch [135/500], Train Loss: 0.1148\n",
      "Epoch [136/500], Train Loss: 0.1148\n",
      "Epoch [137/500], Train Loss: 0.1148\n",
      "Epoch [138/500], Train Loss: 0.1149\n",
      "Epoch [139/500], Train Loss: 0.1148\n",
      "Epoch [140/500], Train Loss: 0.1148\n",
      "Epoch [141/500], Train Loss: 0.1148\n",
      "Epoch [142/500], Train Loss: 0.1147\n",
      "Epoch [143/500], Train Loss: 0.1147\n",
      "Epoch [144/500], Train Loss: 0.1147\n",
      "Epoch [145/500], Train Loss: 0.1149\n",
      "Epoch [146/500], Train Loss: 0.1148\n",
      "Epoch [147/500], Train Loss: 0.1148\n",
      "Epoch [148/500], Train Loss: 0.1148\n",
      "Epoch [149/500], Train Loss: 0.1147\n",
      "Epoch [150/500], Train Loss: 0.1146\n",
      "Epoch [151/500], Train Loss: 0.1148\n",
      "Epoch [152/500], Train Loss: 0.1148\n",
      "Epoch [153/500], Train Loss: 0.1147\n",
      "Epoch [154/500], Train Loss: 0.1148\n",
      "Epoch [155/500], Train Loss: 0.1147\n",
      "Epoch [156/500], Train Loss: 0.1148\n",
      "Epoch [157/500], Train Loss: 0.1147\n",
      "Epoch [158/500], Train Loss: 0.1148\n",
      "Epoch [159/500], Train Loss: 0.1148\n",
      "Epoch [160/500], Train Loss: 0.1147\n",
      "Epoch [161/500], Train Loss: 0.1147\n",
      "Epoch [162/500], Train Loss: 0.1148\n",
      "Epoch [163/500], Train Loss: 0.1149\n",
      "Epoch [164/500], Train Loss: 0.1148\n",
      "Epoch [165/500], Train Loss: 0.1148\n",
      "Epoch [166/500], Train Loss: 0.1148\n",
      "Epoch [167/500], Train Loss: 0.1147\n",
      "Early stopping at epoch 167\n",
      "Final model saved at: ../../../Data/model_results/logistic_regression/models/chr10/final_model_chr10.pth\n",
      "Individual R^2 scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr10/individual_r2_scores_chr10.csv\n",
      "Individual IQS scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr10/individual_iqs_scores_chr10.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-18 12:14:07,354] A new study created in RDB with name: chr11_study\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual AUC ROC curves saved in: ../../../Data/model_results/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../../Data/model_results/logistic_regression/csv_files/performance_metrics.csv\n",
      "Total SNPs:  2550\n",
      "PRS313 SNPs:  38\n",
      "Total SNPs used for Training:  2512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-06-18 12:15:05,088] Trial 4 finished with value: 0.5590310394763947 and parameters: {'learning_rate': 0.0017871461918697491, 'l1_coef': 0.09580086744294376, 'patience': 20, 'batch_size': 256}. Best is trial 4 with value: 0.5590310394763947.\n",
      "[I 2024-06-18 12:15:32,691] Trial 0 finished with value: 0.714206212759018 and parameters: {'learning_rate': 0.0008616720743709761, 'l1_coef': 0.029945094962247513, 'patience': 5, 'batch_size': 128}. Best is trial 4 with value: 0.5590310394763947.\n",
      "[I 2024-06-18 12:15:41,871] Trial 9 finished with value: 0.14558064682143074 and parameters: {'learning_rate': 0.044956640417592425, 'l1_coef': 0.00014219997677332013, 'patience': 7, 'batch_size': 64}. Best is trial 9 with value: 0.14558064682143074.\n",
      "[I 2024-06-18 12:15:50,792] Trial 6 finished with value: 0.2547811672091484 and parameters: {'learning_rate': 0.00014842325354664542, 'l1_coef': 8.681198170156922e-05, 'patience': 20, 'batch_size': 256}. Best is trial 9 with value: 0.14558064682143074.\n",
      "[I 2024-06-18 12:16:08,756] Trial 7 finished with value: 0.43906726837158205 and parameters: {'learning_rate': 0.0005167745475209047, 'l1_coef': 0.002256220202293968, 'patience': 11, 'batch_size': 256}. Best is trial 9 with value: 0.14558064682143074.\n",
      "[I 2024-06-18 12:16:11,064] Trial 3 finished with value: 0.31611017669950214 and parameters: {'learning_rate': 0.0006509136572166041, 'l1_coef': 0.0007001048330647639, 'patience': 12, 'batch_size': 64}. Best is trial 9 with value: 0.14558064682143074.\n",
      "[I 2024-06-18 12:16:23,325] Trial 8 finished with value: 0.05903158273015704 and parameters: {'learning_rate': 0.001372434672947406, 'l1_coef': 1.7839007175765143e-05, 'patience': 10, 'batch_size': 64}. Best is trial 8 with value: 0.05903158273015704.\n",
      "[I 2024-06-18 12:16:37,413] Trial 5 finished with value: 0.1131603560277394 and parameters: {'learning_rate': 0.0001593415341131004, 'l1_coef': 2.8665796209135672e-05, 'patience': 18, 'batch_size': 64}. Best is trial 8 with value: 0.05903158273015704.\n",
      "[I 2024-06-18 12:16:44,550] Trial 1 finished with value: 0.527086135319301 and parameters: {'learning_rate': 0.00017020419014293617, 'l1_coef': 0.01415507081418961, 'patience': 12, 'batch_size': 64}. Best is trial 8 with value: 0.05903158273015704.\n",
      "[I 2024-06-18 12:16:53,233] Trial 2 finished with value: 0.11800295492777457 and parameters: {'learning_rate': 0.00024819452147590594, 'l1_coef': 8.497230880120757e-05, 'patience': 10, 'batch_size': 32}. Best is trial 8 with value: 0.05903158273015704.\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 11 - Best hyperparameters: {'learning_rate': 0.001372434672947406, 'l1_coef': 1.7839007175765143e-05, 'patience': 10, 'batch_size': 64}\n",
      "Chr 11 - Best value: 0.0590\n",
      "Epoch [1/500], Train Loss: 0.5093\n",
      "Epoch [2/500], Train Loss: 0.4159\n",
      "Epoch [3/500], Train Loss: 0.3683\n",
      "Epoch [4/500], Train Loss: 0.3323\n",
      "Epoch [5/500], Train Loss: 0.3040\n",
      "Epoch [6/500], Train Loss: 0.2818\n",
      "Epoch [7/500], Train Loss: 0.2640\n",
      "Epoch [8/500], Train Loss: 0.2489\n",
      "Epoch [9/500], Train Loss: 0.2350\n",
      "Epoch [10/500], Train Loss: 0.2235\n",
      "Epoch [11/500], Train Loss: 0.2132\n",
      "Epoch [12/500], Train Loss: 0.2037\n",
      "Epoch [13/500], Train Loss: 0.1962\n",
      "Epoch [14/500], Train Loss: 0.1893\n",
      "Epoch [15/500], Train Loss: 0.1828\n",
      "Epoch [16/500], Train Loss: 0.1767\n",
      "Epoch [17/500], Train Loss: 0.1708\n",
      "Epoch [18/500], Train Loss: 0.1661\n",
      "Epoch [19/500], Train Loss: 0.1617\n",
      "Epoch [20/500], Train Loss: 0.1574\n",
      "Epoch [21/500], Train Loss: 0.1537\n",
      "Epoch [22/500], Train Loss: 0.1504\n",
      "Epoch [23/500], Train Loss: 0.1474\n",
      "Epoch [24/500], Train Loss: 0.1439\n",
      "Epoch [25/500], Train Loss: 0.1406\n",
      "Epoch [26/500], Train Loss: 0.1378\n",
      "Epoch [27/500], Train Loss: 0.1353\n",
      "Epoch [28/500], Train Loss: 0.1330\n",
      "Epoch [29/500], Train Loss: 0.1307\n",
      "Epoch [30/500], Train Loss: 0.1289\n",
      "Epoch [31/500], Train Loss: 0.1261\n",
      "Epoch [32/500], Train Loss: 0.1239\n",
      "Epoch [33/500], Train Loss: 0.1223\n",
      "Epoch [34/500], Train Loss: 0.1206\n",
      "Epoch [35/500], Train Loss: 0.1186\n",
      "Epoch [36/500], Train Loss: 0.1170\n",
      "Epoch [37/500], Train Loss: 0.1157\n",
      "Epoch [38/500], Train Loss: 0.1140\n",
      "Epoch [39/500], Train Loss: 0.1126\n",
      "Epoch [40/500], Train Loss: 0.1115\n",
      "Epoch [41/500], Train Loss: 0.1097\n",
      "Epoch [42/500], Train Loss: 0.1085\n",
      "Epoch [43/500], Train Loss: 0.1077\n",
      "Epoch [44/500], Train Loss: 0.1064\n",
      "Epoch [45/500], Train Loss: 0.1052\n",
      "Epoch [46/500], Train Loss: 0.1045\n",
      "Epoch [47/500], Train Loss: 0.1031\n",
      "Epoch [48/500], Train Loss: 0.1021\n",
      "Epoch [49/500], Train Loss: 0.1008\n",
      "Epoch [50/500], Train Loss: 0.1001\n",
      "Epoch [51/500], Train Loss: 0.0990\n",
      "Epoch [52/500], Train Loss: 0.0983\n",
      "Epoch [53/500], Train Loss: 0.0973\n",
      "Epoch [54/500], Train Loss: 0.0966\n",
      "Epoch [55/500], Train Loss: 0.0955\n",
      "Epoch [56/500], Train Loss: 0.0946\n",
      "Epoch [57/500], Train Loss: 0.0938\n",
      "Epoch [58/500], Train Loss: 0.0932\n",
      "Epoch [59/500], Train Loss: 0.0924\n",
      "Epoch [60/500], Train Loss: 0.0915\n",
      "Epoch [61/500], Train Loss: 0.0909\n",
      "Epoch [62/500], Train Loss: 0.0901\n",
      "Epoch [63/500], Train Loss: 0.0895\n",
      "Epoch [64/500], Train Loss: 0.0889\n",
      "Epoch [65/500], Train Loss: 0.0881\n",
      "Epoch [66/500], Train Loss: 0.0873\n",
      "Epoch [67/500], Train Loss: 0.0869\n",
      "Epoch [68/500], Train Loss: 0.0860\n",
      "Epoch [69/500], Train Loss: 0.0855\n",
      "Epoch [70/500], Train Loss: 0.0852\n",
      "Epoch [71/500], Train Loss: 0.0846\n",
      "Epoch [72/500], Train Loss: 0.0840\n",
      "Epoch [73/500], Train Loss: 0.0835\n",
      "Epoch [74/500], Train Loss: 0.0831\n",
      "Epoch [75/500], Train Loss: 0.0822\n",
      "Epoch [76/500], Train Loss: 0.0819\n",
      "Epoch [77/500], Train Loss: 0.0812\n",
      "Epoch [78/500], Train Loss: 0.0809\n",
      "Epoch [79/500], Train Loss: 0.0802\n",
      "Epoch [80/500], Train Loss: 0.0800\n",
      "Epoch [81/500], Train Loss: 0.0793\n",
      "Epoch [82/500], Train Loss: 0.0790\n",
      "Epoch [83/500], Train Loss: 0.0789\n",
      "Epoch [84/500], Train Loss: 0.0780\n",
      "Epoch [85/500], Train Loss: 0.0777\n",
      "Epoch [86/500], Train Loss: 0.0772\n",
      "Epoch [87/500], Train Loss: 0.0766\n",
      "Epoch [88/500], Train Loss: 0.0763\n",
      "Epoch [89/500], Train Loss: 0.0759\n",
      "Epoch [90/500], Train Loss: 0.0755\n",
      "Epoch [91/500], Train Loss: 0.0750\n",
      "Epoch [92/500], Train Loss: 0.0749\n",
      "Epoch [93/500], Train Loss: 0.0743\n",
      "Epoch [94/500], Train Loss: 0.0739\n",
      "Epoch [95/500], Train Loss: 0.0736\n",
      "Epoch [96/500], Train Loss: 0.0733\n",
      "Epoch [97/500], Train Loss: 0.0727\n",
      "Epoch [98/500], Train Loss: 0.0723\n",
      "Epoch [99/500], Train Loss: 0.0718\n",
      "Epoch [100/500], Train Loss: 0.0716\n",
      "Epoch [101/500], Train Loss: 0.0714\n",
      "Epoch [102/500], Train Loss: 0.0714\n",
      "Epoch [103/500], Train Loss: 0.0707\n",
      "Epoch [104/500], Train Loss: 0.0702\n",
      "Epoch [105/500], Train Loss: 0.0698\n",
      "Epoch [106/500], Train Loss: 0.0700\n",
      "Epoch [107/500], Train Loss: 0.0696\n",
      "Epoch [108/500], Train Loss: 0.0689\n",
      "Epoch [109/500], Train Loss: 0.0686\n",
      "Epoch [110/500], Train Loss: 0.0683\n",
      "Epoch [111/500], Train Loss: 0.0681\n",
      "Epoch [112/500], Train Loss: 0.0684\n",
      "Epoch [113/500], Train Loss: 0.0677\n",
      "Epoch [114/500], Train Loss: 0.0674\n",
      "Epoch [115/500], Train Loss: 0.0669\n",
      "Epoch [116/500], Train Loss: 0.0668\n",
      "Epoch [117/500], Train Loss: 0.0664\n",
      "Epoch [118/500], Train Loss: 0.0667\n",
      "Epoch [119/500], Train Loss: 0.0660\n",
      "Epoch [120/500], Train Loss: 0.0657\n",
      "Epoch [121/500], Train Loss: 0.0656\n",
      "Epoch [122/500], Train Loss: 0.0649\n",
      "Epoch [123/500], Train Loss: 0.0652\n",
      "Epoch [124/500], Train Loss: 0.0649\n",
      "Epoch [125/500], Train Loss: 0.0644\n",
      "Epoch [126/500], Train Loss: 0.0642\n",
      "Epoch [127/500], Train Loss: 0.0640\n",
      "Epoch [128/500], Train Loss: 0.0639\n",
      "Epoch [129/500], Train Loss: 0.0634\n",
      "Epoch [130/500], Train Loss: 0.0633\n",
      "Epoch [131/500], Train Loss: 0.0631\n",
      "Epoch [132/500], Train Loss: 0.0629\n",
      "Epoch [133/500], Train Loss: 0.0625\n",
      "Epoch [134/500], Train Loss: 0.0623\n",
      "Epoch [135/500], Train Loss: 0.0624\n",
      "Epoch [136/500], Train Loss: 0.0624\n",
      "Epoch [137/500], Train Loss: 0.0618\n",
      "Epoch [138/500], Train Loss: 0.0615\n",
      "Epoch [139/500], Train Loss: 0.0615\n",
      "Epoch [140/500], Train Loss: 0.0612\n",
      "Epoch [141/500], Train Loss: 0.0611\n",
      "Epoch [142/500], Train Loss: 0.0609\n",
      "Epoch [143/500], Train Loss: 0.0606\n",
      "Epoch [144/500], Train Loss: 0.0604\n",
      "Epoch [145/500], Train Loss: 0.0604\n",
      "Epoch [146/500], Train Loss: 0.0600\n",
      "Epoch [147/500], Train Loss: 0.0602\n",
      "Epoch [148/500], Train Loss: 0.0601\n",
      "Epoch [149/500], Train Loss: 0.0596\n",
      "Epoch [150/500], Train Loss: 0.0594\n",
      "Epoch [151/500], Train Loss: 0.0593\n",
      "Epoch [152/500], Train Loss: 0.0591\n",
      "Epoch [153/500], Train Loss: 0.0587\n",
      "Epoch [154/500], Train Loss: 0.0585\n",
      "Epoch [155/500], Train Loss: 0.0583\n",
      "Epoch [156/500], Train Loss: 0.0584\n",
      "Epoch [157/500], Train Loss: 0.0581\n",
      "Epoch [158/500], Train Loss: 0.0580\n",
      "Epoch [159/500], Train Loss: 0.0580\n",
      "Epoch [160/500], Train Loss: 0.0583\n",
      "Epoch [161/500], Train Loss: 0.0575\n",
      "Epoch [162/500], Train Loss: 0.0574\n",
      "Epoch [163/500], Train Loss: 0.0572\n",
      "Epoch [164/500], Train Loss: 0.0571\n",
      "Epoch [165/500], Train Loss: 0.0574\n",
      "Epoch [166/500], Train Loss: 0.0568\n",
      "Epoch [167/500], Train Loss: 0.0566\n",
      "Epoch [168/500], Train Loss: 0.0564\n",
      "Epoch [169/500], Train Loss: 0.0564\n",
      "Epoch [170/500], Train Loss: 0.0564\n",
      "Epoch [171/500], Train Loss: 0.0563\n",
      "Epoch [172/500], Train Loss: 0.0560\n",
      "Epoch [173/500], Train Loss: 0.0559\n",
      "Epoch [174/500], Train Loss: 0.0558\n",
      "Epoch [175/500], Train Loss: 0.0559\n",
      "Epoch [176/500], Train Loss: 0.0556\n",
      "Epoch [177/500], Train Loss: 0.0554\n",
      "Epoch [178/500], Train Loss: 0.0552\n",
      "Epoch [179/500], Train Loss: 0.0552\n",
      "Epoch [180/500], Train Loss: 0.0549\n",
      "Epoch [181/500], Train Loss: 0.0552\n",
      "Epoch [182/500], Train Loss: 0.0549\n",
      "Epoch [183/500], Train Loss: 0.0545\n",
      "Epoch [184/500], Train Loss: 0.0546\n",
      "Epoch [185/500], Train Loss: 0.0546\n",
      "Epoch [186/500], Train Loss: 0.0544\n",
      "Epoch [187/500], Train Loss: 0.0542\n",
      "Epoch [188/500], Train Loss: 0.0540\n",
      "Epoch [189/500], Train Loss: 0.0540\n",
      "Epoch [190/500], Train Loss: 0.0540\n",
      "Epoch [191/500], Train Loss: 0.0538\n",
      "Epoch [192/500], Train Loss: 0.0537\n",
      "Epoch [193/500], Train Loss: 0.0535\n",
      "Epoch [194/500], Train Loss: 0.0536\n",
      "Epoch [195/500], Train Loss: 0.0536\n",
      "Epoch [196/500], Train Loss: 0.0532\n",
      "Epoch [197/500], Train Loss: 0.0534\n",
      "Epoch [198/500], Train Loss: 0.0532\n",
      "Epoch [199/500], Train Loss: 0.0530\n",
      "Epoch [200/500], Train Loss: 0.0530\n",
      "Epoch [201/500], Train Loss: 0.0529\n",
      "Epoch [202/500], Train Loss: 0.0526\n",
      "Epoch [203/500], Train Loss: 0.0528\n",
      "Epoch [204/500], Train Loss: 0.0528\n",
      "Epoch [205/500], Train Loss: 0.0526\n",
      "Epoch [206/500], Train Loss: 0.0524\n",
      "Epoch [207/500], Train Loss: 0.0522\n",
      "Epoch [208/500], Train Loss: 0.0524\n",
      "Epoch [209/500], Train Loss: 0.0523\n",
      "Epoch [210/500], Train Loss: 0.0522\n",
      "Epoch [211/500], Train Loss: 0.0521\n",
      "Epoch [212/500], Train Loss: 0.0519\n",
      "Epoch [213/500], Train Loss: 0.0517\n",
      "Epoch [214/500], Train Loss: 0.0516\n",
      "Epoch [215/500], Train Loss: 0.0518\n",
      "Epoch [216/500], Train Loss: 0.0517\n",
      "Epoch [217/500], Train Loss: 0.0516\n",
      "Epoch [218/500], Train Loss: 0.0513\n",
      "Epoch [219/500], Train Loss: 0.0512\n",
      "Epoch [220/500], Train Loss: 0.0512\n",
      "Epoch [221/500], Train Loss: 0.0513\n",
      "Epoch [222/500], Train Loss: 0.0511\n",
      "Epoch [223/500], Train Loss: 0.0509\n",
      "Epoch [224/500], Train Loss: 0.0510\n",
      "Epoch [225/500], Train Loss: 0.0510\n",
      "Epoch [226/500], Train Loss: 0.0508\n",
      "Epoch [227/500], Train Loss: 0.0508\n",
      "Epoch [228/500], Train Loss: 0.0508\n",
      "Epoch [229/500], Train Loss: 0.0505\n",
      "Epoch [230/500], Train Loss: 0.0506\n",
      "Epoch [231/500], Train Loss: 0.0507\n",
      "Epoch [232/500], Train Loss: 0.0503\n",
      "Epoch [233/500], Train Loss: 0.0505\n",
      "Epoch [234/500], Train Loss: 0.0505\n",
      "Epoch [235/500], Train Loss: 0.0501\n",
      "Epoch [236/500], Train Loss: 0.0501\n",
      "Epoch [237/500], Train Loss: 0.0502\n",
      "Epoch [238/500], Train Loss: 0.0500\n",
      "Epoch [239/500], Train Loss: 0.0501\n",
      "Epoch [240/500], Train Loss: 0.0500\n",
      "Epoch [241/500], Train Loss: 0.0501\n",
      "Epoch [242/500], Train Loss: 0.0499\n",
      "Epoch [243/500], Train Loss: 0.0497\n",
      "Epoch [244/500], Train Loss: 0.0496\n",
      "Epoch [245/500], Train Loss: 0.0496\n",
      "Epoch [246/500], Train Loss: 0.0497\n",
      "Epoch [247/500], Train Loss: 0.0496\n",
      "Epoch [248/500], Train Loss: 0.0495\n",
      "Epoch [249/500], Train Loss: 0.0497\n",
      "Epoch [250/500], Train Loss: 0.0494\n",
      "Epoch [251/500], Train Loss: 0.0498\n",
      "Epoch [252/500], Train Loss: 0.0495\n",
      "Epoch [253/500], Train Loss: 0.0493\n",
      "Epoch [254/500], Train Loss: 0.0490\n",
      "Epoch [255/500], Train Loss: 0.0491\n",
      "Epoch [256/500], Train Loss: 0.0492\n",
      "Epoch [257/500], Train Loss: 0.0489\n",
      "Epoch [258/500], Train Loss: 0.0488\n",
      "Epoch [259/500], Train Loss: 0.0490\n",
      "Epoch [260/500], Train Loss: 0.0489\n",
      "Epoch [261/500], Train Loss: 0.0491\n",
      "Epoch [262/500], Train Loss: 0.0487\n",
      "Epoch [263/500], Train Loss: 0.0486\n",
      "Epoch [264/500], Train Loss: 0.0486\n",
      "Epoch [265/500], Train Loss: 0.0485\n",
      "Epoch [266/500], Train Loss: 0.0487\n",
      "Epoch [267/500], Train Loss: 0.0486\n",
      "Epoch [268/500], Train Loss: 0.0487\n",
      "Epoch [269/500], Train Loss: 0.0487\n",
      "Epoch [270/500], Train Loss: 0.0485\n",
      "Epoch [271/500], Train Loss: 0.0484\n",
      "Epoch [272/500], Train Loss: 0.0483\n",
      "Epoch [273/500], Train Loss: 0.0483\n",
      "Epoch [274/500], Train Loss: 0.0481\n",
      "Epoch [275/500], Train Loss: 0.0482\n",
      "Epoch [276/500], Train Loss: 0.0480\n",
      "Epoch [277/500], Train Loss: 0.0481\n",
      "Epoch [278/500], Train Loss: 0.0480\n",
      "Epoch [279/500], Train Loss: 0.0485\n",
      "Epoch [280/500], Train Loss: 0.0482\n",
      "Epoch [281/500], Train Loss: 0.0481\n",
      "Epoch [282/500], Train Loss: 0.0481\n",
      "Epoch [283/500], Train Loss: 0.0476\n",
      "Epoch [284/500], Train Loss: 0.0479\n",
      "Epoch [285/500], Train Loss: 0.0480\n",
      "Epoch [286/500], Train Loss: 0.0479\n",
      "Epoch [287/500], Train Loss: 0.0476\n",
      "Epoch [288/500], Train Loss: 0.0479\n",
      "Epoch [289/500], Train Loss: 0.0478\n",
      "Epoch [290/500], Train Loss: 0.0474\n",
      "Epoch [291/500], Train Loss: 0.0475\n",
      "Epoch [292/500], Train Loss: 0.0477\n",
      "Epoch [293/500], Train Loss: 0.0476\n",
      "Epoch [294/500], Train Loss: 0.0477\n",
      "Epoch [295/500], Train Loss: 0.0475\n",
      "Epoch [296/500], Train Loss: 0.0474\n",
      "Epoch [297/500], Train Loss: 0.0475\n",
      "Epoch [298/500], Train Loss: 0.0475\n",
      "Epoch [299/500], Train Loss: 0.0472\n",
      "Epoch [300/500], Train Loss: 0.0473\n",
      "Epoch [301/500], Train Loss: 0.0474\n",
      "Epoch [302/500], Train Loss: 0.0472\n",
      "Epoch [303/500], Train Loss: 0.0472\n",
      "Epoch [304/500], Train Loss: 0.0473\n",
      "Epoch [305/500], Train Loss: 0.0473\n",
      "Epoch [306/500], Train Loss: 0.0460\n",
      "Epoch [307/500], Train Loss: 0.0452\n",
      "Epoch [308/500], Train Loss: 0.0451\n",
      "Epoch [309/500], Train Loss: 0.0449\n",
      "Epoch [310/500], Train Loss: 0.0448\n",
      "Epoch [311/500], Train Loss: 0.0448\n",
      "Epoch [312/500], Train Loss: 0.0447\n",
      "Epoch [313/500], Train Loss: 0.0447\n",
      "Epoch [314/500], Train Loss: 0.0448\n",
      "Epoch [315/500], Train Loss: 0.0447\n",
      "Epoch [316/500], Train Loss: 0.0447\n",
      "Epoch [317/500], Train Loss: 0.0447\n",
      "Epoch [318/500], Train Loss: 0.0447\n",
      "Epoch [319/500], Train Loss: 0.0447\n",
      "Epoch [320/500], Train Loss: 0.0446\n",
      "Epoch [321/500], Train Loss: 0.0445\n",
      "Epoch [322/500], Train Loss: 0.0444\n",
      "Epoch [323/500], Train Loss: 0.0445\n",
      "Epoch [324/500], Train Loss: 0.0444\n",
      "Epoch [325/500], Train Loss: 0.0445\n",
      "Epoch [326/500], Train Loss: 0.0445\n",
      "Epoch [327/500], Train Loss: 0.0444\n",
      "Epoch [328/500], Train Loss: 0.0445\n",
      "Epoch [329/500], Train Loss: 0.0444\n",
      "Epoch [330/500], Train Loss: 0.0444\n",
      "Epoch [331/500], Train Loss: 0.0444\n",
      "Epoch [332/500], Train Loss: 0.0445\n",
      "Early stopping at epoch 332\n",
      "Final model saved at: ../../../Data/model_results/logistic_regression/models/chr11/final_model_chr11.pth\n",
      "Individual R^2 scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr11/individual_r2_scores_chr11.csv\n",
      "Individual IQS scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr11/individual_iqs_scores_chr11.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-18 12:17:07,243] A new study created in RDB with name: chr12_study\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual AUC ROC curves saved in: ../../../Data/model_results/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../../Data/model_results/logistic_regression/csv_files/performance_metrics.csv\n",
      "Total SNPs:  1704\n",
      "PRS313 SNPs:  34\n",
      "Total SNPs used for Training:  1670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-06-18 12:18:05,844] Trial 5 finished with value: 0.48401095569133756 and parameters: {'learning_rate': 0.04413119754639475, 'l1_coef': 0.009574731136781377, 'patience': 20, 'batch_size': 128}. Best is trial 5 with value: 0.48401095569133756.\n",
      "[I 2024-06-18 12:18:08,116] Trial 2 finished with value: 0.16865556165575982 and parameters: {'learning_rate': 0.02541031364574953, 'l1_coef': 0.00010491748760437849, 'patience': 18, 'batch_size': 128}. Best is trial 2 with value: 0.16865556165575982.\n",
      "[I 2024-06-18 12:18:26,508] Trial 3 finished with value: 0.18919315785169602 and parameters: {'learning_rate': 0.058028674999873915, 'l1_coef': 0.00015324608951669666, 'patience': 10, 'batch_size': 128}. Best is trial 2 with value: 0.16865556165575982.\n",
      "[I 2024-06-18 12:18:41,749] Trial 1 finished with value: 0.10776926353573799 and parameters: {'learning_rate': 0.0014395840013284337, 'l1_coef': 1.0867185953519262e-05, 'patience': 18, 'batch_size': 256}. Best is trial 1 with value: 0.10776926353573799.\n",
      "[I 2024-06-18 12:19:32,095] Trial 6 finished with value: 0.5036482989788056 and parameters: {'learning_rate': 0.00028246168155195384, 'l1_coef': 0.011795101061339523, 'patience': 19, 'batch_size': 256}. Best is trial 1 with value: 0.10776926353573799.\n",
      "[I 2024-06-18 12:19:45,414] Trial 8 finished with value: 0.33672292487961897 and parameters: {'learning_rate': 0.0005959064461860074, 'l1_coef': 0.0009444050172956216, 'patience': 17, 'batch_size': 64}. Best is trial 1 with value: 0.10776926353573799.\n",
      "[I 2024-06-18 12:19:47,979] Trial 4 finished with value: 0.48527539280744697 and parameters: {'learning_rate': 0.00025267549070338304, 'l1_coef': 0.0737398436904969, 'patience': 12, 'batch_size': 32}. Best is trial 1 with value: 0.10776926353573799.\n",
      "[I 2024-06-18 12:19:48,733] Trial 9 finished with value: 0.4865083960386423 and parameters: {'learning_rate': 0.00020342852377198164, 'l1_coef': 0.09541495993575308, 'patience': 13, 'batch_size': 32}. Best is trial 1 with value: 0.10776926353573799.\n",
      "[I 2024-06-18 12:19:50,137] Trial 0 finished with value: 0.33372598565541783 and parameters: {'learning_rate': 0.0003584100274404831, 'l1_coef': 0.0009221062825234774, 'patience': 6, 'batch_size': 32}. Best is trial 1 with value: 0.10776926353573799.\n",
      "[I 2024-06-18 12:19:58,178] Trial 7 finished with value: 0.12547499675017137 and parameters: {'learning_rate': 0.0005280069323923594, 'l1_coef': 3.793282862648734e-05, 'patience': 15, 'batch_size': 32}. Best is trial 1 with value: 0.10776926353573799.\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 12 - Best hyperparameters: {'learning_rate': 0.0014395840013284337, 'l1_coef': 1.0867185953519262e-05, 'patience': 18, 'batch_size': 256}\n",
      "Chr 12 - Best value: 0.1078\n",
      "Epoch [1/500], Train Loss: 0.5445\n",
      "Epoch [2/500], Train Loss: 0.4869\n",
      "Epoch [3/500], Train Loss: 0.4501\n",
      "Epoch [4/500], Train Loss: 0.4291\n",
      "Epoch [5/500], Train Loss: 0.4128\n",
      "Epoch [6/500], Train Loss: 0.3997\n",
      "Epoch [7/500], Train Loss: 0.3879\n",
      "Epoch [8/500], Train Loss: 0.3771\n",
      "Epoch [9/500], Train Loss: 0.3671\n",
      "Epoch [10/500], Train Loss: 0.3583\n",
      "Epoch [11/500], Train Loss: 0.3497\n",
      "Epoch [12/500], Train Loss: 0.3414\n",
      "Epoch [13/500], Train Loss: 0.3336\n",
      "Epoch [14/500], Train Loss: 0.3263\n",
      "Epoch [15/500], Train Loss: 0.3197\n",
      "Epoch [16/500], Train Loss: 0.3130\n",
      "Epoch [17/500], Train Loss: 0.3072\n",
      "Epoch [18/500], Train Loss: 0.3016\n",
      "Epoch [19/500], Train Loss: 0.2961\n",
      "Epoch [20/500], Train Loss: 0.2910\n",
      "Epoch [21/500], Train Loss: 0.2864\n",
      "Epoch [22/500], Train Loss: 0.2816\n",
      "Epoch [23/500], Train Loss: 0.2768\n",
      "Epoch [24/500], Train Loss: 0.2729\n",
      "Epoch [25/500], Train Loss: 0.2688\n",
      "Epoch [26/500], Train Loss: 0.2647\n",
      "Epoch [27/500], Train Loss: 0.2610\n",
      "Epoch [28/500], Train Loss: 0.2573\n",
      "Epoch [29/500], Train Loss: 0.2537\n",
      "Epoch [30/500], Train Loss: 0.2507\n",
      "Epoch [31/500], Train Loss: 0.2477\n",
      "Epoch [32/500], Train Loss: 0.2446\n",
      "Epoch [33/500], Train Loss: 0.2414\n",
      "Epoch [34/500], Train Loss: 0.2384\n",
      "Epoch [35/500], Train Loss: 0.2357\n",
      "Epoch [36/500], Train Loss: 0.2327\n",
      "Epoch [37/500], Train Loss: 0.2305\n",
      "Epoch [38/500], Train Loss: 0.2276\n",
      "Epoch [39/500], Train Loss: 0.2254\n",
      "Epoch [40/500], Train Loss: 0.2231\n",
      "Epoch [41/500], Train Loss: 0.2207\n",
      "Epoch [42/500], Train Loss: 0.2185\n",
      "Epoch [43/500], Train Loss: 0.2165\n",
      "Epoch [44/500], Train Loss: 0.2142\n",
      "Epoch [45/500], Train Loss: 0.2120\n",
      "Epoch [46/500], Train Loss: 0.2101\n",
      "Epoch [47/500], Train Loss: 0.2083\n",
      "Epoch [48/500], Train Loss: 0.2063\n",
      "Epoch [49/500], Train Loss: 0.2043\n",
      "Epoch [50/500], Train Loss: 0.2025\n",
      "Epoch [51/500], Train Loss: 0.2009\n",
      "Epoch [52/500], Train Loss: 0.1993\n",
      "Epoch [53/500], Train Loss: 0.1976\n",
      "Epoch [54/500], Train Loss: 0.1958\n",
      "Epoch [55/500], Train Loss: 0.1943\n",
      "Epoch [56/500], Train Loss: 0.1931\n",
      "Epoch [57/500], Train Loss: 0.1916\n",
      "Epoch [58/500], Train Loss: 0.1896\n",
      "Epoch [59/500], Train Loss: 0.1881\n",
      "Epoch [60/500], Train Loss: 0.1869\n",
      "Epoch [61/500], Train Loss: 0.1856\n",
      "Epoch [62/500], Train Loss: 0.1842\n",
      "Epoch [63/500], Train Loss: 0.1828\n",
      "Epoch [64/500], Train Loss: 0.1815\n",
      "Epoch [65/500], Train Loss: 0.1803\n",
      "Epoch [66/500], Train Loss: 0.1792\n",
      "Epoch [67/500], Train Loss: 0.1780\n",
      "Epoch [68/500], Train Loss: 0.1766\n",
      "Epoch [69/500], Train Loss: 0.1755\n",
      "Epoch [70/500], Train Loss: 0.1744\n",
      "Epoch [71/500], Train Loss: 0.1733\n",
      "Epoch [72/500], Train Loss: 0.1721\n",
      "Epoch [73/500], Train Loss: 0.1709\n",
      "Epoch [74/500], Train Loss: 0.1697\n",
      "Epoch [75/500], Train Loss: 0.1686\n",
      "Epoch [76/500], Train Loss: 0.1677\n",
      "Epoch [77/500], Train Loss: 0.1667\n",
      "Epoch [78/500], Train Loss: 0.1655\n",
      "Epoch [79/500], Train Loss: 0.1647\n",
      "Epoch [80/500], Train Loss: 0.1638\n",
      "Epoch [81/500], Train Loss: 0.1628\n",
      "Epoch [82/500], Train Loss: 0.1619\n",
      "Epoch [83/500], Train Loss: 0.1609\n",
      "Epoch [84/500], Train Loss: 0.1601\n",
      "Epoch [85/500], Train Loss: 0.1593\n",
      "Epoch [86/500], Train Loss: 0.1584\n",
      "Epoch [87/500], Train Loss: 0.1576\n",
      "Epoch [88/500], Train Loss: 0.1566\n",
      "Epoch [89/500], Train Loss: 0.1559\n",
      "Epoch [90/500], Train Loss: 0.1550\n",
      "Epoch [91/500], Train Loss: 0.1544\n",
      "Epoch [92/500], Train Loss: 0.1536\n",
      "Epoch [93/500], Train Loss: 0.1527\n",
      "Epoch [94/500], Train Loss: 0.1520\n",
      "Epoch [95/500], Train Loss: 0.1513\n",
      "Epoch [96/500], Train Loss: 0.1505\n",
      "Epoch [97/500], Train Loss: 0.1498\n",
      "Epoch [98/500], Train Loss: 0.1490\n",
      "Epoch [99/500], Train Loss: 0.1487\n",
      "Epoch [100/500], Train Loss: 0.1476\n",
      "Epoch [101/500], Train Loss: 0.1470\n",
      "Epoch [102/500], Train Loss: 0.1463\n",
      "Epoch [103/500], Train Loss: 0.1456\n",
      "Epoch [104/500], Train Loss: 0.1450\n",
      "Epoch [105/500], Train Loss: 0.1445\n",
      "Epoch [106/500], Train Loss: 0.1437\n",
      "Epoch [107/500], Train Loss: 0.1430\n",
      "Epoch [108/500], Train Loss: 0.1424\n",
      "Epoch [109/500], Train Loss: 0.1419\n",
      "Epoch [110/500], Train Loss: 0.1414\n",
      "Epoch [111/500], Train Loss: 0.1408\n",
      "Epoch [112/500], Train Loss: 0.1402\n",
      "Epoch [113/500], Train Loss: 0.1397\n",
      "Epoch [114/500], Train Loss: 0.1390\n",
      "Epoch [115/500], Train Loss: 0.1384\n",
      "Epoch [116/500], Train Loss: 0.1376\n",
      "Epoch [117/500], Train Loss: 0.1372\n",
      "Epoch [118/500], Train Loss: 0.1366\n",
      "Epoch [119/500], Train Loss: 0.1360\n",
      "Epoch [120/500], Train Loss: 0.1356\n",
      "Epoch [121/500], Train Loss: 0.1351\n",
      "Epoch [122/500], Train Loss: 0.1345\n",
      "Epoch [123/500], Train Loss: 0.1339\n",
      "Epoch [124/500], Train Loss: 0.1335\n",
      "Epoch [125/500], Train Loss: 0.1331\n",
      "Epoch [126/500], Train Loss: 0.1325\n",
      "Epoch [127/500], Train Loss: 0.1322\n",
      "Epoch [128/500], Train Loss: 0.1317\n",
      "Epoch [129/500], Train Loss: 0.1313\n",
      "Epoch [130/500], Train Loss: 0.1306\n",
      "Epoch [131/500], Train Loss: 0.1303\n",
      "Epoch [132/500], Train Loss: 0.1296\n",
      "Epoch [133/500], Train Loss: 0.1292\n",
      "Epoch [134/500], Train Loss: 0.1287\n",
      "Epoch [135/500], Train Loss: 0.1285\n",
      "Epoch [136/500], Train Loss: 0.1278\n",
      "Epoch [137/500], Train Loss: 0.1276\n",
      "Epoch [138/500], Train Loss: 0.1271\n",
      "Epoch [139/500], Train Loss: 0.1267\n",
      "Epoch [140/500], Train Loss: 0.1263\n",
      "Epoch [141/500], Train Loss: 0.1257\n",
      "Epoch [142/500], Train Loss: 0.1255\n",
      "Epoch [143/500], Train Loss: 0.1249\n",
      "Epoch [144/500], Train Loss: 0.1245\n",
      "Epoch [145/500], Train Loss: 0.1241\n",
      "Epoch [146/500], Train Loss: 0.1238\n",
      "Epoch [147/500], Train Loss: 0.1234\n",
      "Epoch [148/500], Train Loss: 0.1229\n",
      "Epoch [149/500], Train Loss: 0.1226\n",
      "Epoch [150/500], Train Loss: 0.1222\n",
      "Epoch [151/500], Train Loss: 0.1219\n",
      "Epoch [152/500], Train Loss: 0.1214\n",
      "Epoch [153/500], Train Loss: 0.1211\n",
      "Epoch [154/500], Train Loss: 0.1207\n",
      "Epoch [155/500], Train Loss: 0.1203\n",
      "Epoch [156/500], Train Loss: 0.1201\n",
      "Epoch [157/500], Train Loss: 0.1197\n",
      "Epoch [158/500], Train Loss: 0.1193\n",
      "Epoch [159/500], Train Loss: 0.1189\n",
      "Epoch [160/500], Train Loss: 0.1186\n",
      "Epoch [161/500], Train Loss: 0.1182\n",
      "Epoch [162/500], Train Loss: 0.1179\n",
      "Epoch [163/500], Train Loss: 0.1178\n",
      "Epoch [164/500], Train Loss: 0.1174\n",
      "Epoch [165/500], Train Loss: 0.1169\n",
      "Epoch [166/500], Train Loss: 0.1167\n",
      "Epoch [167/500], Train Loss: 0.1163\n",
      "Epoch [168/500], Train Loss: 0.1159\n",
      "Epoch [169/500], Train Loss: 0.1156\n",
      "Epoch [170/500], Train Loss: 0.1154\n",
      "Epoch [171/500], Train Loss: 0.1151\n",
      "Epoch [172/500], Train Loss: 0.1148\n",
      "Epoch [173/500], Train Loss: 0.1144\n",
      "Epoch [174/500], Train Loss: 0.1140\n",
      "Epoch [175/500], Train Loss: 0.1137\n",
      "Epoch [176/500], Train Loss: 0.1135\n",
      "Epoch [177/500], Train Loss: 0.1132\n",
      "Epoch [178/500], Train Loss: 0.1130\n",
      "Epoch [179/500], Train Loss: 0.1126\n",
      "Epoch [180/500], Train Loss: 0.1124\n",
      "Epoch [181/500], Train Loss: 0.1121\n",
      "Epoch [182/500], Train Loss: 0.1118\n",
      "Epoch [183/500], Train Loss: 0.1115\n",
      "Epoch [184/500], Train Loss: 0.1112\n",
      "Epoch [185/500], Train Loss: 0.1109\n",
      "Epoch [186/500], Train Loss: 0.1107\n",
      "Epoch [187/500], Train Loss: 0.1103\n",
      "Epoch [188/500], Train Loss: 0.1101\n",
      "Epoch [189/500], Train Loss: 0.1099\n",
      "Epoch [190/500], Train Loss: 0.1095\n",
      "Epoch [191/500], Train Loss: 0.1094\n",
      "Epoch [192/500], Train Loss: 0.1090\n",
      "Epoch [193/500], Train Loss: 0.1088\n",
      "Epoch [194/500], Train Loss: 0.1085\n",
      "Epoch [195/500], Train Loss: 0.1083\n",
      "Epoch [196/500], Train Loss: 0.1079\n",
      "Epoch [197/500], Train Loss: 0.1078\n",
      "Epoch [198/500], Train Loss: 0.1075\n",
      "Epoch [199/500], Train Loss: 0.1073\n",
      "Epoch [200/500], Train Loss: 0.1070\n",
      "Epoch [201/500], Train Loss: 0.1068\n",
      "Epoch [202/500], Train Loss: 0.1067\n",
      "Epoch [203/500], Train Loss: 0.1064\n",
      "Epoch [204/500], Train Loss: 0.1062\n",
      "Epoch [205/500], Train Loss: 0.1059\n",
      "Epoch [206/500], Train Loss: 0.1057\n",
      "Epoch [207/500], Train Loss: 0.1055\n",
      "Epoch [208/500], Train Loss: 0.1052\n",
      "Epoch [209/500], Train Loss: 0.1050\n",
      "Epoch [210/500], Train Loss: 0.1047\n",
      "Epoch [211/500], Train Loss: 0.1044\n",
      "Epoch [212/500], Train Loss: 0.1042\n",
      "Epoch [213/500], Train Loss: 0.1040\n",
      "Epoch [214/500], Train Loss: 0.1038\n",
      "Epoch [215/500], Train Loss: 0.1036\n",
      "Epoch [216/500], Train Loss: 0.1034\n",
      "Epoch [217/500], Train Loss: 0.1032\n",
      "Epoch [218/500], Train Loss: 0.1029\n",
      "Epoch [219/500], Train Loss: 0.1027\n",
      "Epoch [220/500], Train Loss: 0.1025\n",
      "Epoch [221/500], Train Loss: 0.1025\n",
      "Epoch [222/500], Train Loss: 0.1020\n",
      "Epoch [223/500], Train Loss: 0.1019\n",
      "Epoch [224/500], Train Loss: 0.1016\n",
      "Epoch [225/500], Train Loss: 0.1015\n",
      "Epoch [226/500], Train Loss: 0.1013\n",
      "Epoch [227/500], Train Loss: 0.1010\n",
      "Epoch [228/500], Train Loss: 0.1009\n",
      "Epoch [229/500], Train Loss: 0.1007\n",
      "Epoch [230/500], Train Loss: 0.1004\n",
      "Epoch [231/500], Train Loss: 0.1002\n",
      "Epoch [232/500], Train Loss: 0.1000\n",
      "Epoch [233/500], Train Loss: 0.0998\n",
      "Epoch [234/500], Train Loss: 0.0996\n",
      "Epoch [235/500], Train Loss: 0.0995\n",
      "Epoch [236/500], Train Loss: 0.0993\n",
      "Epoch [237/500], Train Loss: 0.0991\n",
      "Epoch [238/500], Train Loss: 0.0989\n",
      "Epoch [239/500], Train Loss: 0.0986\n",
      "Epoch [240/500], Train Loss: 0.0985\n",
      "Epoch [241/500], Train Loss: 0.0983\n",
      "Epoch [242/500], Train Loss: 0.0982\n",
      "Epoch [243/500], Train Loss: 0.0980\n",
      "Epoch [244/500], Train Loss: 0.0979\n",
      "Epoch [245/500], Train Loss: 0.0976\n",
      "Epoch [246/500], Train Loss: 0.0975\n",
      "Epoch [247/500], Train Loss: 0.0973\n",
      "Epoch [248/500], Train Loss: 0.0971\n",
      "Epoch [249/500], Train Loss: 0.0969\n",
      "Epoch [250/500], Train Loss: 0.0967\n",
      "Epoch [251/500], Train Loss: 0.0965\n",
      "Epoch [252/500], Train Loss: 0.0963\n",
      "Epoch [253/500], Train Loss: 0.0962\n",
      "Epoch [254/500], Train Loss: 0.0960\n",
      "Epoch [255/500], Train Loss: 0.0959\n",
      "Epoch [256/500], Train Loss: 0.0957\n",
      "Epoch [257/500], Train Loss: 0.0956\n",
      "Epoch [258/500], Train Loss: 0.0955\n",
      "Epoch [259/500], Train Loss: 0.0952\n",
      "Epoch [260/500], Train Loss: 0.0951\n",
      "Epoch [261/500], Train Loss: 0.0949\n",
      "Epoch [262/500], Train Loss: 0.0947\n",
      "Epoch [263/500], Train Loss: 0.0945\n",
      "Epoch [264/500], Train Loss: 0.0943\n",
      "Epoch [265/500], Train Loss: 0.0942\n",
      "Epoch [266/500], Train Loss: 0.0940\n",
      "Epoch [267/500], Train Loss: 0.0939\n",
      "Epoch [268/500], Train Loss: 0.0937\n",
      "Epoch [269/500], Train Loss: 0.0935\n",
      "Epoch [270/500], Train Loss: 0.0934\n",
      "Epoch [271/500], Train Loss: 0.0932\n",
      "Epoch [272/500], Train Loss: 0.0931\n",
      "Epoch [273/500], Train Loss: 0.0931\n",
      "Epoch [274/500], Train Loss: 0.0928\n",
      "Epoch [275/500], Train Loss: 0.0927\n",
      "Epoch [276/500], Train Loss: 0.0925\n",
      "Epoch [277/500], Train Loss: 0.0923\n",
      "Epoch [278/500], Train Loss: 0.0922\n",
      "Epoch [279/500], Train Loss: 0.0921\n",
      "Epoch [280/500], Train Loss: 0.0919\n",
      "Epoch [281/500], Train Loss: 0.0917\n",
      "Epoch [282/500], Train Loss: 0.0916\n",
      "Epoch [283/500], Train Loss: 0.0915\n",
      "Epoch [284/500], Train Loss: 0.0914\n",
      "Epoch [285/500], Train Loss: 0.0912\n",
      "Epoch [286/500], Train Loss: 0.0910\n",
      "Epoch [287/500], Train Loss: 0.0909\n",
      "Epoch [288/500], Train Loss: 0.0908\n",
      "Epoch [289/500], Train Loss: 0.0906\n",
      "Epoch [290/500], Train Loss: 0.0904\n",
      "Epoch [291/500], Train Loss: 0.0903\n",
      "Epoch [292/500], Train Loss: 0.0902\n",
      "Epoch [293/500], Train Loss: 0.0901\n",
      "Epoch [294/500], Train Loss: 0.0900\n",
      "Epoch [295/500], Train Loss: 0.0899\n",
      "Epoch [296/500], Train Loss: 0.0897\n",
      "Epoch [297/500], Train Loss: 0.0895\n",
      "Epoch [298/500], Train Loss: 0.0894\n",
      "Epoch [299/500], Train Loss: 0.0892\n",
      "Epoch [300/500], Train Loss: 0.0892\n",
      "Epoch [301/500], Train Loss: 0.0890\n",
      "Epoch [302/500], Train Loss: 0.0888\n",
      "Epoch [303/500], Train Loss: 0.0887\n",
      "Epoch [304/500], Train Loss: 0.0885\n",
      "Epoch [305/500], Train Loss: 0.0884\n",
      "Epoch [306/500], Train Loss: 0.0884\n",
      "Epoch [307/500], Train Loss: 0.0881\n",
      "Epoch [308/500], Train Loss: 0.0880\n",
      "Epoch [309/500], Train Loss: 0.0879\n",
      "Epoch [310/500], Train Loss: 0.0878\n",
      "Epoch [311/500], Train Loss: 0.0877\n",
      "Epoch [312/500], Train Loss: 0.0876\n",
      "Epoch [313/500], Train Loss: 0.0875\n",
      "Epoch [314/500], Train Loss: 0.0873\n",
      "Epoch [315/500], Train Loss: 0.0873\n",
      "Epoch [316/500], Train Loss: 0.0870\n",
      "Epoch [317/500], Train Loss: 0.0869\n",
      "Epoch [318/500], Train Loss: 0.0868\n",
      "Epoch [319/500], Train Loss: 0.0867\n",
      "Epoch [320/500], Train Loss: 0.0866\n",
      "Epoch [321/500], Train Loss: 0.0866\n",
      "Epoch [322/500], Train Loss: 0.0863\n",
      "Epoch [323/500], Train Loss: 0.0861\n",
      "Epoch [324/500], Train Loss: 0.0860\n",
      "Epoch [325/500], Train Loss: 0.0860\n",
      "Epoch [326/500], Train Loss: 0.0858\n",
      "Epoch [327/500], Train Loss: 0.0857\n",
      "Epoch [328/500], Train Loss: 0.0856\n",
      "Epoch [329/500], Train Loss: 0.0855\n",
      "Epoch [330/500], Train Loss: 0.0853\n",
      "Epoch [331/500], Train Loss: 0.0853\n",
      "Epoch [332/500], Train Loss: 0.0851\n",
      "Epoch [333/500], Train Loss: 0.0849\n",
      "Epoch [334/500], Train Loss: 0.0849\n",
      "Epoch [335/500], Train Loss: 0.0847\n",
      "Epoch [336/500], Train Loss: 0.0846\n",
      "Epoch [337/500], Train Loss: 0.0846\n",
      "Epoch [338/500], Train Loss: 0.0845\n",
      "Epoch [339/500], Train Loss: 0.0844\n",
      "Epoch [340/500], Train Loss: 0.0842\n",
      "Epoch [341/500], Train Loss: 0.0841\n",
      "Epoch [342/500], Train Loss: 0.0839\n",
      "Epoch [343/500], Train Loss: 0.0839\n",
      "Epoch [344/500], Train Loss: 0.0838\n",
      "Epoch [345/500], Train Loss: 0.0837\n",
      "Epoch [346/500], Train Loss: 0.0836\n",
      "Epoch [347/500], Train Loss: 0.0834\n",
      "Epoch [348/500], Train Loss: 0.0833\n",
      "Epoch [349/500], Train Loss: 0.0832\n",
      "Epoch [350/500], Train Loss: 0.0831\n",
      "Epoch [351/500], Train Loss: 0.0831\n",
      "Epoch [352/500], Train Loss: 0.0830\n",
      "Epoch [353/500], Train Loss: 0.0830\n",
      "Epoch [354/500], Train Loss: 0.0827\n",
      "Epoch [355/500], Train Loss: 0.0826\n",
      "Epoch [356/500], Train Loss: 0.0825\n",
      "Epoch [357/500], Train Loss: 0.0823\n",
      "Epoch [358/500], Train Loss: 0.0823\n",
      "Epoch [359/500], Train Loss: 0.0822\n",
      "Epoch [360/500], Train Loss: 0.0820\n",
      "Epoch [361/500], Train Loss: 0.0819\n",
      "Epoch [362/500], Train Loss: 0.0819\n",
      "Epoch [363/500], Train Loss: 0.0818\n",
      "Epoch [364/500], Train Loss: 0.0817\n",
      "Epoch [365/500], Train Loss: 0.0816\n",
      "Epoch [366/500], Train Loss: 0.0815\n",
      "Epoch [367/500], Train Loss: 0.0814\n",
      "Epoch [368/500], Train Loss: 0.0813\n",
      "Epoch [369/500], Train Loss: 0.0812\n",
      "Epoch [370/500], Train Loss: 0.0811\n",
      "Epoch [371/500], Train Loss: 0.0811\n",
      "Epoch [372/500], Train Loss: 0.0810\n",
      "Epoch [373/500], Train Loss: 0.0808\n",
      "Epoch [374/500], Train Loss: 0.0809\n",
      "Epoch [375/500], Train Loss: 0.0806\n",
      "Epoch [376/500], Train Loss: 0.0805\n",
      "Epoch [377/500], Train Loss: 0.0805\n",
      "Epoch [378/500], Train Loss: 0.0803\n",
      "Epoch [379/500], Train Loss: 0.0802\n",
      "Epoch [380/500], Train Loss: 0.0801\n",
      "Epoch [381/500], Train Loss: 0.0800\n",
      "Epoch [382/500], Train Loss: 0.0799\n",
      "Epoch [383/500], Train Loss: 0.0799\n",
      "Epoch [384/500], Train Loss: 0.0798\n",
      "Epoch [385/500], Train Loss: 0.0797\n",
      "Epoch [386/500], Train Loss: 0.0795\n",
      "Epoch [387/500], Train Loss: 0.0794\n",
      "Epoch [388/500], Train Loss: 0.0793\n",
      "Epoch [389/500], Train Loss: 0.0793\n",
      "Epoch [390/500], Train Loss: 0.0792\n",
      "Epoch [391/500], Train Loss: 0.0791\n",
      "Epoch [392/500], Train Loss: 0.0791\n",
      "Epoch [393/500], Train Loss: 0.0790\n",
      "Epoch [394/500], Train Loss: 0.0789\n",
      "Epoch [395/500], Train Loss: 0.0787\n",
      "Epoch [396/500], Train Loss: 0.0786\n",
      "Epoch [397/500], Train Loss: 0.0787\n",
      "Epoch [398/500], Train Loss: 0.0787\n",
      "Epoch [399/500], Train Loss: 0.0784\n",
      "Epoch [400/500], Train Loss: 0.0784\n",
      "Epoch [401/500], Train Loss: 0.0782\n",
      "Epoch [402/500], Train Loss: 0.0782\n",
      "Epoch [403/500], Train Loss: 0.0781\n",
      "Epoch [404/500], Train Loss: 0.0780\n",
      "Epoch [405/500], Train Loss: 0.0779\n",
      "Epoch [406/500], Train Loss: 0.0779\n",
      "Epoch [407/500], Train Loss: 0.0777\n",
      "Epoch [408/500], Train Loss: 0.0777\n",
      "Epoch [409/500], Train Loss: 0.0776\n",
      "Epoch [410/500], Train Loss: 0.0775\n",
      "Epoch [411/500], Train Loss: 0.0775\n",
      "Epoch [412/500], Train Loss: 0.0774\n",
      "Epoch [413/500], Train Loss: 0.0774\n",
      "Epoch [414/500], Train Loss: 0.0772\n",
      "Epoch [415/500], Train Loss: 0.0770\n",
      "Epoch [416/500], Train Loss: 0.0770\n",
      "Epoch [417/500], Train Loss: 0.0768\n",
      "Epoch [418/500], Train Loss: 0.0767\n",
      "Epoch [419/500], Train Loss: 0.0767\n",
      "Epoch [420/500], Train Loss: 0.0766\n",
      "Epoch [421/500], Train Loss: 0.0765\n",
      "Epoch [422/500], Train Loss: 0.0765\n",
      "Epoch [423/500], Train Loss: 0.0764\n",
      "Epoch [424/500], Train Loss: 0.0763\n",
      "Epoch [425/500], Train Loss: 0.0763\n",
      "Epoch [426/500], Train Loss: 0.0762\n",
      "Epoch [427/500], Train Loss: 0.0761\n",
      "Epoch [428/500], Train Loss: 0.0761\n",
      "Epoch [429/500], Train Loss: 0.0758\n",
      "Epoch [430/500], Train Loss: 0.0758\n",
      "Epoch [431/500], Train Loss: 0.0757\n",
      "Epoch [432/500], Train Loss: 0.0757\n",
      "Epoch [433/500], Train Loss: 0.0755\n",
      "Epoch [434/500], Train Loss: 0.0755\n",
      "Epoch [435/500], Train Loss: 0.0754\n",
      "Epoch [436/500], Train Loss: 0.0753\n",
      "Epoch [437/500], Train Loss: 0.0753\n",
      "Epoch [438/500], Train Loss: 0.0751\n",
      "Epoch [439/500], Train Loss: 0.0751\n",
      "Epoch [440/500], Train Loss: 0.0751\n",
      "Epoch [441/500], Train Loss: 0.0750\n",
      "Epoch [442/500], Train Loss: 0.0749\n",
      "Epoch [443/500], Train Loss: 0.0749\n",
      "Epoch [444/500], Train Loss: 0.0749\n",
      "Epoch [445/500], Train Loss: 0.0747\n",
      "Epoch [446/500], Train Loss: 0.0747\n",
      "Epoch [447/500], Train Loss: 0.0745\n",
      "Epoch [448/500], Train Loss: 0.0745\n",
      "Epoch [449/500], Train Loss: 0.0743\n",
      "Epoch [450/500], Train Loss: 0.0743\n",
      "Epoch [451/500], Train Loss: 0.0742\n",
      "Epoch [452/500], Train Loss: 0.0742\n",
      "Epoch [453/500], Train Loss: 0.0742\n",
      "Epoch [454/500], Train Loss: 0.0741\n",
      "Epoch [455/500], Train Loss: 0.0741\n",
      "Epoch [456/500], Train Loss: 0.0739\n",
      "Epoch [457/500], Train Loss: 0.0738\n",
      "Epoch [458/500], Train Loss: 0.0736\n",
      "Epoch [459/500], Train Loss: 0.0737\n",
      "Epoch [460/500], Train Loss: 0.0736\n",
      "Epoch [461/500], Train Loss: 0.0736\n",
      "Epoch [462/500], Train Loss: 0.0735\n",
      "Epoch [463/500], Train Loss: 0.0734\n",
      "Epoch [464/500], Train Loss: 0.0733\n",
      "Epoch [465/500], Train Loss: 0.0733\n",
      "Epoch [466/500], Train Loss: 0.0732\n",
      "Epoch [467/500], Train Loss: 0.0731\n",
      "Epoch [468/500], Train Loss: 0.0731\n",
      "Epoch [469/500], Train Loss: 0.0731\n",
      "Epoch [470/500], Train Loss: 0.0730\n",
      "Epoch [471/500], Train Loss: 0.0728\n",
      "Epoch [472/500], Train Loss: 0.0728\n",
      "Epoch [473/500], Train Loss: 0.0726\n",
      "Epoch [474/500], Train Loss: 0.0725\n",
      "Epoch [475/500], Train Loss: 0.0726\n",
      "Epoch [476/500], Train Loss: 0.0725\n",
      "Epoch [477/500], Train Loss: 0.0724\n",
      "Epoch [478/500], Train Loss: 0.0724\n",
      "Epoch [479/500], Train Loss: 0.0722\n",
      "Epoch [480/500], Train Loss: 0.0722\n",
      "Epoch [481/500], Train Loss: 0.0722\n",
      "Epoch [482/500], Train Loss: 0.0722\n",
      "Epoch [483/500], Train Loss: 0.0721\n",
      "Epoch [484/500], Train Loss: 0.0720\n",
      "Epoch [485/500], Train Loss: 0.0720\n",
      "Epoch [486/500], Train Loss: 0.0719\n",
      "Epoch [487/500], Train Loss: 0.0719\n",
      "Epoch [488/500], Train Loss: 0.0718\n",
      "Epoch [489/500], Train Loss: 0.0718\n",
      "Epoch [490/500], Train Loss: 0.0716\n",
      "Epoch [491/500], Train Loss: 0.0715\n",
      "Epoch [492/500], Train Loss: 0.0715\n",
      "Epoch [493/500], Train Loss: 0.0714\n",
      "Epoch [494/500], Train Loss: 0.0715\n",
      "Epoch [495/500], Train Loss: 0.0713\n",
      "Epoch [496/500], Train Loss: 0.0712\n",
      "Epoch [497/500], Train Loss: 0.0712\n",
      "Epoch [498/500], Train Loss: 0.0711\n",
      "Epoch [499/500], Train Loss: 0.0710\n",
      "Epoch [500/500], Train Loss: 0.0710\n",
      "Final model saved at: ../../../Data/model_results/logistic_regression/models/chr12/final_model_chr12.pth\n",
      "Individual R^2 scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr12/individual_r2_scores_chr12.csv\n",
      "Individual IQS scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr12/individual_iqs_scores_chr12.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-18 12:20:07,502] A new study created in RDB with name: chr13_study\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual AUC ROC curves saved in: ../../../Data/model_results/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../../Data/model_results/logistic_regression/csv_files/performance_metrics.csv\n",
      "Total SNPs:  304\n",
      "PRS313 SNPs:  10\n",
      "Total SNPs used for Training:  294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-06-18 12:23:09,875] Trial 6 finished with value: 0.35048297941684725 and parameters: {'learning_rate': 0.0018807895726354535, 'l1_coef': 0.09610981970513735, 'patience': 12, 'batch_size': 256}. Best is trial 6 with value: 0.35048297941684725.\n",
      "[I 2024-06-18 12:23:10,417] Trial 3 finished with value: 0.16070269346237182 and parameters: {'learning_rate': 0.007066957719541967, 'l1_coef': 0.00031266809480140994, 'patience': 19, 'batch_size': 256}. Best is trial 3 with value: 0.16070269346237182.\n",
      "[I 2024-06-18 12:23:48,639] Trial 8 finished with value: 0.2476855754852295 and parameters: {'learning_rate': 0.0021282199415676595, 'l1_coef': 0.0034180019405649882, 'patience': 6, 'batch_size': 256}. Best is trial 3 with value: 0.16070269346237182.\n",
      "[I 2024-06-18 12:24:00,818] Trial 7 finished with value: 0.32873142063617705 and parameters: {'learning_rate': 0.0013510730298077918, 'l1_coef': 0.019459737746326176, 'patience': 10, 'batch_size': 128}. Best is trial 3 with value: 0.16070269346237182.\n",
      "[I 2024-06-18 12:24:01,384] Trial 0 finished with value: 0.3257980227470398 and parameters: {'learning_rate': 0.0009920622185746295, 'l1_coef': 0.047448487864705596, 'patience': 12, 'batch_size': 32}. Best is trial 3 with value: 0.16070269346237182.\n",
      "[I 2024-06-18 12:24:07,800] Trial 9 finished with value: 0.35605916678905486 and parameters: {'learning_rate': 0.00023305778130297222, 'l1_coef': 0.0039059596652582505, 'patience': 15, 'batch_size': 256}. Best is trial 3 with value: 0.16070269346237182.\n",
      "[I 2024-06-18 12:24:11,171] Trial 1 finished with value: 0.227725949883461 and parameters: {'learning_rate': 0.0009935972850224468, 'l1_coef': 0.002527574437256015, 'patience': 13, 'batch_size': 128}. Best is trial 3 with value: 0.16070269346237182.\n",
      "[I 2024-06-18 12:24:17,619] Trial 5 finished with value: 0.11354755035468511 and parameters: {'learning_rate': 0.000279730036447635, 'l1_coef': 1.9175629848811384e-05, 'patience': 19, 'batch_size': 64}. Best is trial 5 with value: 0.11354755035468511.\n",
      "[I 2024-06-18 12:24:19,187] Trial 4 finished with value: 0.36488566739218575 and parameters: {'learning_rate': 0.00016593153543099668, 'l1_coef': 0.023190207018881658, 'patience': 20, 'batch_size': 64}. Best is trial 5 with value: 0.11354755035468511.\n",
      "[I 2024-06-18 12:24:22,391] Trial 2 finished with value: 0.16221560262716733 and parameters: {'learning_rate': 0.05460487163262344, 'l1_coef': 0.0005817761165676465, 'patience': 12, 'batch_size': 32}. Best is trial 5 with value: 0.11354755035468511.\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 13 - Best hyperparameters: {'learning_rate': 0.000279730036447635, 'l1_coef': 1.9175629848811384e-05, 'patience': 19, 'batch_size': 64}\n",
      "Chr 13 - Best value: 0.1135\n",
      "Epoch [1/500], Train Loss: 0.5776\n",
      "Epoch [2/500], Train Loss: 0.4336\n",
      "Epoch [3/500], Train Loss: 0.3766\n",
      "Epoch [4/500], Train Loss: 0.3506\n",
      "Epoch [5/500], Train Loss: 0.3340\n",
      "Epoch [6/500], Train Loss: 0.3234\n",
      "Epoch [7/500], Train Loss: 0.3168\n",
      "Epoch [8/500], Train Loss: 0.3095\n",
      "Epoch [9/500], Train Loss: 0.3041\n",
      "Epoch [10/500], Train Loss: 0.2997\n",
      "Epoch [11/500], Train Loss: 0.2950\n",
      "Epoch [12/500], Train Loss: 0.2911\n",
      "Epoch [13/500], Train Loss: 0.2892\n",
      "Epoch [14/500], Train Loss: 0.2863\n",
      "Epoch [15/500], Train Loss: 0.2821\n",
      "Epoch [16/500], Train Loss: 0.2801\n",
      "Epoch [17/500], Train Loss: 0.2773\n",
      "Epoch [18/500], Train Loss: 0.2746\n",
      "Epoch [19/500], Train Loss: 0.2716\n",
      "Epoch [20/500], Train Loss: 0.2704\n",
      "Epoch [21/500], Train Loss: 0.2688\n",
      "Epoch [22/500], Train Loss: 0.2652\n",
      "Epoch [23/500], Train Loss: 0.2635\n",
      "Epoch [24/500], Train Loss: 0.2610\n",
      "Epoch [25/500], Train Loss: 0.2596\n",
      "Epoch [26/500], Train Loss: 0.2584\n",
      "Epoch [27/500], Train Loss: 0.2555\n",
      "Epoch [28/500], Train Loss: 0.2541\n",
      "Epoch [29/500], Train Loss: 0.2526\n",
      "Epoch [30/500], Train Loss: 0.2510\n",
      "Epoch [31/500], Train Loss: 0.2488\n",
      "Epoch [32/500], Train Loss: 0.2479\n",
      "Epoch [33/500], Train Loss: 0.2473\n",
      "Epoch [34/500], Train Loss: 0.2440\n",
      "Epoch [35/500], Train Loss: 0.2437\n",
      "Epoch [36/500], Train Loss: 0.2414\n",
      "Epoch [37/500], Train Loss: 0.2408\n",
      "Epoch [38/500], Train Loss: 0.2392\n",
      "Epoch [39/500], Train Loss: 0.2369\n",
      "Epoch [40/500], Train Loss: 0.2358\n",
      "Epoch [41/500], Train Loss: 0.2343\n",
      "Epoch [42/500], Train Loss: 0.2337\n",
      "Epoch [43/500], Train Loss: 0.2316\n",
      "Epoch [44/500], Train Loss: 0.2306\n",
      "Epoch [45/500], Train Loss: 0.2284\n",
      "Epoch [46/500], Train Loss: 0.2278\n",
      "Epoch [47/500], Train Loss: 0.2259\n",
      "Epoch [48/500], Train Loss: 0.2253\n",
      "Epoch [49/500], Train Loss: 0.2250\n",
      "Epoch [50/500], Train Loss: 0.2238\n",
      "Epoch [51/500], Train Loss: 0.2219\n",
      "Epoch [52/500], Train Loss: 0.2213\n",
      "Epoch [53/500], Train Loss: 0.2192\n",
      "Epoch [54/500], Train Loss: 0.2189\n",
      "Epoch [55/500], Train Loss: 0.2174\n",
      "Epoch [56/500], Train Loss: 0.2165\n",
      "Epoch [57/500], Train Loss: 0.2145\n",
      "Epoch [58/500], Train Loss: 0.2138\n",
      "Epoch [59/500], Train Loss: 0.2125\n",
      "Epoch [60/500], Train Loss: 0.2134\n",
      "Epoch [61/500], Train Loss: 0.2124\n",
      "Epoch [62/500], Train Loss: 0.2106\n",
      "Epoch [63/500], Train Loss: 0.2104\n",
      "Epoch [64/500], Train Loss: 0.2091\n",
      "Epoch [65/500], Train Loss: 0.2085\n",
      "Epoch [66/500], Train Loss: 0.2060\n",
      "Epoch [67/500], Train Loss: 0.2058\n",
      "Epoch [68/500], Train Loss: 0.2059\n",
      "Epoch [69/500], Train Loss: 0.2030\n",
      "Epoch [70/500], Train Loss: 0.2024\n",
      "Epoch [71/500], Train Loss: 0.2027\n",
      "Epoch [72/500], Train Loss: 0.2006\n",
      "Epoch [73/500], Train Loss: 0.1993\n",
      "Epoch [74/500], Train Loss: 0.1990\n",
      "Epoch [75/500], Train Loss: 0.1978\n",
      "Epoch [76/500], Train Loss: 0.1969\n",
      "Epoch [77/500], Train Loss: 0.1968\n",
      "Epoch [78/500], Train Loss: 0.1956\n",
      "Epoch [79/500], Train Loss: 0.1955\n",
      "Epoch [80/500], Train Loss: 0.1952\n",
      "Epoch [81/500], Train Loss: 0.1930\n",
      "Epoch [82/500], Train Loss: 0.1933\n",
      "Epoch [83/500], Train Loss: 0.1924\n",
      "Epoch [84/500], Train Loss: 0.1905\n",
      "Epoch [85/500], Train Loss: 0.1899\n",
      "Epoch [86/500], Train Loss: 0.1894\n",
      "Epoch [87/500], Train Loss: 0.1887\n",
      "Epoch [88/500], Train Loss: 0.1879\n",
      "Epoch [89/500], Train Loss: 0.1870\n",
      "Epoch [90/500], Train Loss: 0.1870\n",
      "Epoch [91/500], Train Loss: 0.1863\n",
      "Epoch [92/500], Train Loss: 0.1853\n",
      "Epoch [93/500], Train Loss: 0.1842\n",
      "Epoch [94/500], Train Loss: 0.1834\n",
      "Epoch [95/500], Train Loss: 0.1834\n",
      "Epoch [96/500], Train Loss: 0.1830\n",
      "Epoch [97/500], Train Loss: 0.1822\n",
      "Epoch [98/500], Train Loss: 0.1816\n",
      "Epoch [99/500], Train Loss: 0.1810\n",
      "Epoch [100/500], Train Loss: 0.1799\n",
      "Epoch [101/500], Train Loss: 0.1793\n",
      "Epoch [102/500], Train Loss: 0.1790\n",
      "Epoch [103/500], Train Loss: 0.1777\n",
      "Epoch [104/500], Train Loss: 0.1780\n",
      "Epoch [105/500], Train Loss: 0.1761\n",
      "Epoch [106/500], Train Loss: 0.1773\n",
      "Epoch [107/500], Train Loss: 0.1754\n",
      "Epoch [108/500], Train Loss: 0.1748\n",
      "Epoch [109/500], Train Loss: 0.1742\n",
      "Epoch [110/500], Train Loss: 0.1731\n",
      "Epoch [111/500], Train Loss: 0.1730\n",
      "Epoch [112/500], Train Loss: 0.1721\n",
      "Epoch [113/500], Train Loss: 0.1726\n",
      "Epoch [114/500], Train Loss: 0.1718\n",
      "Epoch [115/500], Train Loss: 0.1704\n",
      "Epoch [116/500], Train Loss: 0.1702\n",
      "Epoch [117/500], Train Loss: 0.1701\n",
      "Epoch [118/500], Train Loss: 0.1688\n",
      "Epoch [119/500], Train Loss: 0.1683\n",
      "Epoch [120/500], Train Loss: 0.1680\n",
      "Epoch [121/500], Train Loss: 0.1680\n",
      "Epoch [122/500], Train Loss: 0.1677\n",
      "Epoch [123/500], Train Loss: 0.1669\n",
      "Epoch [124/500], Train Loss: 0.1655\n",
      "Epoch [125/500], Train Loss: 0.1658\n",
      "Epoch [126/500], Train Loss: 0.1646\n",
      "Epoch [127/500], Train Loss: 0.1639\n",
      "Epoch [128/500], Train Loss: 0.1641\n",
      "Epoch [129/500], Train Loss: 0.1633\n",
      "Epoch [130/500], Train Loss: 0.1631\n",
      "Epoch [131/500], Train Loss: 0.1623\n",
      "Epoch [132/500], Train Loss: 0.1620\n",
      "Epoch [133/500], Train Loss: 0.1606\n",
      "Epoch [134/500], Train Loss: 0.1610\n",
      "Epoch [135/500], Train Loss: 0.1604\n",
      "Epoch [136/500], Train Loss: 0.1592\n",
      "Epoch [137/500], Train Loss: 0.1593\n",
      "Epoch [138/500], Train Loss: 0.1589\n",
      "Epoch [139/500], Train Loss: 0.1583\n",
      "Epoch [140/500], Train Loss: 0.1578\n",
      "Epoch [141/500], Train Loss: 0.1575\n",
      "Epoch [142/500], Train Loss: 0.1578\n",
      "Epoch [143/500], Train Loss: 0.1569\n",
      "Epoch [144/500], Train Loss: 0.1566\n",
      "Epoch [145/500], Train Loss: 0.1567\n",
      "Epoch [146/500], Train Loss: 0.1551\n",
      "Epoch [147/500], Train Loss: 0.1550\n",
      "Epoch [148/500], Train Loss: 0.1545\n",
      "Epoch [149/500], Train Loss: 0.1544\n",
      "Epoch [150/500], Train Loss: 0.1535\n",
      "Epoch [151/500], Train Loss: 0.1538\n",
      "Epoch [152/500], Train Loss: 0.1525\n",
      "Epoch [153/500], Train Loss: 0.1513\n",
      "Epoch [154/500], Train Loss: 0.1530\n",
      "Epoch [155/500], Train Loss: 0.1512\n",
      "Epoch [156/500], Train Loss: 0.1507\n",
      "Epoch [157/500], Train Loss: 0.1512\n",
      "Epoch [158/500], Train Loss: 0.1507\n",
      "Epoch [159/500], Train Loss: 0.1500\n",
      "Epoch [160/500], Train Loss: 0.1498\n",
      "Epoch [161/500], Train Loss: 0.1490\n",
      "Epoch [162/500], Train Loss: 0.1490\n",
      "Epoch [163/500], Train Loss: 0.1489\n",
      "Epoch [164/500], Train Loss: 0.1481\n",
      "Epoch [165/500], Train Loss: 0.1474\n",
      "Epoch [166/500], Train Loss: 0.1478\n",
      "Epoch [167/500], Train Loss: 0.1476\n",
      "Epoch [168/500], Train Loss: 0.1460\n",
      "Epoch [169/500], Train Loss: 0.1481\n",
      "Epoch [170/500], Train Loss: 0.1462\n",
      "Epoch [171/500], Train Loss: 0.1461\n",
      "Epoch [172/500], Train Loss: 0.1454\n",
      "Epoch [173/500], Train Loss: 0.1449\n",
      "Epoch [174/500], Train Loss: 0.1448\n",
      "Epoch [175/500], Train Loss: 0.1441\n",
      "Epoch [176/500], Train Loss: 0.1437\n",
      "Epoch [177/500], Train Loss: 0.1432\n",
      "Epoch [178/500], Train Loss: 0.1432\n",
      "Epoch [179/500], Train Loss: 0.1424\n",
      "Epoch [180/500], Train Loss: 0.1424\n",
      "Epoch [181/500], Train Loss: 0.1425\n",
      "Epoch [182/500], Train Loss: 0.1421\n",
      "Epoch [183/500], Train Loss: 0.1413\n",
      "Epoch [184/500], Train Loss: 0.1411\n",
      "Epoch [185/500], Train Loss: 0.1407\n",
      "Epoch [186/500], Train Loss: 0.1414\n",
      "Epoch [187/500], Train Loss: 0.1398\n",
      "Epoch [188/500], Train Loss: 0.1401\n",
      "Epoch [189/500], Train Loss: 0.1396\n",
      "Epoch [190/500], Train Loss: 0.1396\n",
      "Epoch [191/500], Train Loss: 0.1397\n",
      "Epoch [192/500], Train Loss: 0.1392\n",
      "Epoch [193/500], Train Loss: 0.1386\n",
      "Epoch [194/500], Train Loss: 0.1382\n",
      "Epoch [195/500], Train Loss: 0.1384\n",
      "Epoch [196/500], Train Loss: 0.1379\n",
      "Epoch [197/500], Train Loss: 0.1375\n",
      "Epoch [198/500], Train Loss: 0.1373\n",
      "Epoch [199/500], Train Loss: 0.1374\n",
      "Epoch [200/500], Train Loss: 0.1366\n",
      "Epoch [201/500], Train Loss: 0.1363\n",
      "Epoch [202/500], Train Loss: 0.1358\n",
      "Epoch [203/500], Train Loss: 0.1355\n",
      "Epoch [204/500], Train Loss: 0.1355\n",
      "Epoch [205/500], Train Loss: 0.1356\n",
      "Epoch [206/500], Train Loss: 0.1346\n",
      "Epoch [207/500], Train Loss: 0.1350\n",
      "Epoch [208/500], Train Loss: 0.1335\n",
      "Epoch [209/500], Train Loss: 0.1350\n",
      "Epoch [210/500], Train Loss: 0.1344\n",
      "Epoch [211/500], Train Loss: 0.1333\n",
      "Epoch [212/500], Train Loss: 0.1332\n",
      "Epoch [213/500], Train Loss: 0.1328\n",
      "Epoch [214/500], Train Loss: 0.1321\n",
      "Epoch [215/500], Train Loss: 0.1327\n",
      "Epoch [216/500], Train Loss: 0.1322\n",
      "Epoch [217/500], Train Loss: 0.1317\n",
      "Epoch [218/500], Train Loss: 0.1318\n",
      "Epoch [219/500], Train Loss: 0.1316\n",
      "Epoch [220/500], Train Loss: 0.1314\n",
      "Epoch [221/500], Train Loss: 0.1314\n",
      "Epoch [222/500], Train Loss: 0.1307\n",
      "Epoch [223/500], Train Loss: 0.1310\n",
      "Epoch [224/500], Train Loss: 0.1296\n",
      "Epoch [225/500], Train Loss: 0.1301\n",
      "Epoch [226/500], Train Loss: 0.1294\n",
      "Epoch [227/500], Train Loss: 0.1296\n",
      "Epoch [228/500], Train Loss: 0.1290\n",
      "Epoch [229/500], Train Loss: 0.1291\n",
      "Epoch [230/500], Train Loss: 0.1293\n",
      "Epoch [231/500], Train Loss: 0.1287\n",
      "Epoch [232/500], Train Loss: 0.1280\n",
      "Epoch [233/500], Train Loss: 0.1285\n",
      "Epoch [234/500], Train Loss: 0.1282\n",
      "Epoch [235/500], Train Loss: 0.1285\n",
      "Epoch [236/500], Train Loss: 0.1275\n",
      "Epoch [237/500], Train Loss: 0.1281\n",
      "Epoch [238/500], Train Loss: 0.1272\n",
      "Epoch [239/500], Train Loss: 0.1266\n",
      "Epoch [240/500], Train Loss: 0.1266\n",
      "Epoch [241/500], Train Loss: 0.1265\n",
      "Epoch [242/500], Train Loss: 0.1262\n",
      "Epoch [243/500], Train Loss: 0.1256\n",
      "Epoch [244/500], Train Loss: 0.1261\n",
      "Epoch [245/500], Train Loss: 0.1258\n",
      "Epoch [246/500], Train Loss: 0.1264\n",
      "Epoch [247/500], Train Loss: 0.1253\n",
      "Epoch [248/500], Train Loss: 0.1246\n",
      "Epoch [249/500], Train Loss: 0.1250\n",
      "Epoch [250/500], Train Loss: 0.1246\n",
      "Epoch [251/500], Train Loss: 0.1239\n",
      "Epoch [252/500], Train Loss: 0.1238\n",
      "Epoch [253/500], Train Loss: 0.1238\n",
      "Epoch [254/500], Train Loss: 0.1232\n",
      "Epoch [255/500], Train Loss: 0.1234\n",
      "Epoch [256/500], Train Loss: 0.1228\n",
      "Epoch [257/500], Train Loss: 0.1226\n",
      "Epoch [258/500], Train Loss: 0.1227\n",
      "Epoch [259/500], Train Loss: 0.1232\n",
      "Epoch [260/500], Train Loss: 0.1221\n",
      "Epoch [261/500], Train Loss: 0.1218\n",
      "Epoch [262/500], Train Loss: 0.1224\n",
      "Epoch [263/500], Train Loss: 0.1217\n",
      "Epoch [264/500], Train Loss: 0.1221\n",
      "Epoch [265/500], Train Loss: 0.1217\n",
      "Epoch [266/500], Train Loss: 0.1209\n",
      "Epoch [267/500], Train Loss: 0.1211\n",
      "Epoch [268/500], Train Loss: 0.1213\n",
      "Epoch [269/500], Train Loss: 0.1211\n",
      "Epoch [270/500], Train Loss: 0.1211\n",
      "Epoch [271/500], Train Loss: 0.1209\n",
      "Epoch [272/500], Train Loss: 0.1202\n",
      "Epoch [273/500], Train Loss: 0.1201\n",
      "Epoch [274/500], Train Loss: 0.1194\n",
      "Epoch [275/500], Train Loss: 0.1197\n",
      "Epoch [276/500], Train Loss: 0.1206\n",
      "Epoch [277/500], Train Loss: 0.1191\n",
      "Epoch [278/500], Train Loss: 0.1191\n",
      "Epoch [279/500], Train Loss: 0.1196\n",
      "Epoch [280/500], Train Loss: 0.1187\n",
      "Epoch [281/500], Train Loss: 0.1185\n",
      "Epoch [282/500], Train Loss: 0.1182\n",
      "Epoch [283/500], Train Loss: 0.1178\n",
      "Epoch [284/500], Train Loss: 0.1182\n",
      "Epoch [285/500], Train Loss: 0.1175\n",
      "Epoch [286/500], Train Loss: 0.1182\n",
      "Epoch [287/500], Train Loss: 0.1181\n",
      "Epoch [288/500], Train Loss: 0.1170\n",
      "Epoch [289/500], Train Loss: 0.1176\n",
      "Epoch [290/500], Train Loss: 0.1169\n",
      "Epoch [291/500], Train Loss: 0.1168\n",
      "Epoch [292/500], Train Loss: 0.1175\n",
      "Epoch [293/500], Train Loss: 0.1166\n",
      "Epoch [294/500], Train Loss: 0.1163\n",
      "Epoch [295/500], Train Loss: 0.1163\n",
      "Epoch [296/500], Train Loss: 0.1157\n",
      "Epoch [297/500], Train Loss: 0.1165\n",
      "Epoch [298/500], Train Loss: 0.1164\n",
      "Epoch [299/500], Train Loss: 0.1176\n",
      "Epoch [300/500], Train Loss: 0.1155\n",
      "Epoch [301/500], Train Loss: 0.1160\n",
      "Epoch [302/500], Train Loss: 0.1156\n",
      "Epoch [303/500], Train Loss: 0.1145\n",
      "Epoch [304/500], Train Loss: 0.1148\n",
      "Epoch [305/500], Train Loss: 0.1155\n",
      "Epoch [306/500], Train Loss: 0.1144\n",
      "Epoch [307/500], Train Loss: 0.1143\n",
      "Epoch [308/500], Train Loss: 0.1147\n",
      "Epoch [309/500], Train Loss: 0.1140\n",
      "Epoch [310/500], Train Loss: 0.1145\n",
      "Epoch [311/500], Train Loss: 0.1138\n",
      "Epoch [312/500], Train Loss: 0.1140\n",
      "Epoch [313/500], Train Loss: 0.1138\n",
      "Epoch [314/500], Train Loss: 0.1136\n",
      "Epoch [315/500], Train Loss: 0.1135\n",
      "Epoch [316/500], Train Loss: 0.1131\n",
      "Epoch [317/500], Train Loss: 0.1138\n",
      "Epoch [318/500], Train Loss: 0.1137\n",
      "Epoch [319/500], Train Loss: 0.1127\n",
      "Epoch [320/500], Train Loss: 0.1126\n",
      "Epoch [321/500], Train Loss: 0.1123\n",
      "Epoch [322/500], Train Loss: 0.1121\n",
      "Epoch [323/500], Train Loss: 0.1125\n",
      "Epoch [324/500], Train Loss: 0.1123\n",
      "Epoch [325/500], Train Loss: 0.1122\n",
      "Epoch [326/500], Train Loss: 0.1120\n",
      "Epoch [327/500], Train Loss: 0.1127\n",
      "Epoch [328/500], Train Loss: 0.1116\n",
      "Epoch [329/500], Train Loss: 0.1114\n",
      "Epoch [330/500], Train Loss: 0.1114\n",
      "Epoch [331/500], Train Loss: 0.1118\n",
      "Epoch [332/500], Train Loss: 0.1110\n",
      "Epoch [333/500], Train Loss: 0.1109\n",
      "Epoch [334/500], Train Loss: 0.1106\n",
      "Epoch [335/500], Train Loss: 0.1102\n",
      "Epoch [336/500], Train Loss: 0.1107\n",
      "Epoch [337/500], Train Loss: 0.1102\n",
      "Epoch [338/500], Train Loss: 0.1104\n",
      "Epoch [339/500], Train Loss: 0.1106\n",
      "Epoch [340/500], Train Loss: 0.1103\n",
      "Epoch [341/500], Train Loss: 0.1102\n",
      "Epoch [342/500], Train Loss: 0.1095\n",
      "Epoch [343/500], Train Loss: 0.1090\n",
      "Epoch [344/500], Train Loss: 0.1091\n",
      "Epoch [345/500], Train Loss: 0.1089\n",
      "Epoch [346/500], Train Loss: 0.1098\n",
      "Epoch [347/500], Train Loss: 0.1085\n",
      "Epoch [348/500], Train Loss: 0.1093\n",
      "Epoch [349/500], Train Loss: 0.1091\n",
      "Epoch [350/500], Train Loss: 0.1086\n",
      "Epoch [351/500], Train Loss: 0.1083\n",
      "Epoch [352/500], Train Loss: 0.1089\n",
      "Epoch [353/500], Train Loss: 0.1088\n",
      "Epoch [354/500], Train Loss: 0.1090\n",
      "Epoch [355/500], Train Loss: 0.1083\n",
      "Epoch [356/500], Train Loss: 0.1088\n",
      "Epoch [357/500], Train Loss: 0.1077\n",
      "Epoch [358/500], Train Loss: 0.1078\n",
      "Epoch [359/500], Train Loss: 0.1079\n",
      "Epoch [360/500], Train Loss: 0.1075\n",
      "Epoch [361/500], Train Loss: 0.1079\n",
      "Epoch [362/500], Train Loss: 0.1075\n",
      "Epoch [363/500], Train Loss: 0.1074\n",
      "Epoch [364/500], Train Loss: 0.1077\n",
      "Epoch [365/500], Train Loss: 0.1069\n",
      "Epoch [366/500], Train Loss: 0.1073\n",
      "Epoch [367/500], Train Loss: 0.1074\n",
      "Epoch [368/500], Train Loss: 0.1074\n",
      "Epoch [369/500], Train Loss: 0.1064\n",
      "Epoch [370/500], Train Loss: 0.1061\n",
      "Epoch [371/500], Train Loss: 0.1062\n",
      "Epoch [372/500], Train Loss: 0.1067\n",
      "Epoch [373/500], Train Loss: 0.1061\n",
      "Epoch [374/500], Train Loss: 0.1057\n",
      "Epoch [375/500], Train Loss: 0.1059\n",
      "Epoch [376/500], Train Loss: 0.1064\n",
      "Epoch [377/500], Train Loss: 0.1059\n",
      "Epoch [378/500], Train Loss: 0.1058\n",
      "Epoch [379/500], Train Loss: 0.1056\n",
      "Epoch [380/500], Train Loss: 0.1058\n",
      "Epoch [381/500], Train Loss: 0.1058\n",
      "Epoch [382/500], Train Loss: 0.1050\n",
      "Epoch [383/500], Train Loss: 0.1056\n",
      "Epoch [384/500], Train Loss: 0.1050\n",
      "Epoch [385/500], Train Loss: 0.1045\n",
      "Epoch [386/500], Train Loss: 0.1050\n",
      "Epoch [387/500], Train Loss: 0.1046\n",
      "Epoch [388/500], Train Loss: 0.1046\n",
      "Epoch [389/500], Train Loss: 0.1045\n",
      "Epoch [390/500], Train Loss: 0.1045\n",
      "Epoch [391/500], Train Loss: 0.1044\n",
      "Epoch [392/500], Train Loss: 0.1044\n",
      "Epoch [393/500], Train Loss: 0.1034\n",
      "Epoch [394/500], Train Loss: 0.1040\n",
      "Epoch [395/500], Train Loss: 0.1040\n",
      "Epoch [396/500], Train Loss: 0.1038\n",
      "Epoch [397/500], Train Loss: 0.1042\n",
      "Epoch [398/500], Train Loss: 0.1035\n",
      "Epoch [399/500], Train Loss: 0.1033\n",
      "Epoch [400/500], Train Loss: 0.1040\n",
      "Epoch [401/500], Train Loss: 0.1034\n",
      "Epoch [402/500], Train Loss: 0.1032\n",
      "Epoch [403/500], Train Loss: 0.1032\n",
      "Epoch [404/500], Train Loss: 0.1034\n",
      "Epoch [405/500], Train Loss: 0.1033\n",
      "Epoch [406/500], Train Loss: 0.1035\n",
      "Epoch [407/500], Train Loss: 0.1032\n",
      "Epoch [408/500], Train Loss: 0.1029\n",
      "Epoch [409/500], Train Loss: 0.1024\n",
      "Epoch [410/500], Train Loss: 0.1032\n",
      "Epoch [411/500], Train Loss: 0.1032\n",
      "Epoch [412/500], Train Loss: 0.1024\n",
      "Epoch [413/500], Train Loss: 0.1031\n",
      "Epoch [414/500], Train Loss: 0.1019\n",
      "Epoch [415/500], Train Loss: 0.1021\n",
      "Epoch [416/500], Train Loss: 0.1020\n",
      "Epoch [417/500], Train Loss: 0.1023\n",
      "Epoch [418/500], Train Loss: 0.1023\n",
      "Epoch [419/500], Train Loss: 0.1027\n",
      "Epoch [420/500], Train Loss: 0.1019\n",
      "Epoch [421/500], Train Loss: 0.1024\n",
      "Epoch [422/500], Train Loss: 0.1014\n",
      "Epoch [423/500], Train Loss: 0.1013\n",
      "Epoch [424/500], Train Loss: 0.1015\n",
      "Epoch [425/500], Train Loss: 0.1018\n",
      "Epoch [426/500], Train Loss: 0.1011\n",
      "Epoch [427/500], Train Loss: 0.1011\n",
      "Epoch [428/500], Train Loss: 0.1021\n",
      "Epoch [429/500], Train Loss: 0.1010\n",
      "Epoch [430/500], Train Loss: 0.1008\n",
      "Epoch [431/500], Train Loss: 0.1003\n",
      "Epoch [432/500], Train Loss: 0.1006\n",
      "Epoch [433/500], Train Loss: 0.1007\n",
      "Epoch [434/500], Train Loss: 0.1001\n",
      "Epoch [435/500], Train Loss: 0.1000\n",
      "Epoch [436/500], Train Loss: 0.1017\n",
      "Epoch [437/500], Train Loss: 0.1004\n",
      "Epoch [438/500], Train Loss: 0.0996\n",
      "Epoch [439/500], Train Loss: 0.0998\n",
      "Epoch [440/500], Train Loss: 0.0998\n",
      "Epoch [441/500], Train Loss: 0.1000\n",
      "Epoch [442/500], Train Loss: 0.1008\n",
      "Epoch [443/500], Train Loss: 0.0996\n",
      "Epoch [444/500], Train Loss: 0.0997\n",
      "Epoch [445/500], Train Loss: 0.0996\n",
      "Epoch [446/500], Train Loss: 0.0997\n",
      "Epoch [447/500], Train Loss: 0.0997\n",
      "Epoch [448/500], Train Loss: 0.1000\n",
      "Epoch [449/500], Train Loss: 0.0989\n",
      "Epoch [450/500], Train Loss: 0.0997\n",
      "Epoch [451/500], Train Loss: 0.0998\n",
      "Epoch [452/500], Train Loss: 0.0999\n",
      "Epoch [453/500], Train Loss: 0.0988\n",
      "Epoch [454/500], Train Loss: 0.1005\n",
      "Epoch [455/500], Train Loss: 0.0995\n",
      "Epoch [456/500], Train Loss: 0.0996\n",
      "Epoch [457/500], Train Loss: 0.1001\n",
      "Epoch [458/500], Train Loss: 0.0997\n",
      "Epoch [459/500], Train Loss: 0.1004\n",
      "Epoch [460/500], Train Loss: 0.0997\n",
      "Epoch [461/500], Train Loss: 0.0996\n",
      "Epoch [462/500], Train Loss: 0.0997\n",
      "Epoch [463/500], Train Loss: 0.0993\n",
      "Epoch [464/500], Train Loss: 0.0997\n",
      "Epoch [465/500], Train Loss: 0.0998\n",
      "Epoch [466/500], Train Loss: 0.0991\n",
      "Epoch [467/500], Train Loss: 0.0990\n",
      "Epoch [468/500], Train Loss: 0.0997\n",
      "Epoch [469/500], Train Loss: 0.0992\n",
      "Epoch [470/500], Train Loss: 0.0995\n",
      "Epoch [471/500], Train Loss: 0.0999\n",
      "Epoch [472/500], Train Loss: 0.0998\n",
      "Early stopping at epoch 472\n",
      "Final model saved at: ../../../Data/model_results/logistic_regression/models/chr13/final_model_chr13.pth\n",
      "Individual R^2 scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr13/individual_r2_scores_chr13.csv\n",
      "Individual IQS scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr13/individual_iqs_scores_chr13.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-18 12:24:30,589] A new study created in RDB with name: chr14_study\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual AUC ROC curves saved in: ../../../Data/model_results/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../../Data/model_results/logistic_regression/csv_files/performance_metrics.csv\n",
      "Total SNPs:  918\n",
      "PRS313 SNPs:  16\n",
      "Total SNPs used for Training:  902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-06-18 12:26:10,623] Trial 5 finished with value: 0.17179837673902512 and parameters: {'learning_rate': 0.0069193401475989074, 'l1_coef': 0.0003295158549997448, 'patience': 5, 'batch_size': 128}. Best is trial 5 with value: 0.17179837673902512.\n",
      "[I 2024-06-18 12:26:12,154] Trial 1 finished with value: 0.059962207451462746 and parameters: {'learning_rate': 0.011496963189324617, 'l1_coef': 1.5360886664405904e-05, 'patience': 13, 'batch_size': 128}. Best is trial 1 with value: 0.059962207451462746.\n",
      "[I 2024-06-18 12:26:28,062] Trial 3 finished with value: 0.07152817882597447 and parameters: {'learning_rate': 0.010230874250860153, 'l1_coef': 2.2036618579558507e-05, 'patience': 16, 'batch_size': 128}. Best is trial 1 with value: 0.059962207451462746.\n",
      "[I 2024-06-18 12:26:46,337] Trial 0 finished with value: 0.2046399235725403 and parameters: {'learning_rate': 0.0046584544168202, 'l1_coef': 0.0006339367972791854, 'patience': 8, 'batch_size': 128}. Best is trial 1 with value: 0.059962207451462746.\n",
      "[I 2024-06-18 12:26:55,011] Trial 7 finished with value: 0.5256038592411921 and parameters: {'learning_rate': 0.09826326449510074, 'l1_coef': 0.055217996085428246, 'patience': 13, 'batch_size': 32}. Best is trial 1 with value: 0.059962207451462746.\n",
      "[I 2024-06-18 12:26:59,933] Trial 6 finished with value: 0.524032187461853 and parameters: {'learning_rate': 0.0007628123058682854, 'l1_coef': 0.010371191334238729, 'patience': 18, 'batch_size': 128}. Best is trial 1 with value: 0.059962207451462746.\n",
      "[I 2024-06-18 12:27:04,045] Trial 2 finished with value: 0.17852337211370467 and parameters: {'learning_rate': 0.0004575365265223861, 'l1_coef': 0.00021340972408593183, 'patience': 7, 'batch_size': 256}. Best is trial 1 with value: 0.059962207451462746.\n",
      "[I 2024-06-18 12:27:07,788] Trial 4 finished with value: 0.20648108571767806 and parameters: {'learning_rate': 0.0006935449847318068, 'l1_coef': 0.000459652138882943, 'patience': 20, 'batch_size': 256}. Best is trial 1 with value: 0.059962207451462746.\n",
      "[I 2024-06-18 12:27:08,772] Trial 9 finished with value: 0.18759224883147646 and parameters: {'learning_rate': 0.0016970510366134874, 'l1_coef': 0.0004766081751363279, 'patience': 9, 'batch_size': 64}. Best is trial 1 with value: 0.059962207451462746.\n",
      "[I 2024-06-18 12:27:16,168] Trial 8 finished with value: 0.09955951605851834 and parameters: {'learning_rate': 0.03514222875044937, 'l1_coef': 0.00010373957089177125, 'patience': 18, 'batch_size': 32}. Best is trial 1 with value: 0.059962207451462746.\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 14 - Best hyperparameters: {'learning_rate': 0.011496963189324617, 'l1_coef': 1.5360886664405904e-05, 'patience': 13, 'batch_size': 128}\n",
      "Chr 14 - Best value: 0.0600\n",
      "Epoch [1/500], Train Loss: 0.5514\n",
      "Epoch [2/500], Train Loss: 0.3333\n",
      "Epoch [3/500], Train Loss: 0.2540\n",
      "Epoch [4/500], Train Loss: 0.2079\n",
      "Epoch [5/500], Train Loss: 0.1794\n",
      "Epoch [6/500], Train Loss: 0.1591\n",
      "Epoch [7/500], Train Loss: 0.1450\n",
      "Epoch [8/500], Train Loss: 0.1320\n",
      "Epoch [9/500], Train Loss: 0.1227\n",
      "Epoch [10/500], Train Loss: 0.1137\n",
      "Epoch [11/500], Train Loss: 0.1079\n",
      "Epoch [12/500], Train Loss: 0.1016\n",
      "Epoch [13/500], Train Loss: 0.0972\n",
      "Epoch [14/500], Train Loss: 0.0934\n",
      "Epoch [15/500], Train Loss: 0.0896\n",
      "Epoch [16/500], Train Loss: 0.0871\n",
      "Epoch [17/500], Train Loss: 0.0834\n",
      "Epoch [18/500], Train Loss: 0.0809\n",
      "Epoch [19/500], Train Loss: 0.0783\n",
      "Epoch [20/500], Train Loss: 0.0751\n",
      "Epoch [21/500], Train Loss: 0.0737\n",
      "Epoch [22/500], Train Loss: 0.0714\n",
      "Epoch [23/500], Train Loss: 0.0701\n",
      "Epoch [24/500], Train Loss: 0.0688\n",
      "Epoch [25/500], Train Loss: 0.0667\n",
      "Epoch [26/500], Train Loss: 0.0651\n",
      "Epoch [27/500], Train Loss: 0.0640\n",
      "Epoch [28/500], Train Loss: 0.0634\n",
      "Epoch [29/500], Train Loss: 0.0620\n",
      "Epoch [30/500], Train Loss: 0.0609\n",
      "Epoch [31/500], Train Loss: 0.0595\n",
      "Epoch [32/500], Train Loss: 0.0584\n",
      "Epoch [33/500], Train Loss: 0.0579\n",
      "Epoch [34/500], Train Loss: 0.0576\n",
      "Epoch [35/500], Train Loss: 0.0572\n",
      "Epoch [36/500], Train Loss: 0.0569\n",
      "Epoch [37/500], Train Loss: 0.0554\n",
      "Epoch [38/500], Train Loss: 0.0541\n",
      "Epoch [39/500], Train Loss: 0.0537\n",
      "Epoch [40/500], Train Loss: 0.0531\n",
      "Epoch [41/500], Train Loss: 0.0525\n",
      "Epoch [42/500], Train Loss: 0.0518\n",
      "Epoch [43/500], Train Loss: 0.0517\n",
      "Epoch [44/500], Train Loss: 0.0510\n",
      "Epoch [45/500], Train Loss: 0.0503\n",
      "Epoch [46/500], Train Loss: 0.0497\n",
      "Epoch [47/500], Train Loss: 0.0493\n",
      "Epoch [48/500], Train Loss: 0.0488\n",
      "Epoch [49/500], Train Loss: 0.0482\n",
      "Epoch [50/500], Train Loss: 0.0483\n",
      "Epoch [51/500], Train Loss: 0.0481\n",
      "Epoch [52/500], Train Loss: 0.0478\n",
      "Epoch [53/500], Train Loss: 0.0481\n",
      "Epoch [54/500], Train Loss: 0.0473\n",
      "Epoch [55/500], Train Loss: 0.0464\n",
      "Epoch [56/500], Train Loss: 0.0460\n",
      "Epoch [57/500], Train Loss: 0.0459\n",
      "Epoch [58/500], Train Loss: 0.0460\n",
      "Epoch [59/500], Train Loss: 0.0457\n",
      "Epoch [60/500], Train Loss: 0.0450\n",
      "Epoch [61/500], Train Loss: 0.0448\n",
      "Epoch [62/500], Train Loss: 0.0441\n",
      "Epoch [63/500], Train Loss: 0.0438\n",
      "Epoch [64/500], Train Loss: 0.0436\n",
      "Epoch [65/500], Train Loss: 0.0433\n",
      "Epoch [66/500], Train Loss: 0.0430\n",
      "Epoch [67/500], Train Loss: 0.0436\n",
      "Epoch [68/500], Train Loss: 0.0430\n",
      "Epoch [69/500], Train Loss: 0.0428\n",
      "Epoch [70/500], Train Loss: 0.0424\n",
      "Epoch [71/500], Train Loss: 0.0420\n",
      "Epoch [72/500], Train Loss: 0.0418\n",
      "Epoch [73/500], Train Loss: 0.0417\n",
      "Epoch [74/500], Train Loss: 0.0414\n",
      "Epoch [75/500], Train Loss: 0.0416\n",
      "Epoch [76/500], Train Loss: 0.0410\n",
      "Epoch [77/500], Train Loss: 0.0407\n",
      "Epoch [78/500], Train Loss: 0.0414\n",
      "Epoch [79/500], Train Loss: 0.0415\n",
      "Epoch [80/500], Train Loss: 0.0409\n",
      "Epoch [81/500], Train Loss: 0.0403\n",
      "Epoch [82/500], Train Loss: 0.0402\n",
      "Epoch [83/500], Train Loss: 0.0400\n",
      "Epoch [84/500], Train Loss: 0.0401\n",
      "Epoch [85/500], Train Loss: 0.0400\n",
      "Epoch [86/500], Train Loss: 0.0401\n",
      "Epoch [87/500], Train Loss: 0.0398\n",
      "Epoch [88/500], Train Loss: 0.0395\n",
      "Epoch [89/500], Train Loss: 0.0393\n",
      "Epoch [90/500], Train Loss: 0.0392\n",
      "Epoch [91/500], Train Loss: 0.0394\n",
      "Epoch [92/500], Train Loss: 0.0392\n",
      "Epoch [93/500], Train Loss: 0.0391\n",
      "Epoch [94/500], Train Loss: 0.0386\n",
      "Epoch [95/500], Train Loss: 0.0387\n",
      "Epoch [96/500], Train Loss: 0.0386\n",
      "Epoch [97/500], Train Loss: 0.0385\n",
      "Epoch [98/500], Train Loss: 0.0380\n",
      "Epoch [99/500], Train Loss: 0.0382\n",
      "Epoch [100/500], Train Loss: 0.0380\n",
      "Epoch [101/500], Train Loss: 0.0380\n",
      "Epoch [102/500], Train Loss: 0.0379\n",
      "Epoch [103/500], Train Loss: 0.0378\n",
      "Epoch [104/500], Train Loss: 0.0379\n",
      "Epoch [105/500], Train Loss: 0.0376\n",
      "Epoch [106/500], Train Loss: 0.0372\n",
      "Epoch [107/500], Train Loss: 0.0374\n",
      "Epoch [108/500], Train Loss: 0.0373\n",
      "Epoch [109/500], Train Loss: 0.0373\n",
      "Epoch [110/500], Train Loss: 0.0371\n",
      "Epoch [111/500], Train Loss: 0.0369\n",
      "Epoch [112/500], Train Loss: 0.0369\n",
      "Epoch [113/500], Train Loss: 0.0370\n",
      "Epoch [114/500], Train Loss: 0.0369\n",
      "Epoch [115/500], Train Loss: 0.0367\n",
      "Epoch [116/500], Train Loss: 0.0364\n",
      "Epoch [117/500], Train Loss: 0.0367\n",
      "Epoch [118/500], Train Loss: 0.0366\n",
      "Epoch [119/500], Train Loss: 0.0363\n",
      "Epoch [120/500], Train Loss: 0.0367\n",
      "Epoch [121/500], Train Loss: 0.0367\n",
      "Epoch [122/500], Train Loss: 0.0367\n",
      "Epoch [123/500], Train Loss: 0.0364\n",
      "Epoch [124/500], Train Loss: 0.0364\n",
      "Epoch [125/500], Train Loss: 0.0365\n",
      "Epoch [126/500], Train Loss: 0.0351\n",
      "Epoch [127/500], Train Loss: 0.0347\n",
      "Epoch [128/500], Train Loss: 0.0346\n",
      "Epoch [129/500], Train Loss: 0.0345\n",
      "Epoch [130/500], Train Loss: 0.0345\n",
      "Epoch [131/500], Train Loss: 0.0345\n",
      "Epoch [132/500], Train Loss: 0.0345\n",
      "Epoch [133/500], Train Loss: 0.0344\n",
      "Epoch [134/500], Train Loss: 0.0345\n",
      "Epoch [135/500], Train Loss: 0.0344\n",
      "Epoch [136/500], Train Loss: 0.0344\n",
      "Epoch [137/500], Train Loss: 0.0343\n",
      "Epoch [138/500], Train Loss: 0.0344\n",
      "Epoch [139/500], Train Loss: 0.0344\n",
      "Epoch [140/500], Train Loss: 0.0343\n",
      "Epoch [141/500], Train Loss: 0.0343\n",
      "Epoch [142/500], Train Loss: 0.0343\n",
      "Epoch [143/500], Train Loss: 0.0344\n",
      "Epoch [144/500], Train Loss: 0.0343\n",
      "Epoch [145/500], Train Loss: 0.0343\n",
      "Epoch [146/500], Train Loss: 0.0343\n",
      "Epoch [147/500], Train Loss: 0.0343\n",
      "Epoch [148/500], Train Loss: 0.0343\n",
      "Epoch [149/500], Train Loss: 0.0342\n",
      "Epoch [150/500], Train Loss: 0.0342\n",
      "Epoch [151/500], Train Loss: 0.0341\n",
      "Epoch [152/500], Train Loss: 0.0341\n",
      "Epoch [153/500], Train Loss: 0.0342\n",
      "Epoch [154/500], Train Loss: 0.0341\n",
      "Epoch [155/500], Train Loss: 0.0341\n",
      "Epoch [156/500], Train Loss: 0.0341\n",
      "Epoch [157/500], Train Loss: 0.0341\n",
      "Epoch [158/500], Train Loss: 0.0342\n",
      "Epoch [159/500], Train Loss: 0.0341\n",
      "Epoch [160/500], Train Loss: 0.0341\n",
      "Epoch [161/500], Train Loss: 0.0342\n",
      "Epoch [162/500], Train Loss: 0.0341\n",
      "Epoch [163/500], Train Loss: 0.0341\n",
      "Epoch [164/500], Train Loss: 0.0341\n",
      "Epoch [165/500], Train Loss: 0.0341\n",
      "Epoch [166/500], Train Loss: 0.0341\n",
      "Epoch [167/500], Train Loss: 0.0341\n",
      "Epoch [168/500], Train Loss: 0.0341\n",
      "Epoch [169/500], Train Loss: 0.0341\n",
      "Epoch [170/500], Train Loss: 0.0341\n",
      "Epoch [171/500], Train Loss: 0.0341\n",
      "Epoch [172/500], Train Loss: 0.0341\n",
      "Early stopping at epoch 172\n",
      "Final model saved at: ../../../Data/model_results/logistic_regression/models/chr14/final_model_chr14.pth\n",
      "Individual R^2 scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr14/individual_r2_scores_chr14.csv\n",
      "Individual IQS scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr14/individual_iqs_scores_chr14.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-18 12:27:18,909] A new study created in RDB with name: chr15_study\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual AUC ROC curves saved in: ../../../Data/model_results/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../../Data/model_results/logistic_regression/csv_files/performance_metrics.csv\n",
      "Total SNPs:  620\n",
      "PRS313 SNPs:  14\n",
      "Total SNPs used for Training:  606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-06-18 12:27:59,304] Trial 2 finished with value: 0.7909249633550643 and parameters: {'learning_rate': 0.05581513999350153, 'l1_coef': 0.021888442824926885, 'patience': 5, 'batch_size': 256}. Best is trial 2 with value: 0.7909249633550643.\n",
      "[I 2024-06-18 12:28:01,302] Trial 1 finished with value: 0.08222407586872578 and parameters: {'learning_rate': 0.05642866013363076, 'l1_coef': 1.2780374992422405e-05, 'patience': 9, 'batch_size': 128}. Best is trial 1 with value: 0.08222407586872578.\n",
      "[I 2024-06-18 12:29:10,914] Trial 4 finished with value: 0.4632629483938217 and parameters: {'learning_rate': 0.003959967752757223, 'l1_coef': 0.01464351935976468, 'patience': 11, 'batch_size': 256}. Best is trial 1 with value: 0.08222407586872578.\n",
      "[I 2024-06-18 12:29:26,494] Trial 5 finished with value: 0.1082878717354366 and parameters: {'learning_rate': 0.00101408606705295, 'l1_coef': 2.017426294433991e-05, 'patience': 14, 'batch_size': 64}. Best is trial 1 with value: 0.08222407586872578.\n",
      "[I 2024-06-18 12:29:26,556] Trial 3 finished with value: 0.3066879019141197 and parameters: {'learning_rate': 0.0022770919618976524, 'l1_coef': 0.0018522062235131994, 'patience': 9, 'batch_size': 128}. Best is trial 1 with value: 0.08222407586872578.\n",
      "[I 2024-06-18 12:29:33,709] Trial 8 finished with value: 0.11676336377859116 and parameters: {'learning_rate': 0.0009562404898071155, 'l1_coef': 1.005921049830796e-05, 'patience': 18, 'batch_size': 256}. Best is trial 1 with value: 0.08222407586872578.\n",
      "[I 2024-06-18 12:29:44,121] Trial 6 finished with value: 0.5132544070482254 and parameters: {'learning_rate': 0.00042725415343886124, 'l1_coef': 0.07042471564423602, 'patience': 11, 'batch_size': 256}. Best is trial 1 with value: 0.08222407586872578.\n",
      "[I 2024-06-18 12:29:48,432] Trial 7 finished with value: 0.2494065910577774 and parameters: {'learning_rate': 0.0011088741807527658, 'l1_coef': 0.0009156134405140064, 'patience': 18, 'batch_size': 256}. Best is trial 1 with value: 0.08222407586872578.\n",
      "[I 2024-06-18 12:29:52,251] Trial 0 finished with value: 0.24506186991930007 and parameters: {'learning_rate': 0.0011269733958158231, 'l1_coef': 0.0009115315206855733, 'patience': 10, 'batch_size': 256}. Best is trial 1 with value: 0.08222407586872578.\n",
      "[I 2024-06-18 12:29:52,672] Trial 9 finished with value: 0.14400738222258433 and parameters: {'learning_rate': 0.0003449813029282759, 'l1_coef': 5.193767701722787e-05, 'patience': 12, 'batch_size': 64}. Best is trial 1 with value: 0.08222407586872578.\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 15 - Best hyperparameters: {'learning_rate': 0.05642866013363076, 'l1_coef': 1.2780374992422405e-05, 'patience': 9, 'batch_size': 128}\n",
      "Chr 15 - Best value: 0.0822\n",
      "Epoch [1/500], Train Loss: 0.8400\n",
      "Epoch [2/500], Train Loss: 0.4196\n",
      "Epoch [3/500], Train Loss: 0.3395\n",
      "Epoch [4/500], Train Loss: 0.3057\n",
      "Epoch [5/500], Train Loss: 0.2634\n",
      "Epoch [6/500], Train Loss: 0.2426\n",
      "Epoch [7/500], Train Loss: 0.2340\n",
      "Epoch [8/500], Train Loss: 0.2179\n",
      "Epoch [9/500], Train Loss: 0.2068\n",
      "Epoch [10/500], Train Loss: 0.1999\n",
      "Epoch [11/500], Train Loss: 0.1926\n",
      "Epoch [12/500], Train Loss: 0.1841\n",
      "Epoch [13/500], Train Loss: 0.1506\n",
      "Epoch [14/500], Train Loss: 0.1310\n",
      "Epoch [15/500], Train Loss: 0.1112\n",
      "Epoch [16/500], Train Loss: 0.1007\n",
      "Epoch [17/500], Train Loss: 0.0865\n",
      "Epoch [18/500], Train Loss: 0.0839\n",
      "Epoch [19/500], Train Loss: 0.0802\n",
      "Epoch [20/500], Train Loss: 0.0775\n",
      "Epoch [21/500], Train Loss: 0.0755\n",
      "Epoch [22/500], Train Loss: 0.0735\n",
      "Epoch [23/500], Train Loss: 0.0737\n",
      "Epoch [24/500], Train Loss: 0.0728\n",
      "Epoch [25/500], Train Loss: 0.0747\n",
      "Epoch [26/500], Train Loss: 0.0729\n",
      "Epoch [27/500], Train Loss: 0.0741\n",
      "Epoch [28/500], Train Loss: 0.0721\n",
      "Epoch [29/500], Train Loss: 0.0710\n",
      "Epoch [30/500], Train Loss: 0.0726\n",
      "Epoch [31/500], Train Loss: 0.0708\n",
      "Epoch [32/500], Train Loss: 0.0688\n",
      "Epoch [33/500], Train Loss: 0.0684\n",
      "Epoch [34/500], Train Loss: 0.0683\n",
      "Epoch [35/500], Train Loss: 0.0729\n",
      "Epoch [36/500], Train Loss: 0.0758\n",
      "Epoch [37/500], Train Loss: 0.0722\n",
      "Epoch [38/500], Train Loss: 0.0691\n",
      "Epoch [39/500], Train Loss: 0.0689\n",
      "Epoch [40/500], Train Loss: 0.0686\n",
      "Epoch [41/500], Train Loss: 0.0622\n",
      "Epoch [42/500], Train Loss: 0.0600\n",
      "Epoch [43/500], Train Loss: 0.0594\n",
      "Epoch [44/500], Train Loss: 0.0592\n",
      "Epoch [45/500], Train Loss: 0.0590\n",
      "Epoch [46/500], Train Loss: 0.0586\n",
      "Epoch [47/500], Train Loss: 0.0586\n",
      "Epoch [48/500], Train Loss: 0.0586\n",
      "Epoch [49/500], Train Loss: 0.0583\n",
      "Epoch [50/500], Train Loss: 0.0582\n",
      "Epoch [51/500], Train Loss: 0.0583\n",
      "Epoch [52/500], Train Loss: 0.0581\n",
      "Epoch [53/500], Train Loss: 0.0583\n",
      "Epoch [54/500], Train Loss: 0.0582\n",
      "Epoch [55/500], Train Loss: 0.0584\n",
      "Epoch [56/500], Train Loss: 0.0583\n",
      "Epoch [57/500], Train Loss: 0.0583\n",
      "Epoch [58/500], Train Loss: 0.0579\n",
      "Epoch [59/500], Train Loss: 0.0581\n",
      "Epoch [60/500], Train Loss: 0.0582\n",
      "Epoch [61/500], Train Loss: 0.0582\n",
      "Epoch [62/500], Train Loss: 0.0581\n",
      "Epoch [63/500], Train Loss: 0.0581\n",
      "Epoch [64/500], Train Loss: 0.0581\n",
      "Epoch [65/500], Train Loss: 0.0574\n",
      "Epoch [66/500], Train Loss: 0.0574\n",
      "Epoch [67/500], Train Loss: 0.0572\n",
      "Epoch [68/500], Train Loss: 0.0573\n",
      "Epoch [69/500], Train Loss: 0.0573\n",
      "Epoch [70/500], Train Loss: 0.0573\n",
      "Epoch [71/500], Train Loss: 0.0573\n",
      "Epoch [72/500], Train Loss: 0.0572\n",
      "Epoch [73/500], Train Loss: 0.0570\n",
      "Epoch [74/500], Train Loss: 0.0573\n",
      "Epoch [75/500], Train Loss: 0.0573\n",
      "Epoch [76/500], Train Loss: 0.0571\n",
      "Epoch [77/500], Train Loss: 0.0573\n",
      "Epoch [78/500], Train Loss: 0.0572\n",
      "Epoch [79/500], Train Loss: 0.0571\n",
      "Epoch [80/500], Train Loss: 0.0571\n",
      "Epoch [81/500], Train Loss: 0.0569\n",
      "Epoch [82/500], Train Loss: 0.0568\n",
      "Epoch [83/500], Train Loss: 0.0571\n",
      "Epoch [84/500], Train Loss: 0.0572\n",
      "Epoch [85/500], Train Loss: 0.0571\n",
      "Epoch [86/500], Train Loss: 0.0571\n",
      "Epoch [87/500], Train Loss: 0.0572\n",
      "Epoch [88/500], Train Loss: 0.0571\n",
      "Epoch [89/500], Train Loss: 0.0571\n",
      "Epoch [90/500], Train Loss: 0.0572\n",
      "Epoch [91/500], Train Loss: 0.0571\n",
      "Early stopping at epoch 91\n",
      "Final model saved at: ../../../Data/model_results/logistic_regression/models/chr15/final_model_chr15.pth\n",
      "Individual R^2 scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr15/individual_r2_scores_chr15.csv\n",
      "Individual IQS scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr15/individual_iqs_scores_chr15.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-18 12:29:54,458] A new study created in RDB with name: chr16_study\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual AUC ROC curves saved in: ../../../Data/model_results/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../../Data/model_results/logistic_regression/csv_files/performance_metrics.csv\n",
      "Total SNPs:  1212\n",
      "PRS313 SNPs:  28\n",
      "Total SNPs used for Training:  1184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-06-18 12:31:05,040] Trial 4 finished with value: 0.4093294978141785 and parameters: {'learning_rate': 0.010938333539073437, 'l1_coef': 0.002184288403028775, 'patience': 20, 'batch_size': 256}. Best is trial 4 with value: 0.4093294978141785.\n",
      "[I 2024-06-18 12:31:06,672] Trial 3 finished with value: 0.7055125539119427 and parameters: {'learning_rate': 0.023854678810027622, 'l1_coef': 0.009085837311063506, 'patience': 6, 'batch_size': 32}. Best is trial 4 with value: 0.4093294978141785.\n",
      "[I 2024-06-18 12:31:06,866] Trial 1 finished with value: 0.4301452413201332 and parameters: {'learning_rate': 0.0020546956998084825, 'l1_coef': 0.0025115154298340344, 'patience': 6, 'batch_size': 128}. Best is trial 4 with value: 0.4093294978141785.\n",
      "[I 2024-06-18 12:31:22,537] Trial 6 finished with value: 0.300434535741806 and parameters: {'learning_rate': 0.0009134423117999843, 'l1_coef': 0.0007515362867752688, 'patience': 9, 'batch_size': 256}. Best is trial 6 with value: 0.300434535741806.\n",
      "[I 2024-06-18 12:31:37,221] Trial 7 finished with value: 0.4907323941588402 and parameters: {'learning_rate': 0.00043914356956082946, 'l1_coef': 0.010518519243565012, 'patience': 7, 'batch_size': 128}. Best is trial 6 with value: 0.300434535741806.\n",
      "[I 2024-06-18 12:31:40,906] Trial 0 finished with value: 0.30999033004045484 and parameters: {'learning_rate': 0.00020187853280594464, 'l1_coef': 0.000531362726369737, 'patience': 11, 'batch_size': 128}. Best is trial 6 with value: 0.300434535741806.\n",
      "[I 2024-06-18 12:31:48,187] Trial 9 finished with value: 0.1474310125623431 and parameters: {'learning_rate': 0.0006101751786415075, 'l1_coef': 6.899262460943887e-05, 'patience': 9, 'batch_size': 64}. Best is trial 9 with value: 0.1474310125623431.\n",
      "[I 2024-06-18 12:31:53,914] Trial 2 finished with value: 0.304619790499027 and parameters: {'learning_rate': 0.0010156575190037789, 'l1_coef': 0.000890238411708187, 'patience': 10, 'batch_size': 32}. Best is trial 9 with value: 0.1474310125623431.\n",
      "[I 2024-06-18 12:31:59,303] Trial 5 finished with value: 0.17409143958772932 and parameters: {'learning_rate': 0.030640720437331855, 'l1_coef': 0.00013213681746960623, 'patience': 17, 'batch_size': 64}. Best is trial 9 with value: 0.1474310125623431.\n",
      "[I 2024-06-18 12:32:03,796] Trial 8 finished with value: 0.11638630559811225 and parameters: {'learning_rate': 0.0002145507468295692, 'l1_coef': 2.987027699346444e-05, 'patience': 13, 'batch_size': 32}. Best is trial 8 with value: 0.11638630559811225.\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 16 - Best hyperparameters: {'learning_rate': 0.0002145507468295692, 'l1_coef': 2.987027699346444e-05, 'patience': 13, 'batch_size': 32}\n",
      "Chr 16 - Best value: 0.1164\n",
      "Epoch [1/500], Train Loss: 0.5234\n",
      "Epoch [2/500], Train Loss: 0.4661\n",
      "Epoch [3/500], Train Loss: 0.4478\n",
      "Epoch [4/500], Train Loss: 0.4321\n",
      "Epoch [5/500], Train Loss: 0.4191\n",
      "Epoch [6/500], Train Loss: 0.4072\n",
      "Epoch [7/500], Train Loss: 0.3957\n",
      "Epoch [8/500], Train Loss: 0.3858\n",
      "Epoch [9/500], Train Loss: 0.3761\n",
      "Epoch [10/500], Train Loss: 0.3671\n",
      "Epoch [11/500], Train Loss: 0.3583\n",
      "Epoch [12/500], Train Loss: 0.3506\n",
      "Epoch [13/500], Train Loss: 0.3433\n",
      "Epoch [14/500], Train Loss: 0.3363\n",
      "Epoch [15/500], Train Loss: 0.3296\n",
      "Epoch [16/500], Train Loss: 0.3230\n",
      "Epoch [17/500], Train Loss: 0.3174\n",
      "Epoch [18/500], Train Loss: 0.3118\n",
      "Epoch [19/500], Train Loss: 0.3065\n",
      "Epoch [20/500], Train Loss: 0.3014\n",
      "Epoch [21/500], Train Loss: 0.2964\n",
      "Epoch [22/500], Train Loss: 0.2916\n",
      "Epoch [23/500], Train Loss: 0.2871\n",
      "Epoch [24/500], Train Loss: 0.2827\n",
      "Epoch [25/500], Train Loss: 0.2788\n",
      "Epoch [26/500], Train Loss: 0.2748\n",
      "Epoch [27/500], Train Loss: 0.2710\n",
      "Epoch [28/500], Train Loss: 0.2674\n",
      "Epoch [29/500], Train Loss: 0.2642\n",
      "Epoch [30/500], Train Loss: 0.2608\n",
      "Epoch [31/500], Train Loss: 0.2573\n",
      "Epoch [32/500], Train Loss: 0.2541\n",
      "Epoch [33/500], Train Loss: 0.2510\n",
      "Epoch [34/500], Train Loss: 0.2481\n",
      "Epoch [35/500], Train Loss: 0.2454\n",
      "Epoch [36/500], Train Loss: 0.2426\n",
      "Epoch [37/500], Train Loss: 0.2400\n",
      "Epoch [38/500], Train Loss: 0.2373\n",
      "Epoch [39/500], Train Loss: 0.2348\n",
      "Epoch [40/500], Train Loss: 0.2327\n",
      "Epoch [41/500], Train Loss: 0.2302\n",
      "Epoch [42/500], Train Loss: 0.2278\n",
      "Epoch [43/500], Train Loss: 0.2257\n",
      "Epoch [44/500], Train Loss: 0.2235\n",
      "Epoch [45/500], Train Loss: 0.2215\n",
      "Epoch [46/500], Train Loss: 0.2192\n",
      "Epoch [47/500], Train Loss: 0.2175\n",
      "Epoch [48/500], Train Loss: 0.2155\n",
      "Epoch [49/500], Train Loss: 0.2136\n",
      "Epoch [50/500], Train Loss: 0.2116\n",
      "Epoch [51/500], Train Loss: 0.2099\n",
      "Epoch [52/500], Train Loss: 0.2083\n",
      "Epoch [53/500], Train Loss: 0.2064\n",
      "Epoch [54/500], Train Loss: 0.2049\n",
      "Epoch [55/500], Train Loss: 0.2031\n",
      "Epoch [56/500], Train Loss: 0.2017\n",
      "Epoch [57/500], Train Loss: 0.1999\n",
      "Epoch [58/500], Train Loss: 0.1984\n",
      "Epoch [59/500], Train Loss: 0.1970\n",
      "Epoch [60/500], Train Loss: 0.1955\n",
      "Epoch [61/500], Train Loss: 0.1941\n",
      "Epoch [62/500], Train Loss: 0.1928\n",
      "Epoch [63/500], Train Loss: 0.1913\n",
      "Epoch [64/500], Train Loss: 0.1900\n",
      "Epoch [65/500], Train Loss: 0.1886\n",
      "Epoch [66/500], Train Loss: 0.1873\n",
      "Epoch [67/500], Train Loss: 0.1860\n",
      "Epoch [68/500], Train Loss: 0.1850\n",
      "Epoch [69/500], Train Loss: 0.1839\n",
      "Epoch [70/500], Train Loss: 0.1827\n",
      "Epoch [71/500], Train Loss: 0.1816\n",
      "Epoch [72/500], Train Loss: 0.1804\n",
      "Epoch [73/500], Train Loss: 0.1792\n",
      "Epoch [74/500], Train Loss: 0.1781\n",
      "Epoch [75/500], Train Loss: 0.1770\n",
      "Epoch [76/500], Train Loss: 0.1762\n",
      "Epoch [77/500], Train Loss: 0.1750\n",
      "Epoch [78/500], Train Loss: 0.1739\n",
      "Epoch [79/500], Train Loss: 0.1729\n",
      "Epoch [80/500], Train Loss: 0.1720\n",
      "Epoch [81/500], Train Loss: 0.1710\n",
      "Epoch [82/500], Train Loss: 0.1700\n",
      "Epoch [83/500], Train Loss: 0.1691\n",
      "Epoch [84/500], Train Loss: 0.1681\n",
      "Epoch [85/500], Train Loss: 0.1672\n",
      "Epoch [86/500], Train Loss: 0.1665\n",
      "Epoch [87/500], Train Loss: 0.1655\n",
      "Epoch [88/500], Train Loss: 0.1646\n",
      "Epoch [89/500], Train Loss: 0.1638\n",
      "Epoch [90/500], Train Loss: 0.1630\n",
      "Epoch [91/500], Train Loss: 0.1622\n",
      "Epoch [92/500], Train Loss: 0.1614\n",
      "Epoch [93/500], Train Loss: 0.1606\n",
      "Epoch [94/500], Train Loss: 0.1599\n",
      "Epoch [95/500], Train Loss: 0.1591\n",
      "Epoch [96/500], Train Loss: 0.1583\n",
      "Epoch [97/500], Train Loss: 0.1575\n",
      "Epoch [98/500], Train Loss: 0.1568\n",
      "Epoch [99/500], Train Loss: 0.1561\n",
      "Epoch [100/500], Train Loss: 0.1555\n",
      "Epoch [101/500], Train Loss: 0.1545\n",
      "Epoch [102/500], Train Loss: 0.1541\n",
      "Epoch [103/500], Train Loss: 0.1534\n",
      "Epoch [104/500], Train Loss: 0.1528\n",
      "Epoch [105/500], Train Loss: 0.1521\n",
      "Epoch [106/500], Train Loss: 0.1514\n",
      "Epoch [107/500], Train Loss: 0.1509\n",
      "Epoch [108/500], Train Loss: 0.1501\n",
      "Epoch [109/500], Train Loss: 0.1494\n",
      "Epoch [110/500], Train Loss: 0.1489\n",
      "Epoch [111/500], Train Loss: 0.1484\n",
      "Epoch [112/500], Train Loss: 0.1477\n",
      "Epoch [113/500], Train Loss: 0.1471\n",
      "Epoch [114/500], Train Loss: 0.1466\n",
      "Epoch [115/500], Train Loss: 0.1459\n",
      "Epoch [116/500], Train Loss: 0.1454\n",
      "Epoch [117/500], Train Loss: 0.1448\n",
      "Epoch [118/500], Train Loss: 0.1444\n",
      "Epoch [119/500], Train Loss: 0.1436\n",
      "Epoch [120/500], Train Loss: 0.1432\n",
      "Epoch [121/500], Train Loss: 0.1428\n",
      "Epoch [122/500], Train Loss: 0.1423\n",
      "Epoch [123/500], Train Loss: 0.1418\n",
      "Epoch [124/500], Train Loss: 0.1412\n",
      "Epoch [125/500], Train Loss: 0.1407\n",
      "Epoch [126/500], Train Loss: 0.1402\n",
      "Epoch [127/500], Train Loss: 0.1398\n",
      "Epoch [128/500], Train Loss: 0.1392\n",
      "Epoch [129/500], Train Loss: 0.1388\n",
      "Epoch [130/500], Train Loss: 0.1382\n",
      "Epoch [131/500], Train Loss: 0.1379\n",
      "Epoch [132/500], Train Loss: 0.1374\n",
      "Epoch [133/500], Train Loss: 0.1369\n",
      "Epoch [134/500], Train Loss: 0.1366\n",
      "Epoch [135/500], Train Loss: 0.1361\n",
      "Epoch [136/500], Train Loss: 0.1357\n",
      "Epoch [137/500], Train Loss: 0.1352\n",
      "Epoch [138/500], Train Loss: 0.1349\n",
      "Epoch [139/500], Train Loss: 0.1344\n",
      "Epoch [140/500], Train Loss: 0.1340\n",
      "Epoch [141/500], Train Loss: 0.1336\n",
      "Epoch [142/500], Train Loss: 0.1331\n",
      "Epoch [143/500], Train Loss: 0.1330\n",
      "Epoch [144/500], Train Loss: 0.1324\n",
      "Epoch [145/500], Train Loss: 0.1320\n",
      "Epoch [146/500], Train Loss: 0.1317\n",
      "Epoch [147/500], Train Loss: 0.1313\n",
      "Epoch [148/500], Train Loss: 0.1308\n",
      "Epoch [149/500], Train Loss: 0.1305\n",
      "Epoch [150/500], Train Loss: 0.1302\n",
      "Epoch [151/500], Train Loss: 0.1298\n",
      "Epoch [152/500], Train Loss: 0.1295\n",
      "Epoch [153/500], Train Loss: 0.1291\n",
      "Epoch [154/500], Train Loss: 0.1287\n",
      "Epoch [155/500], Train Loss: 0.1284\n",
      "Epoch [156/500], Train Loss: 0.1282\n",
      "Epoch [157/500], Train Loss: 0.1277\n",
      "Epoch [158/500], Train Loss: 0.1275\n",
      "Epoch [159/500], Train Loss: 0.1272\n",
      "Epoch [160/500], Train Loss: 0.1266\n",
      "Epoch [161/500], Train Loss: 0.1265\n",
      "Epoch [162/500], Train Loss: 0.1262\n",
      "Epoch [163/500], Train Loss: 0.1260\n",
      "Epoch [164/500], Train Loss: 0.1256\n",
      "Epoch [165/500], Train Loss: 0.1251\n",
      "Epoch [166/500], Train Loss: 0.1249\n",
      "Epoch [167/500], Train Loss: 0.1246\n",
      "Epoch [168/500], Train Loss: 0.1245\n",
      "Epoch [169/500], Train Loss: 0.1241\n",
      "Epoch [170/500], Train Loss: 0.1237\n",
      "Epoch [171/500], Train Loss: 0.1237\n",
      "Epoch [172/500], Train Loss: 0.1231\n",
      "Epoch [173/500], Train Loss: 0.1230\n",
      "Epoch [174/500], Train Loss: 0.1227\n",
      "Epoch [175/500], Train Loss: 0.1223\n",
      "Epoch [176/500], Train Loss: 0.1220\n",
      "Epoch [177/500], Train Loss: 0.1218\n",
      "Epoch [178/500], Train Loss: 0.1216\n",
      "Epoch [179/500], Train Loss: 0.1213\n",
      "Epoch [180/500], Train Loss: 0.1211\n",
      "Epoch [181/500], Train Loss: 0.1208\n",
      "Epoch [182/500], Train Loss: 0.1205\n",
      "Epoch [183/500], Train Loss: 0.1203\n",
      "Epoch [184/500], Train Loss: 0.1201\n",
      "Epoch [185/500], Train Loss: 0.1198\n",
      "Epoch [186/500], Train Loss: 0.1194\n",
      "Epoch [187/500], Train Loss: 0.1193\n",
      "Epoch [188/500], Train Loss: 0.1190\n",
      "Epoch [189/500], Train Loss: 0.1189\n",
      "Epoch [190/500], Train Loss: 0.1187\n",
      "Epoch [191/500], Train Loss: 0.1184\n",
      "Epoch [192/500], Train Loss: 0.1181\n",
      "Epoch [193/500], Train Loss: 0.1178\n",
      "Epoch [194/500], Train Loss: 0.1177\n",
      "Epoch [195/500], Train Loss: 0.1175\n",
      "Epoch [196/500], Train Loss: 0.1173\n",
      "Epoch [197/500], Train Loss: 0.1170\n",
      "Epoch [198/500], Train Loss: 0.1168\n",
      "Epoch [199/500], Train Loss: 0.1167\n",
      "Epoch [200/500], Train Loss: 0.1163\n",
      "Epoch [201/500], Train Loss: 0.1162\n",
      "Epoch [202/500], Train Loss: 0.1160\n",
      "Epoch [203/500], Train Loss: 0.1158\n",
      "Epoch [204/500], Train Loss: 0.1156\n",
      "Epoch [205/500], Train Loss: 0.1154\n",
      "Epoch [206/500], Train Loss: 0.1151\n",
      "Epoch [207/500], Train Loss: 0.1150\n",
      "Epoch [208/500], Train Loss: 0.1148\n",
      "Epoch [209/500], Train Loss: 0.1147\n",
      "Epoch [210/500], Train Loss: 0.1144\n",
      "Epoch [211/500], Train Loss: 0.1143\n",
      "Epoch [212/500], Train Loss: 0.1141\n",
      "Epoch [213/500], Train Loss: 0.1139\n",
      "Epoch [214/500], Train Loss: 0.1136\n",
      "Epoch [215/500], Train Loss: 0.1135\n",
      "Epoch [216/500], Train Loss: 0.1133\n",
      "Epoch [217/500], Train Loss: 0.1131\n",
      "Epoch [218/500], Train Loss: 0.1130\n",
      "Epoch [219/500], Train Loss: 0.1128\n",
      "Epoch [220/500], Train Loss: 0.1126\n",
      "Epoch [221/500], Train Loss: 0.1124\n",
      "Epoch [222/500], Train Loss: 0.1123\n",
      "Epoch [223/500], Train Loss: 0.1120\n",
      "Epoch [224/500], Train Loss: 0.1119\n",
      "Epoch [225/500], Train Loss: 0.1116\n",
      "Epoch [226/500], Train Loss: 0.1116\n",
      "Epoch [227/500], Train Loss: 0.1114\n",
      "Epoch [228/500], Train Loss: 0.1114\n",
      "Epoch [229/500], Train Loss: 0.1111\n",
      "Epoch [230/500], Train Loss: 0.1110\n",
      "Epoch [231/500], Train Loss: 0.1108\n",
      "Epoch [232/500], Train Loss: 0.1106\n",
      "Epoch [233/500], Train Loss: 0.1104\n",
      "Epoch [234/500], Train Loss: 0.1104\n",
      "Epoch [235/500], Train Loss: 0.1102\n",
      "Epoch [236/500], Train Loss: 0.1101\n",
      "Epoch [237/500], Train Loss: 0.1100\n",
      "Epoch [238/500], Train Loss: 0.1096\n",
      "Epoch [239/500], Train Loss: 0.1095\n",
      "Epoch [240/500], Train Loss: 0.1095\n",
      "Epoch [241/500], Train Loss: 0.1094\n",
      "Epoch [242/500], Train Loss: 0.1092\n",
      "Epoch [243/500], Train Loss: 0.1090\n",
      "Epoch [244/500], Train Loss: 0.1089\n",
      "Epoch [245/500], Train Loss: 0.1087\n",
      "Epoch [246/500], Train Loss: 0.1087\n",
      "Epoch [247/500], Train Loss: 0.1084\n",
      "Epoch [248/500], Train Loss: 0.1084\n",
      "Epoch [249/500], Train Loss: 0.1082\n",
      "Epoch [250/500], Train Loss: 0.1081\n",
      "Epoch [251/500], Train Loss: 0.1079\n",
      "Epoch [252/500], Train Loss: 0.1078\n",
      "Epoch [253/500], Train Loss: 0.1076\n",
      "Epoch [254/500], Train Loss: 0.1075\n",
      "Epoch [255/500], Train Loss: 0.1075\n",
      "Epoch [256/500], Train Loss: 0.1073\n",
      "Epoch [257/500], Train Loss: 0.1073\n",
      "Epoch [258/500], Train Loss: 0.1070\n",
      "Epoch [259/500], Train Loss: 0.1071\n",
      "Epoch [260/500], Train Loss: 0.1068\n",
      "Epoch [261/500], Train Loss: 0.1067\n",
      "Epoch [262/500], Train Loss: 0.1066\n",
      "Epoch [263/500], Train Loss: 0.1064\n",
      "Epoch [264/500], Train Loss: 0.1062\n",
      "Epoch [265/500], Train Loss: 0.1062\n",
      "Epoch [266/500], Train Loss: 0.1061\n",
      "Epoch [267/500], Train Loss: 0.1061\n",
      "Epoch [268/500], Train Loss: 0.1058\n",
      "Epoch [269/500], Train Loss: 0.1057\n",
      "Epoch [270/500], Train Loss: 0.1057\n",
      "Epoch [271/500], Train Loss: 0.1055\n",
      "Epoch [272/500], Train Loss: 0.1055\n",
      "Epoch [273/500], Train Loss: 0.1054\n",
      "Epoch [274/500], Train Loss: 0.1052\n",
      "Epoch [275/500], Train Loss: 0.1052\n",
      "Epoch [276/500], Train Loss: 0.1050\n",
      "Epoch [277/500], Train Loss: 0.1050\n",
      "Epoch [278/500], Train Loss: 0.1049\n",
      "Epoch [279/500], Train Loss: 0.1047\n",
      "Epoch [280/500], Train Loss: 0.1046\n",
      "Epoch [281/500], Train Loss: 0.1045\n",
      "Epoch [282/500], Train Loss: 0.1044\n",
      "Epoch [283/500], Train Loss: 0.1042\n",
      "Epoch [284/500], Train Loss: 0.1042\n",
      "Epoch [285/500], Train Loss: 0.1041\n",
      "Epoch [286/500], Train Loss: 0.1040\n",
      "Epoch [287/500], Train Loss: 0.1038\n",
      "Epoch [288/500], Train Loss: 0.1038\n",
      "Epoch [289/500], Train Loss: 0.1037\n",
      "Epoch [290/500], Train Loss: 0.1036\n",
      "Epoch [291/500], Train Loss: 0.1036\n",
      "Epoch [292/500], Train Loss: 0.1035\n",
      "Epoch [293/500], Train Loss: 0.1033\n",
      "Epoch [294/500], Train Loss: 0.1033\n",
      "Epoch [295/500], Train Loss: 0.1031\n",
      "Epoch [296/500], Train Loss: 0.1030\n",
      "Epoch [297/500], Train Loss: 0.1030\n",
      "Epoch [298/500], Train Loss: 0.1028\n",
      "Epoch [299/500], Train Loss: 0.1029\n",
      "Epoch [300/500], Train Loss: 0.1027\n",
      "Epoch [301/500], Train Loss: 0.1026\n",
      "Epoch [302/500], Train Loss: 0.1026\n",
      "Epoch [303/500], Train Loss: 0.1025\n",
      "Epoch [304/500], Train Loss: 0.1024\n",
      "Epoch [305/500], Train Loss: 0.1023\n",
      "Epoch [306/500], Train Loss: 0.1021\n",
      "Epoch [307/500], Train Loss: 0.1021\n",
      "Epoch [308/500], Train Loss: 0.1020\n",
      "Epoch [309/500], Train Loss: 0.1019\n",
      "Epoch [310/500], Train Loss: 0.1019\n",
      "Epoch [311/500], Train Loss: 0.1017\n",
      "Epoch [312/500], Train Loss: 0.1017\n",
      "Epoch [313/500], Train Loss: 0.1017\n",
      "Epoch [314/500], Train Loss: 0.1015\n",
      "Epoch [315/500], Train Loss: 0.1015\n",
      "Epoch [316/500], Train Loss: 0.1014\n",
      "Epoch [317/500], Train Loss: 0.1014\n",
      "Epoch [318/500], Train Loss: 0.1012\n",
      "Epoch [319/500], Train Loss: 0.1011\n",
      "Epoch [320/500], Train Loss: 0.1011\n",
      "Epoch [321/500], Train Loss: 0.1010\n",
      "Epoch [322/500], Train Loss: 0.1009\n",
      "Epoch [323/500], Train Loss: 0.1008\n",
      "Epoch [324/500], Train Loss: 0.1009\n",
      "Epoch [325/500], Train Loss: 0.1007\n",
      "Epoch [326/500], Train Loss: 0.1005\n",
      "Epoch [327/500], Train Loss: 0.1005\n",
      "Epoch [328/500], Train Loss: 0.1005\n",
      "Epoch [329/500], Train Loss: 0.1003\n",
      "Epoch [330/500], Train Loss: 0.1004\n",
      "Epoch [331/500], Train Loss: 0.1003\n",
      "Epoch [332/500], Train Loss: 0.1002\n",
      "Epoch [333/500], Train Loss: 0.1001\n",
      "Epoch [334/500], Train Loss: 0.0999\n",
      "Epoch [335/500], Train Loss: 0.1000\n",
      "Epoch [336/500], Train Loss: 0.1000\n",
      "Epoch [337/500], Train Loss: 0.0998\n",
      "Epoch [338/500], Train Loss: 0.0997\n",
      "Epoch [339/500], Train Loss: 0.0997\n",
      "Epoch [340/500], Train Loss: 0.0997\n",
      "Epoch [341/500], Train Loss: 0.0997\n",
      "Epoch [342/500], Train Loss: 0.0995\n",
      "Epoch [343/500], Train Loss: 0.0994\n",
      "Epoch [344/500], Train Loss: 0.0994\n",
      "Epoch [345/500], Train Loss: 0.0994\n",
      "Epoch [346/500], Train Loss: 0.0994\n",
      "Epoch [347/500], Train Loss: 0.0992\n",
      "Epoch [348/500], Train Loss: 0.0991\n",
      "Epoch [349/500], Train Loss: 0.0991\n",
      "Epoch [350/500], Train Loss: 0.0990\n",
      "Epoch [351/500], Train Loss: 0.0991\n",
      "Epoch [352/500], Train Loss: 0.0988\n",
      "Epoch [353/500], Train Loss: 0.0989\n",
      "Epoch [354/500], Train Loss: 0.0988\n",
      "Epoch [355/500], Train Loss: 0.0987\n",
      "Epoch [356/500], Train Loss: 0.0987\n",
      "Epoch [357/500], Train Loss: 0.0987\n",
      "Epoch [358/500], Train Loss: 0.0986\n",
      "Epoch [359/500], Train Loss: 0.0985\n",
      "Epoch [360/500], Train Loss: 0.0984\n",
      "Epoch [361/500], Train Loss: 0.0983\n",
      "Epoch [362/500], Train Loss: 0.0983\n",
      "Epoch [363/500], Train Loss: 0.0983\n",
      "Epoch [364/500], Train Loss: 0.0982\n",
      "Epoch [365/500], Train Loss: 0.0982\n",
      "Epoch [366/500], Train Loss: 0.0981\n",
      "Epoch [367/500], Train Loss: 0.0980\n",
      "Epoch [368/500], Train Loss: 0.0979\n",
      "Epoch [369/500], Train Loss: 0.0980\n",
      "Epoch [370/500], Train Loss: 0.0979\n",
      "Epoch [371/500], Train Loss: 0.0979\n",
      "Epoch [372/500], Train Loss: 0.0978\n",
      "Epoch [373/500], Train Loss: 0.0977\n",
      "Epoch [374/500], Train Loss: 0.0977\n",
      "Epoch [375/500], Train Loss: 0.0977\n",
      "Epoch [376/500], Train Loss: 0.0976\n",
      "Epoch [377/500], Train Loss: 0.0976\n",
      "Epoch [378/500], Train Loss: 0.0976\n",
      "Epoch [379/500], Train Loss: 0.0973\n",
      "Epoch [380/500], Train Loss: 0.0974\n",
      "Epoch [381/500], Train Loss: 0.0973\n",
      "Epoch [382/500], Train Loss: 0.0972\n",
      "Epoch [383/500], Train Loss: 0.0972\n",
      "Epoch [384/500], Train Loss: 0.0971\n",
      "Epoch [385/500], Train Loss: 0.0970\n",
      "Epoch [386/500], Train Loss: 0.0971\n",
      "Epoch [387/500], Train Loss: 0.0969\n",
      "Epoch [388/500], Train Loss: 0.0969\n",
      "Epoch [389/500], Train Loss: 0.0969\n",
      "Epoch [390/500], Train Loss: 0.0969\n",
      "Epoch [391/500], Train Loss: 0.0968\n",
      "Epoch [392/500], Train Loss: 0.0967\n",
      "Epoch [393/500], Train Loss: 0.0967\n",
      "Epoch [394/500], Train Loss: 0.0967\n",
      "Epoch [395/500], Train Loss: 0.0966\n",
      "Epoch [396/500], Train Loss: 0.0966\n",
      "Epoch [397/500], Train Loss: 0.0966\n",
      "Epoch [398/500], Train Loss: 0.0965\n",
      "Epoch [399/500], Train Loss: 0.0964\n",
      "Epoch [400/500], Train Loss: 0.0963\n",
      "Epoch [401/500], Train Loss: 0.0963\n",
      "Epoch [402/500], Train Loss: 0.0964\n",
      "Epoch [403/500], Train Loss: 0.0963\n",
      "Epoch [404/500], Train Loss: 0.0962\n",
      "Epoch [405/500], Train Loss: 0.0962\n",
      "Epoch [406/500], Train Loss: 0.0962\n",
      "Epoch [407/500], Train Loss: 0.0961\n",
      "Epoch [408/500], Train Loss: 0.0960\n",
      "Epoch [409/500], Train Loss: 0.0960\n",
      "Epoch [410/500], Train Loss: 0.0959\n",
      "Epoch [411/500], Train Loss: 0.0959\n",
      "Epoch [412/500], Train Loss: 0.0958\n",
      "Epoch [413/500], Train Loss: 0.0958\n",
      "Epoch [414/500], Train Loss: 0.0957\n",
      "Epoch [415/500], Train Loss: 0.0958\n",
      "Epoch [416/500], Train Loss: 0.0956\n",
      "Epoch [417/500], Train Loss: 0.0956\n",
      "Epoch [418/500], Train Loss: 0.0957\n",
      "Epoch [419/500], Train Loss: 0.0956\n",
      "Epoch [420/500], Train Loss: 0.0955\n",
      "Epoch [421/500], Train Loss: 0.0955\n",
      "Epoch [422/500], Train Loss: 0.0955\n",
      "Epoch [423/500], Train Loss: 0.0955\n",
      "Epoch [424/500], Train Loss: 0.0955\n",
      "Epoch [425/500], Train Loss: 0.0953\n",
      "Epoch [426/500], Train Loss: 0.0952\n",
      "Epoch [427/500], Train Loss: 0.0952\n",
      "Epoch [428/500], Train Loss: 0.0952\n",
      "Epoch [429/500], Train Loss: 0.0952\n",
      "Epoch [430/500], Train Loss: 0.0951\n",
      "Epoch [431/500], Train Loss: 0.0951\n",
      "Epoch [432/500], Train Loss: 0.0950\n",
      "Epoch [433/500], Train Loss: 0.0950\n",
      "Epoch [434/500], Train Loss: 0.0951\n",
      "Epoch [435/500], Train Loss: 0.0949\n",
      "Epoch [436/500], Train Loss: 0.0949\n",
      "Epoch [437/500], Train Loss: 0.0948\n",
      "Epoch [438/500], Train Loss: 0.0948\n",
      "Epoch [439/500], Train Loss: 0.0948\n",
      "Epoch [440/500], Train Loss: 0.0948\n",
      "Epoch [441/500], Train Loss: 0.0948\n",
      "Epoch [442/500], Train Loss: 0.0947\n",
      "Epoch [443/500], Train Loss: 0.0947\n",
      "Epoch [444/500], Train Loss: 0.0947\n",
      "Epoch [445/500], Train Loss: 0.0946\n",
      "Epoch [446/500], Train Loss: 0.0946\n",
      "Epoch [447/500], Train Loss: 0.0945\n",
      "Epoch [448/500], Train Loss: 0.0946\n",
      "Epoch [449/500], Train Loss: 0.0944\n",
      "Epoch [450/500], Train Loss: 0.0944\n",
      "Epoch [451/500], Train Loss: 0.0944\n",
      "Epoch [452/500], Train Loss: 0.0944\n",
      "Epoch [453/500], Train Loss: 0.0944\n",
      "Epoch [454/500], Train Loss: 0.0943\n",
      "Epoch [455/500], Train Loss: 0.0943\n",
      "Epoch [456/500], Train Loss: 0.0943\n",
      "Epoch [457/500], Train Loss: 0.0944\n",
      "Epoch [458/500], Train Loss: 0.0943\n",
      "Epoch [459/500], Train Loss: 0.0941\n",
      "Epoch [460/500], Train Loss: 0.0941\n",
      "Epoch [461/500], Train Loss: 0.0942\n",
      "Epoch [462/500], Train Loss: 0.0941\n",
      "Epoch [463/500], Train Loss: 0.0941\n",
      "Epoch [464/500], Train Loss: 0.0940\n",
      "Epoch [465/500], Train Loss: 0.0939\n",
      "Epoch [466/500], Train Loss: 0.0939\n",
      "Epoch [467/500], Train Loss: 0.0939\n",
      "Epoch [468/500], Train Loss: 0.0939\n",
      "Epoch [469/500], Train Loss: 0.0938\n",
      "Epoch [470/500], Train Loss: 0.0938\n",
      "Epoch [471/500], Train Loss: 0.0938\n",
      "Epoch [472/500], Train Loss: 0.0937\n",
      "Epoch [473/500], Train Loss: 0.0937\n",
      "Epoch [474/500], Train Loss: 0.0937\n",
      "Epoch [475/500], Train Loss: 0.0937\n",
      "Epoch [476/500], Train Loss: 0.0936\n",
      "Epoch [477/500], Train Loss: 0.0936\n",
      "Epoch [478/500], Train Loss: 0.0935\n",
      "Epoch [479/500], Train Loss: 0.0936\n",
      "Epoch [480/500], Train Loss: 0.0935\n",
      "Epoch [481/500], Train Loss: 0.0935\n",
      "Epoch [482/500], Train Loss: 0.0935\n",
      "Epoch [483/500], Train Loss: 0.0935\n",
      "Epoch [484/500], Train Loss: 0.0933\n",
      "Epoch [485/500], Train Loss: 0.0933\n",
      "Epoch [486/500], Train Loss: 0.0933\n",
      "Epoch [487/500], Train Loss: 0.0934\n",
      "Epoch [488/500], Train Loss: 0.0933\n",
      "Epoch [489/500], Train Loss: 0.0933\n",
      "Epoch [490/500], Train Loss: 0.0932\n",
      "Epoch [491/500], Train Loss: 0.0932\n",
      "Epoch [492/500], Train Loss: 0.0932\n",
      "Epoch [493/500], Train Loss: 0.0931\n",
      "Epoch [494/500], Train Loss: 0.0931\n",
      "Epoch [495/500], Train Loss: 0.0931\n",
      "Epoch [496/500], Train Loss: 0.0931\n",
      "Epoch [497/500], Train Loss: 0.0930\n",
      "Epoch [498/500], Train Loss: 0.0931\n",
      "Epoch [499/500], Train Loss: 0.0930\n",
      "Epoch [500/500], Train Loss: 0.0930\n",
      "Final model saved at: ../../../Data/model_results/logistic_regression/models/chr16/final_model_chr16.pth\n",
      "Individual R^2 scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr16/individual_r2_scores_chr16.csv\n",
      "Individual IQS scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr16/individual_iqs_scores_chr16.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-18 12:32:21,653] A new study created in RDB with name: chr17_study\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual AUC ROC curves saved in: ../../../Data/model_results/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../../Data/model_results/logistic_regression/csv_files/performance_metrics.csv\n",
      "Total SNPs:  760\n",
      "PRS313 SNPs:  18\n",
      "Total SNPs used for Training:  742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-06-18 12:33:14,270] Trial 4 finished with value: 0.036340780183672906 and parameters: {'learning_rate': 0.0370475206140463, 'l1_coef': 1.0852769314312855e-05, 'patience': 8, 'batch_size': 256}. Best is trial 4 with value: 0.036340780183672906.\n",
      "[I 2024-06-18 12:33:35,567] Trial 2 finished with value: 0.03826434466128166 and parameters: {'learning_rate': 0.0036997686260389195, 'l1_coef': 1.0265277132651641e-05, 'patience': 5, 'batch_size': 32}. Best is trial 4 with value: 0.036340780183672906.\n",
      "[I 2024-06-18 12:33:49,375] Trial 1 finished with value: 0.1528091617992946 and parameters: {'learning_rate': 0.003102010890004305, 'l1_coef': 0.0004818415981078156, 'patience': 6, 'batch_size': 64}. Best is trial 4 with value: 0.036340780183672906.\n",
      "[I 2024-06-18 12:34:14,737] Trial 3 finished with value: 0.3092131957411766 and parameters: {'learning_rate': 0.005885993196287397, 'l1_coef': 0.015554906582993612, 'patience': 17, 'batch_size': 128}. Best is trial 4 with value: 0.036340780183672906.\n",
      "[I 2024-06-18 12:34:36,773] Trial 6 finished with value: 0.3105019665681399 and parameters: {'learning_rate': 0.0013283534100893193, 'l1_coef': 0.03384442277168166, 'patience': 10, 'batch_size': 32}. Best is trial 4 with value: 0.036340780183672906.\n",
      "[I 2024-06-18 12:34:40,675] Trial 5 finished with value: 0.15554574600287846 and parameters: {'learning_rate': 0.005596996350692488, 'l1_coef': 0.0007413116017973148, 'patience': 11, 'batch_size': 64}. Best is trial 4 with value: 0.036340780183672906.\n",
      "[I 2024-06-18 12:34:40,764] Trial 9 finished with value: 0.23156823962926865 and parameters: {'learning_rate': 0.0020469728670289406, 'l1_coef': 0.0025919068099019135, 'patience': 14, 'batch_size': 256}. Best is trial 4 with value: 0.036340780183672906.\n",
      "[I 2024-06-18 12:34:41,392] Trial 8 finished with value: 0.1374426529957698 and parameters: {'learning_rate': 0.004730997355898947, 'l1_coef': 0.0005830343720347583, 'patience': 16, 'batch_size': 32}. Best is trial 4 with value: 0.036340780183672906.\n",
      "[I 2024-06-18 12:34:41,420] Trial 7 finished with value: 0.32174876779317857 and parameters: {'learning_rate': 0.0008502240526223446, 'l1_coef': 0.06114721272006591, 'patience': 8, 'batch_size': 128}. Best is trial 4 with value: 0.036340780183672906.\n",
      "[I 2024-06-18 12:34:42,558] Trial 0 finished with value: 0.279312701523304 and parameters: {'learning_rate': 0.00103557700132916, 'l1_coef': 0.0052182441027866155, 'patience': 18, 'batch_size': 128}. Best is trial 4 with value: 0.036340780183672906.\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 17 - Best hyperparameters: {'learning_rate': 0.0370475206140463, 'l1_coef': 1.0852769314312855e-05, 'patience': 8, 'batch_size': 256}\n",
      "Chr 17 - Best value: 0.0363\n",
      "Epoch [1/500], Train Loss: 0.6761\n",
      "Epoch [2/500], Train Loss: 0.4088\n",
      "Epoch [3/500], Train Loss: 0.2454\n",
      "Epoch [4/500], Train Loss: 0.1596\n",
      "Epoch [5/500], Train Loss: 0.1190\n",
      "Epoch [6/500], Train Loss: 0.1001\n",
      "Epoch [7/500], Train Loss: 0.0878\n",
      "Epoch [8/500], Train Loss: 0.0787\n",
      "Epoch [9/500], Train Loss: 0.0722\n",
      "Epoch [10/500], Train Loss: 0.0675\n",
      "Epoch [11/500], Train Loss: 0.0636\n",
      "Epoch [12/500], Train Loss: 0.0604\n",
      "Epoch [13/500], Train Loss: 0.0575\n",
      "Epoch [14/500], Train Loss: 0.0540\n",
      "Epoch [15/500], Train Loss: 0.0518\n",
      "Epoch [16/500], Train Loss: 0.0495\n",
      "Epoch [17/500], Train Loss: 0.0481\n",
      "Epoch [18/500], Train Loss: 0.0464\n",
      "Epoch [19/500], Train Loss: 0.0449\n",
      "Epoch [20/500], Train Loss: 0.0432\n",
      "Epoch [21/500], Train Loss: 0.0418\n",
      "Epoch [22/500], Train Loss: 0.0405\n",
      "Epoch [23/500], Train Loss: 0.0392\n",
      "Epoch [24/500], Train Loss: 0.0383\n",
      "Epoch [25/500], Train Loss: 0.0373\n",
      "Epoch [26/500], Train Loss: 0.0366\n",
      "Epoch [27/500], Train Loss: 0.0360\n",
      "Epoch [28/500], Train Loss: 0.0355\n",
      "Epoch [29/500], Train Loss: 0.0351\n",
      "Epoch [30/500], Train Loss: 0.0344\n",
      "Epoch [31/500], Train Loss: 0.0338\n",
      "Epoch [32/500], Train Loss: 0.0332\n",
      "Epoch [33/500], Train Loss: 0.0329\n",
      "Epoch [34/500], Train Loss: 0.0325\n",
      "Epoch [35/500], Train Loss: 0.0322\n",
      "Epoch [36/500], Train Loss: 0.0319\n",
      "Epoch [37/500], Train Loss: 0.0318\n",
      "Epoch [38/500], Train Loss: 0.0312\n",
      "Epoch [39/500], Train Loss: 0.0308\n",
      "Epoch [40/500], Train Loss: 0.0306\n",
      "Epoch [41/500], Train Loss: 0.0304\n",
      "Epoch [42/500], Train Loss: 0.0301\n",
      "Epoch [43/500], Train Loss: 0.0298\n",
      "Epoch [44/500], Train Loss: 0.0293\n",
      "Epoch [45/500], Train Loss: 0.0293\n",
      "Epoch [46/500], Train Loss: 0.0290\n",
      "Epoch [47/500], Train Loss: 0.0287\n",
      "Epoch [48/500], Train Loss: 0.0285\n",
      "Epoch [49/500], Train Loss: 0.0284\n",
      "Epoch [50/500], Train Loss: 0.0281\n",
      "Epoch [51/500], Train Loss: 0.0281\n",
      "Epoch [52/500], Train Loss: 0.0278\n",
      "Epoch [53/500], Train Loss: 0.0276\n",
      "Epoch [54/500], Train Loss: 0.0275\n",
      "Epoch [55/500], Train Loss: 0.0274\n",
      "Epoch [56/500], Train Loss: 0.0272\n",
      "Epoch [57/500], Train Loss: 0.0272\n",
      "Epoch [58/500], Train Loss: 0.0269\n",
      "Epoch [59/500], Train Loss: 0.0268\n",
      "Epoch [60/500], Train Loss: 0.0269\n",
      "Epoch [61/500], Train Loss: 0.0269\n",
      "Epoch [62/500], Train Loss: 0.0265\n",
      "Epoch [63/500], Train Loss: 0.0265\n",
      "Epoch [64/500], Train Loss: 0.0263\n",
      "Epoch [65/500], Train Loss: 0.0261\n",
      "Epoch [66/500], Train Loss: 0.0262\n",
      "Epoch [67/500], Train Loss: 0.0261\n",
      "Epoch [68/500], Train Loss: 0.0260\n",
      "Epoch [69/500], Train Loss: 0.0257\n",
      "Epoch [70/500], Train Loss: 0.0255\n",
      "Epoch [71/500], Train Loss: 0.0254\n",
      "Epoch [72/500], Train Loss: 0.0254\n",
      "Epoch [73/500], Train Loss: 0.0252\n",
      "Epoch [74/500], Train Loss: 0.0251\n",
      "Epoch [75/500], Train Loss: 0.0250\n",
      "Epoch [76/500], Train Loss: 0.0250\n",
      "Epoch [77/500], Train Loss: 0.0249\n",
      "Epoch [78/500], Train Loss: 0.0249\n",
      "Epoch [79/500], Train Loss: 0.0247\n",
      "Epoch [80/500], Train Loss: 0.0246\n",
      "Epoch [81/500], Train Loss: 0.0247\n",
      "Epoch [82/500], Train Loss: 0.0246\n",
      "Epoch [83/500], Train Loss: 0.0246\n",
      "Epoch [84/500], Train Loss: 0.0245\n",
      "Epoch [85/500], Train Loss: 0.0245\n",
      "Epoch [86/500], Train Loss: 0.0243\n",
      "Epoch [87/500], Train Loss: 0.0242\n",
      "Epoch [88/500], Train Loss: 0.0241\n",
      "Epoch [89/500], Train Loss: 0.0241\n",
      "Epoch [90/500], Train Loss: 0.0240\n",
      "Epoch [91/500], Train Loss: 0.0240\n",
      "Epoch [92/500], Train Loss: 0.0240\n",
      "Epoch [93/500], Train Loss: 0.0239\n",
      "Epoch [94/500], Train Loss: 0.0238\n",
      "Epoch [95/500], Train Loss: 0.0237\n",
      "Epoch [96/500], Train Loss: 0.0236\n",
      "Epoch [97/500], Train Loss: 0.0237\n",
      "Epoch [98/500], Train Loss: 0.0238\n",
      "Epoch [99/500], Train Loss: 0.0238\n",
      "Epoch [100/500], Train Loss: 0.0237\n",
      "Epoch [101/500], Train Loss: 0.0238\n",
      "Epoch [102/500], Train Loss: 0.0237\n",
      "Epoch [103/500], Train Loss: 0.0233\n",
      "Epoch [104/500], Train Loss: 0.0229\n",
      "Epoch [105/500], Train Loss: 0.0229\n",
      "Epoch [106/500], Train Loss: 0.0227\n",
      "Epoch [107/500], Train Loss: 0.0227\n",
      "Epoch [108/500], Train Loss: 0.0227\n",
      "Epoch [109/500], Train Loss: 0.0227\n",
      "Epoch [110/500], Train Loss: 0.0227\n",
      "Epoch [111/500], Train Loss: 0.0227\n",
      "Epoch [112/500], Train Loss: 0.0227\n",
      "Epoch [113/500], Train Loss: 0.0226\n",
      "Epoch [114/500], Train Loss: 0.0226\n",
      "Epoch [115/500], Train Loss: 0.0226\n",
      "Epoch [116/500], Train Loss: 0.0226\n",
      "Epoch [117/500], Train Loss: 0.0226\n",
      "Epoch [118/500], Train Loss: 0.0226\n",
      "Epoch [119/500], Train Loss: 0.0226\n",
      "Epoch [120/500], Train Loss: 0.0226\n",
      "Epoch [121/500], Train Loss: 0.0226\n",
      "Epoch [122/500], Train Loss: 0.0226\n",
      "Epoch [123/500], Train Loss: 0.0226\n",
      "Epoch [124/500], Train Loss: 0.0226\n",
      "Epoch [125/500], Train Loss: 0.0226\n",
      "Epoch [126/500], Train Loss: 0.0226\n",
      "Epoch [127/500], Train Loss: 0.0226\n",
      "Epoch [128/500], Train Loss: 0.0226\n",
      "Epoch [129/500], Train Loss: 0.0226\n",
      "Epoch [130/500], Train Loss: 0.0226\n",
      "Epoch [131/500], Train Loss: 0.0226\n",
      "Epoch [132/500], Train Loss: 0.0226\n",
      "Epoch [133/500], Train Loss: 0.0225\n",
      "Epoch [134/500], Train Loss: 0.0226\n",
      "Epoch [135/500], Train Loss: 0.0226\n",
      "Epoch [136/500], Train Loss: 0.0225\n",
      "Epoch [137/500], Train Loss: 0.0225\n",
      "Epoch [138/500], Train Loss: 0.0226\n",
      "Epoch [139/500], Train Loss: 0.0225\n",
      "Epoch [140/500], Train Loss: 0.0225\n",
      "Epoch [141/500], Train Loss: 0.0225\n",
      "Epoch [142/500], Train Loss: 0.0224\n",
      "Epoch [143/500], Train Loss: 0.0224\n",
      "Epoch [144/500], Train Loss: 0.0225\n",
      "Epoch [145/500], Train Loss: 0.0224\n",
      "Epoch [146/500], Train Loss: 0.0225\n",
      "Epoch [147/500], Train Loss: 0.0225\n",
      "Epoch [148/500], Train Loss: 0.0224\n",
      "Epoch [149/500], Train Loss: 0.0224\n",
      "Epoch [150/500], Train Loss: 0.0224\n",
      "Epoch [151/500], Train Loss: 0.0225\n",
      "Epoch [152/500], Train Loss: 0.0225\n",
      "Epoch [153/500], Train Loss: 0.0224\n",
      "Epoch [154/500], Train Loss: 0.0225\n",
      "Epoch [155/500], Train Loss: 0.0224\n",
      "Epoch [156/500], Train Loss: 0.0224\n",
      "Epoch [157/500], Train Loss: 0.0224\n",
      "Epoch [158/500], Train Loss: 0.0224\n",
      "Early stopping at epoch 158\n",
      "Final model saved at: ../../../Data/model_results/logistic_regression/models/chr17/final_model_chr17.pth\n",
      "Individual R^2 scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr17/individual_r2_scores_chr17.csv\n",
      "Individual IQS scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr17/individual_iqs_scores_chr17.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-18 12:34:45,108] A new study created in RDB with name: chr18_study\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual AUC ROC curves saved in: ../../../Data/model_results/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../../Data/model_results/logistic_regression/csv_files/performance_metrics.csv\n",
      "Total SNPs:  1178\n",
      "PRS313 SNPs:  18\n",
      "Total SNPs used for Training:  1160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-06-18 12:36:35,532] Trial 6 finished with value: 0.5164165005087853 and parameters: {'learning_rate': 0.061295343930814226, 'l1_coef': 0.004610129002978482, 'patience': 18, 'batch_size': 128}. Best is trial 6 with value: 0.5164165005087853.\n",
      "[I 2024-06-18 12:36:44,851] Trial 7 finished with value: 0.575837322643825 and parameters: {'learning_rate': 0.02458608161481166, 'l1_coef': 0.01905125767364561, 'patience': 14, 'batch_size': 64}. Best is trial 6 with value: 0.5164165005087853.\n",
      "[I 2024-06-18 12:36:46,940] Trial 2 finished with value: 0.587911206483841 and parameters: {'learning_rate': 0.002671332204713552, 'l1_coef': 0.02896195453975584, 'patience': 9, 'batch_size': 256}. Best is trial 6 with value: 0.5164165005087853.\n",
      "[I 2024-06-18 12:36:47,257] Trial 1 finished with value: 0.5758445409628061 and parameters: {'learning_rate': 0.0053941730725796545, 'l1_coef': 0.019374519831483885, 'patience': 8, 'batch_size': 32}. Best is trial 6 with value: 0.5164165005087853.\n",
      "[I 2024-06-18 12:36:53,279] Trial 4 finished with value: 0.5757329225540161 and parameters: {'learning_rate': 0.04608858975688994, 'l1_coef': 0.018593888836336774, 'patience': 19, 'batch_size': 64}. Best is trial 6 with value: 0.5164165005087853.\n",
      "[I 2024-06-18 12:36:56,757] Trial 5 finished with value: 0.42296646833419793 and parameters: {'learning_rate': 0.0023766652563297092, 'l1_coef': 0.002284299569758488, 'patience': 15, 'batch_size': 64}. Best is trial 5 with value: 0.42296646833419793.\n",
      "[I 2024-06-18 12:36:57,439] Trial 8 finished with value: 0.1635846219956875 and parameters: {'learning_rate': 0.005445392036695937, 'l1_coef': 0.00015565603463728921, 'patience': 7, 'batch_size': 128}. Best is trial 8 with value: 0.1635846219956875.\n",
      "[I 2024-06-18 12:37:03,764] Trial 9 finished with value: 0.20984819680452346 and parameters: {'learning_rate': 0.0008860068204884345, 'l1_coef': 0.00028597414566068657, 'patience': 8, 'batch_size': 256}. Best is trial 8 with value: 0.1635846219956875.\n",
      "[I 2024-06-18 12:37:05,816] Trial 0 finished with value: 0.12723615318536757 and parameters: {'learning_rate': 0.0002883877331261967, 'l1_coef': 2.2053185487347745e-05, 'patience': 10, 'batch_size': 128}. Best is trial 0 with value: 0.12723615318536757.\n",
      "[I 2024-06-18 12:37:13,471] Trial 3 finished with value: 0.3228340140410832 and parameters: {'learning_rate': 0.00012725554688548896, 'l1_coef': 0.0009160286936640281, 'patience': 10, 'batch_size': 64}. Best is trial 0 with value: 0.12723615318536757.\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 18 - Best hyperparameters: {'learning_rate': 0.0002883877331261967, 'l1_coef': 2.2053185487347745e-05, 'patience': 10, 'batch_size': 128}\n",
      "Chr 18 - Best value: 0.1272\n",
      "Epoch [1/500], Train Loss: 0.6201\n",
      "Epoch [2/500], Train Loss: 0.5487\n",
      "Epoch [3/500], Train Loss: 0.5219\n",
      "Epoch [4/500], Train Loss: 0.5059\n",
      "Epoch [5/500], Train Loss: 0.4948\n",
      "Epoch [6/500], Train Loss: 0.4855\n",
      "Epoch [7/500], Train Loss: 0.4773\n",
      "Epoch [8/500], Train Loss: 0.4707\n",
      "Epoch [9/500], Train Loss: 0.4641\n",
      "Epoch [10/500], Train Loss: 0.4581\n",
      "Epoch [11/500], Train Loss: 0.4519\n",
      "Epoch [12/500], Train Loss: 0.4467\n",
      "Epoch [13/500], Train Loss: 0.4412\n",
      "Epoch [14/500], Train Loss: 0.4357\n",
      "Epoch [15/500], Train Loss: 0.4305\n",
      "Epoch [16/500], Train Loss: 0.4257\n",
      "Epoch [17/500], Train Loss: 0.4209\n",
      "Epoch [18/500], Train Loss: 0.4158\n",
      "Epoch [19/500], Train Loss: 0.4119\n",
      "Epoch [20/500], Train Loss: 0.4072\n",
      "Epoch [21/500], Train Loss: 0.4032\n",
      "Epoch [22/500], Train Loss: 0.3987\n",
      "Epoch [23/500], Train Loss: 0.3950\n",
      "Epoch [24/500], Train Loss: 0.3910\n",
      "Epoch [25/500], Train Loss: 0.3868\n",
      "Epoch [26/500], Train Loss: 0.3833\n",
      "Epoch [27/500], Train Loss: 0.3796\n",
      "Epoch [28/500], Train Loss: 0.3762\n",
      "Epoch [29/500], Train Loss: 0.3724\n",
      "Epoch [30/500], Train Loss: 0.3685\n",
      "Epoch [31/500], Train Loss: 0.3654\n",
      "Epoch [32/500], Train Loss: 0.3620\n",
      "Epoch [33/500], Train Loss: 0.3589\n",
      "Epoch [34/500], Train Loss: 0.3557\n",
      "Epoch [35/500], Train Loss: 0.3526\n",
      "Epoch [36/500], Train Loss: 0.3495\n",
      "Epoch [37/500], Train Loss: 0.3466\n",
      "Epoch [38/500], Train Loss: 0.3437\n",
      "Epoch [39/500], Train Loss: 0.3405\n",
      "Epoch [40/500], Train Loss: 0.3378\n",
      "Epoch [41/500], Train Loss: 0.3350\n",
      "Epoch [42/500], Train Loss: 0.3327\n",
      "Epoch [43/500], Train Loss: 0.3298\n",
      "Epoch [44/500], Train Loss: 0.3269\n",
      "Epoch [45/500], Train Loss: 0.3247\n",
      "Epoch [46/500], Train Loss: 0.3221\n",
      "Epoch [47/500], Train Loss: 0.3195\n",
      "Epoch [48/500], Train Loss: 0.3169\n",
      "Epoch [49/500], Train Loss: 0.3146\n",
      "Epoch [50/500], Train Loss: 0.3122\n",
      "Epoch [51/500], Train Loss: 0.3101\n",
      "Epoch [52/500], Train Loss: 0.3078\n",
      "Epoch [53/500], Train Loss: 0.3055\n",
      "Epoch [54/500], Train Loss: 0.3033\n",
      "Epoch [55/500], Train Loss: 0.3012\n",
      "Epoch [56/500], Train Loss: 0.2991\n",
      "Epoch [57/500], Train Loss: 0.2970\n",
      "Epoch [58/500], Train Loss: 0.2949\n",
      "Epoch [59/500], Train Loss: 0.2931\n",
      "Epoch [60/500], Train Loss: 0.2913\n",
      "Epoch [61/500], Train Loss: 0.2893\n",
      "Epoch [62/500], Train Loss: 0.2871\n",
      "Epoch [63/500], Train Loss: 0.2849\n",
      "Epoch [64/500], Train Loss: 0.2835\n",
      "Epoch [65/500], Train Loss: 0.2813\n",
      "Epoch [66/500], Train Loss: 0.2797\n",
      "Epoch [67/500], Train Loss: 0.2779\n",
      "Epoch [68/500], Train Loss: 0.2762\n",
      "Epoch [69/500], Train Loss: 0.2746\n",
      "Epoch [70/500], Train Loss: 0.2728\n",
      "Epoch [71/500], Train Loss: 0.2713\n",
      "Epoch [72/500], Train Loss: 0.2695\n",
      "Epoch [73/500], Train Loss: 0.2679\n",
      "Epoch [74/500], Train Loss: 0.2664\n",
      "Epoch [75/500], Train Loss: 0.2650\n",
      "Epoch [76/500], Train Loss: 0.2634\n",
      "Epoch [77/500], Train Loss: 0.2617\n",
      "Epoch [78/500], Train Loss: 0.2600\n",
      "Epoch [79/500], Train Loss: 0.2589\n",
      "Epoch [80/500], Train Loss: 0.2573\n",
      "Epoch [81/500], Train Loss: 0.2558\n",
      "Epoch [82/500], Train Loss: 0.2541\n",
      "Epoch [83/500], Train Loss: 0.2531\n",
      "Epoch [84/500], Train Loss: 0.2514\n",
      "Epoch [85/500], Train Loss: 0.2504\n",
      "Epoch [86/500], Train Loss: 0.2487\n",
      "Epoch [87/500], Train Loss: 0.2476\n",
      "Epoch [88/500], Train Loss: 0.2461\n",
      "Epoch [89/500], Train Loss: 0.2449\n",
      "Epoch [90/500], Train Loss: 0.2434\n",
      "Epoch [91/500], Train Loss: 0.2423\n",
      "Epoch [92/500], Train Loss: 0.2411\n",
      "Epoch [93/500], Train Loss: 0.2397\n",
      "Epoch [94/500], Train Loss: 0.2387\n",
      "Epoch [95/500], Train Loss: 0.2374\n",
      "Epoch [96/500], Train Loss: 0.2362\n",
      "Epoch [97/500], Train Loss: 0.2351\n",
      "Epoch [98/500], Train Loss: 0.2340\n",
      "Epoch [99/500], Train Loss: 0.2327\n",
      "Epoch [100/500], Train Loss: 0.2316\n",
      "Epoch [101/500], Train Loss: 0.2306\n",
      "Epoch [102/500], Train Loss: 0.2295\n",
      "Epoch [103/500], Train Loss: 0.2284\n",
      "Epoch [104/500], Train Loss: 0.2272\n",
      "Epoch [105/500], Train Loss: 0.2262\n",
      "Epoch [106/500], Train Loss: 0.2250\n",
      "Epoch [107/500], Train Loss: 0.2241\n",
      "Epoch [108/500], Train Loss: 0.2229\n",
      "Epoch [109/500], Train Loss: 0.2221\n",
      "Epoch [110/500], Train Loss: 0.2210\n",
      "Epoch [111/500], Train Loss: 0.2201\n",
      "Epoch [112/500], Train Loss: 0.2192\n",
      "Epoch [113/500], Train Loss: 0.2180\n",
      "Epoch [114/500], Train Loss: 0.2172\n",
      "Epoch [115/500], Train Loss: 0.2160\n",
      "Epoch [116/500], Train Loss: 0.2152\n",
      "Epoch [117/500], Train Loss: 0.2139\n",
      "Epoch [118/500], Train Loss: 0.2133\n",
      "Epoch [119/500], Train Loss: 0.2124\n",
      "Epoch [120/500], Train Loss: 0.2119\n",
      "Epoch [121/500], Train Loss: 0.2107\n",
      "Epoch [122/500], Train Loss: 0.2098\n",
      "Epoch [123/500], Train Loss: 0.2088\n",
      "Epoch [124/500], Train Loss: 0.2081\n",
      "Epoch [125/500], Train Loss: 0.2073\n",
      "Epoch [126/500], Train Loss: 0.2063\n",
      "Epoch [127/500], Train Loss: 0.2052\n",
      "Epoch [128/500], Train Loss: 0.2045\n",
      "Epoch [129/500], Train Loss: 0.2037\n",
      "Epoch [130/500], Train Loss: 0.2030\n",
      "Epoch [131/500], Train Loss: 0.2021\n",
      "Epoch [132/500], Train Loss: 0.2014\n",
      "Epoch [133/500], Train Loss: 0.2004\n",
      "Epoch [134/500], Train Loss: 0.1996\n",
      "Epoch [135/500], Train Loss: 0.1989\n",
      "Epoch [136/500], Train Loss: 0.1982\n",
      "Epoch [137/500], Train Loss: 0.1975\n",
      "Epoch [138/500], Train Loss: 0.1969\n",
      "Epoch [139/500], Train Loss: 0.1956\n",
      "Epoch [140/500], Train Loss: 0.1951\n",
      "Epoch [141/500], Train Loss: 0.1944\n",
      "Epoch [142/500], Train Loss: 0.1937\n",
      "Epoch [143/500], Train Loss: 0.1928\n",
      "Epoch [144/500], Train Loss: 0.1924\n",
      "Epoch [145/500], Train Loss: 0.1915\n",
      "Epoch [146/500], Train Loss: 0.1910\n",
      "Epoch [147/500], Train Loss: 0.1901\n",
      "Epoch [148/500], Train Loss: 0.1895\n",
      "Epoch [149/500], Train Loss: 0.1889\n",
      "Epoch [150/500], Train Loss: 0.1882\n",
      "Epoch [151/500], Train Loss: 0.1875\n",
      "Epoch [152/500], Train Loss: 0.1867\n",
      "Epoch [153/500], Train Loss: 0.1857\n",
      "Epoch [154/500], Train Loss: 0.1852\n",
      "Epoch [155/500], Train Loss: 0.1849\n",
      "Epoch [156/500], Train Loss: 0.1840\n",
      "Epoch [157/500], Train Loss: 0.1836\n",
      "Epoch [158/500], Train Loss: 0.1830\n",
      "Epoch [159/500], Train Loss: 0.1824\n",
      "Epoch [160/500], Train Loss: 0.1815\n",
      "Epoch [161/500], Train Loss: 0.1810\n",
      "Epoch [162/500], Train Loss: 0.1804\n",
      "Epoch [163/500], Train Loss: 0.1796\n",
      "Epoch [164/500], Train Loss: 0.1790\n",
      "Epoch [165/500], Train Loss: 0.1786\n",
      "Epoch [166/500], Train Loss: 0.1778\n",
      "Epoch [167/500], Train Loss: 0.1773\n",
      "Epoch [168/500], Train Loss: 0.1767\n",
      "Epoch [169/500], Train Loss: 0.1760\n",
      "Epoch [170/500], Train Loss: 0.1757\n",
      "Epoch [171/500], Train Loss: 0.1750\n",
      "Epoch [172/500], Train Loss: 0.1745\n",
      "Epoch [173/500], Train Loss: 0.1737\n",
      "Epoch [174/500], Train Loss: 0.1733\n",
      "Epoch [175/500], Train Loss: 0.1728\n",
      "Epoch [176/500], Train Loss: 0.1723\n",
      "Epoch [177/500], Train Loss: 0.1717\n",
      "Epoch [178/500], Train Loss: 0.1711\n",
      "Epoch [179/500], Train Loss: 0.1705\n",
      "Epoch [180/500], Train Loss: 0.1700\n",
      "Epoch [181/500], Train Loss: 0.1699\n",
      "Epoch [182/500], Train Loss: 0.1691\n",
      "Epoch [183/500], Train Loss: 0.1685\n",
      "Epoch [184/500], Train Loss: 0.1681\n",
      "Epoch [185/500], Train Loss: 0.1676\n",
      "Epoch [186/500], Train Loss: 0.1668\n",
      "Epoch [187/500], Train Loss: 0.1663\n",
      "Epoch [188/500], Train Loss: 0.1662\n",
      "Epoch [189/500], Train Loss: 0.1656\n",
      "Epoch [190/500], Train Loss: 0.1649\n",
      "Epoch [191/500], Train Loss: 0.1646\n",
      "Epoch [192/500], Train Loss: 0.1640\n",
      "Epoch [193/500], Train Loss: 0.1634\n",
      "Epoch [194/500], Train Loss: 0.1631\n",
      "Epoch [195/500], Train Loss: 0.1626\n",
      "Epoch [196/500], Train Loss: 0.1619\n",
      "Epoch [197/500], Train Loss: 0.1613\n",
      "Epoch [198/500], Train Loss: 0.1611\n",
      "Epoch [199/500], Train Loss: 0.1605\n",
      "Epoch [200/500], Train Loss: 0.1602\n",
      "Epoch [201/500], Train Loss: 0.1597\n",
      "Epoch [202/500], Train Loss: 0.1593\n",
      "Epoch [203/500], Train Loss: 0.1587\n",
      "Epoch [204/500], Train Loss: 0.1584\n",
      "Epoch [205/500], Train Loss: 0.1579\n",
      "Epoch [206/500], Train Loss: 0.1573\n",
      "Epoch [207/500], Train Loss: 0.1571\n",
      "Epoch [208/500], Train Loss: 0.1566\n",
      "Epoch [209/500], Train Loss: 0.1562\n",
      "Epoch [210/500], Train Loss: 0.1559\n",
      "Epoch [211/500], Train Loss: 0.1554\n",
      "Epoch [212/500], Train Loss: 0.1550\n",
      "Epoch [213/500], Train Loss: 0.1543\n",
      "Epoch [214/500], Train Loss: 0.1540\n",
      "Epoch [215/500], Train Loss: 0.1536\n",
      "Epoch [216/500], Train Loss: 0.1533\n",
      "Epoch [217/500], Train Loss: 0.1527\n",
      "Epoch [218/500], Train Loss: 0.1522\n",
      "Epoch [219/500], Train Loss: 0.1518\n",
      "Epoch [220/500], Train Loss: 0.1516\n",
      "Epoch [221/500], Train Loss: 0.1511\n",
      "Epoch [222/500], Train Loss: 0.1507\n",
      "Epoch [223/500], Train Loss: 0.1504\n",
      "Epoch [224/500], Train Loss: 0.1502\n",
      "Epoch [225/500], Train Loss: 0.1496\n",
      "Epoch [226/500], Train Loss: 0.1496\n",
      "Epoch [227/500], Train Loss: 0.1487\n",
      "Epoch [228/500], Train Loss: 0.1486\n",
      "Epoch [229/500], Train Loss: 0.1482\n",
      "Epoch [230/500], Train Loss: 0.1477\n",
      "Epoch [231/500], Train Loss: 0.1473\n",
      "Epoch [232/500], Train Loss: 0.1471\n",
      "Epoch [233/500], Train Loss: 0.1467\n",
      "Epoch [234/500], Train Loss: 0.1464\n",
      "Epoch [235/500], Train Loss: 0.1457\n",
      "Epoch [236/500], Train Loss: 0.1456\n",
      "Epoch [237/500], Train Loss: 0.1454\n",
      "Epoch [238/500], Train Loss: 0.1449\n",
      "Epoch [239/500], Train Loss: 0.1445\n",
      "Epoch [240/500], Train Loss: 0.1438\n",
      "Epoch [241/500], Train Loss: 0.1438\n",
      "Epoch [242/500], Train Loss: 0.1434\n",
      "Epoch [243/500], Train Loss: 0.1430\n",
      "Epoch [244/500], Train Loss: 0.1427\n",
      "Epoch [245/500], Train Loss: 0.1424\n",
      "Epoch [246/500], Train Loss: 0.1420\n",
      "Epoch [247/500], Train Loss: 0.1419\n",
      "Epoch [248/500], Train Loss: 0.1416\n",
      "Epoch [249/500], Train Loss: 0.1408\n",
      "Epoch [250/500], Train Loss: 0.1407\n",
      "Epoch [251/500], Train Loss: 0.1403\n",
      "Epoch [252/500], Train Loss: 0.1401\n",
      "Epoch [253/500], Train Loss: 0.1399\n",
      "Epoch [254/500], Train Loss: 0.1393\n",
      "Epoch [255/500], Train Loss: 0.1389\n",
      "Epoch [256/500], Train Loss: 0.1387\n",
      "Epoch [257/500], Train Loss: 0.1382\n",
      "Epoch [258/500], Train Loss: 0.1380\n",
      "Epoch [259/500], Train Loss: 0.1377\n",
      "Epoch [260/500], Train Loss: 0.1373\n",
      "Epoch [261/500], Train Loss: 0.1372\n",
      "Epoch [262/500], Train Loss: 0.1370\n",
      "Epoch [263/500], Train Loss: 0.1365\n",
      "Epoch [264/500], Train Loss: 0.1361\n",
      "Epoch [265/500], Train Loss: 0.1359\n",
      "Epoch [266/500], Train Loss: 0.1356\n",
      "Epoch [267/500], Train Loss: 0.1354\n",
      "Epoch [268/500], Train Loss: 0.1350\n",
      "Epoch [269/500], Train Loss: 0.1349\n",
      "Epoch [270/500], Train Loss: 0.1344\n",
      "Epoch [271/500], Train Loss: 0.1340\n",
      "Epoch [272/500], Train Loss: 0.1339\n",
      "Epoch [273/500], Train Loss: 0.1334\n",
      "Epoch [274/500], Train Loss: 0.1331\n",
      "Epoch [275/500], Train Loss: 0.1332\n",
      "Epoch [276/500], Train Loss: 0.1325\n",
      "Epoch [277/500], Train Loss: 0.1325\n",
      "Epoch [278/500], Train Loss: 0.1319\n",
      "Epoch [279/500], Train Loss: 0.1319\n",
      "Epoch [280/500], Train Loss: 0.1314\n",
      "Epoch [281/500], Train Loss: 0.1313\n",
      "Epoch [282/500], Train Loss: 0.1310\n",
      "Epoch [283/500], Train Loss: 0.1306\n",
      "Epoch [284/500], Train Loss: 0.1304\n",
      "Epoch [285/500], Train Loss: 0.1299\n",
      "Epoch [286/500], Train Loss: 0.1298\n",
      "Epoch [287/500], Train Loss: 0.1296\n",
      "Epoch [288/500], Train Loss: 0.1295\n",
      "Epoch [289/500], Train Loss: 0.1290\n",
      "Epoch [290/500], Train Loss: 0.1288\n",
      "Epoch [291/500], Train Loss: 0.1284\n",
      "Epoch [292/500], Train Loss: 0.1285\n",
      "Epoch [293/500], Train Loss: 0.1281\n",
      "Epoch [294/500], Train Loss: 0.1276\n",
      "Epoch [295/500], Train Loss: 0.1276\n",
      "Epoch [296/500], Train Loss: 0.1272\n",
      "Epoch [297/500], Train Loss: 0.1271\n",
      "Epoch [298/500], Train Loss: 0.1266\n",
      "Epoch [299/500], Train Loss: 0.1266\n",
      "Epoch [300/500], Train Loss: 0.1262\n",
      "Epoch [301/500], Train Loss: 0.1259\n",
      "Epoch [302/500], Train Loss: 0.1256\n",
      "Epoch [303/500], Train Loss: 0.1255\n",
      "Epoch [304/500], Train Loss: 0.1252\n",
      "Epoch [305/500], Train Loss: 0.1251\n",
      "Epoch [306/500], Train Loss: 0.1247\n",
      "Epoch [307/500], Train Loss: 0.1245\n",
      "Epoch [308/500], Train Loss: 0.1243\n",
      "Epoch [309/500], Train Loss: 0.1241\n",
      "Epoch [310/500], Train Loss: 0.1237\n",
      "Epoch [311/500], Train Loss: 0.1235\n",
      "Epoch [312/500], Train Loss: 0.1233\n",
      "Epoch [313/500], Train Loss: 0.1230\n",
      "Epoch [314/500], Train Loss: 0.1229\n",
      "Epoch [315/500], Train Loss: 0.1225\n",
      "Epoch [316/500], Train Loss: 0.1224\n",
      "Epoch [317/500], Train Loss: 0.1221\n",
      "Epoch [318/500], Train Loss: 0.1218\n",
      "Epoch [319/500], Train Loss: 0.1216\n",
      "Epoch [320/500], Train Loss: 0.1215\n",
      "Epoch [321/500], Train Loss: 0.1210\n",
      "Epoch [322/500], Train Loss: 0.1209\n",
      "Epoch [323/500], Train Loss: 0.1206\n",
      "Epoch [324/500], Train Loss: 0.1206\n",
      "Epoch [325/500], Train Loss: 0.1203\n",
      "Epoch [326/500], Train Loss: 0.1201\n",
      "Epoch [327/500], Train Loss: 0.1201\n",
      "Epoch [328/500], Train Loss: 0.1195\n",
      "Epoch [329/500], Train Loss: 0.1194\n",
      "Epoch [330/500], Train Loss: 0.1192\n",
      "Epoch [331/500], Train Loss: 0.1189\n",
      "Epoch [332/500], Train Loss: 0.1186\n",
      "Epoch [333/500], Train Loss: 0.1184\n",
      "Epoch [334/500], Train Loss: 0.1183\n",
      "Epoch [335/500], Train Loss: 0.1180\n",
      "Epoch [336/500], Train Loss: 0.1178\n",
      "Epoch [337/500], Train Loss: 0.1177\n",
      "Epoch [338/500], Train Loss: 0.1175\n",
      "Epoch [339/500], Train Loss: 0.1173\n",
      "Epoch [340/500], Train Loss: 0.1170\n",
      "Epoch [341/500], Train Loss: 0.1168\n",
      "Epoch [342/500], Train Loss: 0.1167\n",
      "Epoch [343/500], Train Loss: 0.1164\n",
      "Epoch [344/500], Train Loss: 0.1161\n",
      "Epoch [345/500], Train Loss: 0.1160\n",
      "Epoch [346/500], Train Loss: 0.1159\n",
      "Epoch [347/500], Train Loss: 0.1157\n",
      "Epoch [348/500], Train Loss: 0.1154\n",
      "Epoch [349/500], Train Loss: 0.1153\n",
      "Epoch [350/500], Train Loss: 0.1150\n",
      "Epoch [351/500], Train Loss: 0.1151\n",
      "Epoch [352/500], Train Loss: 0.1146\n",
      "Epoch [353/500], Train Loss: 0.1146\n",
      "Epoch [354/500], Train Loss: 0.1142\n",
      "Epoch [355/500], Train Loss: 0.1142\n",
      "Epoch [356/500], Train Loss: 0.1139\n",
      "Epoch [357/500], Train Loss: 0.1136\n",
      "Epoch [358/500], Train Loss: 0.1134\n",
      "Epoch [359/500], Train Loss: 0.1131\n",
      "Epoch [360/500], Train Loss: 0.1129\n",
      "Epoch [361/500], Train Loss: 0.1128\n",
      "Epoch [362/500], Train Loss: 0.1127\n",
      "Epoch [363/500], Train Loss: 0.1126\n",
      "Epoch [364/500], Train Loss: 0.1124\n",
      "Epoch [365/500], Train Loss: 0.1123\n",
      "Epoch [366/500], Train Loss: 0.1119\n",
      "Epoch [367/500], Train Loss: 0.1118\n",
      "Epoch [368/500], Train Loss: 0.1116\n",
      "Epoch [369/500], Train Loss: 0.1115\n",
      "Epoch [370/500], Train Loss: 0.1111\n",
      "Epoch [371/500], Train Loss: 0.1110\n",
      "Epoch [372/500], Train Loss: 0.1110\n",
      "Epoch [373/500], Train Loss: 0.1107\n",
      "Epoch [374/500], Train Loss: 0.1105\n",
      "Epoch [375/500], Train Loss: 0.1102\n",
      "Epoch [376/500], Train Loss: 0.1101\n",
      "Epoch [377/500], Train Loss: 0.1100\n",
      "Epoch [378/500], Train Loss: 0.1100\n",
      "Epoch [379/500], Train Loss: 0.1099\n",
      "Epoch [380/500], Train Loss: 0.1096\n",
      "Epoch [381/500], Train Loss: 0.1094\n",
      "Epoch [382/500], Train Loss: 0.1091\n",
      "Epoch [383/500], Train Loss: 0.1093\n",
      "Epoch [384/500], Train Loss: 0.1088\n",
      "Epoch [385/500], Train Loss: 0.1088\n",
      "Epoch [386/500], Train Loss: 0.1084\n",
      "Epoch [387/500], Train Loss: 0.1082\n",
      "Epoch [388/500], Train Loss: 0.1082\n",
      "Epoch [389/500], Train Loss: 0.1079\n",
      "Epoch [390/500], Train Loss: 0.1078\n",
      "Epoch [391/500], Train Loss: 0.1077\n",
      "Epoch [392/500], Train Loss: 0.1074\n",
      "Epoch [393/500], Train Loss: 0.1072\n",
      "Epoch [394/500], Train Loss: 0.1071\n",
      "Epoch [395/500], Train Loss: 0.1069\n",
      "Epoch [396/500], Train Loss: 0.1069\n",
      "Epoch [397/500], Train Loss: 0.1066\n",
      "Epoch [398/500], Train Loss: 0.1066\n",
      "Epoch [399/500], Train Loss: 0.1063\n",
      "Epoch [400/500], Train Loss: 0.1062\n",
      "Epoch [401/500], Train Loss: 0.1061\n",
      "Epoch [402/500], Train Loss: 0.1060\n",
      "Epoch [403/500], Train Loss: 0.1058\n",
      "Epoch [404/500], Train Loss: 0.1055\n",
      "Epoch [405/500], Train Loss: 0.1056\n",
      "Epoch [406/500], Train Loss: 0.1054\n",
      "Epoch [407/500], Train Loss: 0.1051\n",
      "Epoch [408/500], Train Loss: 0.1050\n",
      "Epoch [409/500], Train Loss: 0.1047\n",
      "Epoch [410/500], Train Loss: 0.1046\n",
      "Epoch [411/500], Train Loss: 0.1043\n",
      "Epoch [412/500], Train Loss: 0.1043\n",
      "Epoch [413/500], Train Loss: 0.1041\n",
      "Epoch [414/500], Train Loss: 0.1041\n",
      "Epoch [415/500], Train Loss: 0.1040\n",
      "Epoch [416/500], Train Loss: 0.1036\n",
      "Epoch [417/500], Train Loss: 0.1037\n",
      "Epoch [418/500], Train Loss: 0.1036\n",
      "Epoch [419/500], Train Loss: 0.1033\n",
      "Epoch [420/500], Train Loss: 0.1032\n",
      "Epoch [421/500], Train Loss: 0.1030\n",
      "Epoch [422/500], Train Loss: 0.1028\n",
      "Epoch [423/500], Train Loss: 0.1028\n",
      "Epoch [424/500], Train Loss: 0.1027\n",
      "Epoch [425/500], Train Loss: 0.1024\n",
      "Epoch [426/500], Train Loss: 0.1024\n",
      "Epoch [427/500], Train Loss: 0.1022\n",
      "Epoch [428/500], Train Loss: 0.1020\n",
      "Epoch [429/500], Train Loss: 0.1019\n",
      "Epoch [430/500], Train Loss: 0.1016\n",
      "Epoch [431/500], Train Loss: 0.1016\n",
      "Epoch [432/500], Train Loss: 0.1014\n",
      "Epoch [433/500], Train Loss: 0.1014\n",
      "Epoch [434/500], Train Loss: 0.1012\n",
      "Epoch [435/500], Train Loss: 0.1011\n",
      "Epoch [436/500], Train Loss: 0.1011\n",
      "Epoch [437/500], Train Loss: 0.1007\n",
      "Epoch [438/500], Train Loss: 0.1007\n",
      "Epoch [439/500], Train Loss: 0.1004\n",
      "Epoch [440/500], Train Loss: 0.1004\n",
      "Epoch [441/500], Train Loss: 0.1003\n",
      "Epoch [442/500], Train Loss: 0.1001\n",
      "Epoch [443/500], Train Loss: 0.0999\n",
      "Epoch [444/500], Train Loss: 0.0997\n",
      "Epoch [445/500], Train Loss: 0.0997\n",
      "Epoch [446/500], Train Loss: 0.0995\n",
      "Epoch [447/500], Train Loss: 0.0995\n",
      "Epoch [448/500], Train Loss: 0.0992\n",
      "Epoch [449/500], Train Loss: 0.0993\n",
      "Epoch [450/500], Train Loss: 0.0991\n",
      "Epoch [451/500], Train Loss: 0.0990\n",
      "Epoch [452/500], Train Loss: 0.0988\n",
      "Epoch [453/500], Train Loss: 0.0987\n",
      "Epoch [454/500], Train Loss: 0.0986\n",
      "Epoch [455/500], Train Loss: 0.0985\n",
      "Epoch [456/500], Train Loss: 0.0982\n",
      "Epoch [457/500], Train Loss: 0.0983\n",
      "Epoch [458/500], Train Loss: 0.0979\n",
      "Epoch [459/500], Train Loss: 0.0980\n",
      "Epoch [460/500], Train Loss: 0.0978\n",
      "Epoch [461/500], Train Loss: 0.0978\n",
      "Epoch [462/500], Train Loss: 0.0977\n",
      "Epoch [463/500], Train Loss: 0.0974\n",
      "Epoch [464/500], Train Loss: 0.0973\n",
      "Epoch [465/500], Train Loss: 0.0972\n",
      "Epoch [466/500], Train Loss: 0.0970\n",
      "Epoch [467/500], Train Loss: 0.0968\n",
      "Epoch [468/500], Train Loss: 0.0967\n",
      "Epoch [469/500], Train Loss: 0.0968\n",
      "Epoch [470/500], Train Loss: 0.0967\n",
      "Epoch [471/500], Train Loss: 0.0965\n",
      "Epoch [472/500], Train Loss: 0.0964\n",
      "Epoch [473/500], Train Loss: 0.0960\n",
      "Epoch [474/500], Train Loss: 0.0962\n",
      "Epoch [475/500], Train Loss: 0.0959\n",
      "Epoch [476/500], Train Loss: 0.0957\n",
      "Epoch [477/500], Train Loss: 0.0958\n",
      "Epoch [478/500], Train Loss: 0.0957\n",
      "Epoch [479/500], Train Loss: 0.0957\n",
      "Epoch [480/500], Train Loss: 0.0955\n",
      "Epoch [481/500], Train Loss: 0.0953\n",
      "Epoch [482/500], Train Loss: 0.0952\n",
      "Epoch [483/500], Train Loss: 0.0950\n",
      "Epoch [484/500], Train Loss: 0.0949\n",
      "Epoch [485/500], Train Loss: 0.0948\n",
      "Epoch [486/500], Train Loss: 0.0947\n",
      "Epoch [487/500], Train Loss: 0.0948\n",
      "Epoch [488/500], Train Loss: 0.0946\n",
      "Epoch [489/500], Train Loss: 0.0943\n",
      "Epoch [490/500], Train Loss: 0.0942\n",
      "Epoch [491/500], Train Loss: 0.0941\n",
      "Epoch [492/500], Train Loss: 0.0941\n",
      "Epoch [493/500], Train Loss: 0.0940\n",
      "Epoch [494/500], Train Loss: 0.0939\n",
      "Epoch [495/500], Train Loss: 0.0938\n",
      "Epoch [496/500], Train Loss: 0.0937\n",
      "Epoch [497/500], Train Loss: 0.0935\n",
      "Epoch [498/500], Train Loss: 0.0934\n",
      "Epoch [499/500], Train Loss: 0.0932\n",
      "Epoch [500/500], Train Loss: 0.0932\n",
      "Final model saved at: ../../../Data/model_results/logistic_regression/models/chr18/final_model_chr18.pth\n",
      "Individual R^2 scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr18/individual_r2_scores_chr18.csv\n",
      "Individual IQS scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr18/individual_iqs_scores_chr18.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-18 12:37:21,188] A new study created in RDB with name: chr19_study\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual AUC ROC curves saved in: ../../../Data/model_results/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../../Data/model_results/logistic_regression/csv_files/performance_metrics.csv\n",
      "Total SNPs:  696\n",
      "PRS313 SNPs:  14\n",
      "Total SNPs used for Training:  682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-06-18 12:38:29,586] Trial 0 finished with value: 0.5925331383943557 and parameters: {'learning_rate': 0.006858832732953398, 'l1_coef': 0.014649477422537407, 'patience': 5, 'batch_size': 128}. Best is trial 0 with value: 0.5925331383943557.\n",
      "[I 2024-06-18 12:38:31,621] Trial 9 finished with value: 0.6982713103294372 and parameters: {'learning_rate': 0.015126462988827504, 'l1_coef': 0.021682178057474553, 'patience': 5, 'batch_size': 128}. Best is trial 0 with value: 0.5925331383943557.\n",
      "[I 2024-06-18 12:38:32,236] Trial 5 finished with value: 0.11571979373693467 and parameters: {'learning_rate': 0.009134210488595348, 'l1_coef': 0.00017770448263949664, 'patience': 6, 'batch_size': 256}. Best is trial 5 with value: 0.11571979373693467.\n",
      "[I 2024-06-18 12:38:52,618] Trial 1 finished with value: 0.3598858565092087 and parameters: {'learning_rate': 0.012353183367505644, 'l1_coef': 0.0037526926266675574, 'patience': 14, 'batch_size': 256}. Best is trial 5 with value: 0.11571979373693467.\n",
      "[I 2024-06-18 12:39:23,119] Trial 3 finished with value: 0.5401881355505723 and parameters: {'learning_rate': 0.0073878830129595775, 'l1_coef': 0.06942876393613796, 'patience': 13, 'batch_size': 32}. Best is trial 5 with value: 0.11571979373693467.\n",
      "[I 2024-06-18 12:39:24,441] Trial 4 finished with value: 0.2533686548471451 and parameters: {'learning_rate': 0.002772231052299873, 'l1_coef': 0.0017482183178401316, 'patience': 11, 'batch_size': 256}. Best is trial 5 with value: 0.11571979373693467.\n",
      "[I 2024-06-18 12:39:25,616] Trial 2 finished with value: 0.5445983818599156 and parameters: {'learning_rate': 0.0006532140389113817, 'l1_coef': 0.018851588204979113, 'patience': 8, 'batch_size': 64}. Best is trial 5 with value: 0.11571979373693467.\n",
      "[I 2024-06-18 12:39:25,815] Trial 6 finished with value: 0.22317873652164755 and parameters: {'learning_rate': 0.001726460098308389, 'l1_coef': 0.0013601740959102875, 'patience': 10, 'batch_size': 32}. Best is trial 5 with value: 0.11571979373693467.\n",
      "[I 2024-06-18 12:39:29,191] Trial 8 finished with value: 0.256686956435442 and parameters: {'learning_rate': 0.0003242597822281256, 'l1_coef': 0.0012043153689821008, 'patience': 18, 'batch_size': 128}. Best is trial 5 with value: 0.11571979373693467.\n",
      "[I 2024-06-18 12:39:43,073] Trial 7 finished with value: 0.21087949872016906 and parameters: {'learning_rate': 0.00016174254585108616, 'l1_coef': 0.001130933541553035, 'patience': 10, 'batch_size': 32}. Best is trial 5 with value: 0.11571979373693467.\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 19 - Best hyperparameters: {'learning_rate': 0.009134210488595348, 'l1_coef': 0.00017770448263949664, 'patience': 6, 'batch_size': 256}\n",
      "Chr 19 - Best value: 0.1157\n",
      "Epoch [1/500], Train Loss: 0.5906\n",
      "Epoch [2/500], Train Loss: 0.4675\n",
      "Epoch [3/500], Train Loss: 0.4032\n",
      "Epoch [4/500], Train Loss: 0.3585\n",
      "Epoch [5/500], Train Loss: 0.3231\n",
      "Epoch [6/500], Train Loss: 0.2986\n",
      "Epoch [7/500], Train Loss: 0.2788\n",
      "Epoch [8/500], Train Loss: 0.2639\n",
      "Epoch [9/500], Train Loss: 0.2506\n",
      "Epoch [10/500], Train Loss: 0.2392\n",
      "Epoch [11/500], Train Loss: 0.2291\n",
      "Epoch [12/500], Train Loss: 0.2211\n",
      "Epoch [13/500], Train Loss: 0.2141\n",
      "Epoch [14/500], Train Loss: 0.2073\n",
      "Epoch [15/500], Train Loss: 0.2011\n",
      "Epoch [16/500], Train Loss: 0.1962\n",
      "Epoch [17/500], Train Loss: 0.1911\n",
      "Epoch [18/500], Train Loss: 0.1866\n",
      "Epoch [19/500], Train Loss: 0.1829\n",
      "Epoch [20/500], Train Loss: 0.1791\n",
      "Epoch [21/500], Train Loss: 0.1769\n",
      "Epoch [22/500], Train Loss: 0.1739\n",
      "Epoch [23/500], Train Loss: 0.1701\n",
      "Epoch [24/500], Train Loss: 0.1671\n",
      "Epoch [25/500], Train Loss: 0.1647\n",
      "Epoch [26/500], Train Loss: 0.1622\n",
      "Epoch [27/500], Train Loss: 0.1598\n",
      "Epoch [28/500], Train Loss: 0.1574\n",
      "Epoch [29/500], Train Loss: 0.1556\n",
      "Epoch [30/500], Train Loss: 0.1546\n",
      "Epoch [31/500], Train Loss: 0.1521\n",
      "Epoch [32/500], Train Loss: 0.1504\n",
      "Epoch [33/500], Train Loss: 0.1488\n",
      "Epoch [34/500], Train Loss: 0.1479\n",
      "Epoch [35/500], Train Loss: 0.1468\n",
      "Epoch [36/500], Train Loss: 0.1453\n",
      "Epoch [37/500], Train Loss: 0.1441\n",
      "Epoch [38/500], Train Loss: 0.1427\n",
      "Epoch [39/500], Train Loss: 0.1413\n",
      "Epoch [40/500], Train Loss: 0.1406\n",
      "Epoch [41/500], Train Loss: 0.1399\n",
      "Epoch [42/500], Train Loss: 0.1386\n",
      "Epoch [43/500], Train Loss: 0.1371\n",
      "Epoch [44/500], Train Loss: 0.1365\n",
      "Epoch [45/500], Train Loss: 0.1368\n",
      "Epoch [46/500], Train Loss: 0.1355\n",
      "Epoch [47/500], Train Loss: 0.1344\n",
      "Epoch [48/500], Train Loss: 0.1332\n",
      "Epoch [49/500], Train Loss: 0.1337\n",
      "Epoch [50/500], Train Loss: 0.1322\n",
      "Epoch [51/500], Train Loss: 0.1325\n",
      "Epoch [52/500], Train Loss: 0.1310\n",
      "Epoch [53/500], Train Loss: 0.1299\n",
      "Epoch [54/500], Train Loss: 0.1295\n",
      "Epoch [55/500], Train Loss: 0.1284\n",
      "Epoch [56/500], Train Loss: 0.1284\n",
      "Epoch [57/500], Train Loss: 0.1277\n",
      "Epoch [58/500], Train Loss: 0.1268\n",
      "Epoch [59/500], Train Loss: 0.1266\n",
      "Epoch [60/500], Train Loss: 0.1266\n",
      "Epoch [61/500], Train Loss: 0.1263\n",
      "Epoch [62/500], Train Loss: 0.1258\n",
      "Epoch [63/500], Train Loss: 0.1262\n",
      "Epoch [64/500], Train Loss: 0.1244\n",
      "Epoch [65/500], Train Loss: 0.1241\n",
      "Epoch [66/500], Train Loss: 0.1238\n",
      "Epoch [67/500], Train Loss: 0.1230\n",
      "Epoch [68/500], Train Loss: 0.1225\n",
      "Epoch [69/500], Train Loss: 0.1220\n",
      "Epoch [70/500], Train Loss: 0.1216\n",
      "Epoch [71/500], Train Loss: 0.1213\n",
      "Epoch [72/500], Train Loss: 0.1213\n",
      "Epoch [73/500], Train Loss: 0.1208\n",
      "Epoch [74/500], Train Loss: 0.1203\n",
      "Epoch [75/500], Train Loss: 0.1200\n",
      "Epoch [76/500], Train Loss: 0.1204\n",
      "Epoch [77/500], Train Loss: 0.1199\n",
      "Epoch [78/500], Train Loss: 0.1196\n",
      "Epoch [79/500], Train Loss: 0.1199\n",
      "Epoch [80/500], Train Loss: 0.1191\n",
      "Epoch [81/500], Train Loss: 0.1186\n",
      "Epoch [82/500], Train Loss: 0.1189\n",
      "Epoch [83/500], Train Loss: 0.1182\n",
      "Epoch [84/500], Train Loss: 0.1176\n",
      "Epoch [85/500], Train Loss: 0.1179\n",
      "Epoch [86/500], Train Loss: 0.1173\n",
      "Epoch [87/500], Train Loss: 0.1171\n",
      "Epoch [88/500], Train Loss: 0.1167\n",
      "Epoch [89/500], Train Loss: 0.1166\n",
      "Epoch [90/500], Train Loss: 0.1165\n",
      "Epoch [91/500], Train Loss: 0.1158\n",
      "Epoch [92/500], Train Loss: 0.1159\n",
      "Epoch [93/500], Train Loss: 0.1156\n",
      "Epoch [94/500], Train Loss: 0.1159\n",
      "Epoch [95/500], Train Loss: 0.1159\n",
      "Epoch [96/500], Train Loss: 0.1160\n",
      "Epoch [97/500], Train Loss: 0.1163\n",
      "Epoch [98/500], Train Loss: 0.1160\n",
      "Epoch [99/500], Train Loss: 0.1156\n",
      "Epoch [100/500], Train Loss: 0.1147\n",
      "Epoch [101/500], Train Loss: 0.1141\n",
      "Epoch [102/500], Train Loss: 0.1138\n",
      "Epoch [103/500], Train Loss: 0.1145\n",
      "Epoch [104/500], Train Loss: 0.1145\n",
      "Epoch [105/500], Train Loss: 0.1143\n",
      "Epoch [106/500], Train Loss: 0.1135\n",
      "Epoch [107/500], Train Loss: 0.1135\n",
      "Epoch [108/500], Train Loss: 0.1133\n",
      "Epoch [109/500], Train Loss: 0.1135\n",
      "Epoch [110/500], Train Loss: 0.1143\n",
      "Epoch [111/500], Train Loss: 0.1134\n",
      "Epoch [112/500], Train Loss: 0.1126\n",
      "Epoch [113/500], Train Loss: 0.1123\n",
      "Epoch [114/500], Train Loss: 0.1124\n",
      "Epoch [115/500], Train Loss: 0.1118\n",
      "Epoch [116/500], Train Loss: 0.1121\n",
      "Epoch [117/500], Train Loss: 0.1121\n",
      "Epoch [118/500], Train Loss: 0.1120\n",
      "Epoch [119/500], Train Loss: 0.1117\n",
      "Epoch [120/500], Train Loss: 0.1117\n",
      "Epoch [121/500], Train Loss: 0.1113\n",
      "Epoch [122/500], Train Loss: 0.1111\n",
      "Epoch [123/500], Train Loss: 0.1109\n",
      "Epoch [124/500], Train Loss: 0.1113\n",
      "Epoch [125/500], Train Loss: 0.1112\n",
      "Epoch [126/500], Train Loss: 0.1109\n",
      "Epoch [127/500], Train Loss: 0.1104\n",
      "Epoch [128/500], Train Loss: 0.1111\n",
      "Epoch [129/500], Train Loss: 0.1106\n",
      "Epoch [130/500], Train Loss: 0.1107\n",
      "Epoch [131/500], Train Loss: 0.1103\n",
      "Epoch [132/500], Train Loss: 0.1103\n",
      "Epoch [133/500], Train Loss: 0.1101\n",
      "Epoch [134/500], Train Loss: 0.1095\n",
      "Epoch [135/500], Train Loss: 0.1093\n",
      "Epoch [136/500], Train Loss: 0.1091\n",
      "Epoch [137/500], Train Loss: 0.1094\n",
      "Epoch [138/500], Train Loss: 0.1098\n",
      "Epoch [139/500], Train Loss: 0.1100\n",
      "Epoch [140/500], Train Loss: 0.1097\n",
      "Epoch [141/500], Train Loss: 0.1097\n",
      "Epoch [142/500], Train Loss: 0.1110\n",
      "Early stopping at epoch 142\n",
      "Final model saved at: ../../../Data/model_results/logistic_regression/models/chr19/final_model_chr19.pth\n",
      "Individual R^2 scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr19/individual_r2_scores_chr19.csv\n",
      "Individual IQS scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr19/individual_iqs_scores_chr19.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-18 12:39:45,851] A new study created in RDB with name: chr20_study\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual AUC ROC curves saved in: ../../../Data/model_results/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../../Data/model_results/logistic_regression/csv_files/performance_metrics.csv\n",
      "Total SNPs:  216\n",
      "PRS313 SNPs:  8\n",
      "Total SNPs used for Training:  208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-06-18 12:40:17,003] Trial 1 finished with value: 0.09218241758644581 and parameters: {'learning_rate': 0.01528516426025495, 'l1_coef': 1.1188529679391656e-05, 'patience': 9, 'batch_size': 128}. Best is trial 1 with value: 0.09218241758644581.\n",
      "[I 2024-06-18 12:40:20,611] Trial 7 finished with value: 0.2089025765657425 and parameters: {'learning_rate': 0.08308703436266394, 'l1_coef': 1.3002263069719351e-05, 'patience': 14, 'batch_size': 256}. Best is trial 1 with value: 0.09218241758644581.\n",
      "[I 2024-06-18 12:40:34,783] Trial 0 finished with value: 0.0951929233968258 and parameters: {'learning_rate': 0.04450661484595062, 'l1_coef': 2.6094386175834953e-05, 'patience': 20, 'batch_size': 128}. Best is trial 1 with value: 0.09218241758644581.\n",
      "[I 2024-06-18 12:40:38,060] Trial 4 finished with value: 0.15643969625234605 and parameters: {'learning_rate': 0.0711970484147741, 'l1_coef': 0.0005512454337160089, 'patience': 13, 'batch_size': 128}. Best is trial 1 with value: 0.09218241758644581.\n",
      "[I 2024-06-18 12:40:54,565] Trial 5 finished with value: 0.24850106409617836 and parameters: {'learning_rate': 0.011881160485406977, 'l1_coef': 0.004649415044653755, 'patience': 19, 'batch_size': 64}. Best is trial 1 with value: 0.09218241758644581.\n",
      "[I 2024-06-18 12:40:58,160] Trial 3 finished with value: 0.10794405660458972 and parameters: {'learning_rate': 0.0007442196713100422, 'l1_coef': 3.3338180528444825e-05, 'patience': 5, 'batch_size': 64}. Best is trial 1 with value: 0.09218241758644581.\n",
      "[I 2024-06-18 12:40:59,883] Trial 9 finished with value: 0.26551840030230006 and parameters: {'learning_rate': 0.006675434954513224, 'l1_coef': 0.03891655863640207, 'patience': 19, 'batch_size': 32}. Best is trial 1 with value: 0.09218241758644581.\n",
      "[I 2024-06-18 12:41:08,736] Trial 2 finished with value: 0.1746788166463375 and parameters: {'learning_rate': 0.0002554105931204861, 'l1_coef': 0.0004683836729004208, 'patience': 13, 'batch_size': 128}. Best is trial 1 with value: 0.09218241758644581.\n",
      "[I 2024-06-18 12:41:19,349] Trial 8 finished with value: 0.43856051862239837 and parameters: {'learning_rate': 0.00010056219530089906, 'l1_coef': 0.02229321922342352, 'patience': 5, 'batch_size': 256}. Best is trial 1 with value: 0.09218241758644581.\n",
      "[I 2024-06-18 12:41:22,546] Trial 6 finished with value: 0.1514380361352648 and parameters: {'learning_rate': 0.08417602673923642, 'l1_coef': 0.0004492673564527438, 'patience': 19, 'batch_size': 64}. Best is trial 1 with value: 0.09218241758644581.\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 20 - Best hyperparameters: {'learning_rate': 0.01528516426025495, 'l1_coef': 1.1188529679391656e-05, 'patience': 9, 'batch_size': 128}\n",
      "Chr 20 - Best value: 0.0922\n",
      "Epoch [1/500], Train Loss: 0.2987\n",
      "Epoch [2/500], Train Loss: 0.2337\n",
      "Epoch [3/500], Train Loss: 0.2021\n",
      "Epoch [4/500], Train Loss: 0.1794\n",
      "Epoch [5/500], Train Loss: 0.1654\n",
      "Epoch [6/500], Train Loss: 0.1551\n",
      "Epoch [7/500], Train Loss: 0.1468\n",
      "Epoch [8/500], Train Loss: 0.1399\n",
      "Epoch [9/500], Train Loss: 0.1342\n",
      "Epoch [10/500], Train Loss: 0.1288\n",
      "Epoch [11/500], Train Loss: 0.1246\n",
      "Epoch [12/500], Train Loss: 0.1220\n",
      "Epoch [13/500], Train Loss: 0.1193\n",
      "Epoch [14/500], Train Loss: 0.1150\n",
      "Epoch [15/500], Train Loss: 0.1127\n",
      "Epoch [16/500], Train Loss: 0.1103\n",
      "Epoch [17/500], Train Loss: 0.1091\n",
      "Epoch [18/500], Train Loss: 0.1057\n",
      "Epoch [19/500], Train Loss: 0.1052\n",
      "Epoch [20/500], Train Loss: 0.1034\n",
      "Epoch [21/500], Train Loss: 0.1024\n",
      "Epoch [22/500], Train Loss: 0.1008\n",
      "Epoch [23/500], Train Loss: 0.1001\n",
      "Epoch [24/500], Train Loss: 0.0989\n",
      "Epoch [25/500], Train Loss: 0.0989\n",
      "Epoch [26/500], Train Loss: 0.0972\n",
      "Epoch [27/500], Train Loss: 0.0971\n",
      "Epoch [28/500], Train Loss: 0.0974\n",
      "Epoch [29/500], Train Loss: 0.0951\n",
      "Epoch [30/500], Train Loss: 0.0947\n",
      "Epoch [31/500], Train Loss: 0.0939\n",
      "Epoch [32/500], Train Loss: 0.0942\n",
      "Epoch [33/500], Train Loss: 0.0943\n",
      "Epoch [34/500], Train Loss: 0.0922\n",
      "Epoch [35/500], Train Loss: 0.0922\n",
      "Epoch [36/500], Train Loss: 0.0926\n",
      "Epoch [37/500], Train Loss: 0.0909\n",
      "Epoch [38/500], Train Loss: 0.0913\n",
      "Epoch [39/500], Train Loss: 0.0903\n",
      "Epoch [40/500], Train Loss: 0.0901\n",
      "Epoch [41/500], Train Loss: 0.0899\n",
      "Epoch [42/500], Train Loss: 0.0900\n",
      "Epoch [43/500], Train Loss: 0.0888\n",
      "Epoch [44/500], Train Loss: 0.0894\n",
      "Epoch [45/500], Train Loss: 0.0901\n",
      "Epoch [46/500], Train Loss: 0.0878\n",
      "Epoch [47/500], Train Loss: 0.0880\n",
      "Epoch [48/500], Train Loss: 0.0885\n",
      "Epoch [49/500], Train Loss: 0.0867\n",
      "Epoch [50/500], Train Loss: 0.0875\n",
      "Epoch [51/500], Train Loss: 0.0872\n",
      "Epoch [52/500], Train Loss: 0.0877\n",
      "Epoch [53/500], Train Loss: 0.0889\n",
      "Epoch [54/500], Train Loss: 0.0876\n",
      "Epoch [55/500], Train Loss: 0.0863\n",
      "Epoch [56/500], Train Loss: 0.0867\n",
      "Epoch [57/500], Train Loss: 0.0862\n",
      "Epoch [58/500], Train Loss: 0.0866\n",
      "Epoch [59/500], Train Loss: 0.0863\n",
      "Epoch [60/500], Train Loss: 0.0858\n",
      "Epoch [61/500], Train Loss: 0.0876\n",
      "Epoch [62/500], Train Loss: 0.0861\n",
      "Epoch [63/500], Train Loss: 0.0847\n",
      "Epoch [64/500], Train Loss: 0.0863\n",
      "Epoch [65/500], Train Loss: 0.0851\n",
      "Epoch [66/500], Train Loss: 0.0854\n",
      "Epoch [67/500], Train Loss: 0.0852\n",
      "Epoch [68/500], Train Loss: 0.0853\n",
      "Epoch [69/500], Train Loss: 0.0853\n",
      "Epoch [70/500], Train Loss: 0.0833\n",
      "Epoch [71/500], Train Loss: 0.0831\n",
      "Epoch [72/500], Train Loss: 0.0832\n",
      "Epoch [73/500], Train Loss: 0.0827\n",
      "Epoch [74/500], Train Loss: 0.0829\n",
      "Epoch [75/500], Train Loss: 0.0827\n",
      "Epoch [76/500], Train Loss: 0.0830\n",
      "Epoch [77/500], Train Loss: 0.0831\n",
      "Epoch [78/500], Train Loss: 0.0829\n",
      "Epoch [79/500], Train Loss: 0.0827\n",
      "Epoch [80/500], Train Loss: 0.0826\n",
      "Epoch [81/500], Train Loss: 0.0831\n",
      "Epoch [82/500], Train Loss: 0.0824\n",
      "Epoch [83/500], Train Loss: 0.0826\n",
      "Epoch [84/500], Train Loss: 0.0829\n",
      "Epoch [85/500], Train Loss: 0.0826\n",
      "Epoch [86/500], Train Loss: 0.0823\n",
      "Epoch [87/500], Train Loss: 0.0826\n",
      "Epoch [88/500], Train Loss: 0.0826\n",
      "Epoch [89/500], Train Loss: 0.0830\n",
      "Epoch [90/500], Train Loss: 0.0823\n",
      "Epoch [91/500], Train Loss: 0.0829\n",
      "Epoch [92/500], Train Loss: 0.0830\n",
      "Epoch [93/500], Train Loss: 0.0826\n",
      "Epoch [94/500], Train Loss: 0.0825\n",
      "Epoch [95/500], Train Loss: 0.0826\n",
      "Early stopping at epoch 95\n",
      "Final model saved at: ../../../Data/model_results/logistic_regression/models/chr20/final_model_chr20.pth\n",
      "Individual R^2 scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr20/individual_r2_scores_chr20.csv\n",
      "Individual IQS scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr20/individual_iqs_scores_chr20.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-18 12:41:23,910] A new study created in RDB with name: chr21_study\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual AUC ROC curves saved in: ../../../Data/model_results/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../../Data/model_results/logistic_regression/csv_files/performance_metrics.csv\n",
      "Total SNPs:  188\n",
      "PRS313 SNPs:  8\n",
      "Total SNPs used for Training:  180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-06-18 12:42:16,206] Trial 2 finished with value: 0.0930142675127302 and parameters: {'learning_rate': 0.014736038144745061, 'l1_coef': 7.112082725602636e-05, 'patience': 15, 'batch_size': 64}. Best is trial 2 with value: 0.0930142675127302.\n",
      "[I 2024-06-18 12:42:33,863] Trial 3 finished with value: 0.373406446831567 and parameters: {'learning_rate': 0.007944531806017544, 'l1_coef': 0.04873923493121475, 'patience': 15, 'batch_size': 64}. Best is trial 2 with value: 0.0930142675127302.\n",
      "[I 2024-06-18 12:43:07,865] Trial 5 finished with value: 0.09902706742286682 and parameters: {'learning_rate': 0.0020142264168256235, 'l1_coef': 7.99761713561818e-05, 'patience': 10, 'batch_size': 64}. Best is trial 2 with value: 0.0930142675127302.\n",
      "[I 2024-06-18 12:43:09,658] Trial 7 finished with value: 0.08571214601397514 and parameters: {'learning_rate': 0.0024498760032761035, 'l1_coef': 1.4500821052766964e-05, 'patience': 5, 'batch_size': 256}. Best is trial 7 with value: 0.08571214601397514.\n",
      "[I 2024-06-18 12:43:32,519] Trial 6 finished with value: 0.17814408093690873 and parameters: {'learning_rate': 0.0005010847497877476, 'l1_coef': 0.00017303971178194675, 'patience': 20, 'batch_size': 256}. Best is trial 7 with value: 0.08571214601397514.\n",
      "[I 2024-06-18 12:43:37,272] Trial 1 finished with value: 0.2467296763108327 and parameters: {'learning_rate': 0.0006565282129299293, 'l1_coef': 0.0022134962486018333, 'patience': 6, 'batch_size': 32}. Best is trial 7 with value: 0.08571214601397514.\n",
      "[I 2024-06-18 12:43:45,366] Trial 0 finished with value: 0.30964144244790076 and parameters: {'learning_rate': 0.0006369097626661387, 'l1_coef': 0.007574365173984536, 'patience': 18, 'batch_size': 128}. Best is trial 7 with value: 0.08571214601397514.\n",
      "[I 2024-06-18 12:43:46,352] Trial 9 finished with value: 0.11955325695184563 and parameters: {'learning_rate': 0.0011223221870205256, 'l1_coef': 0.00020904853256909403, 'patience': 20, 'batch_size': 32}. Best is trial 7 with value: 0.08571214601397514.\n",
      "[I 2024-06-18 12:43:48,366] Trial 4 finished with value: 0.30628066871847426 and parameters: {'learning_rate': 0.0003244880260866313, 'l1_coef': 0.006394831288824858, 'patience': 9, 'batch_size': 64}. Best is trial 7 with value: 0.08571214601397514.\n",
      "[I 2024-06-18 12:43:52,287] Trial 8 finished with value: 0.2943363845348358 and parameters: {'learning_rate': 0.0002261333139039295, 'l1_coef': 0.002115345364304486, 'patience': 6, 'batch_size': 256}. Best is trial 7 with value: 0.08571214601397514.\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 21 - Best hyperparameters: {'learning_rate': 0.0024498760032761035, 'l1_coef': 1.4500821052766964e-05, 'patience': 5, 'batch_size': 256}\n",
      "Chr 21 - Best value: 0.0857\n",
      "Epoch [1/500], Train Loss: 0.5425\n",
      "Epoch [2/500], Train Loss: 0.4060\n",
      "Epoch [3/500], Train Loss: 0.3725\n",
      "Epoch [4/500], Train Loss: 0.3580\n",
      "Epoch [5/500], Train Loss: 0.3462\n",
      "Epoch [6/500], Train Loss: 0.3365\n",
      "Epoch [7/500], Train Loss: 0.3285\n",
      "Epoch [8/500], Train Loss: 0.3219\n",
      "Epoch [9/500], Train Loss: 0.3149\n",
      "Epoch [10/500], Train Loss: 0.3087\n",
      "Epoch [11/500], Train Loss: 0.3033\n",
      "Epoch [12/500], Train Loss: 0.2979\n",
      "Epoch [13/500], Train Loss: 0.2933\n",
      "Epoch [14/500], Train Loss: 0.2883\n",
      "Epoch [15/500], Train Loss: 0.2831\n",
      "Epoch [16/500], Train Loss: 0.2788\n",
      "Epoch [17/500], Train Loss: 0.2746\n",
      "Epoch [18/500], Train Loss: 0.2706\n",
      "Epoch [19/500], Train Loss: 0.2666\n",
      "Epoch [20/500], Train Loss: 0.2629\n",
      "Epoch [21/500], Train Loss: 0.2591\n",
      "Epoch [22/500], Train Loss: 0.2559\n",
      "Epoch [23/500], Train Loss: 0.2527\n",
      "Epoch [24/500], Train Loss: 0.2493\n",
      "Epoch [25/500], Train Loss: 0.2460\n",
      "Epoch [26/500], Train Loss: 0.2431\n",
      "Epoch [27/500], Train Loss: 0.2402\n",
      "Epoch [28/500], Train Loss: 0.2376\n",
      "Epoch [29/500], Train Loss: 0.2352\n",
      "Epoch [30/500], Train Loss: 0.2323\n",
      "Epoch [31/500], Train Loss: 0.2296\n",
      "Epoch [32/500], Train Loss: 0.2274\n",
      "Epoch [33/500], Train Loss: 0.2250\n",
      "Epoch [34/500], Train Loss: 0.2227\n",
      "Epoch [35/500], Train Loss: 0.2207\n",
      "Epoch [36/500], Train Loss: 0.2183\n",
      "Epoch [37/500], Train Loss: 0.2163\n",
      "Epoch [38/500], Train Loss: 0.2140\n",
      "Epoch [39/500], Train Loss: 0.2122\n",
      "Epoch [40/500], Train Loss: 0.2104\n",
      "Epoch [41/500], Train Loss: 0.2082\n",
      "Epoch [42/500], Train Loss: 0.2066\n",
      "Epoch [43/500], Train Loss: 0.2050\n",
      "Epoch [44/500], Train Loss: 0.2029\n",
      "Epoch [45/500], Train Loss: 0.2014\n",
      "Epoch [46/500], Train Loss: 0.1995\n",
      "Epoch [47/500], Train Loss: 0.1981\n",
      "Epoch [48/500], Train Loss: 0.1967\n",
      "Epoch [49/500], Train Loss: 0.1948\n",
      "Epoch [50/500], Train Loss: 0.1935\n",
      "Epoch [51/500], Train Loss: 0.1921\n",
      "Epoch [52/500], Train Loss: 0.1904\n",
      "Epoch [53/500], Train Loss: 0.1887\n",
      "Epoch [54/500], Train Loss: 0.1877\n",
      "Epoch [55/500], Train Loss: 0.1863\n",
      "Epoch [56/500], Train Loss: 0.1848\n",
      "Epoch [57/500], Train Loss: 0.1837\n",
      "Epoch [58/500], Train Loss: 0.1824\n",
      "Epoch [59/500], Train Loss: 0.1814\n",
      "Epoch [60/500], Train Loss: 0.1799\n",
      "Epoch [61/500], Train Loss: 0.1789\n",
      "Epoch [62/500], Train Loss: 0.1774\n",
      "Epoch [63/500], Train Loss: 0.1766\n",
      "Epoch [64/500], Train Loss: 0.1751\n",
      "Epoch [65/500], Train Loss: 0.1742\n",
      "Epoch [66/500], Train Loss: 0.1732\n",
      "Epoch [67/500], Train Loss: 0.1718\n",
      "Epoch [68/500], Train Loss: 0.1710\n",
      "Epoch [69/500], Train Loss: 0.1697\n",
      "Epoch [70/500], Train Loss: 0.1688\n",
      "Epoch [71/500], Train Loss: 0.1678\n",
      "Epoch [72/500], Train Loss: 0.1668\n",
      "Epoch [73/500], Train Loss: 0.1659\n",
      "Epoch [74/500], Train Loss: 0.1649\n",
      "Epoch [75/500], Train Loss: 0.1640\n",
      "Epoch [76/500], Train Loss: 0.1632\n",
      "Epoch [77/500], Train Loss: 0.1620\n",
      "Epoch [78/500], Train Loss: 0.1611\n",
      "Epoch [79/500], Train Loss: 0.1604\n",
      "Epoch [80/500], Train Loss: 0.1595\n",
      "Epoch [81/500], Train Loss: 0.1586\n",
      "Epoch [82/500], Train Loss: 0.1579\n",
      "Epoch [83/500], Train Loss: 0.1569\n",
      "Epoch [84/500], Train Loss: 0.1565\n",
      "Epoch [85/500], Train Loss: 0.1552\n",
      "Epoch [86/500], Train Loss: 0.1544\n",
      "Epoch [87/500], Train Loss: 0.1538\n",
      "Epoch [88/500], Train Loss: 0.1532\n",
      "Epoch [89/500], Train Loss: 0.1524\n",
      "Epoch [90/500], Train Loss: 0.1515\n",
      "Epoch [91/500], Train Loss: 0.1506\n",
      "Epoch [92/500], Train Loss: 0.1501\n",
      "Epoch [93/500], Train Loss: 0.1492\n",
      "Epoch [94/500], Train Loss: 0.1485\n",
      "Epoch [95/500], Train Loss: 0.1476\n",
      "Epoch [96/500], Train Loss: 0.1472\n",
      "Epoch [97/500], Train Loss: 0.1464\n",
      "Epoch [98/500], Train Loss: 0.1457\n",
      "Epoch [99/500], Train Loss: 0.1450\n",
      "Epoch [100/500], Train Loss: 0.1447\n",
      "Epoch [101/500], Train Loss: 0.1439\n",
      "Epoch [102/500], Train Loss: 0.1430\n",
      "Epoch [103/500], Train Loss: 0.1426\n",
      "Epoch [104/500], Train Loss: 0.1418\n",
      "Epoch [105/500], Train Loss: 0.1412\n",
      "Epoch [106/500], Train Loss: 0.1408\n",
      "Epoch [107/500], Train Loss: 0.1399\n",
      "Epoch [108/500], Train Loss: 0.1394\n",
      "Epoch [109/500], Train Loss: 0.1389\n",
      "Epoch [110/500], Train Loss: 0.1383\n",
      "Epoch [111/500], Train Loss: 0.1375\n",
      "Epoch [112/500], Train Loss: 0.1369\n",
      "Epoch [113/500], Train Loss: 0.1364\n",
      "Epoch [114/500], Train Loss: 0.1360\n",
      "Epoch [115/500], Train Loss: 0.1353\n",
      "Epoch [116/500], Train Loss: 0.1348\n",
      "Epoch [117/500], Train Loss: 0.1344\n",
      "Epoch [118/500], Train Loss: 0.1337\n",
      "Epoch [119/500], Train Loss: 0.1332\n",
      "Epoch [120/500], Train Loss: 0.1329\n",
      "Epoch [121/500], Train Loss: 0.1321\n",
      "Epoch [122/500], Train Loss: 0.1316\n",
      "Epoch [123/500], Train Loss: 0.1310\n",
      "Epoch [124/500], Train Loss: 0.1303\n",
      "Epoch [125/500], Train Loss: 0.1301\n",
      "Epoch [126/500], Train Loss: 0.1297\n",
      "Epoch [127/500], Train Loss: 0.1290\n",
      "Epoch [128/500], Train Loss: 0.1287\n",
      "Epoch [129/500], Train Loss: 0.1281\n",
      "Epoch [130/500], Train Loss: 0.1277\n",
      "Epoch [131/500], Train Loss: 0.1273\n",
      "Epoch [132/500], Train Loss: 0.1268\n",
      "Epoch [133/500], Train Loss: 0.1261\n",
      "Epoch [134/500], Train Loss: 0.1258\n",
      "Epoch [135/500], Train Loss: 0.1253\n",
      "Epoch [136/500], Train Loss: 0.1250\n",
      "Epoch [137/500], Train Loss: 0.1245\n",
      "Epoch [138/500], Train Loss: 0.1241\n",
      "Epoch [139/500], Train Loss: 0.1234\n",
      "Epoch [140/500], Train Loss: 0.1231\n",
      "Epoch [141/500], Train Loss: 0.1226\n",
      "Epoch [142/500], Train Loss: 0.1224\n",
      "Epoch [143/500], Train Loss: 0.1218\n",
      "Epoch [144/500], Train Loss: 0.1218\n",
      "Epoch [145/500], Train Loss: 0.1209\n",
      "Epoch [146/500], Train Loss: 0.1206\n",
      "Epoch [147/500], Train Loss: 0.1201\n",
      "Epoch [148/500], Train Loss: 0.1198\n",
      "Epoch [149/500], Train Loss: 0.1195\n",
      "Epoch [150/500], Train Loss: 0.1191\n",
      "Epoch [151/500], Train Loss: 0.1184\n",
      "Epoch [152/500], Train Loss: 0.1183\n",
      "Epoch [153/500], Train Loss: 0.1177\n",
      "Epoch [154/500], Train Loss: 0.1176\n",
      "Epoch [155/500], Train Loss: 0.1173\n",
      "Epoch [156/500], Train Loss: 0.1165\n",
      "Epoch [157/500], Train Loss: 0.1165\n",
      "Epoch [158/500], Train Loss: 0.1159\n",
      "Epoch [159/500], Train Loss: 0.1157\n",
      "Epoch [160/500], Train Loss: 0.1153\n",
      "Epoch [161/500], Train Loss: 0.1149\n",
      "Epoch [162/500], Train Loss: 0.1144\n",
      "Epoch [163/500], Train Loss: 0.1141\n",
      "Epoch [164/500], Train Loss: 0.1141\n",
      "Epoch [165/500], Train Loss: 0.1136\n",
      "Epoch [166/500], Train Loss: 0.1132\n",
      "Epoch [167/500], Train Loss: 0.1127\n",
      "Epoch [168/500], Train Loss: 0.1126\n",
      "Epoch [169/500], Train Loss: 0.1121\n",
      "Epoch [170/500], Train Loss: 0.1117\n",
      "Epoch [171/500], Train Loss: 0.1114\n",
      "Epoch [172/500], Train Loss: 0.1112\n",
      "Epoch [173/500], Train Loss: 0.1106\n",
      "Epoch [174/500], Train Loss: 0.1106\n",
      "Epoch [175/500], Train Loss: 0.1100\n",
      "Epoch [176/500], Train Loss: 0.1100\n",
      "Epoch [177/500], Train Loss: 0.1095\n",
      "Epoch [178/500], Train Loss: 0.1091\n",
      "Epoch [179/500], Train Loss: 0.1088\n",
      "Epoch [180/500], Train Loss: 0.1086\n",
      "Epoch [181/500], Train Loss: 0.1081\n",
      "Epoch [182/500], Train Loss: 0.1080\n",
      "Epoch [183/500], Train Loss: 0.1077\n",
      "Epoch [184/500], Train Loss: 0.1075\n",
      "Epoch [185/500], Train Loss: 0.1072\n",
      "Epoch [186/500], Train Loss: 0.1069\n",
      "Epoch [187/500], Train Loss: 0.1067\n",
      "Epoch [188/500], Train Loss: 0.1068\n",
      "Epoch [189/500], Train Loss: 0.1058\n",
      "Epoch [190/500], Train Loss: 0.1057\n",
      "Epoch [191/500], Train Loss: 0.1052\n",
      "Epoch [192/500], Train Loss: 0.1052\n",
      "Epoch [193/500], Train Loss: 0.1051\n",
      "Epoch [194/500], Train Loss: 0.1048\n",
      "Epoch [195/500], Train Loss: 0.1042\n",
      "Epoch [196/500], Train Loss: 0.1038\n",
      "Epoch [197/500], Train Loss: 0.1040\n",
      "Epoch [198/500], Train Loss: 0.1034\n",
      "Epoch [199/500], Train Loss: 0.1032\n",
      "Epoch [200/500], Train Loss: 0.1030\n",
      "Epoch [201/500], Train Loss: 0.1027\n",
      "Epoch [202/500], Train Loss: 0.1025\n",
      "Epoch [203/500], Train Loss: 0.1022\n",
      "Epoch [204/500], Train Loss: 0.1020\n",
      "Epoch [205/500], Train Loss: 0.1018\n",
      "Epoch [206/500], Train Loss: 0.1014\n",
      "Epoch [207/500], Train Loss: 0.1011\n",
      "Epoch [208/500], Train Loss: 0.1010\n",
      "Epoch [209/500], Train Loss: 0.1008\n",
      "Epoch [210/500], Train Loss: 0.1002\n",
      "Epoch [211/500], Train Loss: 0.1001\n",
      "Epoch [212/500], Train Loss: 0.1002\n",
      "Epoch [213/500], Train Loss: 0.0997\n",
      "Epoch [214/500], Train Loss: 0.0992\n",
      "Epoch [215/500], Train Loss: 0.0994\n",
      "Epoch [216/500], Train Loss: 0.0991\n",
      "Epoch [217/500], Train Loss: 0.0988\n",
      "Epoch [218/500], Train Loss: 0.0985\n",
      "Epoch [219/500], Train Loss: 0.0985\n",
      "Epoch [220/500], Train Loss: 0.0979\n",
      "Epoch [221/500], Train Loss: 0.0977\n",
      "Epoch [222/500], Train Loss: 0.0974\n",
      "Epoch [223/500], Train Loss: 0.0975\n",
      "Epoch [224/500], Train Loss: 0.0973\n",
      "Epoch [225/500], Train Loss: 0.0970\n",
      "Epoch [226/500], Train Loss: 0.0965\n",
      "Epoch [227/500], Train Loss: 0.0968\n",
      "Epoch [228/500], Train Loss: 0.0963\n",
      "Epoch [229/500], Train Loss: 0.0959\n",
      "Epoch [230/500], Train Loss: 0.0959\n",
      "Epoch [231/500], Train Loss: 0.0957\n",
      "Epoch [232/500], Train Loss: 0.0955\n",
      "Epoch [233/500], Train Loss: 0.0953\n",
      "Epoch [234/500], Train Loss: 0.0950\n",
      "Epoch [235/500], Train Loss: 0.0948\n",
      "Epoch [236/500], Train Loss: 0.0947\n",
      "Epoch [237/500], Train Loss: 0.0945\n",
      "Epoch [238/500], Train Loss: 0.0942\n",
      "Epoch [239/500], Train Loss: 0.0944\n",
      "Epoch [240/500], Train Loss: 0.0939\n",
      "Epoch [241/500], Train Loss: 0.0937\n",
      "Epoch [242/500], Train Loss: 0.0934\n",
      "Epoch [243/500], Train Loss: 0.0932\n",
      "Epoch [244/500], Train Loss: 0.0930\n",
      "Epoch [245/500], Train Loss: 0.0927\n",
      "Epoch [246/500], Train Loss: 0.0927\n",
      "Epoch [247/500], Train Loss: 0.0925\n",
      "Epoch [248/500], Train Loss: 0.0924\n",
      "Epoch [249/500], Train Loss: 0.0922\n",
      "Epoch [250/500], Train Loss: 0.0921\n",
      "Epoch [251/500], Train Loss: 0.0919\n",
      "Epoch [252/500], Train Loss: 0.0915\n",
      "Epoch [253/500], Train Loss: 0.0915\n",
      "Epoch [254/500], Train Loss: 0.0913\n",
      "Epoch [255/500], Train Loss: 0.0907\n",
      "Epoch [256/500], Train Loss: 0.0906\n",
      "Epoch [257/500], Train Loss: 0.0906\n",
      "Epoch [258/500], Train Loss: 0.0905\n",
      "Epoch [259/500], Train Loss: 0.0903\n",
      "Epoch [260/500], Train Loss: 0.0902\n",
      "Epoch [261/500], Train Loss: 0.0902\n",
      "Epoch [262/500], Train Loss: 0.0900\n",
      "Epoch [263/500], Train Loss: 0.0897\n",
      "Epoch [264/500], Train Loss: 0.0894\n",
      "Epoch [265/500], Train Loss: 0.0892\n",
      "Epoch [266/500], Train Loss: 0.0889\n",
      "Epoch [267/500], Train Loss: 0.0888\n",
      "Epoch [268/500], Train Loss: 0.0886\n",
      "Epoch [269/500], Train Loss: 0.0888\n",
      "Epoch [270/500], Train Loss: 0.0883\n",
      "Epoch [271/500], Train Loss: 0.0884\n",
      "Epoch [272/500], Train Loss: 0.0879\n",
      "Epoch [273/500], Train Loss: 0.0880\n",
      "Epoch [274/500], Train Loss: 0.0878\n",
      "Epoch [275/500], Train Loss: 0.0877\n",
      "Epoch [276/500], Train Loss: 0.0874\n",
      "Epoch [277/500], Train Loss: 0.0873\n",
      "Epoch [278/500], Train Loss: 0.0873\n",
      "Epoch [279/500], Train Loss: 0.0871\n",
      "Epoch [280/500], Train Loss: 0.0872\n",
      "Epoch [281/500], Train Loss: 0.0869\n",
      "Epoch [282/500], Train Loss: 0.0865\n",
      "Epoch [283/500], Train Loss: 0.0866\n",
      "Epoch [284/500], Train Loss: 0.0862\n",
      "Epoch [285/500], Train Loss: 0.0860\n",
      "Epoch [286/500], Train Loss: 0.0858\n",
      "Epoch [287/500], Train Loss: 0.0858\n",
      "Epoch [288/500], Train Loss: 0.0855\n",
      "Epoch [289/500], Train Loss: 0.0855\n",
      "Epoch [290/500], Train Loss: 0.0852\n",
      "Epoch [291/500], Train Loss: 0.0855\n",
      "Epoch [292/500], Train Loss: 0.0850\n",
      "Epoch [293/500], Train Loss: 0.0848\n",
      "Epoch [294/500], Train Loss: 0.0847\n",
      "Epoch [295/500], Train Loss: 0.0843\n",
      "Epoch [296/500], Train Loss: 0.0846\n",
      "Epoch [297/500], Train Loss: 0.0845\n",
      "Epoch [298/500], Train Loss: 0.0844\n",
      "Epoch [299/500], Train Loss: 0.0840\n",
      "Epoch [300/500], Train Loss: 0.0838\n",
      "Epoch [301/500], Train Loss: 0.0836\n",
      "Epoch [302/500], Train Loss: 0.0837\n",
      "Epoch [303/500], Train Loss: 0.0836\n",
      "Epoch [304/500], Train Loss: 0.0835\n",
      "Epoch [305/500], Train Loss: 0.0833\n",
      "Epoch [306/500], Train Loss: 0.0833\n",
      "Epoch [307/500], Train Loss: 0.0831\n",
      "Epoch [308/500], Train Loss: 0.0829\n",
      "Epoch [309/500], Train Loss: 0.0828\n",
      "Epoch [310/500], Train Loss: 0.0826\n",
      "Epoch [311/500], Train Loss: 0.0825\n",
      "Epoch [312/500], Train Loss: 0.0824\n",
      "Epoch [313/500], Train Loss: 0.0822\n",
      "Epoch [314/500], Train Loss: 0.0821\n",
      "Epoch [315/500], Train Loss: 0.0820\n",
      "Epoch [316/500], Train Loss: 0.0819\n",
      "Epoch [317/500], Train Loss: 0.0817\n",
      "Epoch [318/500], Train Loss: 0.0815\n",
      "Epoch [319/500], Train Loss: 0.0814\n",
      "Epoch [320/500], Train Loss: 0.0814\n",
      "Epoch [321/500], Train Loss: 0.0809\n",
      "Epoch [322/500], Train Loss: 0.0811\n",
      "Epoch [323/500], Train Loss: 0.0810\n",
      "Epoch [324/500], Train Loss: 0.0810\n",
      "Epoch [325/500], Train Loss: 0.0806\n",
      "Epoch [326/500], Train Loss: 0.0806\n",
      "Epoch [327/500], Train Loss: 0.0804\n",
      "Epoch [328/500], Train Loss: 0.0804\n",
      "Epoch [329/500], Train Loss: 0.0803\n",
      "Epoch [330/500], Train Loss: 0.0801\n",
      "Epoch [331/500], Train Loss: 0.0801\n",
      "Epoch [332/500], Train Loss: 0.0798\n",
      "Epoch [333/500], Train Loss: 0.0800\n",
      "Epoch [334/500], Train Loss: 0.0795\n",
      "Epoch [335/500], Train Loss: 0.0794\n",
      "Epoch [336/500], Train Loss: 0.0793\n",
      "Epoch [337/500], Train Loss: 0.0792\n",
      "Epoch [338/500], Train Loss: 0.0793\n",
      "Epoch [339/500], Train Loss: 0.0790\n",
      "Epoch [340/500], Train Loss: 0.0788\n",
      "Epoch [341/500], Train Loss: 0.0790\n",
      "Epoch [342/500], Train Loss: 0.0788\n",
      "Epoch [343/500], Train Loss: 0.0789\n",
      "Epoch [344/500], Train Loss: 0.0789\n",
      "Epoch [345/500], Train Loss: 0.0785\n",
      "Epoch [346/500], Train Loss: 0.0783\n",
      "Epoch [347/500], Train Loss: 0.0786\n",
      "Epoch [348/500], Train Loss: 0.0784\n",
      "Epoch [349/500], Train Loss: 0.0782\n",
      "Epoch [350/500], Train Loss: 0.0781\n",
      "Epoch [351/500], Train Loss: 0.0777\n",
      "Epoch [352/500], Train Loss: 0.0779\n",
      "Epoch [353/500], Train Loss: 0.0776\n",
      "Epoch [354/500], Train Loss: 0.0775\n",
      "Epoch [355/500], Train Loss: 0.0774\n",
      "Epoch [356/500], Train Loss: 0.0774\n",
      "Epoch [357/500], Train Loss: 0.0776\n",
      "Epoch [358/500], Train Loss: 0.0770\n",
      "Epoch [359/500], Train Loss: 0.0772\n",
      "Epoch [360/500], Train Loss: 0.0771\n",
      "Epoch [361/500], Train Loss: 0.0768\n",
      "Epoch [362/500], Train Loss: 0.0769\n",
      "Epoch [363/500], Train Loss: 0.0768\n",
      "Epoch [364/500], Train Loss: 0.0767\n",
      "Epoch [365/500], Train Loss: 0.0764\n",
      "Epoch [366/500], Train Loss: 0.0765\n",
      "Epoch [367/500], Train Loss: 0.0764\n",
      "Epoch [368/500], Train Loss: 0.0764\n",
      "Epoch [369/500], Train Loss: 0.0761\n",
      "Epoch [370/500], Train Loss: 0.0760\n",
      "Epoch [371/500], Train Loss: 0.0758\n",
      "Epoch [372/500], Train Loss: 0.0759\n",
      "Epoch [373/500], Train Loss: 0.0758\n",
      "Epoch [374/500], Train Loss: 0.0755\n",
      "Epoch [375/500], Train Loss: 0.0755\n",
      "Epoch [376/500], Train Loss: 0.0755\n",
      "Epoch [377/500], Train Loss: 0.0753\n",
      "Epoch [378/500], Train Loss: 0.0751\n",
      "Epoch [379/500], Train Loss: 0.0752\n",
      "Epoch [380/500], Train Loss: 0.0750\n",
      "Epoch [381/500], Train Loss: 0.0749\n",
      "Epoch [382/500], Train Loss: 0.0749\n",
      "Epoch [383/500], Train Loss: 0.0745\n",
      "Epoch [384/500], Train Loss: 0.0747\n",
      "Epoch [385/500], Train Loss: 0.0746\n",
      "Epoch [386/500], Train Loss: 0.0744\n",
      "Epoch [387/500], Train Loss: 0.0745\n",
      "Epoch [388/500], Train Loss: 0.0744\n",
      "Epoch [389/500], Train Loss: 0.0744\n",
      "Epoch [390/500], Train Loss: 0.0742\n",
      "Epoch [391/500], Train Loss: 0.0742\n",
      "Epoch [392/500], Train Loss: 0.0741\n",
      "Epoch [393/500], Train Loss: 0.0742\n",
      "Epoch [394/500], Train Loss: 0.0741\n",
      "Epoch [395/500], Train Loss: 0.0738\n",
      "Epoch [396/500], Train Loss: 0.0737\n",
      "Epoch [397/500], Train Loss: 0.0736\n",
      "Epoch [398/500], Train Loss: 0.0736\n",
      "Epoch [399/500], Train Loss: 0.0737\n",
      "Epoch [400/500], Train Loss: 0.0734\n",
      "Epoch [401/500], Train Loss: 0.0734\n",
      "Epoch [402/500], Train Loss: 0.0732\n",
      "Epoch [403/500], Train Loss: 0.0733\n",
      "Epoch [404/500], Train Loss: 0.0730\n",
      "Epoch [405/500], Train Loss: 0.0729\n",
      "Epoch [406/500], Train Loss: 0.0730\n",
      "Epoch [407/500], Train Loss: 0.0732\n",
      "Epoch [408/500], Train Loss: 0.0728\n",
      "Epoch [409/500], Train Loss: 0.0726\n",
      "Epoch [410/500], Train Loss: 0.0726\n",
      "Epoch [411/500], Train Loss: 0.0725\n",
      "Epoch [412/500], Train Loss: 0.0725\n",
      "Epoch [413/500], Train Loss: 0.0725\n",
      "Epoch [414/500], Train Loss: 0.0723\n",
      "Epoch [415/500], Train Loss: 0.0721\n",
      "Epoch [416/500], Train Loss: 0.0722\n",
      "Epoch [417/500], Train Loss: 0.0720\n",
      "Epoch [418/500], Train Loss: 0.0719\n",
      "Epoch [419/500], Train Loss: 0.0718\n",
      "Epoch [420/500], Train Loss: 0.0719\n",
      "Epoch [421/500], Train Loss: 0.0718\n",
      "Epoch [422/500], Train Loss: 0.0717\n",
      "Epoch [423/500], Train Loss: 0.0717\n",
      "Epoch [424/500], Train Loss: 0.0717\n",
      "Epoch [425/500], Train Loss: 0.0715\n",
      "Epoch [426/500], Train Loss: 0.0713\n",
      "Epoch [427/500], Train Loss: 0.0715\n",
      "Epoch [428/500], Train Loss: 0.0710\n",
      "Epoch [429/500], Train Loss: 0.0713\n",
      "Epoch [430/500], Train Loss: 0.0711\n",
      "Epoch [431/500], Train Loss: 0.0712\n",
      "Epoch [432/500], Train Loss: 0.0712\n",
      "Epoch [433/500], Train Loss: 0.0711\n",
      "Early stopping at epoch 433\n",
      "Final model saved at: ../../../Data/model_results/logistic_regression/models/chr21/final_model_chr21.pth\n",
      "Individual R^2 scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr21/individual_r2_scores_chr21.csv\n",
      "Individual IQS scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr21/individual_iqs_scores_chr21.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-18 12:43:56,543] A new study created in RDB with name: chr22_study\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual AUC ROC curves saved in: ../../../Data/model_results/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../../Data/model_results/logistic_regression/csv_files/performance_metrics.csv\n",
      "Total SNPs:  1084\n",
      "PRS313 SNPs:  22\n",
      "Total SNPs used for Training:  1062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-06-18 12:44:47,983] Trial 2 finished with value: 2.283341681957245 and parameters: {'learning_rate': 0.09474070974068888, 'l1_coef': 2.6601248821391705e-05, 'patience': 11, 'batch_size': 128}. Best is trial 2 with value: 2.283341681957245.\n",
      "[I 2024-06-18 12:46:14,489] Trial 7 finished with value: 0.36068163743385906 and parameters: {'learning_rate': 0.002899901938520511, 'l1_coef': 0.01476076256388643, 'patience': 13, 'batch_size': 32}. Best is trial 7 with value: 0.36068163743385906.\n",
      "[I 2024-06-18 12:46:20,381] Trial 5 finished with value: 0.35517905205488204 and parameters: {'learning_rate': 0.055626803048252724, 'l1_coef': 0.09561134654796818, 'patience': 15, 'batch_size': 128}. Best is trial 5 with value: 0.35517905205488204.\n",
      "[I 2024-06-18 12:46:28,877] Trial 3 finished with value: 0.33546311350969166 and parameters: {'learning_rate': 0.062442912744401224, 'l1_coef': 0.00419050465414359, 'patience': 13, 'batch_size': 32}. Best is trial 3 with value: 0.33546311350969166.\n",
      "[I 2024-06-18 12:46:32,939] Trial 0 finished with value: 0.37201606333255766 and parameters: {'learning_rate': 0.0015222444884138938, 'l1_coef': 0.04731955238055238, 'patience': 11, 'batch_size': 128}. Best is trial 3 with value: 0.33546311350969166.\n",
      "[I 2024-06-18 12:46:43,704] Trial 8 finished with value: 0.17428269237279892 and parameters: {'learning_rate': 0.00043583120961510335, 'l1_coef': 0.00015951839793865467, 'patience': 10, 'batch_size': 256}. Best is trial 8 with value: 0.17428269237279892.\n",
      "[I 2024-06-18 12:46:49,275] Trial 1 finished with value: 0.3667631059885025 and parameters: {'learning_rate': 0.0008679842683881005, 'l1_coef': 0.028385855123242917, 'patience': 19, 'batch_size': 128}. Best is trial 8 with value: 0.17428269237279892.\n",
      "[I 2024-06-18 12:47:14,101] Trial 4 finished with value: 0.29666376251440785 and parameters: {'learning_rate': 0.0002290350899380453, 'l1_coef': 0.002355653212059651, 'patience': 7, 'batch_size': 32}. Best is trial 8 with value: 0.17428269237279892.\n",
      "[I 2024-06-18 12:47:15,187] Trial 9 finished with value: 0.1480237694887015 and parameters: {'learning_rate': 0.0009574867321547664, 'l1_coef': 0.000185657780206275, 'patience': 17, 'batch_size': 32}. Best is trial 9 with value: 0.1480237694887015.\n",
      "[I 2024-06-18 12:47:17,101] Trial 6 finished with value: 0.5075638800859451 and parameters: {'learning_rate': 0.000211114532922948, 'l1_coef': 0.06917200408689766, 'patience': 10, 'batch_size': 256}. Best is trial 9 with value: 0.1480237694887015.\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 22 - Best hyperparameters: {'learning_rate': 0.0009574867321547664, 'l1_coef': 0.000185657780206275, 'patience': 17, 'batch_size': 32}\n",
      "Chr 22 - Best value: 0.1480\n",
      "Epoch [1/500], Train Loss: 0.4211\n",
      "Epoch [2/500], Train Loss: 0.3464\n",
      "Epoch [3/500], Train Loss: 0.3202\n",
      "Epoch [4/500], Train Loss: 0.3010\n",
      "Epoch [5/500], Train Loss: 0.2864\n",
      "Epoch [6/500], Train Loss: 0.2740\n",
      "Epoch [7/500], Train Loss: 0.2639\n",
      "Epoch [8/500], Train Loss: 0.2546\n",
      "Epoch [9/500], Train Loss: 0.2474\n",
      "Epoch [10/500], Train Loss: 0.2408\n",
      "Epoch [11/500], Train Loss: 0.2353\n",
      "Epoch [12/500], Train Loss: 0.2300\n",
      "Epoch [13/500], Train Loss: 0.2255\n",
      "Epoch [14/500], Train Loss: 0.2214\n",
      "Epoch [15/500], Train Loss: 0.2177\n",
      "Epoch [16/500], Train Loss: 0.2143\n",
      "Epoch [17/500], Train Loss: 0.2113\n",
      "Epoch [18/500], Train Loss: 0.2087\n",
      "Epoch [19/500], Train Loss: 0.2056\n",
      "Epoch [20/500], Train Loss: 0.2038\n",
      "Epoch [21/500], Train Loss: 0.2015\n",
      "Epoch [22/500], Train Loss: 0.1994\n",
      "Epoch [23/500], Train Loss: 0.1972\n",
      "Epoch [24/500], Train Loss: 0.1956\n",
      "Epoch [25/500], Train Loss: 0.1941\n",
      "Epoch [26/500], Train Loss: 0.1921\n",
      "Epoch [27/500], Train Loss: 0.1910\n",
      "Epoch [28/500], Train Loss: 0.1895\n",
      "Epoch [29/500], Train Loss: 0.1878\n",
      "Epoch [30/500], Train Loss: 0.1867\n",
      "Epoch [31/500], Train Loss: 0.1855\n",
      "Epoch [32/500], Train Loss: 0.1842\n",
      "Epoch [33/500], Train Loss: 0.1830\n",
      "Epoch [34/500], Train Loss: 0.1820\n",
      "Epoch [35/500], Train Loss: 0.1808\n",
      "Epoch [36/500], Train Loss: 0.1800\n",
      "Epoch [37/500], Train Loss: 0.1790\n",
      "Epoch [38/500], Train Loss: 0.1782\n",
      "Epoch [39/500], Train Loss: 0.1776\n",
      "Epoch [40/500], Train Loss: 0.1760\n",
      "Epoch [41/500], Train Loss: 0.1754\n",
      "Epoch [42/500], Train Loss: 0.1748\n",
      "Epoch [43/500], Train Loss: 0.1745\n",
      "Epoch [44/500], Train Loss: 0.1732\n",
      "Epoch [45/500], Train Loss: 0.1722\n",
      "Epoch [46/500], Train Loss: 0.1717\n",
      "Epoch [47/500], Train Loss: 0.1714\n",
      "Epoch [48/500], Train Loss: 0.1706\n",
      "Epoch [49/500], Train Loss: 0.1701\n",
      "Epoch [50/500], Train Loss: 0.1693\n",
      "Epoch [51/500], Train Loss: 0.1687\n",
      "Epoch [52/500], Train Loss: 0.1680\n",
      "Epoch [53/500], Train Loss: 0.1678\n",
      "Epoch [54/500], Train Loss: 0.1675\n",
      "Epoch [55/500], Train Loss: 0.1668\n",
      "Epoch [56/500], Train Loss: 0.1659\n",
      "Epoch [57/500], Train Loss: 0.1655\n",
      "Epoch [58/500], Train Loss: 0.1651\n",
      "Epoch [59/500], Train Loss: 0.1649\n",
      "Epoch [60/500], Train Loss: 0.1644\n",
      "Epoch [61/500], Train Loss: 0.1635\n",
      "Epoch [62/500], Train Loss: 0.1635\n",
      "Epoch [63/500], Train Loss: 0.1628\n",
      "Epoch [64/500], Train Loss: 0.1625\n",
      "Epoch [65/500], Train Loss: 0.1621\n",
      "Epoch [66/500], Train Loss: 0.1620\n",
      "Epoch [67/500], Train Loss: 0.1615\n",
      "Epoch [68/500], Train Loss: 0.1607\n",
      "Epoch [69/500], Train Loss: 0.1606\n",
      "Epoch [70/500], Train Loss: 0.1605\n",
      "Epoch [71/500], Train Loss: 0.1599\n",
      "Epoch [72/500], Train Loss: 0.1597\n",
      "Epoch [73/500], Train Loss: 0.1595\n",
      "Epoch [74/500], Train Loss: 0.1586\n",
      "Epoch [75/500], Train Loss: 0.1585\n",
      "Epoch [76/500], Train Loss: 0.1585\n",
      "Epoch [77/500], Train Loss: 0.1584\n",
      "Epoch [78/500], Train Loss: 0.1580\n",
      "Epoch [79/500], Train Loss: 0.1573\n",
      "Epoch [80/500], Train Loss: 0.1574\n",
      "Epoch [81/500], Train Loss: 0.1567\n",
      "Epoch [82/500], Train Loss: 0.1562\n",
      "Epoch [83/500], Train Loss: 0.1561\n",
      "Epoch [84/500], Train Loss: 0.1558\n",
      "Epoch [85/500], Train Loss: 0.1558\n",
      "Epoch [86/500], Train Loss: 0.1553\n",
      "Epoch [87/500], Train Loss: 0.1550\n",
      "Epoch [88/500], Train Loss: 0.1551\n",
      "Epoch [89/500], Train Loss: 0.1549\n",
      "Epoch [90/500], Train Loss: 0.1548\n",
      "Epoch [91/500], Train Loss: 0.1543\n",
      "Epoch [92/500], Train Loss: 0.1537\n",
      "Epoch [93/500], Train Loss: 0.1539\n",
      "Epoch [94/500], Train Loss: 0.1534\n",
      "Epoch [95/500], Train Loss: 0.1532\n",
      "Epoch [96/500], Train Loss: 0.1535\n",
      "Epoch [97/500], Train Loss: 0.1530\n",
      "Epoch [98/500], Train Loss: 0.1525\n",
      "Epoch [99/500], Train Loss: 0.1529\n",
      "Epoch [100/500], Train Loss: 0.1522\n",
      "Epoch [101/500], Train Loss: 0.1521\n",
      "Epoch [102/500], Train Loss: 0.1520\n",
      "Epoch [103/500], Train Loss: 0.1522\n",
      "Epoch [104/500], Train Loss: 0.1517\n",
      "Epoch [105/500], Train Loss: 0.1514\n",
      "Epoch [106/500], Train Loss: 0.1512\n",
      "Epoch [107/500], Train Loss: 0.1509\n",
      "Epoch [108/500], Train Loss: 0.1508\n",
      "Epoch [109/500], Train Loss: 0.1507\n",
      "Epoch [110/500], Train Loss: 0.1502\n",
      "Epoch [111/500], Train Loss: 0.1503\n",
      "Epoch [112/500], Train Loss: 0.1501\n",
      "Epoch [113/500], Train Loss: 0.1499\n",
      "Epoch [114/500], Train Loss: 0.1499\n",
      "Epoch [115/500], Train Loss: 0.1495\n",
      "Epoch [116/500], Train Loss: 0.1496\n",
      "Epoch [117/500], Train Loss: 0.1494\n",
      "Epoch [118/500], Train Loss: 0.1490\n",
      "Epoch [119/500], Train Loss: 0.1489\n",
      "Epoch [120/500], Train Loss: 0.1491\n",
      "Epoch [121/500], Train Loss: 0.1488\n",
      "Epoch [122/500], Train Loss: 0.1489\n",
      "Epoch [123/500], Train Loss: 0.1484\n",
      "Epoch [124/500], Train Loss: 0.1484\n",
      "Epoch [125/500], Train Loss: 0.1481\n",
      "Epoch [126/500], Train Loss: 0.1481\n",
      "Epoch [127/500], Train Loss: 0.1478\n",
      "Epoch [128/500], Train Loss: 0.1477\n",
      "Epoch [129/500], Train Loss: 0.1477\n",
      "Epoch [130/500], Train Loss: 0.1476\n",
      "Epoch [131/500], Train Loss: 0.1475\n",
      "Epoch [132/500], Train Loss: 0.1472\n",
      "Epoch [133/500], Train Loss: 0.1472\n",
      "Epoch [134/500], Train Loss: 0.1469\n",
      "Epoch [135/500], Train Loss: 0.1471\n",
      "Epoch [136/500], Train Loss: 0.1467\n",
      "Epoch [137/500], Train Loss: 0.1467\n",
      "Epoch [138/500], Train Loss: 0.1466\n",
      "Epoch [139/500], Train Loss: 0.1467\n",
      "Epoch [140/500], Train Loss: 0.1463\n",
      "Epoch [141/500], Train Loss: 0.1463\n",
      "Epoch [142/500], Train Loss: 0.1466\n",
      "Epoch [143/500], Train Loss: 0.1461\n",
      "Epoch [144/500], Train Loss: 0.1461\n",
      "Epoch [145/500], Train Loss: 0.1458\n",
      "Epoch [146/500], Train Loss: 0.1458\n",
      "Epoch [147/500], Train Loss: 0.1457\n",
      "Epoch [148/500], Train Loss: 0.1457\n",
      "Epoch [149/500], Train Loss: 0.1453\n",
      "Epoch [150/500], Train Loss: 0.1450\n",
      "Epoch [151/500], Train Loss: 0.1450\n",
      "Epoch [152/500], Train Loss: 0.1453\n",
      "Epoch [153/500], Train Loss: 0.1450\n",
      "Epoch [154/500], Train Loss: 0.1448\n",
      "Epoch [155/500], Train Loss: 0.1447\n",
      "Epoch [156/500], Train Loss: 0.1446\n",
      "Epoch [157/500], Train Loss: 0.1445\n",
      "Epoch [158/500], Train Loss: 0.1446\n",
      "Epoch [159/500], Train Loss: 0.1444\n",
      "Epoch [160/500], Train Loss: 0.1441\n",
      "Epoch [161/500], Train Loss: 0.1445\n",
      "Epoch [162/500], Train Loss: 0.1442\n",
      "Epoch [163/500], Train Loss: 0.1439\n",
      "Epoch [164/500], Train Loss: 0.1444\n",
      "Epoch [165/500], Train Loss: 0.1439\n",
      "Epoch [166/500], Train Loss: 0.1437\n",
      "Epoch [167/500], Train Loss: 0.1439\n",
      "Epoch [168/500], Train Loss: 0.1436\n",
      "Epoch [169/500], Train Loss: 0.1438\n",
      "Epoch [170/500], Train Loss: 0.1436\n",
      "Epoch [171/500], Train Loss: 0.1435\n",
      "Epoch [172/500], Train Loss: 0.1435\n",
      "Epoch [173/500], Train Loss: 0.1434\n",
      "Epoch [174/500], Train Loss: 0.1437\n",
      "Epoch [175/500], Train Loss: 0.1432\n",
      "Epoch [176/500], Train Loss: 0.1430\n",
      "Epoch [177/500], Train Loss: 0.1430\n",
      "Epoch [178/500], Train Loss: 0.1429\n",
      "Epoch [179/500], Train Loss: 0.1432\n",
      "Epoch [180/500], Train Loss: 0.1429\n",
      "Epoch [181/500], Train Loss: 0.1429\n",
      "Epoch [182/500], Train Loss: 0.1425\n",
      "Epoch [183/500], Train Loss: 0.1423\n",
      "Epoch [184/500], Train Loss: 0.1422\n",
      "Epoch [185/500], Train Loss: 0.1424\n",
      "Epoch [186/500], Train Loss: 0.1424\n",
      "Epoch [187/500], Train Loss: 0.1424\n",
      "Epoch [188/500], Train Loss: 0.1422\n",
      "Epoch [189/500], Train Loss: 0.1421\n",
      "Epoch [190/500], Train Loss: 0.1420\n",
      "Epoch [191/500], Train Loss: 0.1421\n",
      "Epoch [192/500], Train Loss: 0.1419\n",
      "Epoch [193/500], Train Loss: 0.1421\n",
      "Epoch [194/500], Train Loss: 0.1418\n",
      "Epoch [195/500], Train Loss: 0.1415\n",
      "Epoch [196/500], Train Loss: 0.1418\n",
      "Epoch [197/500], Train Loss: 0.1419\n",
      "Epoch [198/500], Train Loss: 0.1416\n",
      "Epoch [199/500], Train Loss: 0.1418\n",
      "Epoch [200/500], Train Loss: 0.1413\n",
      "Epoch [201/500], Train Loss: 0.1413\n",
      "Epoch [202/500], Train Loss: 0.1414\n",
      "Epoch [203/500], Train Loss: 0.1415\n",
      "Epoch [204/500], Train Loss: 0.1414\n",
      "Epoch [205/500], Train Loss: 0.1419\n",
      "Epoch [206/500], Train Loss: 0.1412\n",
      "Epoch [207/500], Train Loss: 0.1413\n",
      "Epoch [208/500], Train Loss: 0.1412\n",
      "Epoch [209/500], Train Loss: 0.1412\n",
      "Epoch [210/500], Train Loss: 0.1413\n",
      "Epoch [211/500], Train Loss: 0.1407\n",
      "Epoch [212/500], Train Loss: 0.1408\n",
      "Epoch [213/500], Train Loss: 0.1410\n",
      "Epoch [214/500], Train Loss: 0.1406\n",
      "Epoch [215/500], Train Loss: 0.1407\n",
      "Epoch [216/500], Train Loss: 0.1405\n",
      "Epoch [217/500], Train Loss: 0.1404\n",
      "Epoch [218/500], Train Loss: 0.1407\n",
      "Epoch [219/500], Train Loss: 0.1403\n",
      "Epoch [220/500], Train Loss: 0.1405\n",
      "Epoch [221/500], Train Loss: 0.1402\n",
      "Epoch [222/500], Train Loss: 0.1405\n",
      "Epoch [223/500], Train Loss: 0.1407\n",
      "Epoch [224/500], Train Loss: 0.1403\n",
      "Epoch [225/500], Train Loss: 0.1407\n",
      "Epoch [226/500], Train Loss: 0.1403\n",
      "Epoch [227/500], Train Loss: 0.1402\n",
      "Epoch [228/500], Train Loss: 0.1400\n",
      "Epoch [229/500], Train Loss: 0.1399\n",
      "Epoch [230/500], Train Loss: 0.1401\n",
      "Epoch [231/500], Train Loss: 0.1404\n",
      "Epoch [232/500], Train Loss: 0.1400\n",
      "Epoch [233/500], Train Loss: 0.1402\n",
      "Epoch [234/500], Train Loss: 0.1404\n",
      "Epoch [235/500], Train Loss: 0.1396\n",
      "Epoch [236/500], Train Loss: 0.1395\n",
      "Epoch [237/500], Train Loss: 0.1399\n",
      "Epoch [238/500], Train Loss: 0.1395\n",
      "Epoch [239/500], Train Loss: 0.1398\n",
      "Epoch [240/500], Train Loss: 0.1397\n",
      "Epoch [241/500], Train Loss: 0.1396\n",
      "Epoch [242/500], Train Loss: 0.1396\n",
      "Epoch [243/500], Train Loss: 0.1376\n",
      "Epoch [244/500], Train Loss: 0.1364\n",
      "Epoch [245/500], Train Loss: 0.1364\n",
      "Epoch [246/500], Train Loss: 0.1363\n",
      "Epoch [247/500], Train Loss: 0.1362\n",
      "Epoch [248/500], Train Loss: 0.1363\n",
      "Epoch [249/500], Train Loss: 0.1363\n",
      "Epoch [250/500], Train Loss: 0.1362\n",
      "Epoch [251/500], Train Loss: 0.1364\n",
      "Epoch [252/500], Train Loss: 0.1363\n",
      "Epoch [253/500], Train Loss: 0.1363\n",
      "Epoch [254/500], Train Loss: 0.1364\n",
      "Epoch [255/500], Train Loss: 0.1363\n",
      "Epoch [256/500], Train Loss: 0.1363\n",
      "Epoch [257/500], Train Loss: 0.1361\n",
      "Epoch [258/500], Train Loss: 0.1358\n",
      "Epoch [259/500], Train Loss: 0.1359\n",
      "Epoch [260/500], Train Loss: 0.1358\n",
      "Epoch [261/500], Train Loss: 0.1357\n",
      "Epoch [262/500], Train Loss: 0.1358\n",
      "Epoch [263/500], Train Loss: 0.1357\n",
      "Epoch [264/500], Train Loss: 0.1358\n",
      "Epoch [265/500], Train Loss: 0.1359\n",
      "Epoch [266/500], Train Loss: 0.1357\n",
      "Epoch [267/500], Train Loss: 0.1358\n",
      "Epoch [268/500], Train Loss: 0.1358\n",
      "Epoch [269/500], Train Loss: 0.1359\n",
      "Epoch [270/500], Train Loss: 0.1358\n",
      "Epoch [271/500], Train Loss: 0.1357\n",
      "Epoch [272/500], Train Loss: 0.1360\n",
      "Epoch [273/500], Train Loss: 0.1358\n",
      "Epoch [274/500], Train Loss: 0.1357\n",
      "Epoch [275/500], Train Loss: 0.1358\n",
      "Epoch [276/500], Train Loss: 0.1359\n",
      "Epoch [277/500], Train Loss: 0.1359\n",
      "Epoch [278/500], Train Loss: 0.1359\n",
      "Epoch [279/500], Train Loss: 0.1357\n",
      "Epoch [280/500], Train Loss: 0.1358\n",
      "Early stopping at epoch 280\n",
      "Final model saved at: ../../../Data/model_results/logistic_regression/models/chr22/final_model_chr22.pth\n",
      "Individual R^2 scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr22/individual_r2_scores_chr22.csv\n",
      "Individual IQS scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr22/individual_iqs_scores_chr22.csv\n",
      "Individual AUC ROC curves saved in: ../../../Data/model_results/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../../Data/model_results/logistic_regression/csv_files/performance_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "from sklearn.metrics import r2_score as sklearn_r2_score\n",
    "import optuna\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# Load and preprocess the data\n",
    "data_directory = '../../../Data/Filtered_split_training_data/'\n",
    "start = 1\n",
    "\n",
    "# Initialize lists to store the performance metrics for each chromosome\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "false_positive_rates = []\n",
    "auc_rocs = []\n",
    "r2_scores = []\n",
    "iqs_scores = []\n",
    "\n",
    "# Create folders for saving files\n",
    "output_folder = \"../../../Data/model_results/logistic_regression/\"\n",
    "model_folder = output_folder + \"models/\"\n",
    "csv_folder = output_folder + \"csv_files/\"\n",
    "curve_folder = output_folder + \"roc_curves/\"\n",
    "\n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "os.makedirs(csv_folder, exist_ok=True)\n",
    "os.makedirs(curve_folder, exist_ok=True)\n",
    "\n",
    "for chromosome_number in range(start, 23):\n",
    "    # Create subfolders for the current chromosome\n",
    "    chr_model_folder = model_folder + f\"chr{chromosome_number}/\"\n",
    "    chr_csv_folder = csv_folder + f\"chr{chromosome_number}/\"\n",
    "    chr_curve_folder = curve_folder + f\"chr{chromosome_number}/\"\n",
    "\n",
    "    os.makedirs(chr_model_folder, exist_ok=True)\n",
    "    os.makedirs(chr_csv_folder, exist_ok=True)\n",
    "    os.makedirs(chr_curve_folder, exist_ok=True)\n",
    "\n",
    "    file_name = data_directory + f\"23AndMe_PRS313_merged_chr{chromosome_number}_matching_split.parquet\"\n",
    "    data = pd.read_parquet(file_name)\n",
    "\n",
    "\n",
    "    # # Split the data into features and target\n",
    "    # X = torch.tensor(data.filter(regex='^(?!.*Unknown)').values, dtype=torch.float32)\n",
    "    # y = torch.tensor(data.filter(regex='Unknown').values, dtype=torch.float32)\n",
    "\n",
    "    # print(\"Unknown PRS313 SNPs: \", y.shape[1])\n",
    "    # print(\"Known PRS313 SNPs: \", data[[col for col in data.columns if \"PRS313_Known\" in col]].shape[1])\n",
    "    # print(\"23AndMe SNPs with LD to Unknown PRS313 SNPs: \", data[[col for col in data.columns if \"PRS313_\" not in col]].shape[1])\n",
    "    # print(\"Total SNPs used for Training: \", X.shape[1])\n",
    "\n",
    "    # Split the data into features and target\n",
    "    X = torch.tensor(data.filter(regex='^(?!.*PRS313_)').values, dtype=torch.float32)\n",
    "    y = torch.tensor(data.filter(regex='PRS313_').values, dtype=torch.float32)\n",
    "\n",
    "    print(\"Total SNPs: \", data.shape[1])\n",
    "    print(\"PRS313 SNPs: \", y.shape[1])\n",
    "    print(\"Total SNPs used for Training: \", X.shape[1])\n",
    "\n",
    "\n",
    "    # Split the data into train-validation and test sets\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Define the logistic regression model with lasso regularization\n",
    "    class LogisticRegression(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim, l1_coef=0.0):\n",
    "            super(LogisticRegression, self).__init__()\n",
    "            self.linear = nn.Linear(input_dim, output_dim)\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "            self.l1_coef = l1_coef\n",
    "\n",
    "        def forward(self, x):\n",
    "            out = self.linear(x)\n",
    "            out = self.sigmoid(out)\n",
    "            return out\n",
    "\n",
    "        def l1_loss(self):\n",
    "            return self.l1_coef * torch.norm(self.linear.weight, p=1)\n",
    "        \n",
    "    # Set the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Set the hyperparameters for tuning\n",
    "    input_dim = X_train_val.shape[1]\n",
    "    output_dim = y_train_val.shape[1]\n",
    "    num_epochs = 500\n",
    "    batch_size = 128\n",
    "\n",
    "    # Define the objective function for Optuna with cross-validation and early stopping\n",
    "    def objective(trial):\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-1, log=True)\n",
    "        l1_coef = trial.suggest_float('l1_coef', 1e-5, 1e-1, log=True)\n",
    "        patience = trial.suggest_int('patience', 5, 20)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "\n",
    "        model = LogisticRegression(input_dim, output_dim, l1_coef).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        criterion = nn.BCELoss()\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=False)\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        fold_losses = []\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_val, y_train_val.argmax(dim=1))):\n",
    "            X_train, X_val = X_train_val[train_idx], X_train_val[val_idx]\n",
    "            y_train, y_val = y_train_val[train_idx], y_train_val[val_idx]\n",
    "\n",
    "            train_dataset = TensorDataset(X_train, y_train)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "            best_val_loss = float('inf')\n",
    "            counter = 0\n",
    "\n",
    "            for epoch in range(num_epochs):\n",
    "                train_loss = 0.0\n",
    "                for batch_X, batch_y in train_loader:\n",
    "                    batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "                    outputs = model(batch_X)\n",
    "                    loss = criterion(outputs, batch_y) + model.l1_loss()\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    train_loss += loss.item()\n",
    "\n",
    "                train_loss /= len(train_loader)\n",
    "\n",
    "                val_dataset = TensorDataset(X_val, y_val)\n",
    "                val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    val_loss = 0.0\n",
    "                    for batch_X, batch_y in val_loader:\n",
    "                        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                        outputs = model(batch_X)\n",
    "                        loss = criterion(outputs, batch_y) + model.l1_loss()\n",
    "                        val_loss += loss.item()\n",
    "\n",
    "                    val_loss /= len(val_loader)\n",
    "                    scheduler.step(val_loss)\n",
    "\n",
    "                    if val_loss < best_val_loss:\n",
    "                        best_val_loss = val_loss\n",
    "                        counter = 0\n",
    "                    else:\n",
    "                        counter += 1\n",
    "\n",
    "                    if counter >= patience:\n",
    "                        # print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                        break\n",
    "\n",
    "            fold_losses.append(best_val_loss)\n",
    "\n",
    "        return np.mean(fold_losses)\n",
    "\n",
    "    # Create the \"optuna_studies\" folder if it doesn't exist\n",
    "    os.makedirs(\"optuna_studies\", exist_ok=True)\n",
    "\n",
    "    # Create an Optuna study and optimize the hyperparameters\n",
    "    study_name = f\"chr{chromosome_number}_study\"\n",
    "    storage_name = f\"sqlite:///optuna_studies/{study_name}.db\"\n",
    "\n",
    "    # Check if the study exists\n",
    "\n",
    "    current_dir = os.getcwd()\n",
    "    study_exists = os.path.exists(current_dir + f\"/optuna_studies/{study_name}.db\")\n",
    "    \n",
    "    if study_exists:\n",
    "        # Load the existing study\n",
    "        study = optuna.load_study(study_name=study_name, storage=storage_name)\n",
    "    else:\n",
    "        # Create a new study\n",
    "        study = optuna.create_study(direction='minimize', study_name=study_name, storage=storage_name)\n",
    "\n",
    "    study.optimize(objective, n_trials=10, n_jobs=-1)\n",
    "\n",
    "    # Print the best hyperparameters and best value\n",
    "    print(f\"Chr {chromosome_number} - Best hyperparameters: {study.best_params}\")\n",
    "    print(f\"Chr {chromosome_number} - Best value: {study.best_value:.4f}\")\n",
    "\n",
    "    # Train the final model with the best hyperparameters and early stopping\n",
    "    best_learning_rate = study.best_params['learning_rate']\n",
    "    best_l1_coef = study.best_params['l1_coef']\n",
    "    best_patience = study.best_params['patience']\n",
    "    best_batch_size = study.best_params['batch_size']\n",
    "\n",
    "    model = LogisticRegression(input_dim, output_dim, best_l1_coef).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_learning_rate)\n",
    "    criterion = nn.BCELoss()\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=False)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_val, y_train_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=best_batch_size, shuffle=True)\n",
    "\n",
    "    best_train_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y) + model.l1_loss()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "        if train_loss < best_train_loss:\n",
    "            best_train_loss = train_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        if counter >= best_patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "        scheduler.step(train_loss)\n",
    "\n",
    "    # Save the final model\n",
    "    model_save_path = chr_model_folder + f'final_model_chr{chromosome_number}.pth'\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"Final model saved at: {model_save_path}\")\n",
    "\n",
    "    # Evaluate the final model on the test set\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test.to(device))\n",
    "        test_preds = (test_outputs > 0.5).float()\n",
    "        test_accuracy = float(((test_preds > 0.5) == y_test).float().mean())\n",
    "        test_precision = precision_score(y_test.cpu().numpy(), test_preds.cpu().numpy(), average='micro')\n",
    "        test_recall = recall_score(y_test.cpu().numpy(), test_preds.cpu().numpy(), average='micro')\n",
    "        test_f1 = f1_score(y_test.cpu().numpy(), test_preds.cpu().numpy(), average='micro')\n",
    "        test_roc_auc = roc_auc_score(y_test.cpu().numpy(), test_outputs.cpu().numpy(), average='micro')\n",
    "        test_r2 = sklearn_r2_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "        test_iqs = calculate_iqs(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "\n",
    "        # Calculate false positive rate\n",
    "        cm = confusion_matrix(y_test.cpu().numpy().ravel(), test_preds.cpu().numpy().ravel())\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        test_fpr = fp / (fp + tn)\n",
    "\n",
    "        # Append performance metrics to the lists\n",
    "        accuracies.append(test_accuracy)\n",
    "        precisions.append(test_precision)\n",
    "        recalls.append(test_recall)\n",
    "        false_positive_rates.append(test_fpr)\n",
    "        auc_rocs.append(test_roc_auc)\n",
    "        r2_scores.append(test_r2)\n",
    "        iqs_scores.append(test_iqs)\n",
    "\n",
    "        # Calculate individual R^2 scores for each SNP\n",
    "        individual_r2_scores = sklearn_r2_score(y_test.cpu().numpy(), test_outputs.cpu().numpy(), multioutput='raw_values')\n",
    "\n",
    "        # Calculate individual IQS scores for each SNP\n",
    "        individual_iqs_scores = np.array([calculate_iqs(y_test.cpu().numpy()[:, i].reshape(-1, 1), test_outputs.cpu().numpy()[:, i].reshape(-1, 1)) for i in range(y_test.shape[1])])\n",
    "\n",
    "        # Get the names of the SNPs from the original dataframe\n",
    "        snp_names = data.filter(regex='Unknown').columns\n",
    "\n",
    "        # Save individual R^2 scores to a CSV file\n",
    "        csv_file = chr_csv_folder + f'individual_r2_scores_chr{chromosome_number}.csv'\n",
    "\n",
    "        with open(csv_file, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['SNP', 'R2 Score'])\n",
    "            for snp, r2_score in zip(snp_names, individual_r2_scores):\n",
    "                writer.writerow([snp, r2_score])\n",
    "\n",
    "        print(f\"Individual R^2 scores saved at: {csv_file}\")\n",
    "\n",
    "        # Save individual IQS scores to a CSV file\n",
    "        iqs_csv_file = chr_csv_folder + f'individual_iqs_scores_chr{chromosome_number}.csv'\n",
    "\n",
    "        with open(iqs_csv_file, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['SNP', 'IQS Score'])\n",
    "            for snp, iqs_score in zip(snp_names, individual_iqs_scores):\n",
    "                writer.writerow([snp, iqs_score])\n",
    "\n",
    "        print(f\"Individual IQS scores saved at: {iqs_csv_file}\")\n",
    "\n",
    "        # Save individual AUC ROC curves for each SNP\n",
    "        for i, snp in enumerate(snp_names):\n",
    "            try: \n",
    "                fpr, tpr, _ = roc_curve(y_test.cpu().numpy()[:, i], test_outputs.cpu().numpy()[:, i])\n",
    "                plt.figure()\n",
    "                plt.plot(fpr, tpr, label=f'AUC ROC = {roc_auc_score(y_test.cpu().numpy()[:, i], test_outputs.cpu().numpy()[:, i]):.4f}')\n",
    "                plt.xlabel('False Positive Rate')\n",
    "                plt.ylabel('True Positive Rate')\n",
    "                plt.title(f'AUC ROC Curve - {snp}')\n",
    "                plt.legend()\n",
    "                \n",
    "                curve_file = chr_curve_folder + f'auc_roc_curve_{snp}_chr{chromosome_number}.png'\n",
    "                plt.savefig(curve_file)\n",
    "                plt.close()\n",
    "            except ValueError:\n",
    "                # Save a placeholder image if there is insufficient data\n",
    "                plt.figure()\n",
    "                plt.axis('off')\n",
    "                plt.text(0.5, 0.5, \"Insufficient data for ROC curve\", ha='center', va='center')\n",
    "                curve_file = chr_curve_folder + f'auc_roc_curve_{snp}_chr{chromosome_number}.png'\n",
    "                plt.savefig(curve_file)\n",
    "                plt.close()\n",
    "\n",
    "                print(f\"Skipping SNP {snp} due to insufficient data\")\n",
    "\n",
    "\n",
    "        print(f\"Individual AUC ROC curves saved in: {curve_folder}\")\n",
    "\n",
    "        # Create a DataFrame to store the performance metrics for each chromosome\n",
    "        performance_df = pd.DataFrame({\n",
    "            'Chromosome': list(range(start, chromosome_number + 1)),\n",
    "            'R2 Score': r2_scores,\n",
    "            'IQS Score': iqs_scores,\n",
    "\n",
    "            'Accuracy': accuracies,\n",
    "            # 'Precision': precisions,\n",
    "            # 'Recall': recalls,\n",
    "            # 'False Positive Rate': false_positive_rates,\n",
    "            'AUC ROC': auc_rocs,\n",
    "        })\n",
    "\n",
    "        # Save the performance metrics to a CSV file\n",
    "        performance_csv_file = csv_folder + 'performance_metrics.csv'\n",
    "        performance_df.to_csv(performance_csv_file, index=False)\n",
    "        print(f\"Performance metrics saved at: {performance_csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance metrics saved at: ../../../Data/model_results/logistic_regression/csv_files/performance_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, r2_score as sklearn_r2_score\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# Define the logistic regression model class\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, l1_coef=0.0):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.l1_coef = l1_coef\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "    def l1_loss(self):\n",
    "        return self.l1_coef * torch.norm(self.linear.weight, p=1)\n",
    "\n",
    "# Function to load and evaluate the model\n",
    "def load_and_evaluate_model(chromosome_number, data_directory, model_folder):\n",
    "    # Load the data\n",
    "    file_name = data_directory + f\"23AndMe_PRS313_merged_chr{chromosome_number}_matching_split.parquet\"\n",
    "    data = pd.read_parquet(file_name)\n",
    "\n",
    "    # Split the data into features and target\n",
    "    X = torch.tensor(data.filter(regex='^(?!.*PRS313_)').values, dtype=torch.float32)\n",
    "    y = torch.tensor(data.filter(regex='PRS313_').values, dtype=torch.float32)\n",
    "\n",
    "    # Split the data into train-validation and test sets\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Set the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load the saved model\n",
    "    input_dim = X_train_val.shape[1]\n",
    "    output_dim = y_train_val.shape[1]\n",
    "    model_save_path = model_folder + f'chr{chromosome_number}/final_model_chr{chromosome_number}.pth'\n",
    "    model = LogisticRegression(input_dim, output_dim).to(device)\n",
    "    model.load_state_dict(torch.load(model_save_path))\n",
    "    model.eval()\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test.to(device))\n",
    "        test_preds = (test_outputs > 0.5).float()\n",
    "        test_accuracy = float(((test_preds > 0.5) == y_test).float().mean())\n",
    "        test_precision = precision_score(y_test.cpu().numpy(), test_preds.cpu().numpy(), average='micro')\n",
    "        test_recall = recall_score(y_test.cpu().numpy(), test_preds.cpu().numpy(), average='micro')\n",
    "        test_f1 = f1_score(y_test.cpu().numpy(), test_preds.cpu().numpy(), average='micro')\n",
    "        test_roc_auc = roc_auc_score(y_test.cpu().numpy(), test_outputs.cpu().numpy(), average='micro')\n",
    "        test_r2 = sklearn_r2_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "        test_iqs = calculate_iqs(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "\n",
    "        # Calculate false positive rate\n",
    "        cm = confusion_matrix(y_test.cpu().numpy().ravel(), test_preds.cpu().numpy().ravel())\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        test_fpr = fp / (fp + tn)\n",
    "\n",
    "    return {\n",
    "        'Chromosome': chromosome_number,\n",
    "        'R2 Score': test_r2,\n",
    "        'IQS Score': test_iqs,\n",
    "        'Accuracy': test_accuracy,\n",
    "        # 'Precision': test_precision,\n",
    "        # 'Recall': test_recall,\n",
    "        # 'False Positive Rate': test_fpr,\n",
    "        'AUC ROC': test_roc_auc,\n",
    "    }\n",
    "\n",
    "# Main function to load and evaluate models for all chromosomes and save results to a CSV file\n",
    "def evaluate_all_chromosomes_and_save_to_csv(start_chromosome, end_chromosome, data_directory, model_folder, csv_file):\n",
    "    results = []\n",
    "    for chromosome_number in range(start_chromosome, end_chromosome + 1):\n",
    "        result = load_and_evaluate_model(chromosome_number, data_directory, model_folder)\n",
    "        results.append(result)\n",
    "\n",
    "    # Create a DataFrame to store the performance metrics for each chromosome\n",
    "    performance_df = pd.DataFrame(results)\n",
    "\n",
    "    # Save the performance metrics to a CSV file\n",
    "    performance_df.to_csv(csv_file, index=False)\n",
    "    print(f\"Performance metrics saved at: {csv_file}\")\n",
    "\n",
    "# Example usage\n",
    "start_chromosome = 1  # Replace with the starting chromosome number\n",
    "end_chromosome = 22   # Replace with the ending chromosome number\n",
    "data_directory = '../../../Data/Filtered_split_training_data/'\n",
    "model_folder = \"../../../Data/model_results/logistic_regression/models/\"\n",
    "csv_file = \"../../../Data/model_results/logistic_regression/csv_files/performance_metrics.csv\"\n",
    "\n",
    "evaluate_all_chromosomes_and_save_to_csv(start_chromosome, end_chromosome, data_directory, model_folder, csv_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
