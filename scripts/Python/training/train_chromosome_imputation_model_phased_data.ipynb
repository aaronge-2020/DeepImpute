{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IQS Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_iqs(true_genotypes, imputed_dosages):\n",
    "    # Convert imputed dosages to discrete values\n",
    "    imputed_discrete = np.round(imputed_dosages).astype(int)\n",
    "\n",
    "    # Clip the imputed discrete values to be within the range of 0 to 2\n",
    "    imputed_discrete = np.clip(imputed_discrete, 0, 2)\n",
    "\n",
    "    # Create a contingency table\n",
    "    contingency_table = np.zeros((3, 3), dtype=int)\n",
    "\n",
    "    # Fill the contingency table\n",
    "    for true_geno, imputed_geno in zip(true_genotypes, imputed_discrete):\n",
    "        for true_allele, imputed_allele in zip(true_geno, imputed_geno):\n",
    "            contingency_table[int(true_allele), int(imputed_allele)] += 1\n",
    "\n",
    "    # Calculate the total number of genotypes\n",
    "    total_genotypes = np.sum(contingency_table)\n",
    "\n",
    "    # Calculate observed proportion of agreement (Po)\n",
    "    observed_agreement = np.trace(contingency_table) / total_genotypes\n",
    "\n",
    "    # Calculate marginal sums\n",
    "    row_marginals = np.sum(contingency_table, axis=1)\n",
    "    col_marginals = np.sum(contingency_table, axis=0)\n",
    "\n",
    "    # Calculate chance agreement (Pc)\n",
    "    chance_agreement = np.sum((row_marginals * col_marginals) / (total_genotypes ** 2))\n",
    "\n",
    "    # Calculate IQS\n",
    "    if chance_agreement == 1:  # To prevent division by zero in case of perfect chance agreement\n",
    "        iqs_score = 0\n",
    "    else:\n",
    "        iqs_score = (observed_agreement - chance_agreement) / (1 - chance_agreement)\n",
    "\n",
    "    return iqs_score\n",
    "\n",
    "# Example usage:\n",
    "true_genotypes = np.array([[0, 1, 2], [1, 2, 0], [2, 0, 1]])\n",
    "imputed_dosages = np.array([[0.1, 1.2, 1.9], [1.0, 1.8, 0.3], [2.0, 0.5, 1.4]])\n",
    "\n",
    "iqs_score = calculate_iqs(true_genotypes, imputed_dosages)\n",
    "print(f\"IQS Score: {iqs_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total SNPs:  1714\n",
      "PRS313 SNPs:  40\n",
      "Total SNPs used for Training:  1674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-05-30 13:49:12,711] Trial 10 finished with value: 0.22871639476372643 and parameters: {'learning_rate': 0.009375212909338682, 'l1_coef': 4.393958333523146e-05, 'patience': 5, 'batch_size': 32}. Best is trial 0 with value: 0.17545781341882855.\n",
      "[I 2024-05-30 13:49:51,343] Trial 4 finished with value: 0.552144455909729 and parameters: {'learning_rate': 0.010779792685448014, 'l1_coef': 0.04157067353646598, 'patience': 13, 'batch_size': 256}. Best is trial 0 with value: 0.17545781341882855.\n",
      "[I 2024-05-30 13:49:56,615] Trial 6 finished with value: 0.552363109588623 and parameters: {'learning_rate': 0.002239794865613517, 'l1_coef': 0.004876956611674172, 'patience': 20, 'batch_size': 256}. Best is trial 0 with value: 0.17545781341882855.\n",
      "[I 2024-05-30 13:49:58,433] Trial 1 finished with value: 0.5473843425512314 and parameters: {'learning_rate': 0.03835060051676701, 'l1_coef': 0.07468179533000352, 'patience': 19, 'batch_size': 128}. Best is trial 0 with value: 0.17545781341882855.\n",
      "[I 2024-05-30 13:50:08,337] Trial 5 finished with value: 0.2898898899555206 and parameters: {'learning_rate': 0.0019321014636761611, 'l1_coef': 0.00029382463931867616, 'patience': 10, 'batch_size': 256}. Best is trial 0 with value: 0.17545781341882855.\n",
      "[I 2024-05-30 13:50:27,110] Trial 9 finished with value: 0.17811475172638894 and parameters: {'learning_rate': 0.052688902725254304, 'l1_coef': 5.202706255661094e-05, 'patience': 18, 'batch_size': 128}. Best is trial 0 with value: 0.17545781341882855.\n",
      "[I 2024-05-30 13:50:29,833] Trial 7 finished with value: 0.3187492206692696 and parameters: {'learning_rate': 0.0005318796351113497, 'l1_coef': 0.00039628766373440744, 'patience': 12, 'batch_size': 128}. Best is trial 0 with value: 0.17545781341882855.\n",
      "[I 2024-05-30 13:50:41,287] Trial 2 finished with value: 0.5501657210863554 and parameters: {'learning_rate': 0.0008916408477510999, 'l1_coef': 0.03001994634736173, 'patience': 16, 'batch_size': 32}. Best is trial 0 with value: 0.17545781341882855.\n",
      "[I 2024-05-30 13:50:45,721] Trial 3 finished with value: 0.31469905035836354 and parameters: {'learning_rate': 0.05667028944071789, 'l1_coef': 0.0003862815737428994, 'patience': 12, 'batch_size': 64}. Best is trial 0 with value: 0.17545781341882855.\n",
      "[I 2024-05-30 13:50:56,693] Trial 8 finished with value: 0.15761085152626036 and parameters: {'learning_rate': 0.08707712544870178, 'l1_coef': 3.1310975887339915e-05, 'patience': 15, 'batch_size': 32}. Best is trial 8 with value: 0.15761085152626036.\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 6 - Best hyperparameters: {'learning_rate': 0.08707712544870178, 'l1_coef': 3.1310975887339915e-05, 'patience': 15, 'batch_size': 32}\n",
      "Chr 6 - Best value: 0.1576\n",
      "Epoch [1/500], Train Loss: 21.1955\n",
      "Epoch [2/500], Train Loss: 14.4986\n",
      "Epoch [3/500], Train Loss: 13.1855\n",
      "Epoch [4/500], Train Loss: 12.6010\n",
      "Epoch [5/500], Train Loss: 9.6711\n",
      "Epoch [6/500], Train Loss: 8.9244\n",
      "Epoch [7/500], Train Loss: 8.4733\n",
      "Epoch [8/500], Train Loss: 9.2435\n",
      "Epoch [9/500], Train Loss: 10.3568\n",
      "Epoch [10/500], Train Loss: 9.0054\n",
      "Epoch [11/500], Train Loss: 8.3581\n",
      "Epoch [12/500], Train Loss: 8.6895\n",
      "Epoch [13/500], Train Loss: 8.5220\n",
      "Epoch [14/500], Train Loss: 8.4626\n",
      "Epoch [15/500], Train Loss: 7.5776\n",
      "Epoch [16/500], Train Loss: 9.1982\n",
      "Epoch [17/500], Train Loss: 8.2002\n",
      "Epoch [18/500], Train Loss: 7.6238\n",
      "Epoch [19/500], Train Loss: 6.3554\n",
      "Epoch [20/500], Train Loss: 5.7224\n",
      "Epoch [21/500], Train Loss: 5.9113\n",
      "Epoch [22/500], Train Loss: 6.9852\n",
      "Epoch [23/500], Train Loss: 7.4490\n",
      "Epoch [24/500], Train Loss: 6.5283\n",
      "Epoch [25/500], Train Loss: 4.9298\n",
      "Epoch [26/500], Train Loss: 4.7885\n",
      "Epoch [27/500], Train Loss: 5.0477\n",
      "Epoch [28/500], Train Loss: 5.1399\n",
      "Epoch [29/500], Train Loss: 4.5654\n",
      "Epoch [30/500], Train Loss: 5.6134\n",
      "Epoch [31/500], Train Loss: 5.3593\n",
      "Epoch [32/500], Train Loss: 4.4650\n",
      "Epoch [33/500], Train Loss: 3.8842\n",
      "Epoch [34/500], Train Loss: 2.2370\n",
      "Epoch [35/500], Train Loss: 1.9161\n",
      "Epoch [36/500], Train Loss: 1.8201\n",
      "Epoch [37/500], Train Loss: 2.8150\n",
      "Epoch [38/500], Train Loss: 3.1299\n",
      "Epoch [39/500], Train Loss: 3.1571\n",
      "Epoch [40/500], Train Loss: 3.1422\n",
      "Epoch [41/500], Train Loss: 2.3146\n",
      "Epoch [42/500], Train Loss: 2.4983\n",
      "Epoch [43/500], Train Loss: 2.1423\n",
      "Epoch [44/500], Train Loss: 1.9700\n",
      "Epoch [45/500], Train Loss: 1.8420\n",
      "Epoch [46/500], Train Loss: 1.7348\n",
      "Epoch [47/500], Train Loss: 1.4417\n",
      "Epoch [48/500], Train Loss: 1.3771\n",
      "Epoch [49/500], Train Loss: 1.3544\n",
      "Epoch [50/500], Train Loss: 1.3362\n",
      "Epoch [51/500], Train Loss: 1.3149\n",
      "Epoch [52/500], Train Loss: 1.2970\n",
      "Epoch [53/500], Train Loss: 1.2649\n",
      "Epoch [54/500], Train Loss: 1.2522\n",
      "Epoch [55/500], Train Loss: 1.2390\n",
      "Epoch [56/500], Train Loss: 1.2169\n",
      "Epoch [57/500], Train Loss: 1.2062\n",
      "Epoch [58/500], Train Loss: 1.1828\n",
      "Epoch [59/500], Train Loss: 1.1471\n",
      "Epoch [60/500], Train Loss: 1.0409\n",
      "Epoch [61/500], Train Loss: 0.9467\n",
      "Epoch [62/500], Train Loss: 0.8618\n",
      "Epoch [63/500], Train Loss: 0.7769\n",
      "Epoch [64/500], Train Loss: 0.6985\n",
      "Epoch [65/500], Train Loss: 0.5257\n",
      "Epoch [66/500], Train Loss: 0.3863\n",
      "Epoch [67/500], Train Loss: 0.3386\n",
      "Epoch [68/500], Train Loss: 0.3341\n",
      "Epoch [69/500], Train Loss: 0.3266\n",
      "Epoch [70/500], Train Loss: 0.3176\n",
      "Epoch [71/500], Train Loss: 0.3188\n",
      "Epoch [72/500], Train Loss: 0.3151\n",
      "Epoch [73/500], Train Loss: 0.3122\n",
      "Epoch [74/500], Train Loss: 0.3079\n",
      "Epoch [75/500], Train Loss: 0.3061\n",
      "Epoch [76/500], Train Loss: 0.3048\n",
      "Epoch [77/500], Train Loss: 0.2981\n",
      "Epoch [78/500], Train Loss: 0.2962\n",
      "Epoch [79/500], Train Loss: 0.2860\n",
      "Epoch [80/500], Train Loss: 0.2906\n",
      "Epoch [81/500], Train Loss: 0.2930\n",
      "Epoch [82/500], Train Loss: 0.2876\n",
      "Epoch [83/500], Train Loss: 0.2821\n",
      "Epoch [84/500], Train Loss: 0.2785\n",
      "Epoch [85/500], Train Loss: 0.2779\n",
      "Epoch [86/500], Train Loss: 0.2743\n",
      "Epoch [87/500], Train Loss: 0.2753\n",
      "Epoch [88/500], Train Loss: 0.2706\n",
      "Epoch [89/500], Train Loss: 0.2687\n",
      "Epoch [90/500], Train Loss: 0.2684\n",
      "Epoch [91/500], Train Loss: 0.2663\n",
      "Epoch [92/500], Train Loss: 0.2617\n",
      "Epoch [93/500], Train Loss: 0.2624\n",
      "Epoch [94/500], Train Loss: 0.2660\n",
      "Epoch [95/500], Train Loss: 0.2610\n",
      "Epoch [96/500], Train Loss: 0.2539\n",
      "Epoch [97/500], Train Loss: 0.2574\n",
      "Epoch [98/500], Train Loss: 0.2507\n",
      "Epoch [99/500], Train Loss: 0.2473\n",
      "Epoch [100/500], Train Loss: 0.2461\n",
      "Epoch [101/500], Train Loss: 0.2439\n",
      "Epoch [102/500], Train Loss: 0.2450\n",
      "Epoch [103/500], Train Loss: 0.2417\n",
      "Epoch [104/500], Train Loss: 0.2401\n",
      "Epoch [105/500], Train Loss: 0.2359\n",
      "Epoch [106/500], Train Loss: 0.2363\n",
      "Epoch [107/500], Train Loss: 0.2401\n",
      "Epoch [108/500], Train Loss: 0.2369\n",
      "Epoch [109/500], Train Loss: 0.2285\n",
      "Epoch [110/500], Train Loss: 0.2284\n",
      "Epoch [111/500], Train Loss: 0.2293\n",
      "Epoch [112/500], Train Loss: 0.2313\n",
      "Epoch [113/500], Train Loss: 0.2247\n",
      "Epoch [114/500], Train Loss: 0.2211\n",
      "Epoch [115/500], Train Loss: 0.2223\n",
      "Epoch [116/500], Train Loss: 0.2250\n",
      "Epoch [117/500], Train Loss: 0.2222\n",
      "Epoch [118/500], Train Loss: 0.2197\n",
      "Epoch [119/500], Train Loss: 0.2223\n",
      "Epoch [120/500], Train Loss: 0.2157\n",
      "Epoch [121/500], Train Loss: 0.2098\n",
      "Epoch [122/500], Train Loss: 0.2161\n",
      "Epoch [123/500], Train Loss: 0.2195\n",
      "Epoch [124/500], Train Loss: 0.2085\n",
      "Epoch [125/500], Train Loss: 0.2053\n",
      "Epoch [126/500], Train Loss: 0.2090\n",
      "Epoch [127/500], Train Loss: 0.2090\n",
      "Epoch [128/500], Train Loss: 0.2014\n",
      "Epoch [129/500], Train Loss: 0.2109\n",
      "Epoch [130/500], Train Loss: 0.2043\n",
      "Epoch [131/500], Train Loss: 0.2012\n",
      "Epoch [132/500], Train Loss: 0.1981\n",
      "Epoch [133/500], Train Loss: 0.1952\n",
      "Epoch [134/500], Train Loss: 0.1963\n",
      "Epoch [135/500], Train Loss: 0.1964\n",
      "Epoch [136/500], Train Loss: 0.1996\n",
      "Epoch [137/500], Train Loss: 0.1957\n",
      "Epoch [138/500], Train Loss: 0.1898\n",
      "Epoch [139/500], Train Loss: 0.1934\n",
      "Epoch [140/500], Train Loss: 0.1946\n",
      "Epoch [141/500], Train Loss: 0.1950\n",
      "Epoch [142/500], Train Loss: 0.1878\n",
      "Epoch [143/500], Train Loss: 0.1911\n",
      "Epoch [144/500], Train Loss: 0.1906\n",
      "Epoch [145/500], Train Loss: 0.1886\n",
      "Epoch [146/500], Train Loss: 0.1841\n",
      "Epoch [147/500], Train Loss: 0.1894\n",
      "Epoch [148/500], Train Loss: 0.1895\n",
      "Epoch [149/500], Train Loss: 0.1816\n",
      "Epoch [150/500], Train Loss: 0.1828\n",
      "Epoch [151/500], Train Loss: 0.1844\n",
      "Epoch [152/500], Train Loss: 0.1819\n",
      "Epoch [153/500], Train Loss: 0.1862\n",
      "Epoch [154/500], Train Loss: 0.1827\n",
      "Epoch [155/500], Train Loss: 0.1847\n",
      "Epoch [156/500], Train Loss: 0.1583\n",
      "Epoch [157/500], Train Loss: 0.1492\n",
      "Epoch [158/500], Train Loss: 0.1465\n",
      "Epoch [159/500], Train Loss: 0.1449\n",
      "Epoch [160/500], Train Loss: 0.1439\n",
      "Epoch [161/500], Train Loss: 0.1433\n",
      "Epoch [162/500], Train Loss: 0.1428\n",
      "Epoch [163/500], Train Loss: 0.1424\n",
      "Epoch [164/500], Train Loss: 0.1424\n",
      "Epoch [165/500], Train Loss: 0.1423\n",
      "Epoch [166/500], Train Loss: 0.1421\n",
      "Epoch [167/500], Train Loss: 0.1419\n",
      "Epoch [168/500], Train Loss: 0.1417\n",
      "Epoch [169/500], Train Loss: 0.1417\n",
      "Epoch [170/500], Train Loss: 0.1416\n",
      "Epoch [171/500], Train Loss: 0.1416\n",
      "Epoch [172/500], Train Loss: 0.1414\n",
      "Epoch [173/500], Train Loss: 0.1413\n",
      "Epoch [174/500], Train Loss: 0.1413\n",
      "Epoch [175/500], Train Loss: 0.1413\n",
      "Epoch [176/500], Train Loss: 0.1413\n",
      "Epoch [177/500], Train Loss: 0.1414\n",
      "Epoch [178/500], Train Loss: 0.1411\n",
      "Epoch [179/500], Train Loss: 0.1413\n",
      "Epoch [180/500], Train Loss: 0.1411\n",
      "Epoch [181/500], Train Loss: 0.1409\n",
      "Epoch [182/500], Train Loss: 0.1409\n",
      "Epoch [183/500], Train Loss: 0.1406\n",
      "Epoch [184/500], Train Loss: 0.1410\n",
      "Epoch [185/500], Train Loss: 0.1407\n",
      "Epoch [186/500], Train Loss: 0.1405\n",
      "Epoch [187/500], Train Loss: 0.1408\n",
      "Epoch [188/500], Train Loss: 0.1408\n",
      "Epoch [189/500], Train Loss: 0.1405\n",
      "Epoch [190/500], Train Loss: 0.1406\n",
      "Epoch [191/500], Train Loss: 0.1405\n",
      "Epoch [192/500], Train Loss: 0.1406\n",
      "Epoch [193/500], Train Loss: 0.1401\n",
      "Epoch [194/500], Train Loss: 0.1406\n",
      "Epoch [195/500], Train Loss: 0.1405\n",
      "Epoch [196/500], Train Loss: 0.1404\n",
      "Epoch [197/500], Train Loss: 0.1405\n",
      "Epoch [198/500], Train Loss: 0.1403\n",
      "Epoch [199/500], Train Loss: 0.1405\n",
      "Epoch [200/500], Train Loss: 0.1374\n",
      "Epoch [201/500], Train Loss: 0.1368\n",
      "Epoch [202/500], Train Loss: 0.1366\n",
      "Epoch [203/500], Train Loss: 0.1364\n",
      "Epoch [204/500], Train Loss: 0.1365\n",
      "Epoch [205/500], Train Loss: 0.1365\n",
      "Epoch [206/500], Train Loss: 0.1364\n",
      "Epoch [207/500], Train Loss: 0.1364\n",
      "Epoch [208/500], Train Loss: 0.1363\n",
      "Epoch [209/500], Train Loss: 0.1364\n",
      "Epoch [210/500], Train Loss: 0.1363\n",
      "Epoch [211/500], Train Loss: 0.1363\n",
      "Epoch [212/500], Train Loss: 0.1363\n",
      "Epoch [213/500], Train Loss: 0.1362\n",
      "Epoch [214/500], Train Loss: 0.1363\n",
      "Epoch [215/500], Train Loss: 0.1362\n",
      "Epoch [216/500], Train Loss: 0.1363\n",
      "Epoch [217/500], Train Loss: 0.1362\n",
      "Epoch [218/500], Train Loss: 0.1362\n",
      "Epoch [219/500], Train Loss: 0.1362\n",
      "Epoch [220/500], Train Loss: 0.1360\n",
      "Epoch [221/500], Train Loss: 0.1359\n",
      "Epoch [222/500], Train Loss: 0.1358\n",
      "Epoch [223/500], Train Loss: 0.1358\n",
      "Epoch [224/500], Train Loss: 0.1357\n",
      "Epoch [225/500], Train Loss: 0.1358\n",
      "Epoch [226/500], Train Loss: 0.1358\n",
      "Epoch [227/500], Train Loss: 0.1358\n",
      "Epoch [228/500], Train Loss: 0.1358\n",
      "Epoch [229/500], Train Loss: 0.1358\n",
      "Epoch [230/500], Train Loss: 0.1358\n",
      "Epoch [231/500], Train Loss: 0.1357\n",
      "Epoch [232/500], Train Loss: 0.1358\n",
      "Epoch [233/500], Train Loss: 0.1358\n",
      "Epoch [234/500], Train Loss: 0.1357\n",
      "Epoch [235/500], Train Loss: 0.1357\n",
      "Epoch [236/500], Train Loss: 0.1357\n",
      "Epoch [237/500], Train Loss: 0.1357\n",
      "Epoch [238/500], Train Loss: 0.1358\n",
      "Epoch [239/500], Train Loss: 0.1358\n",
      "Epoch [240/500], Train Loss: 0.1357\n",
      "Epoch [241/500], Train Loss: 0.1358\n",
      "Epoch [242/500], Train Loss: 0.1358\n",
      "Epoch [243/500], Train Loss: 0.1358\n",
      "Epoch [244/500], Train Loss: 0.1358\n",
      "Epoch [245/500], Train Loss: 0.1358\n",
      "Epoch [246/500], Train Loss: 0.1357\n",
      "Early stopping at epoch 246\n",
      "Final model saved at: ../../../Data/model_results/logistic_regression/models/chr6/final_model_chr6.pth\n",
      "Individual R^2 scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr6/individual_r2_scores_chr6.csv\n",
      "Individual IQS scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr6/individual_iqs_scores_chr6.csv\n",
      "Individual AUC ROC curves saved in: ../../../Data/model_results/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../../Data/model_results/logistic_regression/csv_files/performance_metrics.csv\n",
      "Total SNPs:  1476\n",
      "PRS313 SNPs:  28\n",
      "Total SNPs used for Training:  1448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-30 13:51:12,199] A new study created in RDB with name: chr7_study\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2024-05-30 13:52:00,389] Trial 1 finished with value: 0.45895434767007826 and parameters: {'learning_rate': 0.06789925410251739, 'l1_coef': 0.003303112958291555, 'patience': 10, 'batch_size': 128}. Best is trial 1 with value: 0.45895434767007826.\n",
      "[I 2024-05-30 13:52:06,570] Trial 7 finished with value: 4.659018802642822 and parameters: {'learning_rate': 0.07885376895907296, 'l1_coef': 2.875412588146054e-05, 'patience': 20, 'batch_size': 64}. Best is trial 1 with value: 0.45895434767007826.\n",
      "[I 2024-05-30 13:52:10,194] Trial 3 finished with value: 0.12991864010691642 and parameters: {'learning_rate': 0.019064635865524475, 'l1_coef': 5.3120990680199706e-05, 'patience': 15, 'batch_size': 128}. Best is trial 3 with value: 0.12991864010691642.\n",
      "[I 2024-05-30 13:52:11,318] Trial 4 finished with value: 0.5769830822944642 and parameters: {'learning_rate': 0.0016518075391926082, 'l1_coef': 0.03236144996571162, 'patience': 6, 'batch_size': 256}. Best is trial 3 with value: 0.12991864010691642.\n",
      "[I 2024-05-30 13:52:12,908] Trial 2 finished with value: 0.5042693212628364 and parameters: {'learning_rate': 0.0014057288055090195, 'l1_coef': 0.005488018010169867, 'patience': 5, 'batch_size': 128}. Best is trial 3 with value: 0.12991864010691642.\n",
      "[I 2024-05-30 13:52:35,708] Trial 0 finished with value: 0.3688818437712534 and parameters: {'learning_rate': 0.0027571368698515765, 'l1_coef': 0.001482723572464706, 'patience': 18, 'batch_size': 64}. Best is trial 3 with value: 0.12991864010691642.\n",
      "[I 2024-05-30 13:52:37,210] Trial 6 finished with value: 0.4868455320596695 and parameters: {'learning_rate': 0.00046183541291497084, 'l1_coef': 0.004595163686107394, 'patience': 18, 'batch_size': 128}. Best is trial 3 with value: 0.12991864010691642.\n",
      "[I 2024-05-30 13:52:43,041] Trial 5 finished with value: 0.5026808894597568 and parameters: {'learning_rate': 0.0015265172966092639, 'l1_coef': 0.04468102270265237, 'patience': 16, 'batch_size': 32}. Best is trial 3 with value: 0.12991864010691642.\n",
      "[I 2024-05-30 13:53:10,536] Trial 9 finished with value: 0.12938632162717673 and parameters: {'learning_rate': 0.04237722490218332, 'l1_coef': 5.949874007250977e-05, 'patience': 8, 'batch_size': 32}. Best is trial 9 with value: 0.12938632162717673.\n",
      "[I 2024-05-30 13:53:13,119] Trial 8 finished with value: 0.17969528207412128 and parameters: {'learning_rate': 0.0008386742019317988, 'l1_coef': 0.00016179597284842913, 'patience': 7, 'batch_size': 32}. Best is trial 9 with value: 0.12938632162717673.\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr 7 - Best hyperparameters: {'learning_rate': 0.04237722490218332, 'l1_coef': 5.949874007250977e-05, 'patience': 8, 'batch_size': 32}\n",
      "Chr 7 - Best value: 0.1294\n",
      "Epoch [1/500], Train Loss: 0.9855\n",
      "Epoch [2/500], Train Loss: 0.3288\n",
      "Epoch [3/500], Train Loss: 0.2752\n",
      "Epoch [4/500], Train Loss: 0.2819\n",
      "Epoch [5/500], Train Loss: 0.2769\n",
      "Epoch [6/500], Train Loss: 0.2739\n",
      "Epoch [7/500], Train Loss: 0.2853\n",
      "Epoch [8/500], Train Loss: 0.2996\n",
      "Epoch [9/500], Train Loss: 0.2838\n",
      "Epoch [10/500], Train Loss: 0.2973\n",
      "Epoch [11/500], Train Loss: 0.3025\n",
      "Epoch [12/500], Train Loss: 0.2958\n",
      "Epoch [13/500], Train Loss: 0.2171\n",
      "Epoch [14/500], Train Loss: 0.1765\n",
      "Epoch [15/500], Train Loss: 0.1625\n",
      "Epoch [16/500], Train Loss: 0.1547\n",
      "Epoch [17/500], Train Loss: 0.1507\n",
      "Epoch [18/500], Train Loss: 0.1475\n",
      "Epoch [19/500], Train Loss: 0.1453\n",
      "Epoch [20/500], Train Loss: 0.1438\n",
      "Epoch [21/500], Train Loss: 0.1431\n",
      "Epoch [22/500], Train Loss: 0.1424\n",
      "Epoch [23/500], Train Loss: 0.1418\n",
      "Epoch [24/500], Train Loss: 0.1403\n",
      "Epoch [25/500], Train Loss: 0.1404\n",
      "Epoch [26/500], Train Loss: 0.1391\n",
      "Epoch [27/500], Train Loss: 0.1397\n",
      "Epoch [28/500], Train Loss: 0.1394\n",
      "Epoch [29/500], Train Loss: 0.1393\n",
      "Epoch [30/500], Train Loss: 0.1393\n",
      "Epoch [31/500], Train Loss: 0.1382\n",
      "Epoch [32/500], Train Loss: 0.1387\n",
      "Epoch [33/500], Train Loss: 0.1378\n",
      "Epoch [34/500], Train Loss: 0.1377\n",
      "Epoch [35/500], Train Loss: 0.1378\n",
      "Epoch [36/500], Train Loss: 0.1390\n",
      "Epoch [37/500], Train Loss: 0.1382\n",
      "Epoch [38/500], Train Loss: 0.1384\n",
      "Epoch [39/500], Train Loss: 0.1381\n",
      "Epoch [40/500], Train Loss: 0.1389\n",
      "Epoch [41/500], Train Loss: 0.1283\n",
      "Epoch [42/500], Train Loss: 0.1253\n",
      "Epoch [43/500], Train Loss: 0.1244\n",
      "Epoch [44/500], Train Loss: 0.1243\n",
      "Epoch [45/500], Train Loss: 0.1238\n",
      "Epoch [46/500], Train Loss: 0.1238\n",
      "Epoch [47/500], Train Loss: 0.1236\n",
      "Epoch [48/500], Train Loss: 0.1236\n",
      "Epoch [49/500], Train Loss: 0.1237\n",
      "Epoch [50/500], Train Loss: 0.1235\n",
      "Epoch [51/500], Train Loss: 0.1235\n",
      "Epoch [52/500], Train Loss: 0.1234\n",
      "Epoch [53/500], Train Loss: 0.1232\n",
      "Epoch [54/500], Train Loss: 0.1233\n",
      "Epoch [55/500], Train Loss: 0.1232\n",
      "Epoch [56/500], Train Loss: 0.1233\n",
      "Epoch [57/500], Train Loss: 0.1231\n",
      "Epoch [58/500], Train Loss: 0.1232\n",
      "Epoch [59/500], Train Loss: 0.1231\n",
      "Epoch [60/500], Train Loss: 0.1231\n",
      "Epoch [61/500], Train Loss: 0.1232\n",
      "Epoch [62/500], Train Loss: 0.1232\n",
      "Epoch [63/500], Train Loss: 0.1232\n",
      "Epoch [64/500], Train Loss: 0.1232\n",
      "Epoch [65/500], Train Loss: 0.1230\n",
      "Epoch [66/500], Train Loss: 0.1232\n",
      "Epoch [67/500], Train Loss: 0.1230\n",
      "Epoch [68/500], Train Loss: 0.1230\n",
      "Epoch [69/500], Train Loss: 0.1230\n",
      "Epoch [70/500], Train Loss: 0.1230\n",
      "Epoch [71/500], Train Loss: 0.1229\n",
      "Epoch [72/500], Train Loss: 0.1228\n",
      "Epoch [73/500], Train Loss: 0.1231\n",
      "Epoch [74/500], Train Loss: 0.1230\n",
      "Epoch [75/500], Train Loss: 0.1229\n",
      "Epoch [76/500], Train Loss: 0.1228\n",
      "Epoch [77/500], Train Loss: 0.1230\n",
      "Epoch [78/500], Train Loss: 0.1229\n",
      "Epoch [79/500], Train Loss: 0.1231\n",
      "Epoch [80/500], Train Loss: 0.1228\n",
      "Epoch [81/500], Train Loss: 0.1228\n",
      "Epoch [82/500], Train Loss: 0.1229\n",
      "Epoch [83/500], Train Loss: 0.1229\n",
      "Epoch [84/500], Train Loss: 0.1227\n",
      "Epoch [85/500], Train Loss: 0.1228\n",
      "Epoch [86/500], Train Loss: 0.1228\n",
      "Epoch [87/500], Train Loss: 0.1229\n",
      "Epoch [88/500], Train Loss: 0.1228\n",
      "Epoch [89/500], Train Loss: 0.1227\n",
      "Epoch [90/500], Train Loss: 0.1227\n",
      "Epoch [91/500], Train Loss: 0.1215\n",
      "Epoch [92/500], Train Loss: 0.1211\n",
      "Epoch [93/500], Train Loss: 0.1209\n",
      "Epoch [94/500], Train Loss: 0.1209\n",
      "Epoch [95/500], Train Loss: 0.1210\n",
      "Epoch [96/500], Train Loss: 0.1208\n",
      "Epoch [97/500], Train Loss: 0.1209\n",
      "Epoch [98/500], Train Loss: 0.1209\n",
      "Epoch [99/500], Train Loss: 0.1208\n",
      "Epoch [100/500], Train Loss: 0.1208\n",
      "Epoch [101/500], Train Loss: 0.1209\n",
      "Epoch [102/500], Train Loss: 0.1208\n",
      "Epoch [103/500], Train Loss: 0.1208\n",
      "Epoch [104/500], Train Loss: 0.1209\n",
      "Epoch [105/500], Train Loss: 0.1208\n",
      "Epoch [106/500], Train Loss: 0.1206\n",
      "Epoch [107/500], Train Loss: 0.1207\n",
      "Epoch [108/500], Train Loss: 0.1206\n",
      "Epoch [109/500], Train Loss: 0.1206\n",
      "Epoch [110/500], Train Loss: 0.1207\n",
      "Epoch [111/500], Train Loss: 0.1207\n",
      "Epoch [112/500], Train Loss: 0.1206\n",
      "Epoch [113/500], Train Loss: 0.1207\n",
      "Epoch [114/500], Train Loss: 0.1205\n",
      "Epoch [115/500], Train Loss: 0.1206\n",
      "Epoch [116/500], Train Loss: 0.1207\n",
      "Epoch [117/500], Train Loss: 0.1207\n",
      "Epoch [118/500], Train Loss: 0.1206\n",
      "Epoch [119/500], Train Loss: 0.1207\n",
      "Epoch [120/500], Train Loss: 0.1207\n",
      "Epoch [121/500], Train Loss: 0.1206\n",
      "Epoch [122/500], Train Loss: 0.1207\n",
      "Early stopping at epoch 122\n",
      "Final model saved at: ../../../Data/model_results/logistic_regression/models/chr7/final_model_chr7.pth\n",
      "Individual R^2 scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr7/individual_r2_scores_chr7.csv\n",
      "Individual IQS scores saved at: ../../../Data/model_results/logistic_regression/csv_files/chr7/individual_iqs_scores_chr7.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-30 13:53:19,970] A new study created in RDB with name: chr8_study\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual AUC ROC curves saved in: ../../../Data/model_results/logistic_regression/roc_curves/\n",
      "Performance metrics saved at: ../../../Data/model_results/logistic_regression/csv_files/performance_metrics.csv\n",
      "Total SNPs:  1614\n",
      "PRS313 SNPs:  42\n",
      "Total SNPs used for Training:  1572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/Users/gea2/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-05-30 13:53:41,452] Trial 8 finished with value: 17.74535541534424 and parameters: {'learning_rate': 0.0627810549564541, 'l1_coef': 1.369331145091169e-05, 'patience': 13, 'batch_size': 256}. Best is trial 8 with value: 17.74535541534424.\n",
      "[I 2024-05-30 13:54:06,393] Trial 4 finished with value: 0.5366251051425934 and parameters: {'learning_rate': 0.08806299222279251, 'l1_coef': 0.008663605059545164, 'patience': 13, 'batch_size': 256}. Best is trial 4 with value: 0.5366251051425934.\n",
      "[I 2024-05-30 13:54:08,626] Trial 6 finished with value: 0.5366137862205506 and parameters: {'learning_rate': 0.058834991339273834, 'l1_coef': 0.013411984672026773, 'patience': 13, 'batch_size': 256}. Best is trial 6 with value: 0.5366137862205506.\n",
      "[I 2024-05-30 13:54:13,646] Trial 3 finished with value: 0.536030201613903 and parameters: {'learning_rate': 0.09166841542245097, 'l1_coef': 0.02900524354645823, 'patience': 9, 'batch_size': 128}. Best is trial 3 with value: 0.536030201613903.\n",
      "[I 2024-05-30 13:54:26,054] Trial 5 finished with value: 0.5361868543284279 and parameters: {'learning_rate': 0.016212379049962405, 'l1_coef': 0.01516964603777673, 'patience': 14, 'batch_size': 64}. Best is trial 3 with value: 0.536030201613903.\n",
      "[I 2024-05-30 13:54:34,127] Trial 1 finished with value: 0.5381692379713059 and parameters: {'learning_rate': 0.0004280701556859422, 'l1_coef': 0.0048431188325918405, 'patience': 6, 'batch_size': 128}. Best is trial 3 with value: 0.536030201613903.\n",
      "[I 2024-05-30 13:54:39,137] Trial 7 finished with value: 0.2656093148084787 and parameters: {'learning_rate': 0.0017804515149529162, 'l1_coef': 0.00029396409163183745, 'patience': 5, 'batch_size': 32}. Best is trial 7 with value: 0.2656093148084787.\n",
      "[I 2024-05-30 13:54:39,580] Trial 0 finished with value: 0.1952469066931651 and parameters: {'learning_rate': 0.050667312255680816, 'l1_coef': 0.00012844467863483436, 'patience': 10, 'batch_size': 32}. Best is trial 0 with value: 0.1952469066931651.\n",
      "[I 2024-05-30 13:54:44,193] Trial 9 finished with value: 0.5366821825504303 and parameters: {'learning_rate': 0.0010744491622470178, 'l1_coef': 0.05501513480536039, 'patience': 13, 'batch_size': 32}. Best is trial 0 with value: 0.1952469066931651.\n",
      "[I 2024-05-30 13:54:51,581] Trial 2 finished with value: 0.35245607495307923 and parameters: {'learning_rate': 0.000417319651095719, 'l1_coef': 0.0007141089227091424, 'patience': 6, 'batch_size': 32}. Best is trial 0 with value: 0.1952469066931651.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "from sklearn.metrics import r2_score as sklearn_r2_score\n",
    "import optuna\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# Load and preprocess the data\n",
    "data_directory = '../../../Data/Filtered_split_training_data/'\n",
    "start = 6\n",
    "\n",
    "# Initialize lists to store the performance metrics for each chromosome\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "false_positive_rates = []\n",
    "auc_rocs = []\n",
    "r2_scores = []\n",
    "iqs_scores = []\n",
    "\n",
    "# Create folders for saving files\n",
    "output_folder = \"../../../Data/model_results/logistic_regression/\"\n",
    "model_folder = output_folder + \"models/\"\n",
    "csv_folder = output_folder + \"csv_files/\"\n",
    "curve_folder = output_folder + \"roc_curves/\"\n",
    "\n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "os.makedirs(csv_folder, exist_ok=True)\n",
    "os.makedirs(curve_folder, exist_ok=True)\n",
    "\n",
    "for chromosome_number in range(start, 23):\n",
    "    # Create subfolders for the current chromosome\n",
    "    chr_model_folder = model_folder + f\"chr{chromosome_number}/\"\n",
    "    chr_csv_folder = csv_folder + f\"chr{chromosome_number}/\"\n",
    "    chr_curve_folder = curve_folder + f\"chr{chromosome_number}/\"\n",
    "\n",
    "    os.makedirs(chr_model_folder, exist_ok=True)\n",
    "    os.makedirs(chr_csv_folder, exist_ok=True)\n",
    "    os.makedirs(chr_curve_folder, exist_ok=True)\n",
    "\n",
    "    file_name = data_directory + f\"23AndMe_PRS313_merged_chr{chromosome_number}_matching_split.parquet\"\n",
    "    data = pd.read_parquet(file_name)\n",
    "\n",
    "\n",
    "    # # Split the data into features and target\n",
    "    # X = torch.tensor(data.filter(regex='^(?!.*Unknown)').values, dtype=torch.float32)\n",
    "    # y = torch.tensor(data.filter(regex='Unknown').values, dtype=torch.float32)\n",
    "\n",
    "    # print(\"Unknown PRS313 SNPs: \", y.shape[1])\n",
    "    # print(\"Known PRS313 SNPs: \", data[[col for col in data.columns if \"PRS313_Known\" in col]].shape[1])\n",
    "    # print(\"23AndMe SNPs with LD to Unknown PRS313 SNPs: \", data[[col for col in data.columns if \"PRS313_\" not in col]].shape[1])\n",
    "    # print(\"Total SNPs used for Training: \", X.shape[1])\n",
    "\n",
    "    # Split the data into features and target\n",
    "    X = torch.tensor(data.filter(regex='^(?!.*PRS313_)').values, dtype=torch.float32)\n",
    "    y = torch.tensor(data.filter(regex='PRS313_').values, dtype=torch.float32)\n",
    "\n",
    "    print(\"Total SNPs: \", data.shape[1])\n",
    "    print(\"PRS313 SNPs: \", y.shape[1])\n",
    "    print(\"Total SNPs used for Training: \", X.shape[1])\n",
    "\n",
    "\n",
    "    # Split the data into train-validation and test sets\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Define the logistic regression model with lasso regularization\n",
    "    class LogisticRegression(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim, l1_coef=0.0):\n",
    "            super(LogisticRegression, self).__init__()\n",
    "            self.linear = nn.Linear(input_dim, output_dim)\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "            self.l1_coef = l1_coef\n",
    "\n",
    "        def forward(self, x):\n",
    "            out = self.linear(x)\n",
    "            out = self.sigmoid(out)\n",
    "            return out\n",
    "\n",
    "        def l1_loss(self):\n",
    "            return self.l1_coef * torch.norm(self.linear.weight, p=1)\n",
    "        \n",
    "    # Set the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Set the hyperparameters for tuning\n",
    "    input_dim = X_train_val.shape[1]\n",
    "    output_dim = y_train_val.shape[1]\n",
    "    num_epochs = 500\n",
    "    batch_size = 128\n",
    "\n",
    "    # Define the objective function for Optuna with cross-validation and early stopping\n",
    "    def objective(trial):\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-1, log=True)\n",
    "        l1_coef = trial.suggest_float('l1_coef', 1e-5, 1e-1, log=True)\n",
    "        patience = trial.suggest_int('patience', 5, 20)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "\n",
    "        model = LogisticRegression(input_dim, output_dim, l1_coef).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        criterion = nn.BCELoss()\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=False)\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        fold_losses = []\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_val, y_train_val.argmax(dim=1))):\n",
    "            X_train, X_val = X_train_val[train_idx], X_train_val[val_idx]\n",
    "            y_train, y_val = y_train_val[train_idx], y_train_val[val_idx]\n",
    "\n",
    "            train_dataset = TensorDataset(X_train, y_train)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "            best_val_loss = float('inf')\n",
    "            counter = 0\n",
    "\n",
    "            for epoch in range(num_epochs):\n",
    "                train_loss = 0.0\n",
    "                for batch_X, batch_y in train_loader:\n",
    "                    batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "                    outputs = model(batch_X)\n",
    "                    loss = criterion(outputs, batch_y) + model.l1_loss()\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    train_loss += loss.item()\n",
    "\n",
    "                train_loss /= len(train_loader)\n",
    "\n",
    "                val_dataset = TensorDataset(X_val, y_val)\n",
    "                val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    val_loss = 0.0\n",
    "                    for batch_X, batch_y in val_loader:\n",
    "                        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                        outputs = model(batch_X)\n",
    "                        loss = criterion(outputs, batch_y) + model.l1_loss()\n",
    "                        val_loss += loss.item()\n",
    "\n",
    "                    val_loss /= len(val_loader)\n",
    "                    scheduler.step(val_loss)\n",
    "\n",
    "                    if val_loss < best_val_loss:\n",
    "                        best_val_loss = val_loss\n",
    "                        counter = 0\n",
    "                    else:\n",
    "                        counter += 1\n",
    "\n",
    "                    if counter >= patience:\n",
    "                        # print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                        break\n",
    "\n",
    "            fold_losses.append(best_val_loss)\n",
    "\n",
    "        return np.mean(fold_losses)\n",
    "\n",
    "    # Create the \"optuna_studies\" folder if it doesn't exist\n",
    "    os.makedirs(\"optuna_studies\", exist_ok=True)\n",
    "\n",
    "    # Create an Optuna study and optimize the hyperparameters\n",
    "    study_name = f\"chr{chromosome_number}_study\"\n",
    "    storage_name = f\"sqlite:///optuna_studies/{study_name}.db\"\n",
    "\n",
    "    # Check if the study exists\n",
    "\n",
    "    current_dir = os.getcwd()\n",
    "    study_exists = os.path.exists(current_dir + f\"/optuna_studies/{study_name}.db\")\n",
    "    \n",
    "    if study_exists:\n",
    "        # Load the existing study\n",
    "        study = optuna.load_study(study_name=study_name, storage=storage_name)\n",
    "    else:\n",
    "        # Create a new study\n",
    "        study = optuna.create_study(direction='minimize', study_name=study_name, storage=storage_name)\n",
    "\n",
    "    study.optimize(objective, n_trials=10, n_jobs=-1)\n",
    "\n",
    "    # Print the best hyperparameters and best value\n",
    "    print(f\"Chr {chromosome_number} - Best hyperparameters: {study.best_params}\")\n",
    "    print(f\"Chr {chromosome_number} - Best value: {study.best_value:.4f}\")\n",
    "\n",
    "    # Train the final model with the best hyperparameters and early stopping\n",
    "    best_learning_rate = study.best_params['learning_rate']\n",
    "    best_l1_coef = study.best_params['l1_coef']\n",
    "    best_patience = study.best_params['patience']\n",
    "    best_batch_size = study.best_params['batch_size']\n",
    "\n",
    "    model = LogisticRegression(input_dim, output_dim, best_l1_coef).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_learning_rate)\n",
    "    criterion = nn.BCELoss()\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=False)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_val, y_train_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=best_batch_size, shuffle=True)\n",
    "\n",
    "    best_train_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y) + model.l1_loss()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "        if train_loss < best_train_loss:\n",
    "            best_train_loss = train_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        if counter >= best_patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "        scheduler.step(train_loss)\n",
    "\n",
    "    # Save the final model\n",
    "    model_save_path = chr_model_folder + f'final_model_chr{chromosome_number}.pth'\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"Final model saved at: {model_save_path}\")\n",
    "\n",
    "    # Evaluate the final model on the test set\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test.to(device))\n",
    "        test_preds = (test_outputs > 0.5).float()\n",
    "        test_accuracy = float(((test_preds > 0.5) == y_test).float().mean())\n",
    "        test_precision = precision_score(y_test.cpu().numpy(), test_preds.cpu().numpy(), average='micro')\n",
    "        test_recall = recall_score(y_test.cpu().numpy(), test_preds.cpu().numpy(), average='micro')\n",
    "        test_f1 = f1_score(y_test.cpu().numpy(), test_preds.cpu().numpy(), average='micro')\n",
    "        test_roc_auc = roc_auc_score(y_test.cpu().numpy(), test_outputs.cpu().numpy(), average='micro')\n",
    "        test_r2 = sklearn_r2_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "        test_iqs = calculate_iqs(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "\n",
    "        # Calculate false positive rate\n",
    "        cm = confusion_matrix(y_test.cpu().numpy().ravel(), test_preds.cpu().numpy().ravel())\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        test_fpr = fp / (fp + tn)\n",
    "\n",
    "        # Append performance metrics to the lists\n",
    "        accuracies.append(test_accuracy)\n",
    "        precisions.append(test_precision)\n",
    "        recalls.append(test_recall)\n",
    "        false_positive_rates.append(test_fpr)\n",
    "        auc_rocs.append(test_roc_auc)\n",
    "        r2_scores.append(test_r2)\n",
    "        iqs_scores.append(test_iqs)\n",
    "\n",
    "        # Calculate individual R^2 scores for each SNP\n",
    "        individual_r2_scores = sklearn_r2_score(y_test.cpu().numpy(), test_outputs.cpu().numpy(), multioutput='raw_values')\n",
    "\n",
    "        # Calculate individual IQS scores for each SNP\n",
    "        individual_iqs_scores = np.array([calculate_iqs(y_test.cpu().numpy()[:, i].reshape(-1, 1), test_outputs.cpu().numpy()[:, i].reshape(-1, 1)) for i in range(y_test.shape[1])])\n",
    "\n",
    "        # Get the names of the SNPs from the original dataframe\n",
    "        snp_names = data.filter(regex='Unknown').columns\n",
    "\n",
    "        # Save individual R^2 scores to a CSV file\n",
    "        csv_file = chr_csv_folder + f'individual_r2_scores_chr{chromosome_number}.csv'\n",
    "\n",
    "        with open(csv_file, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['SNP', 'R2 Score'])\n",
    "            for snp, r2_score in zip(snp_names, individual_r2_scores):\n",
    "                writer.writerow([snp, r2_score])\n",
    "\n",
    "        print(f\"Individual R^2 scores saved at: {csv_file}\")\n",
    "\n",
    "        # Save individual IQS scores to a CSV file\n",
    "        iqs_csv_file = chr_csv_folder + f'individual_iqs_scores_chr{chromosome_number}.csv'\n",
    "\n",
    "        with open(iqs_csv_file, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['SNP', 'IQS Score'])\n",
    "            for snp, iqs_score in zip(snp_names, individual_iqs_scores):\n",
    "                writer.writerow([snp, iqs_score])\n",
    "\n",
    "        print(f\"Individual IQS scores saved at: {iqs_csv_file}\")\n",
    "\n",
    "        # Save individual AUC ROC curves for each SNP\n",
    "        for i, snp in enumerate(snp_names):\n",
    "            try: \n",
    "                fpr, tpr, _ = roc_curve(y_test.cpu().numpy()[:, i], test_outputs.cpu().numpy()[:, i])\n",
    "                plt.figure()\n",
    "                plt.plot(fpr, tpr, label=f'AUC ROC = {roc_auc_score(y_test.cpu().numpy()[:, i], test_outputs.cpu().numpy()[:, i]):.4f}')\n",
    "                plt.xlabel('False Positive Rate')\n",
    "                plt.ylabel('True Positive Rate')\n",
    "                plt.title(f'AUC ROC Curve - {snp}')\n",
    "                plt.legend()\n",
    "                \n",
    "                curve_file = chr_curve_folder + f'auc_roc_curve_{snp}_chr{chromosome_number}.png'\n",
    "                plt.savefig(curve_file)\n",
    "                plt.close()\n",
    "            except ValueError:\n",
    "                # Save a placeholder image if there is insufficient data\n",
    "                plt.figure()\n",
    "                plt.axis('off')\n",
    "                plt.text(0.5, 0.5, \"Insufficient data for ROC curve\", ha='center', va='center')\n",
    "                curve_file = chr_curve_folder + f'auc_roc_curve_{snp}_chr{chrom0osome_number}.png'\n",
    "                plt.savefig(curve_file)\n",
    "                plt.close()\n",
    "\n",
    "                print(f\"Skipping SNP {snp} due to insufficient data\")\n",
    "\n",
    "\n",
    "        print(f\"Individual AUC ROC curves saved in: {curve_folder}\")\n",
    "\n",
    "        # Create a DataFrame to store the performance metrics for each chromosome\n",
    "        performance_df = pd.DataFrame({\n",
    "            'Chromosome': list(range(start, chromosome_number + 1)),\n",
    "            'Accuracy': accuracies,\n",
    "            'Precision': precisions,\n",
    "            'Recall': recalls,\n",
    "            'False Positive Rate': false_positive_rates,\n",
    "            'AUC ROC': auc_rocs,\n",
    "            'R2 Score': r2_scores,\n",
    "            'IQS Score': iqs_scores\n",
    "        })\n",
    "\n",
    "        # Save the performance metrics to a CSV file\n",
    "        performance_csv_file = csv_folder + 'performance_metrics.csv'\n",
    "        performance_df.to_csv(performance_csv_file, index=False)\n",
    "        print(f\"Performance metrics saved at: {performance_csv_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
