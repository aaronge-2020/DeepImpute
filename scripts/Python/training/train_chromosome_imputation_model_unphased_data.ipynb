{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IQS Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_iqs_unphased(true_genotypes, imputed_dosages):\n",
    "    # Convert imputed dosages to discrete values\n",
    "    imputed_discrete = np.round(imputed_dosages).astype(int)\n",
    "\n",
    "    # Clip the imputed discrete values to be within the range of 0 to 2\n",
    "    imputed_discrete = np.clip(imputed_discrete, 0, 2)\n",
    "\n",
    "    # Create a contingency table\n",
    "    contingency_table = np.zeros((3, 3), dtype=int)\n",
    "\n",
    "    # Fill the contingency table\n",
    "    for true_geno, imputed_geno in zip(true_genotypes, imputed_discrete):\n",
    "        for true_allele, imputed_allele in zip(true_geno, imputed_geno):\n",
    "            contingency_table[int(true_allele), int(imputed_allele)] += 1\n",
    "\n",
    "    # Calculate the total number of genotypes\n",
    "    total_genotypes = np.sum(contingency_table)\n",
    "\n",
    "    # Calculate observed proportion of agreement (Po)\n",
    "    observed_agreement = np.trace(contingency_table) / total_genotypes\n",
    "\n",
    "    # Calculate marginal sums\n",
    "    row_marginals = np.sum(contingency_table, axis=1)\n",
    "    col_marginals = np.sum(contingency_table, axis=0)\n",
    "\n",
    "    # Calculate chance agreement (Pc)\n",
    "    chance_agreement = np.sum((row_marginals * col_marginals) / (total_genotypes ** 2))\n",
    "\n",
    "    # Calculate IQS\n",
    "    if chance_agreement == 1:  # To prevent division by zero in case of perfect chance agreement\n",
    "        iqs_score = 0\n",
    "    else:\n",
    "        iqs_score = (observed_agreement - chance_agreement) / (1 - chance_agreement)\n",
    "\n",
    "    return iqs_score\n",
    "\n",
    "# Example usage:\n",
    "true_genotypes = np.array([[0, 1, 2], [1, 2, 0], [2, 0, 1]])\n",
    "imputed_dosages = np.array([[0.1, 1.2, 1.9], [1.0, 1.8, 0.3], [2.0, 0.5, 1.4]])\n",
    "\n",
    "iqs_score = calculate_iqs_unphased(true_genotypes, imputed_dosages)\n",
    "print(f\"IQS Score: {iqs_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'optuna'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader, TensorDataset\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split, KFold\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01moptuna\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'optuna'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import optuna\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "\n",
    "def calculate_pearson_r_squared(true_genotypes, imputed_genotypes):\n",
    "    \"\"\"\n",
    "    Calculate Pearson's r² for each SNP separately and return the median.\n",
    "    \n",
    "    Args:\n",
    "        true_genotypes: numpy array of shape (n_samples, n_snps) with true genotypes (0,1,2)\n",
    "        imputed_genotypes: numpy array of shape (n_samples, n_snps) with imputed genotypes (continuous values)\n",
    "    \n",
    "    Returns:\n",
    "        float: Median Pearson's r² across all SNPs, avoiding NaN values\n",
    "    \"\"\"\n",
    "    from scipy.stats import pearsonr\n",
    "    import numpy as np\n",
    "    \n",
    "    true_genotypes = np.array(true_genotypes)\n",
    "    imputed_genotypes = np.array(imputed_genotypes)  # Not rounding for correlation calculation\n",
    "\n",
    "    num_snps = true_genotypes.shape[1]\n",
    "    r_squared_scores = []\n",
    "\n",
    "    for i in range(num_snps):\n",
    "        true_col = true_genotypes[:, i]\n",
    "        imputed_col = imputed_genotypes[:, i]\n",
    "\n",
    "        # Ensure both arrays have at least two unique values\n",
    "        if len(np.unique(true_col)) > 1 and len(np.unique(imputed_col)) > 1:\n",
    "            # Calculate Pearson's r and square it to get r²\n",
    "            r, _ = pearsonr(true_col, imputed_col)\n",
    "            r_squared = r**2\n",
    "            r_squared_scores.append(r_squared)\n",
    "\n",
    "    # median_score = np.median(r_squared_scores) if r_squared_scores else 0.0  # Return 0.0 if all SNPs were skipped\n",
    "    return r_squared_scores\n",
    "\n",
    "\n",
    "# Load and preprocess the data\n",
    "data_directory = '../../../Data/Filtered_unphased_training_data_union_final/'\n",
    "start = 8\n",
    "\n",
    "# Initialize lists to store the performance metrics for each chromosome\n",
    "r2_scores = []\n",
    "iqs_scores = []\n",
    "accuracy_scores = []\n",
    "\n",
    "# Create folders for saving files\n",
    "output_folder = \"../../../Data/model_results_unphased_all_PRS/linear_regression/\"\n",
    "model_folder = output_folder + \"models_unphased/\"\n",
    "csv_folder = output_folder + \"csv_files/\"\n",
    "\n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "os.makedirs(csv_folder, exist_ok=True)\n",
    "\n",
    "for chromosome_number in range(start, 23):\n",
    "    # Create subfolders for the current chromosome\n",
    "    chr_model_folder = model_folder + f\"chr{chromosome_number}/\"\n",
    "    chr_csv_folder = csv_folder + f\"chr{chromosome_number}/\"\n",
    "\n",
    "    os.makedirs(chr_model_folder, exist_ok=True)\n",
    "    os.makedirs(chr_csv_folder, exist_ok=True)\n",
    "\n",
    "    file_name = data_directory + f\"23AndMe_PRS313_merged_chr{chromosome_number}_matching_combined.parquet\"\n",
    "    data = pd.read_parquet(file_name)\n",
    "\n",
    "    # Split the data into features and target\n",
    "    X = torch.tensor(data.filter(regex='^(?!.*PRS313_)').values, dtype=torch.float32)\n",
    "    y = torch.tensor(data.filter(regex='PRS313_').values, dtype=torch.float32)\n",
    "\n",
    "    print(\"Total SNPs: \", data.shape[1])\n",
    "    print(\"PRS313 SNPs: \", y.shape[1])\n",
    "    print(\"Total SNPs used for Training: \", X.shape[1])\n",
    "\n",
    "    # Split the data into train-validation and test sets\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Define the linear regression model with L1 regularization\n",
    "    class LinearRegression(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim, l1_coef=0.0):\n",
    "            super(LinearRegression, self).__init__()\n",
    "            self.linear = nn.Linear(input_dim, output_dim)\n",
    "            self.l1_coef = l1_coef\n",
    "\n",
    "        def forward(self, x):\n",
    "            out = self.linear(x)\n",
    "            return out\n",
    "\n",
    "        def l1_loss(self):\n",
    "            return self.l1_coef * torch.norm(self.linear.weight, p=1)\n",
    "\n",
    "    # Set the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Set the hyperparameters for tuning\n",
    "    input_dim = X_train_val.shape[1]\n",
    "    output_dim = y_train_val.shape[1]\n",
    "    num_epochs = 500\n",
    "\n",
    "    # Define the objective function for Optuna with cross-validation and early stopping\n",
    "    def objective(trial):\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-6, 1e-3, log=True)\n",
    "        l1_coef = trial.suggest_float('l1_coef', 1e-20, 1e-1, log=True)\n",
    "        patience = trial.suggest_int('patience', 5, 50)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "        lr_factor = trial.suggest_float('lr_factor', 0.1, 0.7)\n",
    "        num_epochs = trial.suggest_int('num_epochs', 200, 800)\n",
    "\n",
    "        model = LinearRegression(input_dim, output_dim, l1_coef).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        criterion = nn.MSELoss()\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=lr_factor, patience=patience, verbose=False)\n",
    "\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        fold_losses = []\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_val)):\n",
    "            X_train, X_val = X_train_val[train_idx], X_train_val[val_idx]\n",
    "            y_train, y_val = y_train_val[train_idx], y_train_val[val_idx]\n",
    "\n",
    "            train_dataset = TensorDataset(X_train, y_train)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "            best_val_loss = float('inf')\n",
    "            counter = 0\n",
    "\n",
    "            for epoch in range(num_epochs):\n",
    "                train_loss = 0.0\n",
    "                for batch_X, batch_y in train_loader:\n",
    "                    batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "                    outputs = model(batch_X)\n",
    "                    loss = criterion(outputs, batch_y) + model.l1_loss()\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    train_loss += loss.item()\n",
    "\n",
    "                train_loss /= len(train_loader)\n",
    "\n",
    "                val_dataset = TensorDataset(X_val, y_val)\n",
    "                val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    val_loss = 0.0\n",
    "                    for batch_X, batch_y in val_loader:\n",
    "                        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                        outputs = model(batch_X)\n",
    "                        loss = criterion(outputs, batch_y) + model.l1_loss()\n",
    "                        val_loss += loss.item()\n",
    "\n",
    "                    val_loss /= len(val_loader)\n",
    "                    scheduler.step(val_loss)\n",
    "\n",
    "                    if val_loss < best_val_loss:\n",
    "                        best_val_loss = val_loss\n",
    "                        counter = 0\n",
    "                    else:\n",
    "                        counter += 1\n",
    "\n",
    "                    if counter >= patience:\n",
    "                        break\n",
    "\n",
    "            fold_losses.append(best_val_loss)\n",
    "\n",
    "        return np.mean(fold_losses)\n",
    "    # Create the \"optuna_studies\" folder if it doesn't exist\n",
    "    os.makedirs(\"optuna_studies\", exist_ok=True)\n",
    "\n",
    "    # Create an Optuna study and optimize the hyperparameters\n",
    "    study_name = f\"unphased_full_23andMe_chr{chromosome_number}_study_linear_regression\"\n",
    "    storage_name = f\"sqlite:///optuna_studies/{study_name}.db\"\n",
    "\n",
    "    # Check if the study exists\n",
    "    current_dir = os.getcwd()\n",
    "    study_exists = os.path.exists(current_dir + f\"/optuna_studies/{study_name}.db\")\n",
    "\n",
    "    if study_exists:\n",
    "        # Load the existing study\n",
    "        study = optuna.load_study(study_name=study_name, storage=storage_name)\n",
    "    else:\n",
    "        # Create a new study\n",
    "        study = optuna.create_study(direction='minimize', study_name=study_name, storage=storage_name, load_if_exists= True)\n",
    "\n",
    "    study.optimize(objective, n_trials=25, n_jobs=-1)\n",
    "\n",
    "    # Print the best hyperparameters and best value\n",
    "    print(f\"Chr {chromosome_number} - Best hyperparameters: {study.best_params}\")\n",
    "    print(f\"Chr {chromosome_number} - Best value: {study.best_value:.4f}\")\n",
    "\n",
    "    # Train the final model with the best hyperparameters and early stopping\n",
    "    best_learning_rate = study.best_params['learning_rate']\n",
    "    best_l1_coef = study.best_params['l1_coef']\n",
    "    best_patience = study.best_params['patience']\n",
    "    best_batch_size = study.best_params['batch_size']\n",
    "    best_lr_factor = study.best_params['lr_factor']\n",
    "    best_num_epochs = study.best_params['num_epochs']\n",
    "\n",
    "    model = LinearRegression(input_dim, output_dim, best_l1_coef).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=best_lr_factor, patience=best_patience, verbose=False)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_val, y_train_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=best_batch_size, shuffle=True)\n",
    "\n",
    "    best_train_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "    for epoch in range(best_num_epochs):\n",
    "        train_loss = 0.0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y) + model.l1_loss()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        if train_loss < best_train_loss:\n",
    "            best_train_loss = train_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        if counter >= best_patience:\n",
    "            break\n",
    "\n",
    "        scheduler.step(train_loss)\n",
    "\n",
    "    # Save the final model\n",
    "    model_save_path = chr_model_folder + f'final_model_chr{chromosome_number}.pth'\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"Final model saved at: {model_save_path}\")\n",
    "\n",
    "    # Evaluate the final model on the test set\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test.to(device))\n",
    "        test_r2 = calculate_pearson_r_squared(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "        test_iqs = calculate_iqs_unphased(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "        test_accuracy = ((y_test == test_outputs.round()).sum() / (test_outputs.shape[0] * test_outputs.shape[1])).numpy()\n",
    "\n",
    "        # Append performance metrics to the lists\n",
    "        r2_scores.append(test_r2)\n",
    "        iqs_scores.append(test_iqs)\n",
    "        accuracy_scores.append(test_accuracy)\n",
    "\n",
    "        # Calculate individual R^2 scores for each SNP\n",
    "        individual_r2_scores = pearsonr(y_test.cpu().numpy(), test_outputs.cpu().numpy())**2\n",
    "\n",
    "        # Calculate individual IQS scores for each SNP\n",
    "        individual_iqs_scores = np.array([calculate_iqs_unphased(y_test.cpu().numpy()[:, i].reshape(-1, 1), test_outputs.cpu().numpy()[:, i].reshape(-1, 1)) for i in range(y_test.shape[1])])\n",
    "\n",
    "        # Get the names of the SNPs from the original dataframe\n",
    "        snp_names = data.filter(regex='PRS').columns\n",
    "\n",
    "        # Save individual R^2 scores to a CSV file\n",
    "        csv_file = chr_csv_folder + f'individual_r2_scores_chr{chromosome_number}.csv'\n",
    "\n",
    "        with open(csv_file, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['SNP', 'R2 Score'])\n",
    "            for snp, r2_score in zip(snp_names, individual_r2_scores):\n",
    "                writer.writerow([snp, r2_score])\n",
    "\n",
    "        print(f\"Individual R^2 scores saved at: {csv_file}\")\n",
    "\n",
    "        # Save individual IQS scores to a CSV file\n",
    "        iqs_csv_file = chr_csv_folder + f'individual_iqs_scores_chr{chromosome_number}.csv'\n",
    "\n",
    "        with open(iqs_csv_file, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['SNP', 'IQS Score'])\n",
    "            for snp, iqs_score in zip(snp_names, individual_iqs_scores):\n",
    "                writer.writerow([snp, iqs_score])\n",
    "\n",
    "        print(f\"Individual IQS scores saved at: {iqs_csv_file}\")\n",
    "\n",
    "    # Create a DataFrame to store the performance metrics for each chromosome\n",
    "    performance_df = pd.DataFrame({\n",
    "        'Chromosome': list(range(start, chromosome_number + 1)),\n",
    "        'R2 Score': r2_scores,\n",
    "        'IQS Score': iqs_scores,\n",
    "        'Accuracy Score': accuracy_scores\n",
    "    })\n",
    "\n",
    "    # Save the performance metrics to a CSV file\n",
    "    performance_csv_file = csv_folder + 'performance_metrics.csv'\n",
    "    performance_df.to_csv(performance_csv_file, index=False)\n",
    "    print(f\"Performance metrics saved at: {performance_csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total SNPs:  2028\n",
      "PRS313 SNPs:  30\n",
      "Total SNPs used for Training:  1998\n",
      "All true labels are negative. Returning NaN.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../../Data/model_results_unphased_all_PRS/linear_regression/csv_files/chr1/individual_r2_scores_chr1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 159\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# Save individual R^2 scores to a CSV file\u001b[39;00m\n\u001b[1;32m    156\u001b[0m csv_file \u001b[38;5;241m=\u001b[39m chr_csv_folder \u001b[38;5;241m+\u001b[39m \\\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindividual_r2_scores_chr\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchromosome_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 159\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m    160\u001b[0m     writer \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mwriter(file)\n\u001b[1;32m    161\u001b[0m     writer\u001b[38;5;241m.\u001b[39mwriterow([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSNP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR2 Score\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/Documents/GitHub/DeepImpute/.venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../../Data/model_results_unphased_all_PRS/linear_regression/csv_files/chr1/individual_r2_scores_chr1.csv'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchmetrics.classification import BinaryAUROC\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# Load and preprocess the data\n",
    "data_directory = '../../../Data/Filtered_unphased_training_data_union_final/'\n",
    "start = 1\n",
    "\n",
    "# Initialize lists to store the performance metrics for each chromosome\n",
    "r2_scores = []\n",
    "iqs_scores = []\n",
    "accuracy_scores = []\n",
    "auc_scores = []\n",
    "\n",
    "# Folders for saved models and CSV files\n",
    "output_folder = \"../../../Data/model_results_unphased_all_PRS/linear_regression/\"\n",
    "model_folder = output_folder + \"models_unphased/\"\n",
    "csv_folder = output_folder + \"csv_files/\"\n",
    "\n",
    "# Function to calculate Pearson's r²\n",
    "def pearson_r2_score(y_true, y_pred, multioutput=None):\n",
    "    if multioutput == 'raw_values':\n",
    "        # Calculate r² for each output column separately\n",
    "        r2_values = []\n",
    "        for i in range(y_true.shape[1]):\n",
    "            r, _ = pearsonr(y_true[:, i], y_pred[:, i])\n",
    "            r2_values.append(r**2)\n",
    "        return np.array(r2_values)\n",
    "    else:\n",
    "        # Flatten arrays if we have multiple outputs\n",
    "        if y_true.ndim > 1 and y_true.shape[1] > 1:\n",
    "            y_true = y_true.flatten()\n",
    "            y_pred = y_pred.flatten()\n",
    "        \n",
    "        r, _ = pearsonr(y_true, y_pred)\n",
    "        return r**2\n",
    "\n",
    "def safe_auc_metric(y_pred, y_true):\n",
    "    if y_true.sum() == 0:\n",
    "        print(\"All true labels are negative. Returning NaN.\")\n",
    "        return np.nan\n",
    "    return auc_metric(y_pred, y_true)\n",
    "\n",
    "for chromosome_number in range(start, 23):\n",
    "    # Paths for the current chromosome\n",
    "    chr_model_folder = model_folder + f\"chr{chromosome_number}/\"\n",
    "    chr_csv_folder = csv_folder + f\"chr{chromosome_number}/\"\n",
    "\n",
    "    file_name = data_directory + \\\n",
    "        f\"23AndMe_PRS313_merged_chr{chromosome_number}_matching_combined.parquet\"\n",
    "    data = pd.read_parquet(file_name)\n",
    "\n",
    "    # Split the data into features and target\n",
    "    X = torch.tensor(data.filter(\n",
    "        regex='^(?!.*PRS313_)').values, dtype=torch.float32)\n",
    "    y = torch.tensor(data.filter(regex='PRS313_').values, dtype=torch.float32)\n",
    "\n",
    "    print(\"Total SNPs: \", data.shape[1])\n",
    "    print(\"PRS313 SNPs: \", y.shape[1])\n",
    "    print(\"Total SNPs used for Training: \", X.shape[1])\n",
    "\n",
    "    # Split the data into train-validation and test sets\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Define the linear regression model with L1 regularization\n",
    "    class LinearRegression(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim, l1_coef=0.0):\n",
    "            super(LinearRegression, self).__init__()\n",
    "            self.linear = nn.Linear(input_dim, output_dim)\n",
    "            self.l1_coef = l1_coef\n",
    "\n",
    "        def forward(self, x):\n",
    "            out = self.linear(x)\n",
    "            return out\n",
    "\n",
    "        def l1_loss(self):\n",
    "            return self.l1_coef * torch.norm(self.linear.weight, p=1)\n",
    "\n",
    "    # Set the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load the final model\n",
    "    model = LinearRegression(X_train_val.shape[1], y_train_val.shape[1])\n",
    "    model_save_path = chr_model_folder + \\\n",
    "        f'final_model_chr{chromosome_number}.pth'\n",
    "    model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Evaluate the final model on the test set\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test.to(device))\n",
    "        # Use Pearson's r² instead of sklearn's R²\n",
    "        test_r2 = pearson_r2_score(\n",
    "            y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "        test_iqs = calculate_iqs_unphased(\n",
    "            y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "        test_accuracy = ((y_test == test_outputs.round()).sum(\n",
    "        ) / (test_outputs.shape[0] * test_outputs.shape[1])).numpy()\n",
    "\n",
    "        # Calculate the AUC for each SNP individually and then average them\n",
    "        y_test_np = y_test.cpu().numpy()\n",
    "        test_outputs_np = test_outputs.cpu().numpy()\n",
    "        auc_metric = BinaryAUROC(thresholds=None)\n",
    "\n",
    "        individual_aucs = [\n",
    "            [\n",
    "                safe_auc_metric(\n",
    "                    (test_outputs[:, j].round() == i).float(), (y_test[:, j] == i).float())\n",
    "                for i in range(3)\n",
    "            ]\n",
    "            for j in range(y_test_np.shape[1])\n",
    "        ]\n",
    "        # Flatten the list of individual AUCs\n",
    "        flattened_aucs = [auc for sublist in individual_aucs for auc in sublist]\n",
    "\n",
    "        # Calculate the mean while ignoring NaN values\n",
    "        test_auc = np.nanmean(flattened_aucs)\n",
    "        auc_scores.append(test_auc)\n",
    "\n",
    "        # Append performance metrics to the lists\n",
    "        r2_scores.append(test_r2)\n",
    "        iqs_scores.append(test_iqs)\n",
    "        accuracy_scores.append(test_accuracy)\n",
    "\n",
    "        # Calculate individual Pearson's r² scores for each SNP\n",
    "        individual_r2_scores = pearson_r2_score(\n",
    "            y_test_np, test_outputs_np, multioutput='raw_values')\n",
    "\n",
    "        # Calculate individual IQS scores for each SNP\n",
    "        individual_iqs_scores = np.array([calculate_iqs_unphased(y_test_np[:, i].reshape(\n",
    "            -1, 1), test_outputs_np[:, i].reshape(-1, 1)) for i in range(y_test_np.shape[1])])\n",
    "\n",
    "        # Calculate individual accuracy for each SNP\n",
    "        individual_accuracy_scores = np.array([\n",
    "            ((y_test_np[:, i] == np.round(test_outputs_np[:, i])).sum() / y_test_np.shape[0])\n",
    "            for i in range(y_test_np.shape[1])\n",
    "        ])\n",
    "\n",
    "        # Flatten individual AUC scores into a single array for each SNP\n",
    "        individual_auc_scores = np.nanmean(np.array(individual_aucs), axis=1)\n",
    "\n",
    "        # Get the names of the SNPs from the original dataframe\n",
    "        snp_names = data.filter(regex='PRS').columns\n",
    "\n",
    "        # Save individual R^2 scores to a CSV file\n",
    "        csv_file = chr_csv_folder + \\\n",
    "            f'individual_r2_scores_chr{chromosome_number}.csv'\n",
    "\n",
    "        with open(csv_file, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['SNP', 'R2 Score'])\n",
    "            for snp, r2_score in zip(snp_names, individual_r2_scores):\n",
    "                writer.writerow([snp, r2_score])\n",
    "\n",
    "        print(f\"Individual R^2 scores saved at: {csv_file}\")\n",
    "\n",
    "        # Save individual IQS scores to a CSV file\n",
    "        iqs_csv_file = chr_csv_folder + \\\n",
    "            f'individual_iqs_scores_chr{chromosome_number}.csv'\n",
    "\n",
    "        with open(iqs_csv_file, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['SNP', 'IQS Score'])\n",
    "            for snp, iqs_score in zip(snp_names, individual_iqs_scores):\n",
    "                writer.writerow([snp, iqs_score])\n",
    "\n",
    "        print(f\"Individual IQS scores saved at: {iqs_csv_file}\")\n",
    "\n",
    "        # Save individual accuracy scores to a CSV file\n",
    "        accuracy_csv_file = chr_csv_folder + \\\n",
    "            f'individual_accuracy_scores_chr{chromosome_number}.csv'\n",
    "\n",
    "        with open(accuracy_csv_file, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['SNP', 'Accuracy Score'])\n",
    "            for snp, accuracy_score in zip(snp_names, individual_accuracy_scores):\n",
    "                writer.writerow([snp, accuracy_score])\n",
    "\n",
    "        print(f\"Individual Accuracy scores saved at: {accuracy_csv_file}\")\n",
    "\n",
    "        # Save individual AUC scores to a CSV file\n",
    "        auc_csv_file = chr_csv_folder + \\\n",
    "            f'individual_auc_scores_chr{chromosome_number}.csv'\n",
    "\n",
    "        with open(auc_csv_file, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['SNP', 'AUC Score'])\n",
    "            for snp, auc_score in zip(snp_names, individual_auc_scores):\n",
    "                writer.writerow([snp, auc_score])\n",
    "\n",
    "        print(f\"Individual AUC scores saved at: {auc_csv_file}\")\n",
    "\n",
    "# Create a DataFrame to store the performance metrics for each chromosome\n",
    "performance_df = pd.DataFrame({\n",
    "    'Chromosome': list(range(start, 23)),\n",
    "    'R2 Score': r2_scores,\n",
    "    'IQS Score': iqs_scores,\n",
    "    'Accuracy Score': accuracy_scores,\n",
    "    'AUC Score': auc_scores\n",
    "})\n",
    "\n",
    "# Save the performance metrics to a CSV file\n",
    "performance_csv_file = csv_folder + 'performance_metrics.csv'\n",
    "performance_df.to_csv(performance_csv_file, index=False)\n",
    "print(f\"Performance metrics saved at: {performance_csv_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'optuna'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader, TensorDataset\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split, KFold\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01moptuna\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'optuna'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import optuna\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# Load and preprocess the data\n",
    "data_directory = '../../../Data/Filtered_unphased_training_data_union_final/'\n",
    "start = 1\n",
    "\n",
    "# Initialize lists to store the performance metrics for each chromosome\n",
    "r2_scores = []\n",
    "iqs_scores = []\n",
    "accuracy_scores = []\n",
    "\n",
    "# Create folders for saving files\n",
    "output_folder = \"../../../Data/model_results_unphased_all_PRS/logistic_regression/\"\n",
    "model_folder = output_folder + \"models_unphased/\"\n",
    "csv_folder = output_folder + \"csv_files/\"\n",
    "\n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "os.makedirs(csv_folder, exist_ok=True)\n",
    "\n",
    "for chromosome_number in range(start, 23):\n",
    "    # Create subfolders for the current chromosome\n",
    "    chr_model_folder = model_folder + f\"chr{chromosome_number}/\"\n",
    "    chr_csv_folder = csv_folder + f\"chr{chromosome_number}/\"\n",
    "\n",
    "    os.makedirs(chr_model_folder, exist_ok=True)\n",
    "    os.makedirs(chr_csv_folder, exist_ok=True)\n",
    "\n",
    "    file_name = data_directory + f\"23AndMe_PRS313_merged_chr{chromosome_number}_matching_combined.parquet\"\n",
    "    data = pd.read_parquet(file_name)\n",
    "\n",
    "    # Split the data into features and target\n",
    "    X = torch.tensor(data.filter(regex='^(?!.*PRS313_)').values, dtype=torch.float32)\n",
    "    y = torch.tensor(data.filter(regex='PRS313_').values, dtype=torch.long)\n",
    "\n",
    "    print(\"Total SNPs: \", data.shape[1])\n",
    "    print(\"PRS313 SNPs: \", y.shape[1])\n",
    "    print(\"Total SNPs used for Training: \", X.shape[1])\n",
    "\n",
    "    # Split the data into train-validation and test sets\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Define the multinomial logistic regression model with L1 regularization\n",
    "    class MulticlassLogisticRegression(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim, num_classes=3, l1_coef=0.0):\n",
    "            super(MulticlassLogisticRegression, self).__init__()\n",
    "            self.num_classes = num_classes\n",
    "            self.linear = nn.ModuleList([nn.Linear(input_dim, output_dim) for _ in range(num_classes)])\n",
    "            self.l1_coef = l1_coef\n",
    "\n",
    "        def forward(self, x):\n",
    "            out = torch.stack([linear(x) for linear in self.linear], dim=-1)\n",
    "            return out\n",
    "\n",
    "        def l1_loss(self):\n",
    "            return self.l1_coef * sum(torch.norm(linear.weight, p=1) for linear in self.linear)\n",
    "\n",
    "    # Set the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Set the hyperparameters for tuning\n",
    "    input_dim = X_train_val.shape[1]\n",
    "    output_dim = y_train_val.shape[1]\n",
    "    num_epochs = 500\n",
    "\n",
    "    # Define the objective function for Optuna with cross-validation and early stopping\n",
    "    def objective(trial):\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-6, 1e-1, log=True)\n",
    "        l1_coef = trial.suggest_float('l1_coef', 1e-10, 1e-1, log=True)\n",
    "        patience = trial.suggest_int('patience', 5, 20)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "        lr_factor = trial.suggest_float('lr_factor', 0.1, 0.9)\n",
    "        num_epochs = trial.suggest_int('num_epochs', 100, 500)\n",
    "\n",
    "        model = MulticlassLogisticRegression(input_dim, output_dim, l1_coef=l1_coef).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=lr_factor, patience=patience, verbose=False)\n",
    "\n",
    "        kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "        fold_losses = []\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_val)):\n",
    "            X_train, X_val = X_train_val[train_idx], X_train_val[val_idx]\n",
    "            y_train, y_val = y_train_val[train_idx], y_train_val[val_idx]\n",
    "\n",
    "            train_dataset = TensorDataset(X_train, y_train)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "            best_val_loss = float('inf')\n",
    "            counter = 0\n",
    "\n",
    "            for epoch in range(num_epochs):\n",
    "                train_loss = 0.0\n",
    "                for batch_X, batch_y in train_loader:\n",
    "                    batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "                    outputs = model(batch_X)\n",
    "                    loss = criterion(outputs.transpose(1, 2), batch_y) + model.l1_loss()\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    train_loss += loss.item()\n",
    "\n",
    "                train_loss /= len(train_loader)\n",
    "\n",
    "                val_dataset = TensorDataset(X_val, y_val)\n",
    "                val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    val_loss = 0.0\n",
    "                    for batch_X, batch_y in val_loader:\n",
    "                        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                        outputs = model(batch_X)\n",
    "                        loss = criterion(outputs.transpose(1, 2), batch_y) + model.l1_loss()\n",
    "                        val_loss += loss.item()\n",
    "\n",
    "                    val_loss /= len(val_loader)\n",
    "                    scheduler.step(val_loss)\n",
    "\n",
    "                    if val_loss < best_val_loss:\n",
    "                        best_val_loss = val_loss\n",
    "                        counter = 0\n",
    "                    else:\n",
    "                        counter += 1\n",
    "\n",
    "                    if counter >= patience:\n",
    "                        break\n",
    "\n",
    "            fold_losses.append(best_val_loss)\n",
    "\n",
    "        return np.mean(fold_losses)\n",
    "    \n",
    "    # Create the \"optuna_studies\" folder if it doesn't exist\n",
    "    os.makedirs(\"optuna_studies_logistic\", exist_ok=True)\n",
    "\n",
    "    # Create an Optuna study and optimize the hyperparameters\n",
    "    study_name = f\"unphased_full_23andMe_chr{chromosome_number}_study_logistic_regression\"\n",
    "    storage_name = f\"sqlite:///optuna_studies_logistic/{study_name}.db\"\n",
    "\n",
    "    # Check if the study exists\n",
    "    current_dir = os.getcwd()\n",
    "    study_exists = os.path.exists(current_dir + f\"/optuna_studies/{study_name}.db\")\n",
    "\n",
    "    if study_exists:\n",
    "        # Load the existing study\n",
    "        study = optuna.load_study(study_name=study_name, storage=storage_name)\n",
    "    else:\n",
    "        # Create a new study\n",
    "        study = optuna.create_study(direction='minimize', study_name=study_name, storage=storage_name, load_if_exists= True)\n",
    "\n",
    "    study.optimize(objective, n_trials=25, n_jobs=-1)\n",
    "\n",
    "    # Print the best hyperparameters and best value\n",
    "    print(f\"Chr {chromosome_number} - Best hyperparameters: {study.best_params}\")\n",
    "    print(f\"Chr {chromosome_number} - Best value: {study.best_value:.4f}\")\n",
    "\n",
    "    # Train the final model with the best hyperparameters and early stopping\n",
    "    best_learning_rate = study.best_params['learning_rate']\n",
    "    best_l1_coef = study.best_params['l1_coef']\n",
    "    best_patience = study.best_params['patience']\n",
    "    best_batch_size = study.best_params['batch_size']\n",
    "    best_lr_factor = study.best_params['lr_factor']\n",
    "    best_num_epochs = study.best_params['num_epochs']\n",
    "\n",
    "    model = MulticlassLogisticRegression(input_dim, output_dim, l1_coef=best_l1_coef).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=best_lr_factor, patience=best_patience, verbose=False)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_val, y_train_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=best_batch_size, shuffle=True)\n",
    "\n",
    "    best_train_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "    for epoch in range(best_num_epochs):\n",
    "        train_loss = 0.0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs.transpose(1, 2), batch_y) + model.l1_loss()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        if train_loss < best_train_loss:\n",
    "            best_train_loss = train_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        if counter >= best_patience:\n",
    "            break\n",
    "\n",
    "        scheduler.step(train_loss)\n",
    "\n",
    "    # Save the final model\n",
    "    model_save_path = chr_model_folder + f'final_model_chr{chromosome_number}.pth'\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"Final model saved at: {model_save_path}\")\n",
    "\n",
    "    # Evaluate the final model on the test set\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test.to(device)).softmax(dim=2)\n",
    "\n",
    "        test_outputs = test_outputs.argmax(dim=-1)\n",
    "        test_r2 = calculate_pearson_r_squared(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "        test_iqs = calculate_iqs_unphased(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "\n",
    "        test_accuracy = ((y_test == test_outputs).sum() / (test_outputs.shape[0] * test_outputs.shape[1])).numpy()\n",
    "\n",
    "        # Append performance metrics to the lists\n",
    "        r2_scores.append(test_r2)\n",
    "        iqs_scores.append(test_iqs)\n",
    "        accuracy_scores.append(test_accuracy)\n",
    "\n",
    "        # Calculate individual R^2 scores for each SNP\n",
    "        individual_r2_scores = pearsonr(y_test.cpu().numpy(), test_outputs.cpu().numpy())**2\n",
    "\n",
    "        # Calculate individual IQS scores for each SNP\n",
    "        individual_iqs_scores = np.array([calculate_iqs_unphased(y_test.cpu().numpy()[:, i].reshape(-1, 1), test_outputs.cpu().numpy()[:, i].reshape(-1, 1)) for i in range(y_test.shape[1])])\n",
    "\n",
    "        # Get the names of the SNPs from the original dataframe\n",
    "        snp_names = data.filter(regex='PRS').columns\n",
    "\n",
    "        # Save individual R^2 scores to a CSV file\n",
    "        csv_file = chr_csv_folder + f'individual_r2_scores_chr{chromosome_number}.csv'\n",
    "\n",
    "        with open(csv_file, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['SNP', 'R2 Score'])\n",
    "            for snp, r2_score in zip(snp_names, individual_r2_scores):\n",
    "                writer.writerow([snp, r2_score])\n",
    "\n",
    "        print(f\"Individual R^2 scores saved at: {csv_file}\")\n",
    "\n",
    "        # Save individual IQS scores to a CSV file\n",
    "        iqs_csv_file = chr_csv_folder + f'individual_iqs_scores_chr{chromosome_number}.csv'\n",
    "\n",
    "        with open(iqs_csv_file, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['SNP', 'IQS Score'])\n",
    "            for snp, iqs_score in zip(snp_names, individual_iqs_scores):\n",
    "                writer.writerow([snp, iqs_score])\n",
    "\n",
    "        print(f\"Individual IQS scores saved at: {iqs_csv_file}\")\n",
    "\n",
    "    # Create a DataFrame to store the performance metrics for each chromosome\n",
    "    performance_df = pd.DataFrame({\n",
    "        'Chromosome': list(range(start, chromosome_number + 1)),\n",
    "        'R2 Score': r2_scores,\n",
    "        'IQS Score': iqs_scores,\n",
    "        'Accuracy Score': accuracy_scores\n",
    "    })\n",
    "\n",
    "    # Save the performance metrics to a CSV file\n",
    "    performance_csv_file = csv_folder + 'performance_metrics.csv'\n",
    "    performance_df.to_csv(performance_csv_file, index=False)\n",
    "    print(f\"Performance metrics saved at: {performance_csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../Data/Filtered_unphased_training_data_union_final/23AndMe_PRS313_merged_chr1_matching_combined.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m chr_csv_folder \u001b[38;5;241m=\u001b[39m csv_folder \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchr\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchromosome_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m file_name \u001b[38;5;241m=\u001b[39m data_directory \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m23AndMe_PRS313_merged_chr\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchromosome_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_matching_combined.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 35\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Split the data into features and target\u001b[39;00m\n\u001b[1;32m     38\u001b[0m X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(data\u001b[38;5;241m.\u001b[39mfilter(regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m^(?!.*PRS313_)\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m~/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/pandas/io/parquet.py:667\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    665\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m--> 667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/pandas/io/parquet.py:267\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    265\u001b[0m     to_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[1;32m    275\u001b[0m         path_or_handle,\n\u001b[1;32m    276\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    280\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/pandas/io/parquet.py:140\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    130\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m     handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/Documents/medicaid/DeepImpute/.venv/lib/python3.11/site-packages/pandas/io/common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../Data/Filtered_unphased_training_data_union_final/23AndMe_PRS313_merged_chr1_matching_combined.parquet'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# Load and preprocess the data\n",
    "data_directory = '../../Data/Filtered_unphased_training_data_union_final/'\n",
    "start = 1\n",
    "\n",
    "# Initialize lists to store the performance metrics for each chromosome\n",
    "r2_scores = []\n",
    "iqs_scores = []\n",
    "accuracy_scores = []\n",
    "\n",
    "# Initialize lists to store predictions and SNP names for all chromosomes\n",
    "all_predictions = []\n",
    "all_snp_names = []\n",
    "\n",
    "# Folders for saved models and CSV files\n",
    "output_folder = \"../../Data/model_results_unphased_all_PRS/logistic_regression/\"\n",
    "model_folder = output_folder + \"models_unphased/\"\n",
    "csv_folder = output_folder + \"csv_files/\"\n",
    "\n",
    "for chromosome_number in range(start, 23):\n",
    "    # Paths for the current chromosome\n",
    "    chr_model_folder = model_folder + f\"chr{chromosome_number}/\"\n",
    "    chr_csv_folder = csv_folder + f\"chr{chromosome_number}/\"\n",
    "\n",
    "    file_name = data_directory + f\"23AndMe_PRS313_merged_chr{chromosome_number}_matching_combined.parquet\"\n",
    "    data = pd.read_parquet(file_name)\n",
    "\n",
    "    # Split the data into features and target\n",
    "    X = torch.tensor(data.filter(regex='^(?!.*PRS313_)').values, dtype=torch.float32)\n",
    "    y = torch.tensor(data.filter(regex='PRS313_').values, dtype=torch.long)\n",
    "\n",
    "    print(\"Total SNPs: \", data.shape[1])\n",
    "    print(\"PRS313 SNPs: \", y.shape[1])\n",
    "    print(\"Total SNPs used for Training: \", X.shape[1])\n",
    "\n",
    "    # Split the data into train-validation and test sets\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Define the multinomial logistic regression model with L1 regularization\n",
    "    class MulticlassLogisticRegression(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim, num_classes=3, l1_coef=0.0):\n",
    "            super(MulticlassLogisticRegression, self).__init__()\n",
    "            self.num_classes = num_classes\n",
    "            self.linear = nn.ModuleList([nn.Linear(input_dim, output_dim) for _ in range(num_classes)])\n",
    "            self.l1_coef = l1_coef\n",
    "\n",
    "        def forward(self, x):\n",
    "            out = torch.stack([linear(x) for linear in self.linear], dim=-1)\n",
    "            out = nn.functional.softmax(out, dim=-1)\n",
    "            return out\n",
    "\n",
    "        def l1_loss(self):\n",
    "            return self.l1_coef * sum(torch.norm(linear.weight, p=1) for linear in self.linear)\n",
    "\n",
    "    # Set the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load the final model\n",
    "    model = MulticlassLogisticRegression(X_train_val.shape[1], y_train_val.shape[1])\n",
    "    model_save_path = chr_model_folder + f'final_model_chr{chromosome_number}.pth'\n",
    "    model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Evaluate the final model on the test set\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test.to(device))\n",
    "        test_outputs = test_outputs.argmax(dim=-1)\n",
    "        test_r2 = calculate_pearson_r_squared(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "        test_iqs = calculate_iqs_unphased(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "        test_accuracy = ((y_test == test_outputs).sum() / (test_outputs.shape[0] * test_outputs.shape[1])).numpy()\n",
    "\n",
    "        # Append performance metrics to the lists\n",
    "        r2_scores.append(test_r2)\n",
    "        iqs_scores.append(test_iqs)\n",
    "        accuracy_scores.append(test_accuracy)\n",
    "\n",
    "        # Get the names of the SNPs from the original dataframe\n",
    "        snp_names = data.filter(regex='PRS').columns\n",
    "\n",
    "        # Append predictions and SNP names to the respective lists\n",
    "        all_predictions.append(test_outputs.cpu().numpy())\n",
    "        all_snp_names.extend(snp_names)\n",
    "\n",
    "# Concatenate all predictions and SNP names\n",
    "all_predictions = np.concatenate(all_predictions, axis=1)\n",
    "all_snp_names = np.array(all_snp_names)\n",
    "\n",
    "# Create a DataFrame for the concatenated predictions and SNP names\n",
    "predictions_df = pd.DataFrame(all_predictions, columns=all_snp_names)\n",
    "\n",
    "# Save the concatenated predictions and SNP names to a single CSV file\n",
    "predictions_csv_file = csv_folder + 'imputed_snps_all_chromosomes.csv'\n",
    "predictions_df.to_csv(predictions_csv_file, index=False)\n",
    "print(f\"Imputed SNPs saved at: {predictions_csv_file}\")\n",
    "\n",
    "# Create a DataFrame to store the performance metrics for each chromosome\n",
    "performance_df = pd.DataFrame({\n",
    "    'Chromosome': list(range(start, 23)),\n",
    "    'R2 Score': r2_scores,\n",
    "    'IQS Score': iqs_scores,\n",
    "    'Accuracy Score': accuracy_scores\n",
    "})\n",
    "\n",
    "# Save the performance metrics to a CSV file\n",
    "performance_csv_file = csv_folder + 'performance_metrics.csv'\n",
    "performance_df.to_csv(performance_csv_file, index=False)\n",
    "print(f\"Performance metrics saved at: {performance_csv_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRS313 SNPs saved at: ../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/prs313_snps.csv\n",
      "Total number of PRS313 SNPs: 313\n"
     ]
    }
   ],
   "source": [
    "# Loop through all the training datasets and document the PRS313 SNPs in each dataset. Save this to a CSV file.\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data_directory = '../../Data/Filtered_unphased_training_data_union_final/'\n",
    "output_folder = \"../../Data/model_results_unphased_all_PRS/logistic_regression/csv_files/\"\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Initialize a list to store the PRS313 SNPs in each dataset\n",
    "prs313_snps = []\n",
    "\n",
    "for chromosome_number in range(1, 23):\n",
    "    file_name = data_directory + \\\n",
    "        f\"23AndMe_PRS313_merged_chr{chromosome_number}_matching_combined.parquet\"\n",
    "    data = pd.read_parquet(file_name)\n",
    "\n",
    "    prs313_snps.append(data.filter(regex='PRS313_').columns)\n",
    "\n",
    "# Create a DataFrame to store the PRS313 SNPs in each dataset\n",
    "prs313_df = pd.DataFrame({\n",
    "    'Chromosome': list(range(1, 23)),\n",
    "    'PRS313 SNPs': prs313_snps,\n",
    "    \"Number of PRS313 SNPs\": [len(snps) for snps in prs313_snps]\n",
    "})\n",
    "\n",
    "# Save the PRS313 SNPs to a CSV file\n",
    "prs313_csv_file = output_folder + 'prs313_snps.csv'\n",
    "prs313_df.to_csv(prs313_csv_file, index=False)\n",
    "print(f\"PRS313 SNPs saved at: {prs313_csv_file}\")\n",
    "\n",
    "# Print the total number of PRS313 SNPs in all datasets\n",
    "total_prs313_snps = sum(prs313_df[\"Number of PRS313 SNPs\"])\n",
    "print(f\"Total number of PRS313 SNPs: {total_prs313_snps}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
